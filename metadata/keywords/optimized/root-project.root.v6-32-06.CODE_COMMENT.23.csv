quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,filename,wiki,url,total_similar,target_keywords,target_matched_words
Performance,// Clear instruction cache before code will be executed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp:21,cache,cache,21,interpreter/llvm-project/llvm/tools/lli/lli.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp,1,['cache'],['cache']
Performance,// Clear kill flags between store and load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,// Clear out any cached analyses.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ModuleInliner.cpp:17,cache,cached,17,interpreter/llvm-project/llvm/lib/Transforms/IPO/ModuleInliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ModuleInliner.cpp,1,['cache'],['cached']
Performance,// Clear out old caches and data.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/PrecompiledPreamble.cpp:17,cache,caches,17,interpreter/llvm-project/clang/lib/Frontend/PrecompiledPreamble.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/PrecompiledPreamble.cpp,1,['cache'],['caches']
Performance,// Clear out the source line cache if it's already been computed.; // FIXME: Allow this to be incrementally extended.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/ScratchBuffer.cpp:29,cache,cache,29,interpreter/llvm-project/clang/lib/Lex/ScratchBuffer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/ScratchBuffer.cpp,1,['cache'],['cache']
Performance,"// Clear the body and queue the function itself for deletion when we; // finish inlining and call graph updates.; // Note that after this point, it is an error to do anything other; // than use the callee's address or delete it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp:22,queue,queue,22,interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp,1,['queue'],['queue']
Performance,"// Clear the body and queue the function itself for deletion when we; // finish inlining.; // Note that after this point, it is an error to do anything other; // than use the callee's address or delete it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ModuleInliner.cpp:22,queue,queue,22,interpreter/llvm-project/llvm/lib/Transforms/IPO/ModuleInliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ModuleInliner.cpp,1,['queue'],['queue']
Performance,"// Clear the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchDataSet.cxx:13,cache,cache,13,proof/proofbench/src/TProofBenchDataSet.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchDataSet.cxx,1,['cache'],['cache']
Performance,"// Clear the list of performances points",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/sessionviewer/src/TProofProgressDialog.cxx:21,perform,performances,21,gui/sessionviewer/src/TProofProgressDialog.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/sessionviewer/src/TProofProgressDialog.cxx,1,['perform'],['performances']
Performance,// Clear the registration list and release the lock once we're done. Any; // pending updates from other threads will safely take effect after we return.; // That might not be what the user wants if they're measuring a compilation; // but it's their responsibility to prevent concurrent compilations to make; // a single compilation measurable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Statistic.cpp:275,concurren,concurrent,275,interpreter/llvm-project/llvm/lib/Support/Statistic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Statistic.cpp,1,['concurren'],['concurrent']
Performance,"// Clear the rest of the baskets. While it would be ideal to reuse these baskets; // for other baskets in the new cluster. It would require the function to go; // beyond its current scope. In the ideal case when each cluster only has 1 basket; // this will perform well",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBranch.cxx:257,perform,perform,257,tree/tree/src/TBranch.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBranch.cxx,1,['perform'],['perform']
Performance,"// Clear the rewriter cache, because values that are in the rewriter's cache; // can be deleted below, causing the AssertingVH in the cache to trigger.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:22,cache,cache,22,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,6,['cache'],['cache']
Performance,"// Clear the rewriter cache, because values that are in the rewriter's cache; // can be deleted in the loop below, causing the AssertingVH in the cache to; // trigger.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp:22,cache,cache,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp,3,['cache'],['cache']
Performance,// Clear the tune CPU.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp:13,tune,tune,13,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,1,['tune'],['tune']
Performance,"// ClearSysIncFiles(); <-- this one stays cached; // ClearUsers(); <-- this one stays cached",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/rint/src/TTabCom.cxx:42,cache,cached,42,core/rint/src/TTabCom.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/rint/src/TTabCom.cxx,2,['cache'],['cached']
Performance,// Clients are responsible for avoid race conditions in registration.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/TargetRegistry.cpp:37,race condition,race conditions,37,interpreter/llvm-project/llvm/lib/MC/TargetRegistry.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/TargetRegistry.cpp,1,['race condition'],['race conditions']
Performance,"// Clip away scene outside of the clip object.; // Load all clip planes (up to max) at once.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScene.cxx:51,Load,Load,51,graf3d/gl/src/TGLScene.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScene.cxx,1,['Load'],['Load']
Performance,// Clone all the blocks. The original blocks will be the hot-path; // CHR-optimized code and the cloned blocks will be the original unoptimized; // code. This is so that the block pointers from the; // CHRScope/Region/RegionInfo can stay valid in pointing to the hot-path code; // which CHR should apply to.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/ControlHeightReduction.cpp:74,optimiz,optimized,74,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/ControlHeightReduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/ControlHeightReduction.cpp,1,['optimiz'],['optimized']
Performance,// Clone the MMO and unset the load flag.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,// Clone the MMO into two separate MMOs for loading and storing,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:44,load,loading,44,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loading']
Performance,// Cluster adjacent cases with the same destination. We do this at all; // optimization levels because it's cheap to do and will make codegen faster; // if there are many clusters.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:75,optimiz,optimization,75,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,2,['optimiz'],['optimization']
Performance,"// Cluster if the memory operations are on the same or a neighbouring cache; // line, but limit the maximum ClusterSize to avoid creating too much; // additional register pressure.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:70,cache,cache,70,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,1,['cache'],['cache']
Performance,// Cluster loads by adding MVT::Glue outputs and inputs. This also; // ensure they are scheduled in order of increasing addresses.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,1,['load'],['loads']
Performance,"// Cluster loads from ""near"" addresses into combined SUnits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,1,['load'],['loads']
Performance,// Cluster the load/store only when they have the same base; // register or FI.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,"// Cluster the load/store only when they have the same opcode, and they are; // clusterable opcode according to PowerPC specification.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,"// Code in an unnamed namespace shows up automatically in; // decls_begin()/decls_end(). Thus we don't need to recurse on; // D->getAnonymousNamespace().; // If the traversal scope is set, then consider them to be the children of; // the TUDecl, rather than traversing (and loading?) all top-level decls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/RecursiveASTVisitor.h:274,load,loading,274,interpreter/llvm-project/clang/include/clang/AST/RecursiveASTVisitor.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/RecursiveASTVisitor.h,1,['load'],['loading']
Performance,"// Code motion for interleaved accesses can potentially hoist strided loads; // and sink strided stores. The code below checks the legality of the; // following two conditions:; //; // 1. Potentially moving a strided load (B) before any store (A) that; // precedes B, or; //; // 2. Potentially moving a strided store (A) after any load or store (B); // that A precedes.; //; // It's legal to reorder A and B if we know there isn't a dependence from A; // to B. Note that this determination is conservative since some; // dependences could potentially be reordered safely.; // A is potentially the source of a dependence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/VectorUtils.h:70,load,loads,70,interpreter/llvm-project/llvm/include/llvm/Analysis/VectorUtils.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/VectorUtils.h,3,['load'],"['load', 'loads']"
Performance,"// CodeView can only express variables in register and variables in memory; // at a constant offset from a register. However, for variables passed; // indirectly by pointer, it is common for that pointer to be spilled to a; // stack location. For the special case of one offseted load followed by a; // zero offset load (a pointer spilled to the stack), we change the type of; // the local variable from a value type to a reference type. This tricks the; // debugger into doing the load for us.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/CodeViewDebug.cpp:280,load,load,280,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/CodeViewDebug.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/CodeViewDebug.cpp,3,['load'],['load']
Performance,// Codes for which we want to perform some z-specific combinations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:30,perform,perform,30,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['perform'],['perform']
Performance,"// Coerce the arguments, llvm optimizations seem to ignore the types in; // vaarg functions and throws away casts in optimized mode.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroSplit.cpp:30,optimiz,optimizations,30,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroSplit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroSplit.cpp,2,['optimiz'],"['optimizations', 'optimized']"
Performance,// Coerce the shift amount to the right type if we can. This exposes the; // truncate or zext to optimization early.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:97,optimiz,optimization,97,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['optimiz'],['optimization']
Performance,"// Collect UsingDirectiveDecls in all scopes, and recursively all; // nominated namespaces by those using-directives.; //; // FIXME: Cache this sorted list in Scope structure, and DeclContext, so we; // don't build it for each lookup!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:133,Cache,Cache,133,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['Cache'],['Cache']
Performance,// Collect all aggregate loads and mem* calls.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerAggrCopies.cpp:25,load,loads,25,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerAggrCopies.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerAggrCopies.cpp,1,['load'],['loads']
Performance,"// Collect all of the overridders from the base class subobject; // and merge them into the set of overridders for this class.; // For virtual base classes, populate or use the cached virtual; // overrides so that we do not walk the virtual base class (and; // its base classes) more than once.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/CXXInheritance.cpp:177,cache,cached,177,interpreter/llvm-project/clang/lib/AST/CXXInheritance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/CXXInheritance.cpp,1,['cache'],['cached']
Performance,// Collect all scheduling regions. The actual scheduling is performed in; // GCNScheduleDAGMILive::finalizeSchedule.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:60,perform,performed,60,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,1,['perform'],['performed']
Performance,// Collect all the clusterable loads/stores,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:31,load,loads,31,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,1,['load'],['loads']
Performance,"// Collect all used nodes together with the uses from loads and stores,; // where the GEP node could be folded into the load/store instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp:54,load,loads,54,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp,2,['load'],"['load', 'loads']"
Performance,// Collect buckets of comparable addresses used by loads and stores for DQ; // form.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,1,['load'],['loads']
Performance,// Collect buckets of comparable addresses used by loads and stores for DS; // form.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,1,['load'],['loads']
Performance,"// Collect buckets of comparable addresses used by loads and stores for chain; // commoning. With chain commoning, we reuse offsets between the chains, so; // the register pressure will be reduced.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,1,['load'],['loads']
Performance,// Collect buckets of comparable addresses used by loads and stores for update; // form.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,1,['load'],['loads']
Performance,// Collect info for variables/labels that were optimized out.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/DwarfDebug.cpp:47,optimiz,optimized,47,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/DwarfDebug.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/DwarfDebug.cpp,1,['optimiz'],['optimized']
Performance,// Collect information about load/store queues.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp:29,load,load,29,interpreter/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenSchedule.cpp,2,"['load', 'queue']","['load', 'queues']"
Performance,"// Collect loads and instruction that may write to memory. For now we only; // record loads which are simple, sign-extended and have a single user.; // TODO: Allow zero-extended loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,3,['load'],['loads']
Performance,// Collect names of runtime library functions. User-defined functions with the; // same names are added to llvm.compiler.used to prevent them from being; // deleted by optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/UpdateCompilerUsed.cpp:168,optimiz,optimizations,168,interpreter/llvm-project/llvm/lib/LTO/UpdateCompilerUsed.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/UpdateCompilerUsed.cpp,1,['optimiz'],['optimizations']
Performance,// Collect potentially blocking stores.; // Limit the number of instructions backwards we want to inspect; // since the effect of store block won't be visible if the store; // and load instructions have enough instructions in between to; // keep the core busy.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp:180,load,load,180,interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp,1,['load'],['load']
Performance,"// Collect register usage information and produce a register mask of; // clobbered registers, to be used to optimize call sites.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/CodeGenPassBuilder.h:108,optimiz,optimize,108,interpreter/llvm-project/llvm/include/llvm/CodeGen/CodeGenPassBuilder.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/CodeGenPassBuilder.h,2,['optimiz'],['optimize']
Performance,// Collect sizes of all sections to be loaded;; // also determine the max alignment of all sections,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:39,load,loaded,39,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['loaded']
Performance,// Collect the pointers of the candidate loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp:41,load,loads,41,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,1,['load'],['loads']
Performance,// Collect the registers can be optimized,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp:32,optimiz,optimized,32,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeVGPRLiveRange.cpp,1,['optimiz'],['optimized']
Performance,"// Collect the set of virtual functions that are eligible for virtual constant; // propagation. Each eligible function must not access memory, must return; // an integer of width <=64 bits, must take at least one argument, must not; // use its first argument (assumed to be ""this"") and all arguments other than; // the first one must be of <=64 bit integer type.; //; // Note that we test whether this copy of the function is readnone, rather; // than testing function attributes, which must hold for any copy of the; // function, even a less optimized version substituted at link time. This is; // sound because the virtual constant propagation optimizations effectively; // inline all implementations of the virtual function into each call site,; // rather than using function attributes to perform local optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ThinLTOBitcodeWriter.cpp:543,optimiz,optimized,543,interpreter/llvm-project/llvm/lib/Transforms/IPO/ThinLTOBitcodeWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ThinLTOBitcodeWriter.cpp,4,"['optimiz', 'perform']","['optimization', 'optimizations', 'optimized', 'perform']"
Performance,// Combine BSWAP (LOAD) into LRVH/LRV/LRVG/VLBR,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:18,LOAD,LOAD,18,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['LOAD'],['LOAD']
Performance,"// Combine GA + Constant -> GA+Offset, but only if GA is not used elsewhere; // and the root node itself is not used more than twice. This reduces the; // amount of additional constant extenders introduced by this optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:214,optimiz,optimization,214,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['optimiz'],['optimization']
Performance,// Combine a new optimization goal with existing ones.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:17,optimiz,optimization,17,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,1,['optimiz'],['optimization']
Performance,"// Combine a vector ops (shuffles etc.) that is equal to build_vector load1,; // load2, load3, load4, <0, 1, 2, 3> into a vector load if the load addresses; // are consecutive, non-overlapping, and in the right order.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:129,load,load,129,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,// Combine element-swap (LOAD) into VLER,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:25,LOAD,LOAD,25,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['LOAD'],['LOAD']
Performance,"// Combine insert(shuffle(load, <u,0,1,2>), load, 0) into a single load if; // possible and the new load will be quick. We use more loads but less shuffles; // and inserts.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,5,['load'],"['load', 'loads']"
Performance,"// Combine into mla/mls.; // This works on the patterns of:; // add v1, (mul v2, v3); // sub v1, (mul v2, v3); // for vectors of type <1 x i64> and <2 x i64> when SVE is available.; // It will transform the add/sub to a scalable version, so that we can; // make use of SVE's MLA/MLS that will be generated for that pattern",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:220,scalab,scalable,220,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,// Combine load followed by zero- or sign-extend.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,1,['load'],['load']
Performance,"// Combine load followed by zero- or sign-extend.; // ldrb r1, [r0] ldrb r1, [r0]; // uxtb r2, r1 =>; // mov r3, r2 mov r3, r1",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,1,['load'],['load']
Performance,"// Combine patterns like:; // %0 = load <4 x i32>, <4 x i32>* %a; // %1 = insertelement <4 x i32> %0, i32 %b, i32 1; // store <4 x i32> %1, <4 x i32>* %a; // to:; // %0 = bitcast <4 x i32>* %a to i32*; // %1 = getelementptr inbounds i32, i32* %0, i64 0, i64 1; // store i32 %b, i32* %1",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,1,['load'],['load']
Performance,// Combine strided loads with unit-stride to a regular VP load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],"['load', 'loads']"
Performance,"// Combine the 8-bit registers into 16-bit register pairs.; // This done either from LSB to MSB or from MSB to LSB, depending on the; // shift. It's an optimization so that the register allocator will use the; // fewest movs possible (which order we use isn't a correctness issue, just an; // optimization issue).; // - lsl prefers starting from the most significant byte (2nd case).; // - lshr prefers starting from the least significant byte (1st case).; // - for ashr it depends on the number of shifted bytes.; // Some shift operations still don't get the most optimal mov sequences even; // with this distinction. TODO: figure out why and try to fix it (but we're; // already equal to or faster than avr-gcc in all cases except ashr 8).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp:152,optimiz,optimization,152,interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp,2,['optimiz'],['optimization']
Performance,"// Combine the two ranges, but limit the result to the type in which we; // performed the computation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp:76,perform,performed,76,interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,1,['perform'],['performed']
Performance,// Combine unsigned buffer load and signed extension instructions to generate; // signed buffer laod instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPostLegalizerCombiner.cpp,1,['load'],['load']
Performance,"// Combines two comparison operation and logic operation to one selection; // operation(min, max) and logic operation. Returns new constructed Node if; // conditions for optimization are satisfied.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:170,optimiz,optimization,170,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// Combines vecreduce_add(mul(ext(x), ext(y))) -> vecreduce_add(udot(x, y)); // Or vecreduce_add(ext(x)) -> vecreduce_add(udot(x, 1)); // Similar to performVecReduceAddCombine in SelectionDAG",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp:149,perform,performVecReduceAddCombine,149,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PreLegalizerCombiner.cpp,1,['perform'],['performVecReduceAddCombine']
Performance,// Command line option to disable memory intrinsic optimization. The default is; // false. This is for debug purpose.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/PGOMemOPSizeOpt.cpp:51,optimiz,optimization,51,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/PGOMemOPSizeOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/PGOMemOPSizeOpt.cpp,1,['optimiz'],['optimization']
Performance,// Command-line parameter overrides instruction attribute.; // This can't be moved to optimizeFloatingPointLibCall() because it may be; // used by the intrinsic optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp:86,optimiz,optimizeFloatingPointLibCall,86,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,2,['optimiz'],"['optimizations', 'optimizeFloatingPointLibCall']"
Performance,// Commit to the cache (if enabled),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp:17,cache,cache,17,interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,1,['cache'],['cache']
Performance,"// Common (latency, uops) code for LEA templates. `GetDestReg` takes the; // addressing base and index registers and returns the LEA destination register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/Target.cpp:11,latency,latency,11,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/Target.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/Target.cpp,1,['latency'],['latency']
Performance,"// Compare compares the result of MI against zero. If MI is a suitable load; // instruction and if CCUsers is a single conditional trap on zero, eliminate; // the load and convert the branch to a load-and-trap. Return true on success.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp:71,load,load,71,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp,3,['load'],"['load', 'load-and-trap']"
Performance,"// Compare every group in the result with the rest. If one groups contains; // another group, we only need to return the bigger group.; // Note: This doesn't scale well, so if possible avoid calling any heavy; // function from this loop to minimize the performance impact.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CloneDetection.cpp:253,perform,performance,253,interpreter/llvm-project/clang/lib/Analysis/CloneDetection.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CloneDetection.cpp,1,['perform'],['performance']
Performance,"// Compare the AdaptiveIntegratorMultiDim and; // TF1::IntegralMultiple performance and results; // Compares time performance; // for different dimensions; // draws a graph; //; // Author: David Gonzalez Maline; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/mathcore/test/testIntegrationMultiDim.cxx:72,perform,performance,72,math/mathcore/test/testIntegrationMultiDim.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/mathcore/test/testIntegrationMultiDim.cxx,2,['perform'],['performance']
Performance,"// Compare the contents of the cached buffer and the string we should; // process. If there are hash collisions this assert should trigger; // making it easier to debug.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/LookupHelper.cpp:31,cache,cached,31,interpreter/cling/lib/Interpreter/LookupHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/LookupHelper.cpp,1,['cache'],['cached']
Performance,// Compare the expression type for anything but load and store.; // For load and store we set the opcode to zero to make them equal.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Scalar/GVNExpression.h:48,load,load,48,interpreter/llvm-project/llvm/include/llvm/Transforms/Scalar/GVNExpression.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Scalar/GVNExpression.h,2,['load'],['load']
Performance,// Compare the locations within the common file and cache them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:52,cache,cache,52,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['cache'],['cache']
Performance,"// Compare the vptr against the expected vptr for the destination type at; // this offset. Note that we do not know what type ThisAddr points to in; // the case where the derived class multiply inherits from the base class; // so we can't use GetVTablePtr, so we load the vptr directly instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp:263,load,load,263,interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,1,['load'],['load']
Performance,"// Compared with jump tables, the DFA optimizer removes an indirect branch; // on each loop iteration, thus making branch prediction more precise. The; // more branch targets there are, the more likely it is for the branch; // predictor to make a mistake, and the more benefit there is in the DFA; // optimizer. Thus, the more branch targets there are, the lower is the; // cost of the DFA opt.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DFAJumpThreading.cpp:38,optimiz,optimizer,38,interpreter/llvm-project/llvm/lib/Transforms/Scalar/DFAJumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DFAJumpThreading.cpp,2,['optimiz'],['optimizer']
Performance,// Comparing latency against a call makes little sense unless the node; // is register pressure-neutral.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:13,latency,latency,13,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,1,['latency'],['latency']
Performance,// Comparison optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h:14,optimiz,optimization,14,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h,1,['optimiz'],['optimization']
Performance,"// Compatible - Add current set1/2 to cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooNormSetCache.cxx:38,cache,cache,38,roofit/roofitcore/src/RooNormSetCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooNormSetCache.cxx,1,['cache'],['cache']
Performance,// Complete translation units and modules define vtables and perform implicit; // instantiations. PCH files do not.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp:61,perform,perform,61,interpreter/llvm-project/clang/lib/Sema/Sema.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp,1,['perform'],['perform']
Performance,// ComplexPattern used on BPF Load/Store instructions,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFISelDAGToDAG.cpp:30,Load,Load,30,interpreter/llvm-project/llvm/lib/Target/BPF/BPFISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFISelDAGToDAG.cpp,1,['Load'],['Load']
Performance,"// Components of composite shape hierarchies for local frame viewers are painted; // in coordinate frame of the top level composite shape. So we force; // conversion to this. See TGeoPainter::PaintNode for loading of GLMatrix.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/src/TGeoBoolNode.cxx:206,load,loading,206,geom/geom/src/TGeoBoolNode.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/src/TGeoBoolNode.cxx,1,['load'],['loading']
Performance,"// Composite shapes have their own internal hierarchy of shapes, each; // of which generate a filled TBuffer3D. Therefore we can't pass up a; // single buffer to here. So as a special case the TGeoCompositeShape; // performs it's own painting & negotiation with the viewer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geompainter/src/TGeoPainter.cxx:216,perform,performs,216,geom/geompainter/src/TGeoPainter.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geompainter/src/TGeoPainter.cxx,1,['perform'],['performs']
Performance,// Compute an upper bound of the memory size that is required to load all; // sections,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:65,load,load,65,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['load']
Performance,// Compute an upper bound of the memory that is required to load all; // sections,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldImpl.h:60,load,load,60,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldImpl.h,1,['load'],['load']
Performance,// Compute and cache live-ins and pressure for all regions in block.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h:15,cache,cache,15,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.h,1,['cache'],['cache']
Performance,"// Compute and cache the constant value, and remember that we have a; // constant initializer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:15,cache,cache,15,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['cache'],['cache']
Performance,// Compute left/right load/store offsets.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,1,['load'],['load']
Performance,// Compute lengths and cache them; // We cannot overwrite getLengths() because UtoT mapping uses it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:23,cache,cache,23,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['cache'],['cache']
Performance,"// Compute numerical throughput.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/MethodDL.cxx:21,throughput,throughput,21,tmva/tmva/src/MethodDL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/MethodDL.cxx,3,['throughput'],['throughput']
Performance,// Compute output latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCDisassembler/Disassembler.cpp:18,latency,latency,18,interpreter/llvm-project/llvm/lib/MC/MCDisassembler/Disassembler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCDisassembler/Disassembler.cpp,1,['latency'],['latency']
Performance,// Compute the byval alignment. We specify the alignment of the byval in all; // cases so that the mid-level optimizer knows the alignment of the byval.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/X86.cpp:109,optimiz,optimizer,109,interpreter/llvm-project/clang/lib/CodeGen/Targets/X86.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/X86.cpp,1,['optimiz'],['optimizer']
Performance,// Compute the cached properties and then set the cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp:15,cache,cached,15,interpreter/llvm-project/clang/lib/AST/Type.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp,2,['cache'],"['cache', 'cached']"
Performance,"// Compute the equivalence relation for the gep nodes. Use two caches,; // one for equality and the other for non-equality.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp:63,cache,caches,63,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp,1,['cache'],['caches']
Performance,"// Compute the hash value of RVVType, used for cache the result of computeType.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Support/RISCVVIntrinsicUtils.cpp:47,cache,cache,47,interpreter/llvm-project/clang/lib/Support/RISCVVIntrinsicUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Support/RISCVVIntrinsicUtils.cpp,1,['cache'],['cache']
Performance,// Compute the latency for the node. We use the sum of the latencies for; // all nodes glued together into this SUnit.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp:15,latency,latency,15,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,1,['latency'],['latency']
Performance,"// Compute the leaf node branching factor that makes a node fit in three; // cache lines. The branching factor must be at least 3, or some B+-tree; // balancing algorithms won't work.; // LeafSize can't be larger than CacheLineBytes. This is required by the; // PointerIntPair used by NodeRef.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/IntervalMap.h:77,cache,cache,77,interpreter/llvm-project/llvm/include/llvm/ADT/IntervalMap.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/IntervalMap.h,2,"['Cache', 'cache']","['CacheLineBytes', 'cache']"
Performance,// Compute the memory size required to load all sections to be loaded; // and pass this information to the memory manager,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,2,['load'],"['load', 'loaded']"
Performance,// Compute the module access path corresponding to this module.; // FIXME: Should we have a second loadModule() overload to avoid this; // extra lookup step?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp:99,load,loadModule,99,interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,1,['load'],['loadModule']
Performance,// Compute the number of load commands we will need.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/MachOUtils.cpp:25,load,load,25,interpreter/llvm-project/llvm/tools/dsymutil/MachOUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/MachOUtils.cpp,1,['load'],['load']
Performance,// Compute the ordering we will process the inputs: the rough heuristic here; // is to sort them per size so that the largest module get schedule as soon as; // possible. This is purely a compile-time optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp:201,optimiz,optimization,201,interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,1,['optimiz'],['optimization']
Performance,// Compute the smallest VF at which the store and load would be misaligned.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:50,load,load,50,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,1,['load'],['load']
Performance,// Compute which bits of the stored value are being used by the load. Convert; // to an integer type to start with.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp:64,load,load,64,interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,1,['load'],['load']
Performance,// ComputeNumSignBits not yet implemented for scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:46,scalab,scalable,46,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,// Computes the CastContextHint from a Load/Store instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:39,Load,Load,39,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['Load'],['Load']
Performance,// Computes the idealized ProcRes Unit pressure. This is the expected; // distribution if the CPU scheduler can distribute the load as evenly as; // possible.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/SchedClassResolution.h:127,load,load,127,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/SchedClassResolution.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/SchedClassResolution.h,1,['load'],['load']
Performance,"// Computes whether and how we can vectorize the loads/stores of a; // flattened function parameter or return value.; //; // The flattened parameter is represented as the list of ValueVTs and; // Offsets, and is aligned to ParamAlignment bytes. We return a vector; // of the same size as ValueVTs indicating how each piece should be; // loaded/stored (i.e. as a scalar, or as part of a vector; // load/store).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:49,load,loads,49,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,3,['load'],"['load', 'loaded', 'loads']"
Performance,// Computing the value outside of the loop brings no benefit if it is; // definitely used inside the loop in a way which can not be optimized; // away. Avoid doing so unless we know we have a value which computes; // the ExitValue already. TODO: This should be merged into SCEV; // expander to leverage its knowledge of existing expressions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp:132,optimiz,optimized,132,interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,1,['optimiz'],['optimized']
Performance,// Concatenate the AATags of the Merged Loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp:40,Load,Loads,40,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,1,['Load'],['Loads']
Performance,"// Conceptually these next two fields could be in a union. However, this; // causes gcc 4.2 to pessimize LexTokenInternal, a very performance critical; // routine. Keeping as separate members with casts until a more beautiful fix; // presents itself.; /// UintData - This holds either the length of the token text, when; /// a normal token, or the end of the SourceRange when an annotation; /// token.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Lex/Token.h:130,perform,performance,130,interpreter/llvm-project/clang/include/clang/Lex/Token.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Lex/Token.h,1,['perform'],['performance']
Performance,"// Conceptually, there is a vector of N bytes covering the addresses; // starting from the minimum offset (i.e. Base.Addr+Start). This vector; // represents a contiguous memory region that spans all accessed memory; // locations.; // The correspondence between loaded or stored values will be expressed; // in terms of this vector. For example, the 0th element of the vector; // from the Base address info will start at byte Start from the beginning; // of this conceptual vector.; //; // This vector will be loaded/stored starting at the nearest down-aligned; // address and the amount od the down-alignment will be AlignVal:; // valign(load_vector(align_down(Base+Start)), AlignVal)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:261,load,loaded,261,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,2,['load'],['loaded']
Performance,"// Concurrent writes to the same cache element can result in invalid cache; // elements, causing pointer address not being available in the cache even; // though they should be, i.e. false cache misses. While can cause a; // slow-down, the cost for keeping the cache thread-local or atomic is; // much higher (yes, this was measured).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Utils/PlatformPosix.cpp:3,Concurren,Concurrent,3,interpreter/cling/lib/Utils/PlatformPosix.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Utils/PlatformPosix.cpp,6,"['Concurren', 'cache']","['Concurrent', 'cache']"
Performance,"// Concurrently append to this array",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/tpython/test/testTPython.cxx:3,Concurren,Concurrently,3,bindings/tpython/test/testTPython.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/tpython/test/testTPython.cxx,1,['Concurren'],['Concurrently']
Performance,"// Condition reversed because priority queue has the *highest* element at; // the front, rather than the lowest.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocPBQP.cpp:39,queue,queue,39,interpreter/llvm-project/llvm/lib/CodeGen/RegAllocPBQP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocPBQP.cpp,1,['queue'],['queue']
Performance,// Conditional select increment.; // Vector load N-element structure to all lanes:,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.h:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.h,1,['load'],['load']
Performance,"// Conditions under which the scheduler should eagerly advance the cycle:; // (1) No available instructions; // (2) All pipelines full, so available instructions must have hazards.; //; // If HazardRec is disabled, the cycle was pre-advanced before calling; // ReleasePredecessors. In that case, IssueCount should remain 0.; //; // Check AvailableQueue after ReleasePredecessors in case of zero latency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:395,latency,latency,395,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,1,['latency'],['latency']
Performance,"// Confirm the invariant.start location size contains the load operand size; // in bits. Also, the invariant.start should dominate the load, and we; // should not hoist the load out of a loop that contains this dominating; // invariant.start.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:58,load,load,58,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,3,['load'],['load']
Performance,"// Congruence classes represent the set of expressions/instructions; // that are all the same *during some scope in the function*.; // That is, because of the way we perform equality propagation, and; // because of memory value numbering, it is not correct to assume; // you can willy-nilly replace any member with any other at any; // point in the function.; //; // For any Value in the Member set, it is valid to replace any dominated member; // with that Value.; //; // Every congruence class has a leader, and the leader is used to symbolize; // instructions in a canonical way (IE every operand of an instruction that is a; // member of the same congruence class will always be replaced with leader; // during symbolization). To simplify symbolization, we keep the leader as a; // constant if class can be proved to be a constant value. Otherwise, the; // leader is the member of the value set with the smallest DFS number. Each; // congruence class also has a defining expression, though the expression may be; // null. If it exists, it can be used for forward propagation and reassociation; // of values.; // For memory, we also track a representative MemoryAccess, and a set of memory; // members for MemoryPhis (which have no real instructions). Note that for; // memory, it seems tempting to try to split the memory members into a; // MemoryCongruenceClass or something. Unfortunately, this does not work; // easily. The value numbering of a given memory expression depends on the; // leader of the memory congruence class, and the leader of memory congruence; // class depends on the value numbering of a given memory expression. This; // leads to wasted propagation, and in some cases, missed optimization. For; // example: If we had value numbered two stores together before, but now do not,; // we move them to a new value congruence class. This in turn will move at one; // of the memorydefs to a new memory congruence class. Which in turn, affects; // the value numbering of the stores",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp:166,perform,perform,166,interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,1,['perform'],['perform']
Performance,"// Connect the preheader to the exit block. Keep the old edge to the header; // around to perform the dominator tree update in two separate steps; // -- #1 insertion of the edge preheader -> exit and #2 deletion of the edge; // preheader -> header.; //; //; // 0. Preheader 1. Preheader 2. Preheader; // | | | |; // V | V |; // Header <--\ | Header <--\ | Header <--\; // | | | | | | | | | | |; // | V | | | V | | | V |; // | Body --/ | | Body --/ | | Body --/; // V V V V V; // Exit Exit Exit; //; // By doing this is two separate steps we can perform the dominator tree; // update without using the batch update API.; //; // Even when the loop is never executed, we cannot remove the edge from the; // source block to the exit block. Consider the case where the unexecuted loop; // branches back to an outer loop. If we deleted the loop and removed the edge; // coming to this inner loop, this will break the outer loop structure (by; // deleting the backedge of the outer loop). If the outer loop is indeed a; // non-loop, it will be deleted in a future iteration of loop deletion pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp:90,perform,perform,90,interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,2,['perform'],['perform']
Performance,"// Connections for rebinning are created - i.e. slider is; // connected to the slots that perform the rebinning in the; // case of a histogram not derived from an ntuple.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/ged/src/TH1Editor.cxx:90,perform,perform,90,gui/ged/src/TH1Editor.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/ged/src/TH1Editor.cxx,1,['perform'],['perform']
Performance,// Consecutive loads can contain UNDEFS but not ZERO elements.; // Consecutive loads with UNDEFs and ZEROs elements require a; // an additional shuffle stage to clear the ZERO elements.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:15,load,loads,15,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['loads']
Performance,"// Conservatively allocatable nodes will never spill. For now just; // take the first node in the set and push it on the stack. When we; // start optimizing more heavily for register preferencing, it may; // would be better to push nodes with lower 'expected' or worst-case; // register costs first (since early nodes are the most; // constrained).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/RegAllocPBQP.h:146,optimiz,optimizing,146,interpreter/llvm-project/llvm/include/llvm/CodeGen/RegAllocPBQP.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/RegAllocPBQP.h,1,['optimiz'],['optimizing']
Performance,"// Conservatively append user-supplied runtime library functions (supplied; // either directly, or via a function alias) to llvm.compiler.used. These; // could be internalized and deleted by optimizations like -globalopt,; // causing problems when later optimizations add new library calls (e.g.,; // llvm.memset => memset and printf => puts).; // Leave it to the linker to remove any dead code (e.g. with -dead_strip).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/UpdateCompilerUsed.cpp:191,optimiz,optimizations,191,interpreter/llvm-project/llvm/lib/LTO/UpdateCompilerUsed.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/UpdateCompilerUsed.cpp,2,['optimiz'],['optimizations']
Performance,"// Conservatively discard any optimization hints, they may differ on the; // other paths.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp:30,optimiz,optimization,30,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,1,['optimiz'],['optimization']
Performance,"// Conservatively estimate whether the negative offset from the frame; // pointer will be sufficient to reach. If a function has a smallish; // frame, it's less likely to have lots of spills and callee saved; // space, so it's all more likely to be within range of the frame pointer.; // If it's wrong, we'll materialize the constant and still get to the; // object; it's just suboptimal. Negative offsets use the unscaled; // load/store instructions, which have a 9-bit signed immediate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:427,load,load,427,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,1,['load'],['load']
Performance,// Conservatively identifies any definitions which might be live at the; // given instruction. The analysis is performed immediately before the; // given instruction. Values defined by that instruction are not considered; // live. Values used by that instruction are considered live.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp:111,perform,performed,111,interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,1,['perform'],['performed']
Performance,// Conservatively load a constant offset greater than 32 bits into a; // register below.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,// Conservatively refuse to perform a polymorphic operation if we would; // not be able to read a notional 'vptr' value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:28,perform,perform,28,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['perform'],['perform']
Performance,// Conservatively set memVT to the entire set of vectors loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:57,load,loaded,57,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,3,['load'],['loaded']
Performance,"// Conservatively, fences are always clobbers, so don't perform the walk if; // we hit a fence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:56,perform,perform,56,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['perform'],['perform']
Performance,"// Conservatively, mark all functions and calls in CUDA and OpenCL as; // convergent (meaning, they may call an intrinsically convergent op, such; // as __syncthreads() / barrier(), and so can't have certain optimizations; // applied around them). LLVM will remove this attribute where it safely; // can.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:208,optimiz,optimizations,208,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['optimiz'],['optimizations']
Performance,"// Conservatively, mark all inline asm blocks in CUDA or OpenCL as; // convergent (meaning, they may call an intrinsically convergent op, such; // as bar.sync, and so can't have certain optimizations applied around; // them).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp:186,optimiz,optimizations,186,interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,1,['optimiz'],['optimizations']
Performance,"// Conservatively, only handle scalar loads/stores for now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:38,load,loads,38,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,1,['load'],['loads']
Performance,// Consider additional vector types where the element type size is a; // multiple of load/store element size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:85,load,load,85,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,// Consider any loads or stores that are the exact size of the slice.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:16,load,loads,16,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,// Consider only the sections that are required to be loaded for execution,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:54,load,loaded,54,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['loaded']
Performance,"// ConstAttr is enabled in fast-math mode. In fast-math mode, math-errno is; // disabled.; // Math intrinsics are generated only when math-errno is disabled. Any pragmas; // or attributes that affect math-errno should prevent or allow math; // intrincs to be generated. Intrinsics are generated:; // 1- In fast math mode, unless math-errno is overriden; // via '#pragma float_control(precise, on)', or via an; // 'attribute__((optnone))'.; // 2- If math-errno was enabled on command line but overriden; // to false via '#pragma float_control(precise, off))' and; // 'attribute__((optnone))' hasn't been used.; // 3- If we are compiling with optimization and errno has been disabled; // via '#pragma float_control(precise, off)', and; // 'attribute__((optnone))' hasn't been used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:641,optimiz,optimization,641,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['optimiz'],['optimization']
Performance,"// Constant ISD::SRA can be performed efficiently on vXi16 vectors as we; // can replace with ISD::MULHS, creating scale factor from (NumEltBits - Amt).; // TODO: Special case handling for shift by 0/1, really we can afford either; // of these cases in pre-SSE41/XOP/AVX512 but not both.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:28,perform,performed,28,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performed']
Performance,// Constant ISD::SRA/SRL can be performed efficiently on vXi8 vectors as we; // extend to vXi16 to perform a MUL scale effectively as a MUL_LOHI.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:32,perform,performed,32,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['perform'],"['perform', 'performed']"
Performance,"// Constant ISD::SRL can be performed efficiently on vXi16 vectors as we; // can replace with ISD::MULHU, creating scale factor from (NumEltBits - Amt).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:28,perform,performed,28,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performed']
Performance,// Constant being loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:18,load,loaded,18,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,2,['load'],['loaded']
Performance,// Constant fold NOT(N0) to allow us to use AND.; // Ensure this is only performed if we can confirm that the bitcasted source; // has oneuse to prevent an infinite loop with canonicalizeBitSelect.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:73,perform,performed,73,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performed']
Performance,// Constant offsets of scalable types are not really constant.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp:23,scalab,scalable,23,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,2,['scalab'],['scalable']
Performance,"// Constant term optimizer interface",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooAbsData.h:17,optimiz,optimizer,17,roofit/roofitcore/inc/RooAbsData.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooAbsData.h,4,['optimiz'],['optimizer']
Performance,"// Constant vectors are generated as loads from constant pools or as; // splats of a constant value. Since they are generated during the; // selection process, the main selection algorithm is not aware of them.; // Select them directly here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAGHVX.cpp:37,load,loads,37,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAGHVX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAGHVX.cpp,1,['load'],['loads']
Performance,"// ConstantFP nodes default to expand. Targets can either change this to; // Legal, in which case all fp constants are legal, or use isFPImmLegal(); // to optimize expansions for certain constants.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp:155,optimiz,optimize,155,interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,1,['optimiz'],['optimize']
Performance,"// Constants for register spacing in NEON load/store instructions.; // For quad-register load-lane and store-lane pseudo instructors, the; // spacing is initially assumed to be EvenDblSpc, and that is changed to; // OddDblSpc depending on the lane number operand.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,2,['load'],"['load', 'load-lane']"
Performance,// Constants loaded via lgfi.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:13,load,loaded,13,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,1,['load'],['loaded']
Performance,// Constants loaded via llihf:,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:13,load,loaded,13,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,1,['load'],['loaded']
Performance,// Constants loaded via llilf.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:13,load,loaded,13,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,1,['load'],['loaded']
Performance,"// Constants not loadable via lui, addiu, or ori",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/Targets/Mips.h:17,load,loadable,17,interpreter/llvm-project/clang/lib/Basic/Targets/Mips.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/Targets/Mips.h,1,['load'],['loadable']
Performance,// Construct MMO for the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['load'],['load']
Performance,"// Construct a FrameIndexDbgValue for FrameIndexSDNodes so we can describe; // stack slot locations.; //; // Consider ""int x = 0; int *px = &x;"". There are two kinds of interesting; // debug values here after optimization:; //; // dbg.value(i32* %px, !""int *px"", !DIExpression()), and; // dbg.value(i32* %px, !""int x"", !DIExpression(DW_OP_deref)); //; // Both describe the direct values of their associated variables.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:209,optimiz,optimization,209,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['optimiz'],['optimization']
Performance,"// Construct a FrameIndexDbgValue for FrameIndexSDNodes so we can; // describe stack slot locations.; //; // Consider ""int x = 0; int *px = &x;"". There are two kinds of; // interesting debug values here after optimization:; //; // dbg.value(i32* %px, !""int *px"", !DIExpression()), and; // dbg.value(i32* %px, !""int x"", !DIExpression(DW_OP_deref)); //; // Both describe the direct values of their associated variables.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:209,optimiz,optimization,209,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['optimiz'],['optimization']
Performance,"// Construct a compiler instance that will be used to actually create the; // module. Since we're sharing an in-memory module cache,; // CompilerInstance::CompilerInstance is responsible for finalizing the; // buffers to prevent use-after-frees.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:126,cache,cache,126,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,1,['cache'],['cache']
Performance,// Construct a load command.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ObjCopy/MachO/MachOWriter.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/ObjCopy/MachO/MachOWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ObjCopy/MachO/MachOWriter.cpp,1,['load'],['load']
Performance,// Construct the actions to perform.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp:28,perform,perform,28,interpreter/llvm-project/clang/lib/Driver/Driver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp,1,['perform'],['perform']
Performance,// Construct the list of abstract actions to perform for this compilation. On; // Darwin OSes this uses the driver-driver and builds universal actions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp:45,perform,perform,45,interpreter/llvm-project/clang/lib/Driver/Driver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp,1,['perform'],['perform']
Performance,// Construct the list of abstract actions to perform for this compilation. On; // MachO targets this uses the driver-driver and universal actions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp:45,perform,perform,45,interpreter/llvm-project/clang/lib/Driver/Driver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp,1,['perform'],['perform']
Performance,"// Construct the new indexed instruction.; // First, add the def for loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstExtenders.cpp:69,load,loads,69,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstExtenders.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstExtenders.cpp,1,['load'],['loads']
Performance,"// Construct two halves in parallel, then or them together. Rn and Rm count; // number of rotations needed before the next element. One last rotation is; // performed post-loop to position the last element.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp:157,perform,performed,157,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp,1,['perform'],['performed']
Performance,"// Constructor and ""create"" factory function. The constructor is only a thin; // wrapper around the base constructor. The ""create"" function fills out the; // XCOFF-specific information and performs the error checking along the way.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Object/XCOFFObjectFile.h:189,perform,performs,189,interpreter/llvm-project/llvm/include/llvm/Object/XCOFFObjectFile.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Object/XCOFFObjectFile.h,1,['perform'],['performs']
Performance,"// Constructor from existing data set with list of variables that preserves the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx:80,cache,cache,80,roofit/roofitcore/src/RooTreeDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx,1,['cache'],['cache']
Performance,"// Constructor homing can be used for classes that cannnot be constructed; // without emitting code for one of their constructors. This is classes that; // don't have trivial or constexpr constructors, or can be created from; // aggregate initialization. Also skip lambda objects because they don't call; // constructors.; // Skip this optimization if the class or any of its methods are marked; // dllimport.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp:336,optimiz,optimization,336,interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,1,['optimiz'],['optimization']
Performance,"// Constructor of optimization generator context for RooProdPdf objects; //Build an array of generator contexts for each product component PDF",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooProdGenContext.cxx:18,optimiz,optimization,18,roofit/roofitcore/src/RooProdGenContext.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooProdGenContext.cxx,1,['optimiz'],['optimization']
Performance,// Consume and cache the starting token.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseDeclCXX.cpp:15,cache,cache,15,interpreter/llvm-project/clang/lib/Parse/ParseDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseDeclCXX.cpp,1,['cache'],['cache']
Performance,// Consume queues.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseOpenACC.cpp:11,queue,queues,11,interpreter/llvm-project/clang/lib/Parse/ParseOpenACC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseOpenACC.cpp,1,['queue'],['queues']
Performance,// Consumes inotify events and pushes directory watcher events to the Queue.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp:70,Queue,Queue,70,interpreter/llvm-project/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp,1,['Queue'],['Queue']
Performance,// Container for loads to widen.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp:17,load,loads,17,interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,1,['load'],['loads']
Performance,"// Contains all the needed information to create a stack for doing a depth; // first traversal of the tree. This includes scopes for values, loads, and; // calls as well as the generation. There is a child iterator so that the; // children do not need to be store separately.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:141,load,loads,141,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,1,['load'],['loads']
Performance,// Contains the set of symbols loaded from file,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Internalize.cpp:31,load,loaded,31,interpreter/llvm-project/llvm/lib/Transforms/IPO/Internalize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Internalize.cpp,1,['load'],['loaded']
Performance,// Contiguous gather => masked load.; // (sve.ld1.gather.index Mask BasePtr (sve.index IndexBase 1)); // => (masked.load (gep BasePtr IndexBase) Align Mask zeroinitializer),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,2,['load'],['load']
Performance,"// Continually inlining through an SCC can result in huge compile; // times and bloated code since we arbitrarily stop at some point; // when the inliner decides it's not profitable to inline anymore.; // We attempt to mitigate this by making these calls exponentially; // more expensive.; // This doesn't apply to calls in the same SCC since if we do; // inline through the SCC the function will end up being; // self-recursive which the inliner bails out on, and inlining; // within an SCC is necessary for performance.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp:509,perform,performance,509,interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp,1,['perform'],['performance']
Performance,"// Control usage of the tree cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/inc/TEventIter.h:29,cache,cache,29,proof/proofplayer/inc/TEventIter.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/inc/TEventIter.h,1,['cache'],['cache']
Performance,// Controls how to track origins.; // * 0: do not track origins.; // * 1: track origins at memory store operations.; // * 2: track origins at memory load and store operations.; // TODO: track callsites.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:149,load,load,149,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['load'],['load']
Performance,// Controls whether the pass includes or ignores the labels of pointers in load; // instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:75,load,load,75,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['load'],['load']
Performance,"// Convert a bitcasted integer logic operation that has one bitcasted; // floating-point operand into a floating-point logic operation. This may; // create a load of a constant, but that is cheaper than materializing the; // constant in an integer register and transferring it to an SSE register or; // transferring the SSE operand to integer register and back.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:158,load,load,158,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// Convert a full vector load into vzload when not all bits are needed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,['load'],['load']
Performance,"// Convert a masked load with a constant mask into a masked load and a select.; // This allows the select operation to use a faster kind of select instruction; // (for example, vblendvps -> vblendps).; // Don't try this if the pass-through operand is already undefined. That would; // cause an infinite loop because that's what we're about to create.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,// Convert all fixed length vector loads larger than NEON to masked_loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['loads']
Performance,// Convert all loads and intermediate operations to use parameter AS and; // skip creation of a local copy of the argument.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerArgs.cpp:15,load,loads,15,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerArgs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerArgs.cpp,1,['load'],['loads']
Performance,// Convert callee-save register save/restore instruction to do stack pointer; // decrement/increment to allocate/deallocate the callee-save stack area by; // converting store/load to use pre/post increment version.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:175,load,load,175,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,1,['load'],['load']
Performance,// Convert fixed length vector operands to scalable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:43,scalab,scalable,43,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['scalab'],['scalable']
Performance,// Convert fixed vectors to scalable if needed,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:28,scalab,scalable,28,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// Convert loads directly. This is normally done by DAGCombiner,; // but we need this case for bitcasts that are created during lowering; // and which are then lowered themselves.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['loads']
Performance,"// Convert one MVE vector type into another by reinterpreting its in-register; // format.; //; // Little-endian, this is identical to a bitcast (which reinterprets the; // memory format). But big-endian, they're not necessarily the same, because; // the register and memory formats map to each other differently depending on; // the lane size.; //; // We generate a bitcast whenever we can (if we're little-endian, or if the; // lane sizes are the same anyway). Otherwise we fall back to an IR intrinsic; // that performs the different kind of reinterpretation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:513,perform,performs,513,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['perform'],['performs']
Performance,// Convert operands to Scalable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,Scalab,Scalable,23,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['Scalab'],['Scalable']
Performance,"// Convert right out of the scalable type so we can use standard ISD; // nodes for the rest of the computation. If we used scalable types with; // these, we'd lose the fixed-length vector info and generate worse; // vsetvli code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:28,scalab,scalable,28,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,['scalab'],['scalable']
Performance,// Convert sext.w+add/sub/mul to their W instructions. This will create; // a new independent instruction. This improves latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp:121,latency,latency,121,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,1,['latency'],['latency']
Performance,"// Convert sve_tbl(OpVal sve_dup_x(SplatValue)) to; // splat_vector(extractelement(OpVal, SplatValue)) for further optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:115,optimiz,optimization,115,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['optimiz'],['optimization']
Performance,"// Convert the V_ADD_CO_U32_e64 into V_ADD_CO_U32_e32. This allows; // isConvertibleToSDWA to perform its transformation on V_ADD_CO_U32_e32 into; // V_ADD_CO_U32_sdwa.; //; // We are transforming from a VOP3 into a VOP2 form of the instruction.; // %19:vgpr_32 = V_AND_B32_e32 255,; // killed %16:vgpr_32, implicit $exec; // %47:vgpr_32, %49:sreg_64_xexec = V_ADD_CO_U32_e64; // %26.sub0:vreg_64, %19:vgpr_32, implicit $exec; // %48:vgpr_32, dead %50:sreg_64_xexec = V_ADDC_U32_e64; // %26.sub1:vreg_64, %54:vgpr_32, killed %49:sreg_64_xexec, implicit $exec; //; // becomes; // %47:vgpr_32 = V_ADD_CO_U32_sdwa; // 0, %26.sub0:vreg_64, 0, killed %16:vgpr_32, 0, 6, 0, 6, 0,; // implicit-def $vcc, implicit $exec; // %48:vgpr_32, dead %50:sreg_64_xexec = V_ADDC_U32_e64; // %26.sub1:vreg_64, %54:vgpr_32, killed $vcc, implicit $exec",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp:94,perform,perform,94,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIPeepholeSDWA.cpp,1,['perform'],['perform']
Performance,// Convert the bit-reverse load intrinsic to appropriate target instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// Convert the byte-offset used by unscaled into an ""element"" offset used; // by the scaled pair load/store instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:97,load,load,97,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,2,['load'],['load']
Performance,// Convert the comparison and its user to a compare against zero with the; // appropriate predicate on the branch. Zero comparison might provide; // optimization opportunities post-RA (see optimization in; // PPCPreEmitPeephole.cpp).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:149,optimiz,optimization,149,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,2,['optimiz'],['optimization']
Performance,// Convert this to two 32-bit bswap loads and a BUILD_PAIR. Do this only; // before legalization so that the BUILD_PAIR is handled correctly.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:36,load,loads,36,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loads']
Performance,// Convert to the appropriate type; this could be an lvalue for; // an integer. FIXME: performAddrSpaceCast,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprConstant.cpp:87,perform,performAddrSpaceCast,87,interpreter/llvm-project/clang/lib/CodeGen/CGExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprConstant.cpp,1,['perform'],['performAddrSpaceCast']
Performance,"// Convert to vectors, do a VSELECT, and convert back to scalar.; // All of the conversions should be optimized away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:102,optimiz,optimized,102,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimized']
Performance,"// Convert vector increment or decrement to sub/add with an all-ones; // constant:; // add X, <1, 1...> --> sub X, <-1, -1...>; // sub X, <1, 1...> --> add X, <-1, -1...>; // The all-ones vector constant can be materialized using a pcmpeq; // instruction that is commonly recognized as an idiom (has no register; // dependency), so that's better/smaller than loading a splat 1 constant.; //; // But don't do this if it would inhibit a potentially profitable load; // folding opportunity for the other operand. That only occurs with the; // intersection of:; // (1) The other operand (op0) is load foldable.; // (2) The op is an add (otherwise, we are *creating* an add and can still; // load fold the other op).; // (3) The target has AVX (otherwise, we have a destructive add and can't; // load fold the other op without killing the constant op).; // (4) The constant 1 vector has multiple uses (so it is profitable to load; // into a register anyway).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:359,load,loading,359,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,6,['load'],"['load', 'loading']"
Performance,"// Convey the information that a branch was not found either when; // switching to a new tree (i.e. when trying to load its first entry) or; // even if we are in the middle of the tree (e.g. by calling SetEntriesRange; // beforehand) but a proxy was not created because of the missing branch",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeReader.cxx:115,load,load,115,tree/treeplayer/src/TTreeReader.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeReader.cxx,1,['load'],['load']
Performance,"// Copied from Frontend/FrontendAction.cpp.; // FIXME: Remove when we switch to a tools-based cling driver.; // If the FrontendPluginRegistry has plugins before loading any shared library; // this means we have linked our plugins. This is useful when cling runs in; // embedded mode (in a shared library). This is the only feasible way to have; // plugins if cling is in a single shared library which is dlopen-ed with; // RTLD_LOCAL. In that situation plugins can still find the cling, clang and; // llvm symbols opened with local visibility.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/IncrementalParser.cpp:161,load,loading,161,interpreter/cling/lib/Interpreter/IncrementalParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/IncrementalParser.cpp,1,['load'],['loading']
Performance,"// Copies/moves of physical accumulators are expensive operations; // that should be avoided whenever possible. MMA instructions are; // meant to be used in performance-sensitive computational kernels.; // This option is provided, at least for the time being, to give the; // user a tool to detect this expensive operation and either rework; // their code or report a compiler bug if that turns out to be the; // cause.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp:157,perform,performance-sensitive,157,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp,1,['perform'],['performance-sensitive']
Performance,"// Copy any metadata that is valid for the new load. This may require; // conversion to a different kind of metadata, e.g. !nonnull might change; // to !range or vice versa.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,"// Copy failed (could be because the CacheEntry was removed from the cache; // in the meantime by another process), fall back and try to write down the; // buffer to the output.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp:37,Cache,CacheEntry,37,interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,2,"['Cache', 'cache']","['CacheEntry', 'cache']"
Performance,// Copy load operand to new alloca.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,1,['load'],['load']
Performance,// Copy successor edges from SUa to SUb. Interleaving computation; // dependent on SUa can prevent load combining due to register reuse.; // Predecessor edges do not need to be copied from SUb to SUa since; // nearby loads should have effectively the same inputs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:99,load,load,99,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,2,['load'],"['load', 'loads']"
Performance,// Copy the candidates as our processing of them may load new declarations; // from an external source and invalidate lookup_result.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:53,load,load,53,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['load'],['load']
Performance,// Copy the load command as it is.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ObjCopy/MachO/MachOWriter.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/ObjCopy/MachO/MachOWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ObjCopy/MachO/MachOWriter.cpp,1,['load'],['load']
Performance,// Copy the memoperands from the load to the folded instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,1,['load'],['load']
Performance,"// Copy the optimized out elements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfoActions.cxx:12,optimiz,optimized,12,io/io/src/TStreamerInfoActions.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfoActions.cxx,2,['optimiz'],['optimized']
Performance,// Copy the remainder of the byval argument with sub-word loads and shifts.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:58,load,loads,58,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['loads']
Performance,// Copy the residual with single byte load/store loop.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LowerMemIntrinsics.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/Transforms/Utils/LowerMemIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LowerMemIntrinsics.cpp,1,['load'],['load']
Performance,"// Copy the value to a (aligned) stack slot using (unaligned) integer; // loads and stores, then do a (aligned) load from the stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:74,load,loads,74,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,2,['load'],"['load', 'loads']"
Performance,"// Copying of byval aggregates + SROA may result in pointers being loaded as; // integers, followed by intotoptr. We may want to mark those as global, too,; // but only if the loaded integer is used exclusively for conversion to a; // pointer with inttoptr.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerArgs.cpp:67,load,loaded,67,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerArgs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXLowerArgs.cpp,2,['load'],['loaded']
Performance,"// Cost is proportional to the number of memory operations implied. For; // scalable vectors, we use an estimate on that number since we don't; // know exactly what VL will be.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:76,scalab,scalable,76,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['scalab'],['scalable']
Performance,// Cost of HVX loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonTargetTransformInfo.cpp:15,load,loads,15,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonTargetTransformInfo.cpp,1,['load'],['loads']
Performance,// Cost of constructing HVX vector from scalar loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonTargetTransformInfo.cpp:47,load,loads,47,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonTargetTransformInfo.cpp,1,['load'],['loads']
Performance,// Cost of load/store operations and the permutations needed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,1,['load'],['load']
Performance,// Costs for loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:13,load,loads,13,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,1,['load'],['loads']
Performance,"// Could change cached volume/axes",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLBoundingBox.cxx:16,cache,cached,16,graf3d/gl/src/TGLBoundingBox.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLBoundingBox.cxx,8,['cache'],['cached']
Performance,"// Could generate ""BUFFER_INV"" but it would do nothing as there are no; // caches to invalidate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:75,cache,caches,75,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,1,['cache'],['caches']
Performance,// Could not evaluate load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Evaluator.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Transforms/Utils/Evaluator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Evaluator.cpp,1,['load'],['load']
Performance,// Could not optimize.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:13,optimiz,optimize,13,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['optimiz'],['optimize']
Performance,"// Couldn't load an index, fallback to loading all the block ""old-style"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,2,['load'],"['load', 'loading']"
Performance,// Couldn't optimize. Emit a compare + a Bcc.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:12,optimiz,optimize,12,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['optimiz'],['optimize']
Performance,"// Count of resets performed of basket size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/inc/TBasket.h:19,perform,performed,19,tree/tree/inc/TBasket.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/inc/TBasket.h,1,['perform'],['performed']
Performance,// Count scheduled resources that have been executed. Resources are; // considered executed if they become ready in the time that it takes to; // saturate any resource including the one in question. Counts are scaled; // for direct comparison with other resources. Counts can be compared with; // MOps * getMicroOpFactor and Latency * getLatencyFactor.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineScheduler.h:325,Latency,Latency,325,interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineScheduler.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineScheduler.h,1,['Latency'],['Latency']
Performance,// Counter to track num of load & store,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopVersioningLICM.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopVersioningLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopVersioningLICM.cpp,1,['load'],['load']
Performance,"// Create ""cond"" block; //; // %EltAddr = getelementptr i32* %1, i32 0; // %Elt = load i32* %EltAddr; // VResult = insertelement <16 x i32> VResult, i32 %Elt, i32 Idx; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp:82,load,load,82,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,3,['load'],['load']
Performance,"// Create CompileUnit structures to keep information about source; // DWARFUnit`s, load line tables.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/DWARFLinkerImpl.cpp:83,load,load,83,interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/DWARFLinkerImpl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Parallel/DWARFLinkerImpl.cpp,1,['load'],['load']
Performance,"// Create GC pool and pre-load some GCs...",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGResourcePool.cxx:26,load,load,26,gui/gui/src/TGResourcePool.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGResourcePool.cxx,1,['load'],['load']
Performance,// Create LLVM's optimization remarks streamer.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/LLVMRemarkStreamer.cpp:17,optimiz,optimization,17,interpreter/llvm-project/llvm/lib/IR/LLVMRemarkStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/LLVMRemarkStreamer.cpp,2,['optimiz'],['optimization']
Performance,// Create VPWidenMemoryInstructionRecipe for loads and stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanTransforms.cpp,1,['load'],['loads']
Performance,// Create a GEP with a byte offset between the FAM and count and; // use that to load the count value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:81,load,load,81,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,// Create a PHI node at the start of the block for the PRE'd load value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:61,load,load,61,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// Create a PassManager to hold and optimize the collection of passes we are; // about to build. If the -debugify-each option is set, wrap each pass with; // the (-check)-debugify passes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/opt.cpp:36,optimiz,optimize,36,interpreter/llvm-project/llvm/tools/opt/opt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/opt.cpp,1,['optimiz'],['optimize']
Performance,"// Create a cache entry. This compute a unique hash for the Module considering; // the current list of export/import, and offer an interface to query to; // access the content in the cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp:12,cache,cache,12,interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,2,['cache'],['cache']
Performance,"// Create a cache object",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/RooBarlowBeestonLL.cxx:12,cache,cache,12,roofit/histfactory/src/RooBarlowBeestonLL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/RooBarlowBeestonLL.cxx,1,['cache'],['cache']
Performance,// Create a call to load.relative intrinsic that computes the target address; // by adding base address (lookup table address) and relative offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp,1,['load'],['load']
Performance,"// Create a clone for two reasons:; // * If the optimization passes delete any function, the deleted function; // will be in the clone and Funcs will still point to valid memory; // * If the optimization passes use interprocedural information to break; // a function, we want to continue with the original function. Otherwise; // we can conclude that a function triggers the bug when in fact one; // needs a larger set of original functions to do so.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/Miscompilation.cpp:48,optimiz,optimization,48,interpreter/llvm-project/llvm/tools/bugpoint/Miscompilation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/Miscompilation.cpp,2,['optimiz'],['optimization']
Performance,// Create a constant or a load from the source.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['load'],['load']
Performance,// Create a file manager object to provide access to and cache the filesystem.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:57,cache,cache,57,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,2,['cache'],['cache']
Performance,// Create a function that performs CFI checks for cross-DSO calls with targets; // in the current module.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:26,perform,performs,26,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['perform'],['performs']
Performance,"// Create a io_uring instance that can hold at least `entriesHint` submission entries. The actual; // queue depth is rounded up to the next power of 2. Throws an exception if ring setup fails.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/inc/ROOT/RIoUring.hxx:102,queue,queue,102,io/io/inc/ROOT/RIoUring.hxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/inc/ROOT/RIoUring.hxx,1,['queue'],['queue']
Performance,"// Create a list of unique branches to load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx:39,load,load,39,tree/treeplayer/src/TTreeFormula.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx,1,['load'],['load']
Performance,// Create a load from the temporary.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['load'],['load']
Performance,// Create a module that will run the optimization passes,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp:37,optimiz,optimization,37,interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,1,['optimiz'],['optimization']
Performance,"// Create a new ObjectEntry, but don't add it to the cache yet. Loading of; // the archive members might fail and we don't want to lock the whole archive; // during this operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp:53,cache,cache,53,interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,2,"['Load', 'cache']","['Loading', 'cache']"
Performance,// Create a new global to hold the cached function pointer.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/Miscompilation.cpp:35,cache,cached,35,interpreter/llvm-project/llvm/tools/bugpoint/Miscompilation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/Miscompilation.cpp,1,['cache'],['cached']
Performance,// Create a new optimization pass for each one specified on the command line,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/opt.cpp:16,optimiz,optimization,16,interpreter/llvm-project/llvm/tools/opt/opt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/opt.cpp,1,['optimiz'],['optimization']
Performance,// Create a new simpler BUILD_VECTOR sequence which other optimizations can; // turn into a single shuffle instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:58,optimiz,optimizations,58,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['optimiz'],['optimizations']
Performance,"// Create a node that performs P on operands Op0 and Op1, casting the; // operands to the appropriate type. The type of the result is determined by P.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:22,perform,performs,22,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['perform'],['performs']
Performance,"// Create a note that contains pointers to the list of global; // descriptors. Adding a note to the output file will cause the linker to; // create a PT_NOTE program header pointing to the note that we can use to; // find the descriptor list starting from the program headers. A function; // provided by the runtime initializes the shadow memory for the globals by; // accessing the descriptor list via the note. The dynamic loader needs to; // call this function whenever a library is loaded.; //; // The reason why we use a note for this instead of a more conventional; // approach of having a global constructor pass a descriptor list pointer to; // the runtime is because of an order of initialization problem. With; // constructors we can encounter the following problematic scenario:; //; // 1) library A depends on library B and also interposes one of B's symbols; // 2) B's constructors are called before A's (as required for correctness); // 3) during construction, B accesses one of its ""own"" globals (actually; // interposed by A) and triggers a HWASAN failure due to the initialization; // for A not having happened yet; //; // Even without interposition it is possible to run into similar situations in; // cases where two libraries mutually depend on each other.; //; // We only need one note per binary, so put everything for the note in a; // comdat. This needs to be a comdat with an .init_array section to prevent; // newer versions of lld from discarding the note.; //; // Create the note even if we aren't instrumenting globals. This ensures that; // binaries linked from object files with both instrumented and; // non-instrumented globals will end up with a note, even if a comdat from an; // object file with non-instrumented globals is selected. The note is harmless; // if the runtime doesn't support it, since it will just be ignored.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/HWAddressSanitizer.cpp:425,load,loader,425,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/HWAddressSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/HWAddressSanitizer.cpp,2,['load'],"['loaded', 'loader']"
Performance,// Create a priority queue containing all edges ordered by the merge gain.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp:21,queue,queue,21,interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp,1,['queue'],['queue']
Performance,// Create a read hazard for the allocas. This inhibits dead-store; // optimizations and forces the values to memory. This hazard is; // inserted before any 'throwing' calls in the protected scope to; // reflect the possibility that the variables might be read from the; // catch block if the call throws.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp:70,optimiz,optimizations,70,interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,1,['optimiz'],['optimizations']
Performance,// Create a small function pass pipeline to cleanup after all the global; // optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:77,optimiz,optimizations,77,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['optimiz'],['optimizations']
Performance,// Create a temporary for the node that is referencing the operand we; // will lazy-load. It is needed before recursing in case there are; // uniquing cycles.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp:84,load,load,84,interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,1,['load'],['load']
Performance,// Create a type on which we perform the shuffle.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:29,perform,perform,29,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['perform'],['perform']
Performance,"// Create a v4i32 vector load operation, effectively <4 x v4i8>.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['load']
Performance,// Create a vector of the same type returned by the original load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:61,load,load,61,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['load']
Performance,"// Create a vector sized/aligned stack slot, store the value to element #0,; // then load the whole vector back out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:85,load,load,85,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,// Create a write hazard for the allocas. This inhibits folding; // loads across the hazard. This hazard is inserted at the; // beginning of the catch path to reflect the possibility that the; // variables might have been written within the protected scope.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp:68,load,loads,68,interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,1,['load'],['loads']
Performance,// Create all 256-bit loads starting from offset 0 and up to Num256Loads-1*32.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,load,loads,22,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loads']
Performance,// Create all the blocks and logic.; // ParentBB:; // goto RegionCheckTidBB; // RegionCheckTidBB:; // Tid = __kmpc_hardware_thread_id(); // if (Tid != 0); // goto RegionBarrierBB; // RegionStartBB:; // <execute instructions guarded>; // goto RegionEndBB; // RegionEndBB:; // <store escaping values to shared mem>; // goto RegionBarrierBB; // RegionBarrierBB:; // __kmpc_simple_barrier_spmd(); // // second barrier is omitted if lacking escaping values.; // <load escaping values from shared mem>; // __kmpc_simple_barrier_spmd(); // goto RegionExitBB; // RegionExitBB:; // <execute rest of instructions>,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp:458,load,load,458,interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,1,['load'],['load']
Performance,"// Create all the new phi nodes of the new type, and bitcast any loads to the; // correct type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:65,load,loads,65,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['load'],['loads']
Performance,"// Create an AddExpr for ""PreStart"" after subtracting Step. Full SCEV; // subtraction is expensive. For this purpose, perform a quick and dirty; // difference, by checking for Step in the operand list. Note, that; // SA might have repeated ops, like %a + %a + ..., so only remove one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp:118,perform,perform,118,interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp,1,['perform'],['perform']
Performance,// Create an Attributor and initially empty information cache that is filled; // while we identify default attribute opportunities.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Attributor.cpp:56,cache,cache,56,interpreter/llvm-project/llvm/lib/Transforms/IPO/Attributor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Attributor.cpp,2,['cache'],['cache']
Performance,// Create an anchor point that is in range. Start at 0xffff so that; // can use LLILH to load the immediate.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZRegisterInfo.cpp:89,load,load,89,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZRegisterInfo.cpp,1,['load'],['load']
Performance,"// Create an io_uring instance. The ring selects an appropriate queue depth. which can be queried; // afterwards using GetQueueDepth(). The depth is typically 1024 or lower. Throws an exception if; // ring setup fails.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/inc/ROOT/RIoUring.hxx:64,queue,queue,64,io/io/inc/ROOT/RIoUring.hxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/inc/ROOT/RIoUring.hxx,1,['queue'],['queue']
Performance,"// Create and fill cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsCachedPdf.cxx:19,cache,cache,19,roofit/roofitcore/src/RooAbsCachedPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsCachedPdf.cxx,1,['cache'],['cache']
Performance,"// Create artificial edges between loads that could likely cause a bank; // conflict. Since such loads would normally not have any dependency; // between them, we cannot rely on existing edges.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,2,['load'],['loads']
Performance,"// Create cache object itself -- Default implementation is a RooHistPdf",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsCachedPdf.cxx:10,cache,cache,10,roofit/roofitcore/src/RooAbsCachedPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsCachedPdf.cxx,1,['cache'],['cache']
Performance,"// Create cache storage element",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooAbsCachedPdf.h:10,cache,cache,10,roofit/roofitcore/inc/RooAbsCachedPdf.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooAbsCachedPdf.h,1,['cache'],['cache']
Performance,"// Create clones of the elements in prodSet. These need to be cloned; // because when caching optimisation lvl 2 is activated, pre-computed; // values are side-loaded into the elements.; // Those pre-cached values already contain normalisation constants, so; // the integral comes out wrongly. Therefore, we create here nodes that; // don't participate in any caching, which are used to compute integrals.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooGenProdProj.cxx:160,load,loaded,160,roofit/roofitcore/src/RooGenProdProj.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooGenProdProj.cxx,2,"['cache', 'load']","['cached', 'loaded']"
Performance,"// Create code to check if the memory locations of the Load and Store; // overlap and if they do, copy Load's operand to a new buffer.; // First, create new blocks for 2n part of the check and the copy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:55,Load,Load,55,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,2,['Load'],['Load']
Performance,"// Create following instructions and multiple basic blocks.; //; // thisBB:; // brge.l.t %sp, %sl, sinkBB; // syscallBB:; // ld %s61, 0x18(, %tp) // load param area; // or %s62, 0, %s0 // spill the value of %s0; // lea %s63, 0x13b // syscall # of grow; // shm.l %s63, 0x0(%s61) // store syscall # at addr:0; // shm.l %sl, 0x8(%s61) // store old limit at addr:8; // shm.l %sp, 0x10(%s61) // store new limit at addr:16; // monc // call monitor; // or %s0, 0, %s62 // restore the value of %s0; // sinkBB:; // Create new MBB",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEInstrInfo.cpp:149,load,load,149,interpreter/llvm-project/llvm/lib/Target/VE/VEInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEInstrInfo.cpp,1,['load'],['load']
Performance,"// Create font pool and pre-load some fonts...",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGResourcePool.cxx:28,load,load,28,gui/gui/src/TGResourcePool.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGResourcePool.cxx,1,['load'],['load']
Performance,// Create initialized wrapper structure that points to the loaded GPU binary,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp:59,load,loaded,59,interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp,1,['load'],['loaded']
Performance,"// Create integral performing remaining numeric integration over (partial) analytic product",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooGenProdProj.cxx:19,perform,performing,19,roofit/roofitcore/src/RooGenProdProj.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooGenProdProj.cxx,1,['perform'],['performing']
Performance,// Create list of operands by converting existing ones to scalable types.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:58,scalab,scalable,58,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,['scalab'],['scalable']
Performance,// Create load from the chosen pointer,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/FuzzMutate/RandomIRBuilder.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/FuzzMutate/RandomIRBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/FuzzMutate/RandomIRBuilder.cpp,1,['load'],['load']
Performance,// Create load node to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['load'],['load']
Performance,// Create load nodes to retrieve arguments from the stack,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['load']
Performance,// Create load nodes to retrieve arguments from the stack.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,4,['load'],['load']
Performance,"// Create methods available outside of this file, to use them; // ""include/llvm/LinkAllPasses.h"". Otherwise the pass would be deleted by; // the link time optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CallPrinter.cpp:155,optimiz,optimization,155,interpreter/llvm-project/llvm/lib/Analysis/CallPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CallPrinter.cpp,4,['optimiz'],['optimization']
Performance,// Create narrow loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:17,load,loads,17,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,"// Create new cache element",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/ParamHistFunc.cxx:14,cache,cache,14,roofit/histfactory/src/ParamHistFunc.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/ParamHistFunc.cxx,3,['cache'],['cache']
Performance,"// Create new cache entry",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMultiVarGaussian.cxx:14,cache,cache,14,roofit/roofitcore/src/RooMultiVarGaussian.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMultiVarGaussian.cxx,1,['cache'],['cache']
Performance,"// Create new node SET_FPENV_MEM, which uses the load address to read FP; // environment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:49,load,load,49,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,// Create non-scalable LocationSize,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/MemoryLocation.h:14,scalab,scalable,14,interpreter/llvm-project/llvm/include/llvm/Analysis/MemoryLocation.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/MemoryLocation.h,1,['scalab'],['scalable']
Performance,// Create operands to load from the constant pool entry.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,"// Create picture pool, GC pool, font pool, mime type list, etc.; // Create picture pool and pre-load some pictures...",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGResourcePool.cxx:97,load,load,97,gui/gui/src/TGResourcePool.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGResourcePool.cxx,1,['load'],['load']
Performance,// Create readers and perform requested tasks on them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/llvm-debuginfo-analyzer.cpp:22,perform,perform,22,interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/llvm-debuginfo-analyzer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-debuginfo-analyzer/llvm-debuginfo-analyzer.cpp,1,['perform'],['perform']
Performance,"// Create tensor to load the chunk into",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/RBatchGenerator.hxx:20,load,load,20,tmva/tmva/inc/TMVA/RBatchGenerator.hxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/RBatchGenerator.hxx,1,['load'],['load']
Performance,// Create the AND instruction which performs the actual truncation.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:36,perform,performs,36,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,1,['perform'],['performs']
Performance,"// Create the MBB that will load from and jump through the table.; // Note: We create it here, but it's not inserted into the function yet.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SwitchLoweringUtils.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/CodeGen/SwitchLoweringUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SwitchLoweringUtils.cpp,1,['load'],['load']
Performance,"// Create the MBBs for the dispatch code like following:; //; // ThisMBB:; // Prepare DispatchBB address and store it to buf[1].; // ...; //; // DispatchBB:; // %s15 = GETGOT iff isPositionIndependent; // %callsite = load callsite; // brgt.l.t #size of callsites, %callsite, DispContBB; //; // TrapBB:; // Call abort.; //; // DispContBB:; // %breg = address of jump table; // %pc = load and calculate next pc from %breg and %callsite; // jmp %pc; // Shove the dispatch's address into the return slot in the function context.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp:217,load,load,217,interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,2,['load'],['load']
Performance,// Create the SelectionDAG nodes corresponding to a load; // from this parameter,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCISelLowering.cpp,2,['load'],['load']
Performance,// Create the SelectionDAG nodes corresponding to a load; // from this parameter.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp,1,['load'],['load']
Performance,// Create the SelectionDAG nodes corresponding to a load; // from this parameter. Unpromoted ints and floats are; // passed as right-justified 8-byte values.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,// Create the SelectionDAG nodes corresponding to a load; //from this parameter,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430ISelLowering.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430ISelLowering.cpp,2,['load'],['load']
Performance,"// Create the blocks that we're going to need:; // 1. A block for checking the zero-extended length exceeds 0; // 2. A block to check that the start and end addresses of a given array; // lie on the same page.; // 3. The SVE loop preheader.; // 4. The first SVE loop block.; // 5. The SVE loop increment block.; // 6. A block we can jump to from the SVE loop when a mismatch is found.; // 7. The first block of the scalar loop itself, containing PHIs , loads; // and cmp.; // 8. A scalar loop increment block to increment the PHIs and go back; // around the loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:453,load,loads,453,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,1,['load'],['loads']
Performance,// Create the byte-swapping load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,// Create the cache directory if not already done. Doing this lazily; // ensures the filesystem isn't mutated until the cache is.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Caching.cpp:14,cache,cache,14,interpreter/llvm-project/llvm/lib/Support/Caching.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Caching.cpp,2,['cache'],['cache']
Performance,// Create the call.; // Note that unordered atomic loads/stores are *required* by the spec to; // have an alignment but non-atomic loads/stores may not.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,2,['load'],['loads']
Performance,// Create the element-swapping load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,// Create the load for the slice.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,// Create the load from the constant pool.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,2,['load'],['load']
Performance,// Create the load function calling the runtime entry point with the module; // structure,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp:14,load,load,14,interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,1,['load'],['load']
Performance,// Create the load of the global variable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,1,['load'],['load']
Performance,// Create the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,2,['load'],['load']
Performance,// Create the new Load and Store operations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:18,Load,Load,18,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['Load'],['Load']
Performance,"// Create the new MMO for the new base load. It is like the original MMO,; // but represents an area in memory almost twice the vector size centered; // on the original address. If the address is unaligned, we might start; // reading up to (sizeof(vector)-1) bytes below the address of the; // original unaligned load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,// Create the new base load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// Create the new basic blocks. One block contains all the XMM stores,; // and another block is the final destination regardless of whether any; // stores were performed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ExpandPseudo.cpp:160,perform,performed,160,interpreter/llvm-project/llvm/lib/Target/X86/X86ExpandPseudo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ExpandPseudo.cpp,1,['perform'],['performed']
Performance,"// Create the new updating load/store node.; // First, create an SDVTList for the new updating node's results.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['load'],['load']
Performance,// Create the nodes corresponding to a load from this parameter slot.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,2,['load'],['load']
Performance,// Create the number of required load compare basic blocks.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['load'],['load']
Performance,"// Create the parameter slot and register its destruction. For a vararg; // argument, create a temporary.; // FIXME: For calling conventions that destroy parameters in the callee,; // should we consider performing destruction when the function returns; // instead?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:203,perform,performing,203,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['perform'],['performing']
Performance,// Create the type of the loaded slice according to its size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:26,load,loaded,26,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loaded']
Performance,// Create the wide load and update the MemorySSA.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp,1,['load'],['load']
Performance,"// Create the wide load, while making sure to maintain the original alignment; // as this prevents ldrd from being generated when it could be illegal due to; // memory alignment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,1,['load'],['load']
Performance,// Create two loads for the pair subregisters accounting for endianness and; // then prime the accumulator register being restored.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp,1,['load'],['loads']
Performance,// Create user-defined cache dir.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp:23,cache,cache,23,interpreter/llvm-project/llvm/tools/lli/lli.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp,1,['cache'],['cache']
Performance,"// Create value caches for _intList and _sumList",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooRealIntegral.cxx:16,cache,caches,16,roofit/roofitcore/src/RooRealIntegral.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooRealIntegral.cxx,1,['cache'],['caches']
Performance,"// Created load does not have to be ""Instruction"" (e.g. ""undef"").",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,1,['load'],['load']
Performance,// Created state cached out.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountChecker.cpp:17,cache,cached,17,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountChecker.cpp,1,['cache'],['cached']
Performance,// Creates the more optimized patterns and generally does all the code; // transformations in this pass.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp:20,optimiz,optimized,20,interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/A15SDOptimizer.cpp,1,['optimiz'],['optimized']
Performance,"// Creating a RooDataSet from an RDataFrame should be consistent with the; // creation from a TTree. The construction from a TTree discards entries; // outside the variable definition range, so we have to do that too (see; // also RooTreeDataStore::loadValues).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsDataHelper.cxx:249,load,loadValues,249,roofit/roofitcore/src/RooAbsDataHelper.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsDataHelper.cxx,1,['load'],['loadValues']
Performance,"// Creating a predicated vector loop is the first step for generating a; // tail-predicated hardware loop, for which we need the MVE masked; // load/stores instructions:",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:144,load,load,144,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['load'],['load']
Performance,"// Creating types might create further types - invalidating the current; // element and the size(), so don't cache/reference them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp:109,cache,cache,109,interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,1,['cache'],['cache']
Performance,// Critical path through the DAG in expected latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineScheduler.h:45,latency,latency,45,interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineScheduler.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineScheduler.h,1,['latency'],['latency']
Performance,"// CurMBB needs to add an unconditional branch to SuccMBB (we removed these; // branches temporarily for tail merging). In the case where CurMBB ends; // with a conditional branch to the next block, optimize by reversing the; // test and conditionally branching to SuccMBB instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp:199,optimiz,optimize,199,interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,1,['optimiz'],['optimize']
Performance,// CurPtr - Cache BufferPtr in an automatic variable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Lexer.cpp:12,Cache,Cache,12,interpreter/llvm-project/clang/lib/Lex/Lexer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Lexer.cpp,2,['Cache'],['Cache']
Performance,"// Current magnetic field, cached.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/eve/inc/TEveTrackPropagator.h:27,cache,cached,27,graf3d/eve/inc/TEveTrackPropagator.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/eve/inc/TEveTrackPropagator.h,2,['cache'],['cached']
Performance,"// Current morph cache element in use",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofit/inc/RooIntegralMorph.h:17,cache,cache,17,roofit/roofit/inc/RooIntegralMorph.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofit/inc/RooIntegralMorph.h,1,['cache'],['cache']
Performance,// Current-value load instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonBaseInfo.h:17,load,load,17,interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonBaseInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonBaseInfo.h,1,['load'],['load']
Performance,// CurrentDef is the earliest write clobber of KillingDef. Use it as; // optimized access. Do not optimize if CurrentDef is already the; // defining access of KillingDef.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp:73,optimiz,optimized,73,interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp,2,['optimiz'],"['optimize', 'optimized']"
Performance,"// Currently all users of a global variable have to be non-volatile loads; // or stores of the global type, and the global cannot be stored itself.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueLatticeUtils.cpp:68,load,loads,68,interpreter/llvm-project/llvm/lib/Analysis/ValueLatticeUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueLatticeUtils.cpp,1,['load'],['loads']
Performance,// Currently the check is performed for apple targets only.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers/unix_api_example.c:26,perform,performed,26,interpreter/llvm-project/clang/docs/analyzer/checkers/unix_api_example.c,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/docs/analyzer/checkers/unix_api_example.c,1,['perform'],['performed']
Performance,"// Currently the module file are never unloaded (even if the library is; // unloaded) and, of course, never reloaded.; // Consequently, we must NOT remove the `pendingRdict` from the list; // of pending dictionary, otherwise if a library is unloaded and then; // reload we will be unable to update properly the TClass object; // (because we wont be able to load the rootpcm file by executing the; // above lines)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:357,load,load,357,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,1,['load'],['load']
Performance,"// Currently the transformation only works on scalable vector types, although; // there is no fundamental reason why it cannot be made to work for fixed; // width too.; // We also need to know the minimum page size for the target in order to; // generate runtime memory checks to ensure the vector version won't fault.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:46,scalab,scalable,46,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,1,['scalab'],['scalable']
Performance,"// Currently there is no support for enabling whole program visibility via a; // linker option in the old LTO API, but this call allows it to be specified; // via the internal option. Must be done before WPD invoked via the optimizer; // pipeline run below.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTOCodeGenerator.cpp:224,optimiz,optimizer,224,interpreter/llvm-project/llvm/lib/LTO/LTOCodeGenerator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTOCodeGenerator.cpp,1,['optimiz'],['optimizer']
Performance,"// Currently this only tries to ensure we don't undo the GEP splits done by; // CodeGenPrepare when shouldConsiderGEPOffsetSplit is true. To ensure this,; // we check if the following transformation would be problematic:; // (load/store (add, (add, x, offset1), offset2)) ->; // (load/store (add, x, offset1+offset2)).; // (load/store (add, (add, x, y), offset2)) ->; // (load/store (add, (add, x, offset2), y)).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:226,load,load,226,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],['load']
Performance,"// Currently to save compiling time, MBB's register pressure will not change; // in one ProcessBlock iteration because of CachedRegisterPressure. but MBB's; // register pressure is changed after sinking any instructions into it.; // FIXME: need a accurate and cheap register pressure estiminate model here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp:122,Cache,CachedRegisterPressure,122,interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,1,['Cache'],['CachedRegisterPressure']
Performance,"// Currently we choose the most profitable base as the one which has the max; // number of load/store with same remainder.; // FIXME: adjust the base selection strategy according to load/store offset; // distribution.; // For example, if we have one candidate chain for DS form preparation, which; // contains following load/stores with different remainders:; // 1: 10 load/store whose remainder is 1;; // 2: 9 load/store whose remainder is 2;; // 3: 1 for remainder 3 and 0 for remainder 0;; // Now we will choose the first load/store whose remainder is 1 as base and; // adjust all other load/stores according to new base, so we will get 10 DS; // form and 10 X form.; // But we should be more clever, for this case we could use two bases, one for; // remainder 1 and the other for remainder 2, thus we could get 19 DS form and; // 1 X form.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:91,load,load,91,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,7,['load'],['load']
Performance,"// Currently we disable the optimization for classes with virtual; // bases because (1) the addresses of parameter variables need to be; // consistent across all initializers but (2) the delegate function; // call necessarily creates a second copy of the parameter variable.; //; // The limiting example (purely theoretical AFAIK):; // struct A { A(int &c) { c++; } };; // struct B : virtual A {; // B(int count) : A(count) { printf(""%d\n"", count); }; // };; // ...although even this example could in principle be emitted as a; // delegation since the address of the parameter doesn't escape.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp:28,optimiz,optimization,28,interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,1,['optimiz'],['optimization']
Performance,"// Currently, if Cling sees the ""key function"" of a virtual class, it; // emits typeinfo and vtable variables in every transaction llvm::Module; // that reference them. Turn them into weak linkage to avoid duplicate; // symbol errors from the JIT linker.; // FIXME: This is a hack, we should teach the frontend to emit these; // only once, or mark all duplicates as available_externally (if that; // improves performance due to optimizations).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/BackendPasses.cpp:409,perform,performance,409,interpreter/cling/lib/Interpreter/BackendPasses.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/BackendPasses.cpp,2,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"// Currently, lowering is supported for the following vectors:; // Stride 4:; // 1. Store and load of 4-element vectors of 64 bits on AVX.; // 2. Store of 16/32-element vectors of 8 bits on AVX.; // Stride 3:; // 1. Load of 16/32-element vectors of 8 bits on AVX.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp:94,load,load,94,interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,2,"['Load', 'load']","['Load', 'load']"
Performance,"// Currently, only load/store and addasl are handled.; // Some other instructions to consider -; // A2_add -> A2_addi; // M4_mpyrr_addr -> M4_mpyrr_addi",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonOptAddrMode.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonOptAddrMode.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonOptAddrMode.cpp,1,['load'],['load']
Performance,"// Currently, the ExpandReductions pass can't expand scalable-vector; // reductions, but we still request expansion as RVV doesn't support certain; // reductions and the SelectionDAG can't legalize them either.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:53,scalab,scalable-vector,53,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['scalab'],['scalable-vector']
Performance,"// Currently, these represent both throughput and codesize costs; // for the respective intrinsics. The costs in this table are simply; // instruction counts with the following adjustments made:; // * One vsetvli is considered free.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:35,throughput,throughput,35,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['throughput'],['throughput']
Performance,"// Currently, two use-cases possible:; // Case #1. Non-var-args function, and we meet first byval parameter.; // Setup first unallocated register as first byval register;; // eat all remained registers; // (these two actions are performed by HandleByVal method).; // Then, here, we initialize stack frame with; // ""store-reg"" instructions.; // Case #2. Var-args function, that doesn't contain byval parameters.; // The same: eat all remained unallocated registers,; // initialize stack frame.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:229,perform,performed,229,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['perform'],['performed']
Performance,"// Currently, we only enable register pressure reducing in machine combiner; // for: 1: PPC64; 2: Code Model is Medium; 3: Power9 which also has vector; // support.; //; // So we need following instructions to access a TOC entry:; //; // %6:g8rc_and_g8rc_nox0 = ADDIStocHA8 $x2, %const.0; // %7:vssrc = DFLOADf32 target-flags(ppc-toc-lo) %const.0,; // killed %6:g8rc_and_g8rc_nox0, implicit $x2 :: (load 4 from constant-pool); //; // FIXME: add more supported targets, like Small and Large code model, PPC32,; // AIX.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:399,load,load,399,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,"// Currently, we only support relative addressing with statepoints.; // Otherwise, we'll need a scratch register to hold the target; // address. You'll fail asserts during load & relocation if this; // symbol is to far away. (TODO: support non-relative addressing)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp:172,load,load,172,interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,1,['load'],['load']
Performance,"// Currently, we only support relative addressing with statepoints.; // Otherwise, we'll need a scratch register to hold the target; // immediate. You'll fail asserts during load & relocation if this; // address is to far away. (TODO: support non-relative addressing)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp:174,load,load,174,interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,1,['load'],['load']
Performance,"// Currently, we only track data-dependent loads within a basic block.; // FIXME: We should see if this is necessary or if we could be more; // aggressive here without opening up attack avenues.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:43,load,loads,43,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['loads']
Performance,"// Custom DFS implementation which can skip nodes based on a provided; // predicate. It also collects ReverseChildren so that we don't have to spend; // time getting predecessors in SemiNCA.; //; // If IsReverse is set to true, the DFS walk will be performed backwards; // relative to IsPostDom -- using reverse edges for dominators and forward; // edges for postdominators.; //; // If SuccOrder is specified then in this order the DFS traverses the children; // otherwise the order is implied by the results of getChildren().",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/GenericDomTreeConstruction.h:249,perform,performed,249,interpreter/llvm-project/llvm/include/llvm/Support/GenericDomTreeConstruction.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/GenericDomTreeConstruction.h,1,['perform'],['performed']
Performance,// Custom Lower Atomic LOAD/STORE,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp:23,LOAD,LOAD,23,interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,1,['LOAD'],['LOAD']
Performance,// Custom expand misaligned loads / stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/XCore/XCoreISelLowering.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Target/XCore/XCoreISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/XCore/XCoreISelLowering.cpp,1,['load'],['loads']
Performance,"// Custom handling only for i64: turn i64 load into a v2i32 load,; // and a bitcast.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,2,['load'],['load']
Performance,"// Custom lower MGATHER/VP_GATHER to a legalized form for RVV. It will then be; // matched to a RVV indexed load. The RVV indexed load instructions only; // support the ""unsigned unscaled"" addressing mode; indices are implicitly; // zero-extended or truncated to XLEN and are treated as byte offsets. Any; // signed or scaled indexing is extended to the XLEN value type and scaled; // accordingly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:108,load,load,108,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,['load'],['load']
Performance,// Custom lower fixed vector undefs to scalable vector undefs to avoid; // expansion to a build_vector of 0s.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:39,scalab,scalable,39,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,['scalab'],['scalable']
Performance,"// Custom lower unaligned loads.; // Also, for both loads and stores, verify the alignment of the address; // in case it is a compile-time constant. This is a usability feature to; // provide a meaningful error message to users.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,2,['load'],['loads']
Performance,// Custom lowering for extending v4i8 vector loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loads']
Performance,"// Custom-lower BUILD_VECTOR for vector pairs. The standard (target-; // independent) handling of it would convert it to a load, which is; // not always the optimal choice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp:123,load,load,123,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp,1,['load'],['load']
Performance,"// Custom-lower BUILD_VECTOR. The standard (target-independent); // handling of it would convert it to a load, which is not always; // the optimal choice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp:105,load,load,105,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLoweringHVX.cpp,2,['load'],['load']
Performance,// Custom-lower load/stores of boolean vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['load'],['load']
Performance,"// Customize printing of the addis instruction on AIX. When an operand is a; // symbol reference, the instruction syntax is changed to look like a load; // operation, i.e:; // Transform: addis $rD, $rA, $src --> addis $rD, $src($rA).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCInstPrinter.cpp:147,load,load,147,interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCInstPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCInstPrinter.cpp,1,['load'],['load']
Performance,// D registers loaded or stored,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:15,load,loaded,15,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,1,['load'],['loaded']
Performance,"// DAG combiner will fold:; // (shl (add x, c1), c2) -> (add (shl x, c2), c1 << c2); // (shl (or x, c1), c2) -> (or (shl x, c2), c1 << c2; // Other code patterns that can be also be modified have the following form:; // b + ((a << 1) | 510); // b + ((a << 1) & 510); // b + ((a << 1) ^ 510); // b + ((a << 1) + 510); // Many instructions can perform the shift for free, but it requires both; // the operands to be registers. If c1 << c2 is too large, a mov immediate; // instruction will needed. So, unfold back to the original pattern if:; // - if c1 and c2 are small enough that they don't require mov imms.; // - the user(s) of the node can perform an shl; // No shifted operands for 16-bit instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:342,perform,perform,342,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['perform'],['perform']
Performance,"// DAG optimizations should be able to handle these cases better, especially; // for function arguments.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TypePromotion.cpp:7,optimiz,optimizations,7,interpreter/llvm-project/llvm/lib/CodeGen/TypePromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TypePromotion.cpp,1,['optimiz'],['optimizations']
Performance,"// DAGCombiner will revert the combination when Z is constant cause; // dead loop. So don't enable the combination when Z is constant.; // If Z is one use shift C, we also can't do the optimization.; // It will falling to self infinite loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:185,optimiz,optimization,185,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// DEFLATE is a complex format; to read this code, you should probably check the RFC first:; // https://tools.ietf.org/html/rfc1951; // You may also wish to take a look at the guide I made about this program:; // https://gist.github.com/101arrowz/253f31eb5abc3d9275ab943003ffecad; // Much of the following code is similar to that of UZIP.js:; // https://github.com/photopea/UZIP.js; // Many optimizations have been made, so the bundle size is ultimately smaller but performance is similar.; // Sometimes 0 will appear where -1 would be more appropriate. This is because using a uint; // is better for memory in most engines (I *think*).; // Mediocre shim",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:391,optimiz,optimizations,391,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,4,"['optimiz', 'perform']","['optimizations', 'performance']"
Performance,"// DO NOT DO THIS - seems to mess with the NLL function in a way that breaks the cache - reactivating wont fix; // if(constOptimize) { _minimizer.optimizeConst(0); } // doing this because saw happens in RooAbsPdf::minimizeNLL; // method; // signal(SIGINT,gOldHandlerr);",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/xroofit/src/xRooFit.cxx:81,cache,cache,81,roofit/xroofit/src/xRooFit.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/xroofit/src/xRooFit.cxx,2,"['cache', 'optimiz']","['cache', 'optimizeConst']"
Performance,// DPP/Iterative option enables the atomic optimizer with given strategy; // whereas None disables the atomic optimizer.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h:43,optimiz,optimizer,43,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPU.h,2,['optimiz'],['optimizer']
Performance,// Data dpendence ok if we have load.cur.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,1,['load'],['load']
Performance,// Data structures for table-driven optimizations.; // FuncTbl works for both f32 and f64 functions with 1 input argument,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp:36,optimiz,optimizations,36,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULibCalls.cpp,1,['optimiz'],['optimizations']
Performance,// Dead dependent mem-op disguise as a load evaluating the same value; // as the load in question.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],['load']
Performance,// Debug info intrinsics do not get in the way of tail call optimization.; // Pseudo probe intrinsics do not block tail call optimization either.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/Analysis.cpp:60,optimiz,optimization,60,interpreter/llvm-project/llvm/lib/CodeGen/Analysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/Analysis.cpp,2,['optimiz'],['optimization']
Performance,// Debug info sections are linked as if their load address was zero,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:46,load,load,46,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['load']
Performance,"// Debug value instruction is permitted to use undefined vregs.; // This is a performance measure to skip the overhead of immediately; // pruning unused debug operands. The final undef substitution occurs; // when debug values are allocated in LDVImpl::handleDebugValue, so; // these verifications always apply after this pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp:78,perform,performance,78,interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp,1,['perform'],['performance']
Performance,// Decide whether PRE is profitable for this load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:45,load,load,45,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['load'],['load']
Performance,// Declaration of class/meta-class metadata; /* struct _objc_class {; struct _objc_class *isa; // or const char *root_class_name when metadata; const char *super_class_name;; char *name;; long version;; long info;; long instance_size;; struct _objc_ivar_list *ivars;; struct _objc_method_list *methods;; struct objc_cache *cache;; struct objc_protocol_list *protocols;; const char *ivar_layout;; struct _objc_class_ext *ext;; };; */,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/Rewrite/RewriteObjC.cpp:323,cache,cache,323,interpreter/llvm-project/clang/lib/Frontend/Rewrite/RewriteObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/Rewrite/RewriteObjC.cpp,1,['cache'],['cache']
Performance,"// Decode the following 14 instructions. Bit 9 indicates load(0) or store(1),; // bits 8~4 indicate the value register, bits 3-2 indicate the base address; // register (11-X, 10-Y, 00-Z), bits 1~0 indicate the mode (00-basic,; // 01-postinc, 10-predec).; // ST X, Rr : 1001 001r rrrr 1100; // ST X+, Rr : 1001 001r rrrr 1101; // ST -X, Rr : 1001 001r rrrr 1110; // ST Y+, Rr : 1001 001r rrrr 1001; // ST -Y, Rr : 1001 001r rrrr 1010; // ST Z+, Rr : 1001 001r rrrr 0001; // ST -Z, Rr : 1001 001r rrrr 0010; // LD Rd, X : 1001 000d dddd 1100; // LD Rd, X+ : 1001 000d dddd 1101; // LD Rd, -X : 1001 000d dddd 1110; // LD Rd, Y+ : 1001 000d dddd 1001; // LD Rd, -Y : 1001 000d dddd 1010; // LD Rd, Z+ : 1001 000d dddd 0001; // LD Rd, -Z : 1001 000d dddd 0010",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/Disassembler/AVRDisassembler.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/Target/AVR/Disassembler/AVRDisassembler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/Disassembler/AVRDisassembler.cpp,1,['load'],['load']
Performance,// Decompose string pseudo-instruction MI into a loop that continually performs; // Opcode until CC != 3.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:71,perform,performs,71,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['perform'],['performs']
Performance,// Decompose the load instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp:17,load,load,17,interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,1,['load'],['load']
Performance,// Decreases by one the number of used scheduler queue slots of every; // buffered resource in the Buffers set.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.h:49,queue,queue,49,interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.h,1,['queue'],['queue']
Performance,"// Deduction of index used for the line records.; //; // For the following test case: test.cpp; // void foo(void ParamPtr) { }; // Both GCC and Clang generate DWARF-5 .debug_line layout.; // * GCC (GNU C++17 11.3.0) - All DW_AT_decl_file use index 1.; //; // .debug_info:; // format = DWARF32, version = 0x0005; // DW_TAG_compile_unit; // DW_AT_name	(""test.cpp""); // DW_TAG_subprogram (""foo""); // DW_AT_decl_file (1); // DW_TAG_formal_parameter (""ParamPtr""); // DW_AT_decl_file (1); // .debug_line:; // Line table prologue: format (DWARF32), version (5); // include_directories[0] = ""...""; // file_names[0]: name (""test.cpp""), dir_index (0); // file_names[1]: name (""test.cpp""), dir_index (0); // * Clang (14.0.6) - All DW_AT_decl_file use index 0.; //; // .debug_info:; // format = DWARF32, version = 0x0005; // DW_AT_producer	(""clang version 14.0.6""); // DW_AT_name	(""test.cpp""); //; // DW_TAG_subprogram (""foo""); // DW_AT_decl_file (0); // DW_TAG_formal_parameter (""ParamPtr""); // DW_AT_decl_file (0); // .debug_line:; // Line table prologue: format (DWARF32), version (5); // include_directories[0] = ""...""; // file_names[0]: name (""test.cpp""), dir_index (0); // From DWARFDebugLine::getFileNameByIndex documentation:; // In Dwarf 4, the files are 1-indexed.; // In Dwarf 5, the files are 0-indexed.; // Additional discussions here:; // https://www.mail-archive.com/dwarf-discuss@lists.dwarfstd.org/msg00883.html; // The ELF Reader is expecting the files are 1-indexed, so using; // the .debug_line header information decide if the indexed require; // an internal adjustment.; // For the case of GCC (DWARF5), if the entries[0] and [1] are the; // same, do not perform any adjustment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVELFReader.cpp:1665,perform,perform,1665,interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVELFReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVELFReader.cpp,1,['perform'],['perform']
Performance,"// Def(U) can't be returned here because it is non-local. If local; // dependency won't be found then return nonLocal counting that the; // user will call getNonLocalPointerDependency, which will return cached; // result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:203,cache,cached,203,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['cache'],['cached']
Performance,// Default all indexed load / store to expand.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,1,['load'],['load']
Performance,// Default handling for scalable and single-element vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h:24,scalab,scalable,24,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h,1,['scalab'],['scalable']
Performance,// Default implementation: no optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:30,optimiz,optimization,30,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['optimiz'],['optimization']
Performance,"// Default machine value number is <None> -- if no instruction defines; // the corresponding value, it must have been optimized out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp:118,optimiz,optimized,118,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp,2,['optimiz'],['optimized']
Performance,"// Default to cheap (throughput/size of 1 instruction) but adjust throughput; // for ""multiple beats"" potentially needed by MVE instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:21,throughput,throughput,21,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,4,['throughput'],['throughput']
Performance,// Default to the same logic as loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h:32,load,loads,32,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,1,['load'],['loads']
Performance,"// Default validity of the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TDataSetManagerFile.cxx:27,cache,cache,27,proof/proof/src/TDataSetManagerFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TDataSetManagerFile.cxx,1,['cache'],['cache']
Performance,"// DefaultArgumentPromotion (C99 6.5.2.2p6). Used for function calls that; // do not have a prototype. Integer promotions are performed on each; // argument, and arguments that have type float are promoted to double.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Sema.h:126,perform,performed,126,interpreter/llvm-project/clang/include/clang/Sema/Sema.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Sema.h,1,['perform'],['performed']
Performance,// DefaultFunctionArrayLvalueConversion - converts functions and; // arrays to their respective pointers and performs the; // lvalue-to-rvalue conversion.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Sema.h:109,perform,performs,109,interpreter/llvm-project/clang/include/clang/Sema/Sema.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Sema.h,1,['perform'],['performs']
Performance,// DefaultLvalueConversion - performs lvalue-to-rvalue conversion on; // the operand. This function is a no-op if the operand has a function type; // or an array type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Sema.h:29,perform,performs,29,interpreter/llvm-project/clang/include/clang/Sema/Sema.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Sema.h,1,['perform'],['performs']
Performance,"// Defer code generation to first use when possible, e.g. if this is an inline; // function. If the global must always be emitted, do it eagerly if possible; // to benefit from cache locality.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp:177,cache,cache,177,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,1,['cache'],['cache']
Performance,"// Defer loading the anonymous namespace until we've finished merging; // this namespace; loading it might load a later declaration of the; // same namespace, and we have an invariant that older declarations; // get merged before newer ones try to merge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:9,load,loading,9,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,3,['load'],"['load', 'loading']"
Performance,"// Defer the creation of the bitcast from X to combineExtract,; // which might be able to optimize the extraction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:90,optimiz,optimize,90,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['optimiz'],['optimize']
Performance,// Define a union of all load command structs,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/MachO.h:25,load,load,25,interpreter/llvm-project/llvm/include/llvm/BinaryFormat/MachO.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/MachO.h,1,['load'],['load']
Performance,"// Define alternate numeric integrator configuration for bin integration; // We expect bin contents to very only very slowly so a non-adaptive; // Gauss-Kronrod integrator is expected to perform well (if RooFitMore is available)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooXYChi2Var.cxx:187,perform,perform,187,roofit/roofitcore/src/RooXYChi2Var.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooXYChi2Var.cxx,1,['perform'],['perform']
Performance,// Define list of load and store spill opcodes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h,1,['load'],['load']
Performance,"// Define the input data for subsequent calls to DoUnfold(Double_t); // input: input distribution with errors; // scaleBias: scale factor applied to the bias; // oneOverZeroError: for bins with zero error, this number defines 1/error.; // Return value: number of bins with bad error; // +10000*number of unconstrained output bins; // Note: return values>=10000 are fatal errors,; // for the given input, the unfolding can not be done!; // Calls the SetInput method of the base class, then renames the input; // vectors fY and fVyy, then performs the background subtraction; // Data members modified:; // fYData,fY,fVyyData,fVyy,fVyyinvData,fVyyinv; // and those modified by TUnfold::SetInput(); // and those modified by DoBackgroundSubtraction()",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/unfold/src/TUnfoldSys.cxx:537,perform,performs,537,hist/unfold/src/TUnfoldSys.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/unfold/src/TUnfoldSys.cxx,1,['perform'],['performs']
Performance,"// Define the root of the tree. This identifies the tree, so that; // if our LLVM IR is linked with LLVM IR from a different front-end; // (or a different version of this front-end), their TBAA trees will; // remain distinct, and the optimizer will treat them conservatively.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenTBAA.cpp:234,optimiz,optimizer,234,interpreter/llvm-project/clang/lib/CodeGen/CodeGenTBAA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenTBAA.cpp,1,['optimiz'],['optimizer']
Performance,// Delay VSX load for LE combine until after LegalizeOps to prioritize other; // load combines.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,"// Delay optimization, so we don't have to deal with illegal types, or block; // optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:9,optimiz,optimization,9,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['optimiz'],"['optimization', 'optimizations']"
Performance,// Delay this optimization as late as possible.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:14,optimiz,optimization,14,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['optimiz'],['optimization']
Performance,// Delay this optimization to as late as possible.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:14,optimiz,optimization,14,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['optimiz'],['optimization']
Performance,// Delay-load DSO by default.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/ELF.h:9,load,load,9,interpreter/llvm-project/llvm/include/llvm/BinaryFormat/ELF.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/ELF.h,1,['load'],['load']
Performance,"// Delete & recreate cache tree",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx:21,cache,cache,21,roofit/roofitcore/src/RooTreeDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx,1,['cache'],['cache']
Performance,"// Delete FileEntry objects corresponding to content caches. Since the actual; // content cache objects are bump pointer allocated, we just have to run the; // dtors, but we call the deallocate method for completeness.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:53,cache,caches,53,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,2,['cache'],"['cache', 'caches']"
Performance,"// Delete all instructions. On the first pass, new dummy loads may have been; // added so we need to collect them too.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:57,load,loads,57,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,1,['load'],['loads']
Performance,"// Delete basic blocks, which optimization passes may have killed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:30,optimiz,optimization,30,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['optimiz'],['optimization']
Performance,// Delete cached satisfactions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp:10,cache,cached,10,interpreter/llvm-project/clang/lib/Sema/Sema.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp,1,['cache'],['cached']
Performance,"// Delete no-op casts. These function calls have special semantics, but; // the semantics are entirely implemented via lowering in the front-end,; // so by the time they reach the optimizer, they are just no-op calls; // which return their argument.; //; // There are gray areas here, as the ability to cast reference-counted; // pointers to raw void* and back allows code to break ARC assumptions,; // however these are currently considered to be unimportant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:180,optimiz,optimizer,180,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['optimiz'],['optimizer']
Performance,// Delete or cache the now-dead macro expander.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPLexerChange.cpp:13,cache,cache,13,interpreter/llvm-project/clang/lib/Lex/PPLexerChange.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPLexerChange.cpp,2,['cache'],['cache']
Performance,"// Delete or move the file cache if it points to this Tree",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx:27,cache,cache,27,tree/tree/src/TTree.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx,2,['cache'],['cache']
Performance,"// Delete parameters cache if we have one",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooRealIntegral.cxx:21,cache,cache,21,roofit/roofitcore/src/RooRealIntegral.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooRealIntegral.cxx,1,['cache'],['cache']
Performance,"// Delete previous cache, if any",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooVectorDataStore.cxx:19,cache,cache,19,roofit/roofitcore/src/RooVectorDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooVectorDataStore.cxx,1,['cache'],['cache']
Performance,"// Delete source clone if we don't cache it",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooCachedReal.cxx:35,cache,cache,35,roofit/roofitcore/src/RooCachedReal.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooCachedReal.cxx,1,['cache'],['cache']
Performance,"// Delete the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsOptTestStatistic.cxx:14,cache,cache,14,roofit/roofitcore/src/RooAbsOptTestStatistic.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsOptTestStatistic.cxx,2,['cache'],['cache']
Performance,"// Delete the tree cache ...",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TEventIter.cxx:19,cache,cache,19,proof/proofplayer/src/TEventIter.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TEventIter.cxx,1,['cache'],['cache']
Performance,"// DemandedElts and UndefElts are ignored for scalable vectors, since; // the only supported cases are SPLAT_VECTOR nodes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:46,scalab,scalable,46,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// Deopt bundle operands are intended to capture state with minimal; // perturbance of the code otherwise. If we can find a constant value for; // any such operand and remove a use of the original value, that's; // desireable since it may allow further optimization of that value (e.g. via; // single use rules in instcombine). Since deopt uses tend to,; // idiomatically, appear along rare conditional paths, it's reasonable likely; // we may have a conditional fact with which LVI can fold.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CorrelatedValuePropagation.cpp:253,optimiz,optimization,253,interpreter/llvm-project/llvm/lib/Transforms/Scalar/CorrelatedValuePropagation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/CorrelatedValuePropagation.cpp,1,['optimiz'],['optimization']
Performance,// Depend on LiveIntervals and perform some optimizations on it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyTargetMachine.cpp:31,perform,perform,31,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyTargetMachine.cpp,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"// Dependent of the caller of this function, a gather instruction will; // either have opcode Instruction::Load or be a call to the masked_gather; // intrinsic",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:107,Load,Load,107,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['Load'],['Load']
Performance,"// Dependent on peeling being performed on the first loop, and; // assuming all other conditions for fusion return true.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFuse.cpp:30,perform,performed,30,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFuse.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFuse.cpp,1,['perform'],['performed']
Performance,"// Depending on which version of the ELF ABI is in use, we need to; // generate one of two variants of the stub. They both start with; // the same sequence to load the target address into r12.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:159,load,load,159,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['load']
Performance,"// Depends only on libCore.so but libCore.so is loaded and thus missing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/test/TClingTests.cxx:48,load,loaded,48,core/metacling/test/TClingTests.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/test/TClingTests.cxx,1,['load'],['loaded']
Performance,"// Deprecated, but ignoring here to preserve loading older textual llvm; // ASM file",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/DataLayout.cpp:45,load,loading,45,interpreter/llvm-project/llvm/lib/IR/DataLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/DataLayout.cpp,1,['load'],['loading']
Performance,"// Description of the extra info, used to interpret the actual optional; // data appended.; //; // Note that this is not terribly space optimized. This leaves a great deal; // of flexibility to fit more in here later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineInstr.h:136,optimiz,optimized,136,interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineInstr.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/MachineInstr.h,1,['optimiz'],['optimized']
Performance,// Destroy the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCleanup.cpp:15,load,load,15,interpreter/llvm-project/clang/lib/CodeGen/CGCleanup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCleanup.cpp,1,['load'],['load']
Performance,// Detailed record is important since it is used for the module cache hash.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:64,cache,cache,64,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,2,['cache'],['cache']
Performance,// Details about the old load,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['load'],['load']
Performance,"// Details on which baskets was used, cached, 'miss-cached' or read uncached.Browse",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/inc/TTreePerfStats.h:38,cache,cached,38,tree/treeplayer/inc/TTreePerfStats.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/inc/TTreePerfStats.h,2,['cache'],['cached']
Performance,"// Detect cases where we're performing call slot forwarding, but; // happen to be using a load-store pair to implement it, rather than; // a memcpy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:28,perform,performing,28,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,2,"['load', 'perform']","['load-store', 'performing']"
Performance,"// Detect whether RD is a bitfield extract (sign- or zero-extended) of; // some register from the AVs set. Create a new corresponding instruction; // at the location of MI. The intent is to recognize situations where; // a sequence of instructions performs an operation that is equivalent to; // an extract operation, such as a shift left followed by a shift right.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp:248,perform,performs,248,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,1,['perform'],['performs']
Performance,// Determine any common known bits from the loaded constant pool value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:44,load,loaded,44,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['load'],['loaded']
Performance,"// Determine clusters which get triggered for background loading",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RClusterPool.cxx:57,load,loading,57,tree/ntuple/v7/src/RClusterPool.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RClusterPool.cxx,1,['load'],['loading']
Performance,// Determine if a load can be unfolded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp,1,['load'],['load']
Performance,"// Determine if the callsite is cold relative to caller's entry. We could; // potentially cache the computation of scaled entry frequency, but the added; // complexity is not worth it unless this scaling shows up high in the; // profiles.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp:90,cache,cache,90,interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,1,['cache'],['cache']
Performance,"// Determine if the callsite is hot relative to caller's entry. We could; // potentially cache the computation of scaled entry frequency, but the added; // complexity is not worth it unless this scaling shows up high in the; // profiles.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp:89,cache,cache,89,interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,1,['cache'],['cache']
Performance,"// Determine if there other observables than the convolution observable in the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFFTConvPdf.cxx:79,cache,cache,79,roofit/roofitcore/src/RooFFTConvPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFFTConvPdf.cxx,1,['cache'],['cache']
Performance,// Determine if this an indexed load with an opaque target constant index.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,// Determine if we are introspecting the result of performSelectorXXX.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp:51,perform,performSelectorXXX,51,interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,1,['perform'],['performSelectorXXX']
Performance,// Determine optimization level.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llc/llc.cpp:13,optimiz,optimization,13,interpreter/llvm-project/llvm/tools/llc/llc.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llc/llc.cpp,2,['optimiz'],['optimization']
Performance,// Determine the alignment of the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,// Determine the arguments required to actually perform the constructor; // call.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:48,perform,perform,48,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,2,['perform'],['perform']
Performance,"// Determine the arguments required to actually perform the; // constructor call (we might have derived-to-base conversions, or; // the copy constructor may have default arguments).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:48,perform,perform,48,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['perform'],['perform']
Performance,// Determine the kind of load/store that should be used.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp,1,['load'],['load']
Performance,"// Determine the metadata to describe the position of any padding in this; // memcpy, as well as the TBAA tags for the members of the struct, in case; // the optimizer wishes to expand it in to scalar memory operations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:158,optimiz,optimizer,158,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['optimiz'],['optimizer']
Performance,"// Determine the previous frame's address. If FrameSize can't be; // represented as 16 bits or we need special alignment, then we load the; // previous frame's address from 0(SP). Why not do an addis of the hi?; // Because R0 is our only safe tmp register and addi/addis treat R0 as zero.; // Constructing the constant and adding would take 3 instructions.; // Fortunately, a frame greater than 32K is rare.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp:130,load,load,130,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCRegisterInfo.cpp,1,['load'],['load']
Performance,// Determine where to perform name lookup,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp:22,perform,perform,22,interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp,2,['perform'],['perform']
Performance,"// Determine where to perform name lookup.; // FIXME: This area of the standard is very messy, and the current; // wording is rather unclear about which scopes we search for the; // destructor name; see core issues 399 and 555. Issue 399 in; // particular shows where the current description of destructor name; // lookup is completely out of line with existing practice, e.g.,; // this appears to be ill-formed:; //; // namespace N {; // template <typename T> struct S {; // ~S();; // };; // }; //; // void f(N::S<int>* s) {; // s->N::S<int>::~S();; // }; //; // See also PR6358 and PR6359.; //; // For now, we accept all the cases in which the name given could plausibly; // be interpreted as a correct destructor name, issuing off-by-default; // extension diagnostics on the cases that don't strictly conform to the; // C++20 rules. This basically means we always consider looking in the; // nested-name-specifier prefix, the complete nested-name-specifier, and; // the scope, and accept if we find the expected type in any of the three; // places.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:22,perform,perform,22,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['perform'],['perform']
Performance,// Determine where we will perform name lookup.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:27,perform,perform,27,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['perform'],['perform']
Performance,"// Determine whether the original StartBB post-dominates all of the blocks we; // visited. If not, insert a sentinel indicating that most optimizations are; // not safe.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/DependencyAnalysis.cpp:138,optimiz,optimizations,138,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/DependencyAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/DependencyAnalysis.cpp,1,['optimiz'],['optimizations']
Performance,"// Determine whether the type Ty is simple enough to be handled by; // fast-isel as a load target, and return its equivalent machine type in VT.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:86,load,load,86,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,1,['load'],['load']
Performance,// Determine whether two GEP operations perform the same underlying arithmetic.; // Read method declaration comments for more details.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/FunctionComparator.cpp:40,perform,perform,40,interpreter/llvm-project/llvm/lib/Transforms/Utils/FunctionComparator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/FunctionComparator.cpp,1,['perform'],['perform']
Performance,// Determine whether we'll need to perform derived-to-base adjustments or; // other conversions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:35,perform,perform,35,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['perform'],['perform']
Performance,// Determine which file we're performing consistency checking for.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaType.cpp:30,perform,performing,30,interpreter/llvm-project/clang/lib/Sema/SemaType.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaType.cpp,1,['perform'],['performing']
Performance,"// Determining when to avoid vperm is tricky. Many things affect the cost; // of vperm, particularly how many times the perm mask needs to be; // computed. For example, if the perm mask can be hoisted out of a loop or; // is already used (perhaps because there are multiple permutes with the; // same shuffle mask?) the vperm has a cost of 1. OTOH, hoisting the; // permute mask out of the loop requires an extra register.; //; // As a compromise, we only emit discrete instructions if the shuffle can; // be generated in 3 or fewer operations. When we have loop information; // available, if this block is within a loop, we should avoid using vperm; // for 3-operation perms and use a constant pool load instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:700,load,load,700,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// Developer note on the std::move() warning above:; //; // The point here was that you don't want to clone RooAbsL's too much, because they contain clones of the pdf and; // dataset that may have been mangled for optimization. You probably don't want to be doing that all the time, although; // it is a premature optimization, since we haven't timed its impact. That is the motivation behind using unique_ptrs; // for the components. The way the classes are built, the RooSumL doesn't care about what components it gets, so by; // definition it cannot create them internally, so they have to be passed in somehow. Forcing the user to call the; // function with a std::move is a way to make them fully realize that their local components will be destroyed and the; // contents moved into the RooSumL.; //; // We could change the type to an rvalue reference to make it clearer from the compiler error that std::move is; // necessary, instead of the obscure error that you get now. Compare the compiler error messages from these two types:; //; //#include <vector>; //#include <memory>; //#include <cstdio>; //; // struct Clear {; // Clear(std::vector<std::unique_ptr<int>>&& vec) : vec_(std::move(vec)) {; // printf(""number is %d"", *vec_[0]);; // }; //; // std::vector<std::unique_ptr<int>> vec_;; //};; //; // struct Obscure {; // Obscure(std::vector<std::unique_ptr<int>> vec) : vec_(std::move(vec)) {; // printf(""number is %d"", *vec_[0]);; // }; //; // std::vector<std::unique_ptr<int>> vec_;; //};; //; // int main() {; // std::vector<std::unique_ptr<int>> vec;; // vec.emplace_back(new int(4));; // Clear thing(vec);; // Obscure thingy(vec);; //}; /// \note Compared to the RooAbsTestStatistic implementation that this was taken from, we leave out Hybrid and; /// SimComponents interleaving support here. This should be implemented by a calculator (i.e. LikelihoodWrapper or; /// LikelihoodGradientWrapper derived class), if desired.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/TestStatistics/RooSumL.cxx:214,optimiz,optimization,214,roofit/roofitcore/src/TestStatistics/RooSumL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/TestStatistics/RooSumL.cxx,2,['optimiz'],['optimization']
Performance,// Diagnose availability attributes. Availability cannot be used on functions; // that are run during load/unload.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:102,load,load,102,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['load'],['load']
Performance,"// Diagnose self-import before attempting a load.; // [module.import]/9; // A module implementation unit of a module M that is not a module partition; // shall not contain a module-import-declaration nominating M.; // (for an implementation, the module interface is imported implicitly,; // but that's handled in the module decl code).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaModule.cpp:44,load,load,44,interpreter/llvm-project/clang/lib/Sema/SemaModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaModule.cpp,1,['load'],['load']
Performance,// Diagnose use of tune if target doesn't support it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclAttr.cpp:19,tune,tune,19,interpreter/llvm-project/clang/lib/Sema/SemaDeclAttr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclAttr.cpp,1,['tune'],['tune']
Performance,// Diagnostic optimization remarks file,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/LTO/LTO.h:14,optimiz,optimization,14,interpreter/llvm-project/llvm/include/llvm/LTO/LTO.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/LTO/LTO.h,1,['optimiz'],['optimization']
Performance,"// Dictionary initialization code for loading the module",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/TModuleGenerator.cxx:38,load,loading,38,core/dictgen/src/TModuleGenerator.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/TModuleGenerator.cxx,1,['load'],['loading']
Performance,// Did we cache out?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountChecker.cpp:10,cache,cache,10,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountChecker.cpp,2,['cache'],['cache']
Performance,"// Differently from AddDataSourceColumnReaders, this can be called from multiple threads concurrently; /// \brief Register a new RTreeColumnReader with this RLoopManager.; /// \return A shared pointer to the inserted column reader.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/src/RLoopManager.cxx:89,concurren,concurrently,89,tree/dataframe/src/RLoopManager.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/src/RLoopManager.cxx,1,['concurren'],['concurrently']
Performance,"// Directory containing cache of user files",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProofLite.h:24,cache,cache,24,proof/proof/inc/TProofLite.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProofLite.h,1,['cache'],['cache']
Performance,"// Disable ProfileMergeInlinee if profile is not loaded in top down order,; // because the profile for a function may be used for the profile; // annotation of its outline copy before the profile merging of its; // non-inlined inline instances, and that is not the way how; // ProfileMergeInlinee is supposed to work.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp:49,load,loaded,49,interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,1,['load'],['loaded']
Performance,// Disable a codegen optimization for floating-point casts.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp:21,optimiz,optimization,21,interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,1,['optimiz'],['optimization']
Performance,// Disable all llvm IR level optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp:29,optimiz,optimizations,29,interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,1,['optimiz'],['optimizations']
Performance,// Disable assignment tracking in LTO builds for now as the performance; // cost is too high. Disable for LLDB tuning due to llvm.org/PR43126.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp:60,perform,performance,60,interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,1,['perform'],['performance']
Performance,"// Disable extending masked loads for fixed-width for now, since the code; // quality doesn't look great.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loads']
Performance,// Disable f32->f64 extload as we can only generate this in one instruction; // under optsize. So its easier to pattern match (fpext (load)) for that; // case instead of needing to emit 2 instructions for extload in the; // non-optsize case.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:134,load,load,134,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// Disable hoisting past potentially interfering loads. Optimized; // Uses may point to an access outside the loop, as getClobbering; // checks the previous iteration when walking the backedge.; // FIXME: More precise: no Uses that alias SI.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:49,load,loads,49,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,2,"['Optimiz', 'load']","['Optimized', 'loads']"
Performance,"// Disable instr-ref at -O0: it's very slow (in compile time). We can still; // have optimized code inlined into this unoptimized code, however with; // fewer and less aggressive optimizations happening, coverage and accuracy; // should not suffer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineFunction.cpp:85,optimiz,optimized,85,interpreter/llvm-project/llvm/lib/CodeGen/MachineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineFunction.cpp,2,['optimiz'],"['optimizations', 'optimized']"
Performance,// Disable inttoptr/ptrtoint optimization if enabled.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Instructions.cpp:29,optimiz,optimization,29,interpreter/llvm-project/llvm/lib/IR/Instructions.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Instructions.cpp,1,['optimiz'],['optimization']
Performance,"// Disable optimizations and keep frame pointer when debugging, overriding; // other optimization options that might be in argv",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp:11,optimiz,optimizations,11,interpreter/cling/lib/Interpreter/CIFactory.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp,2,['optimiz'],"['optimization', 'optimizations']"
Performance,// Disable optimizations if requested. We cannot skip the whole pass as some; // fixups are necessary for correctness.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp:11,optimiz,optimizations,11,interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp,1,['optimiz'],['optimizations']
Performance,"// Disable optimized register allocation which is turned on automatically; // by -O1, but seems to require -O2 to not explode in run time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:11,optimiz,optimized,11,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,1,['optimiz'],['optimized']
Performance,// Disable scalable vectorization if the loop contains any instructions; // with element types not supported for scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:11,scalab,scalable,11,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,2,['scalab'],['scalable']
Performance,"// Disable the MachineScheduler for now. Even with ShouldTrackPressure set and; // enableMachineSchedDefaultSched overridden, it appears to have an overall; // negative effect for the kinds of register optimizations we're doing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblySubtarget.cpp:202,optimiz,optimizations,202,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblySubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblySubtarget.cpp,1,['optimiz'],['optimizations']
Performance,"// Disable the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TEventIter.cxx:15,cache,cache,15,proof/proofplayer/src/TEventIter.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TEventIter.cxx,2,['cache'],['cache']
Performance,// Disable the check for -Oz (aka OptimizeForSizeHarder).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp:34,Optimiz,OptimizeForSizeHarder,34,interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,1,['Optimiz'],['OptimizeForSizeHarder']
Performance,"// Disable the module hash. This gives us a flat file layout in the; // modules cache directory. In clang this is used to prevent modules from; // different compiler invocations to not collide, but we only have one; // compiler invocation in cling, so we don't need this.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp:80,cache,cache,80,interpreter/cling/lib/Interpreter/CIFactory.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp,1,['cache'],['cache']
Performance,"// Disable this optimization for ARM64EC. FIXME: This probably should work,; // but getting the symbol table correct is complicated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCXX.cpp:16,optimiz,optimization,16,interpreter/llvm-project/clang/lib/CodeGen/CGCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCXX.cpp,1,['optimiz'],['optimization']
Performance,// Disable vector promotion when there are loads or stores of an FCA.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:43,load,loads,43,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,2,['load'],['loads']
Performance,"// Disable warnings when reading a tree without loading the corresponding library",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TDataSetManager.cxx:48,load,loading,48,proof/proof/src/TDataSetManager.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TDataSetManager.cxx,1,['load'],['loading']
Performance,"// Disabled by default, because the added alignment assumptions may increase; // compile-time and block optimizations. This option is not suitable for use; // with frontends that emit comprehensive parameter alignment annotations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:104,optimiz,optimizations,104,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['optimiz'],['optimizations']
Performance,// Discard bad candidates before we run out of interference cache cursors.; // This will only affect register classes with a lot of registers (>32).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocGreedy.cpp:60,cache,cache,60,interpreter/llvm-project/llvm/lib/CodeGen/RegAllocGreedy.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocGreedy.cpp,1,['cache'],['cache']
Performance,// Dismabiguate if more than 1 latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Analysis.cpp:31,latency,latency,31,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Analysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Analysis.cpp,1,['latency'],['latency']
Performance,"// Display a load error message.; /*; std::stringstream ss;; ss << ""<html><body bgcolor=\""white\"">""; ""<h2>Failed to load URL ""; << failedUrl.ToString().substr(0,100) << "" with error "" << errorText.ToString() << "" ("" << errorCode; << "").</h2></body></html>"";; frame->LoadURL(GetDataURI(ss.str(), ""text/html""));; */",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/cefdisplay/src/gui_handler.cxx:13,load,load,13,gui/cefdisplay/src/gui_handler.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/cefdisplay/src/gui_handler.cxx,3,"['Load', 'load']","['LoadURL', 'load']"
Performance,// Disregard v2i64. Memcpy lowering produces those and splitting; // them regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:84,perform,performance,84,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['perform'],['performance']
Performance,"// Distribute add(vecreduce(load(Y)), vecreduce(load(Z))); // Or add(add(X, vecreduce(load(Y))), vecreduce(load(Z))); // by ascending load offsets. This can help cores prefetch if the order of; // loads is more predictable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,6,['load'],"['load', 'loads']"
Performance,// Distribute loops to allow partial vectorization. I.e. isolate dependences; // into separate loop that would otherwise inhibit vectorization. This is; // currently only performed for loops marked with the metadata; // llvm.loop.distribute=true or when -enable-loop-distribute is specified.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:171,perform,performed,171,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['perform'],['performed']
Performance,// Divide the input until we get to a supported size. This will; // always end up with an EC that represent a scalar or a scalable; // scalar.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp:122,scalab,scalable,122,interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,1,['scalab'],['scalable']
Performance,"// Do /not/ compare MapIterator for equality, as this is very expensive.; // The cached start/stop values make that check unnecessary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/CoalescingBitVector.h:81,cache,cached,81,interpreter/llvm-project/llvm/include/llvm/ADT/CoalescingBitVector.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/CoalescingBitVector.h,1,['cache'],['cached']
Performance,"// Do PHI translation to get its value in the predecessor if necessary. The; // returned pointer (if non-null) is guaranteed to dominate UnavailablePred.; // We do the translation for each edge we skipped by going from Load's block; // to LoadBB, otherwise we might miss pieces needing translation.; // If all preds have a single successor, then we know it is safe to insert; // the load on the pred (?!?), so we can insert code to materialize the; // pointer if it is not available.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:219,Load,Load,219,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,3,"['Load', 'load']","['Load', 'LoadBB', 'load']"
Performance,// Do RPO function attribute inference across the module to forward-propagate; // attributes where applicable.; // FIXME: Is this really an optimization rather than a canonicalization?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:140,optimiz,optimization,140,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,2,['optimiz'],['optimization']
Performance,"// Do a (aligned) store to a stack slot, then copy from the stack slot; // to the final destination using (unaligned) integer loads and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:126,load,loads,126,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['load'],['loads']
Performance,"// Do a binary search to see if we already have an entry for this block in; // the cache set. If so, find it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:83,cache,cache,83,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,['cache'],['cache']
Performance,// Do a lookup to see if the function is in our cache... this should just be a; // deferred annotation!,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Interpreter/ExternalFunctions.cpp:48,cache,cache,48,interpreter/llvm-project/llvm/lib/ExecutionEngine/Interpreter/ExternalFunctions.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Interpreter/ExternalFunctions.cpp,1,['cache'],['cache']
Performance,// Do a non-extending load followed by FP_EXTEND.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeFloatTypes.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeFloatTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeFloatTypes.cpp,1,['load'],['load']
Performance,// Do a quick scan to see if we have any checkable loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['loads']
Performance,// Do a small peephole optimization: re-use the switch table compare if; // possible.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:23,optimiz,optimization,23,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,1,['optimiz'],['optimization']
Performance,// Do a word load with postincrement. This will be lowered to a two byte; // load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRExpandPseudoInsts.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Target/AVR/AVRExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRExpandPseudoInsts.cpp,2,['load'],['load']
Performance,"// Do an extremely restricted form of load folding.; // ISel will often create patterns like:; // movl 4(%edi), %eax; // movl 8(%edi), %ecx; // movl 12(%edi), %edx; // movl %edx, 8(%esp); // movl %ecx, 4(%esp); // movl %eax, (%esp); // call; // Get rid of those with prejudice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CallFrameOptimization.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/Target/X86/X86CallFrameOptimization.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CallFrameOptimization.cpp,1,['load'],['load']
Performance,// Do an initial analysis for each basic block and find all the potential; // candidates to perform if-conversion.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp:92,perform,perform,92,interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp,1,['perform'],['perform']
Performance,// Do an optimization for the most frequently used types.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:9,optimiz,optimization,9,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// Do final scheduling after all other optimizations, to get an; // optimal input for the decoder (branch relaxation must happen; // after block placement).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetMachine.cpp:39,optimiz,optimizations,39,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetMachine.cpp,1,['optimiz'],['optimizations']
Performance,// Do more involved optimizations if the global is internal.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:20,optimiz,optimizations,20,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['optimiz'],['optimizations']
Performance,// Do not CSE normal loads because between them could be store instructions; // that change the loaded value,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:21,load,loads,21,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// Do not apply discount if scalable, because that would lead to; // invalid scalarization costs.; // Do not apply discount logic if hacked cost is needed; // for emulated masked memrefs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:28,scalab,scalable,28,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['scalab'],['scalable']
Performance,"// Do not attempt to close stdout or stderr. We used to try to maintain the; // property that tools that support writing file to stdout should not also; // write informational output to stdout, but in practice we were never able to; // maintain this invariant. Many features have been added to LLVM and clang; // (-fdump-record-layouts, optimization remarks, etc) that print to stdout, so; // users must simply be aware that mixed output and remarks is a possibility.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/raw_ostream.cpp:337,optimiz,optimization,337,interpreter/llvm-project/llvm/lib/Support/raw_ostream.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/raw_ostream.cpp,1,['optimiz'],['optimization']
Performance,// Do not attempt to inline a candidate if; // --disable-sample-loader-inlining is true.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp:64,load,loader-inlining,64,interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,1,['load'],['loader-inlining']
Performance,"// Do not bother looking at stored values that are not constants, loads, or; // extracted vector elements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:66,load,loads,66,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,"// Do not cache a basket if it is bigger than the cache size!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx:10,cache,cache,10,tree/tree/src/TTreeCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx,2,['cache'],['cache']
Performance,"// Do not cache failed stats, it is easy to construct common inconsistent; // situations if we do, and they are not important for PCH performance; // (which currently only needs the stats to construct the initial; // FileManager entries).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp:10,cache,cache,10,interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp,2,"['cache', 'perform']","['cache', 'performance']"
Performance,"// Do not cache this lookup, getValueState calls later in the function might; // invalidate the reference.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp:10,cache,cache,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp,1,['cache'],['cache']
Performance,// Do not change the width of a volatile load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['load']
Performance,// Do not change the width of a volatile or atomic loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,// Do not contract if we're not optimizing the code.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:32,optimiz,optimizing,32,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['optimiz'],['optimizing']
Performance,"// Do not define font set is we're loading the symbol.ttf - it's; // the same in both cases.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf2d/graf/src/TTF.cxx:35,load,loading,35,graf2d/graf/src/TTF.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf2d/graf/src/TTF.cxx,1,['load'],['loading']
Performance,"// Do not delete the directory if it is part of the output; // and we are in incremental mode (because it will be reused; // and has not been written to disk (for performance reason).; // coverity[var_deref_model] the IsA()->InheritsFrom guarantees that the dynamic_cast will succeed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFileMerger.cxx:163,perform,performance,163,io/io/src/TFileMerger.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFileMerger.cxx,1,['perform'],['performance']
Performance,// Do not duplicate volatile and atomic loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,1,['load'],['loads']
Performance,// Do not emit the intrinsic if we're not optimizing.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDeclCXX.cpp:42,optimiz,optimizing,42,interpreter/llvm-project/clang/lib/CodeGen/CGDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDeclCXX.cpp,1,['optimiz'],['optimizing']
Performance,// Do not expand if the total number of loads is larger than what the; // target allows. Note that it's important that we exit before completing; // the expansion to avoid using a ton of memory to store the expansion for; // large sizes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['load'],['loads']
Performance,// Do not expand loads and stores that don't exceed 64 bits.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:17,load,loads,17,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,2,['load'],['loads']
Performance,"// Do not generate ""BUFFER_WBL2"" as there are no caches it would; // writeback, and would require an otherwise unnecessary; // ""S_WAITCNT vmcnt(0)"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:49,cache,caches,49,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,1,['cache'],['caches']
Performance,"// Do not generate inline TP loop if optimizations is disabled,; // or if optimization for size (-Os or -Oz) is on.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp:37,optimiz,optimizations,37,interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp,2,['optimiz'],"['optimization', 'optimizations']"
Performance,// Do not generate loads of non-round integer types since these can; // be expensive (and would be wrong if the type is not byte sized).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['loads']
Performance,"// Do not generate pre-inc forms for specific loads that feed scalar_to_vector; // instructions because we can fold these into a more efficient instruction; // instead, (such as LXSD).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:46,load,loads,46,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loads']
Performance,// Do not handle extending vector loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:34,load,loads,34,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['loads']
Performance,"// Do not handle the case where the LHS of the and is not a shift. While; // it would be trivial to handle this case, it would just transform; // 'and' -> 'bfe', but 'and' has higher-throughput.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp:183,throughput,throughput,183,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,1,['throughput'],['throughput']
Performance,// Do not instrument the load fetching the dynamic shadow address.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,3,['load'],['load']
Performance,// Do not introduce a slow unaligned load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,// Do not iterate on scalable vector. The num of elements is unknown at; // compile-time.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp:21,scalab,scalable,21,interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,2,['scalab'],['scalable']
Performance,// Do not iterate on scalable vector. The number of elements is unknown at; // compile-time.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp:21,scalab,scalable,21,interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,1,['scalab'],['scalable']
Performance,// Do not let this transformation reduce the number of volatile loads.; // Be conservative for atomics for the moment; // TODO: This does appear to be legal for unordered atomics (see D66309),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:64,load,loads,64,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,// Do not load if the limit is reached.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp:10,load,load,10,interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp,1,['load'],['load']
Performance,"// Do not load non-FS profiles. A line or probe can get a zero-valued; // discriminator at certain pass which could result in accidentally loading; // the corresponding base counter in the non-FS profile, while a non-zero; // discriminator would end up getting zero samples. This could in turn undo; // the sample distribution effort done by previous BFI maintenance and the; // probe distribution factor work for pseudo probes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MIRSampleProfile.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/MIRSampleProfile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MIRSampleProfile.cpp,2,['load'],"['load', 'loading']"
Performance,// Do not optimize atomic loads to non-atomic memcmp,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp:10,optimiz,optimize,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,2,"['load', 'optimiz']","['loads', 'optimize']"
Performance,// Do not optimize away the return value. Inspired by; // https://github.com/google/benchmark/blob/main/include/benchmark/benchmark.h#L307-L345,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/vfabi-demangle-fuzzer/vfabi-demangler-fuzzer.cpp:10,optimiz,optimize,10,interpreter/llvm-project/llvm/tools/vfabi-demangle-fuzzer/vfabi-demangler-fuzzer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/vfabi-demangle-fuzzer/vfabi-demangler-fuzzer.cpp,1,['optimiz'],['optimize']
Performance,// Do not optimize if the calling conventions do not match and the conventions; // used are not C or Fast.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:10,optimiz,optimize,10,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['optimiz'],['optimize']
Performance,"// Do not optimize out type conversion of f128 type yet.; // For some targets like x86_64, configuration is changed to keep one f128; // value in one SSE register, but instruction selection cannot handle; // FCOPYSIGN on SSE registers yet.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,optimiz,optimize,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['optimiz'],['optimize']
Performance,"// Do not perform phi translation across a loop header phi, because this; // may result in comparison of values from two different loop iterations.; // FIXME: This check is broken if LoopHeaders is not populated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:10,perform,perform,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['perform'],['perform']
Performance,"// Do not perform this transformation if it would require; // insertion of a large number of select instructions. For targets; // without predication/cmovs, this is a big pessimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:10,perform,perform,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,1,['perform'],['perform']
Performance,// Do not perform transform!,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp:10,perform,perform,10,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,1,['perform'],['perform']
Performance,"// Do not process non-executable instructions. They can become exceutable; // later (via a flow-edge in the work queue). In such case, the instruc-; // tion will be visited at that time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstPropagation.cpp:113,queue,queue,113,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstPropagation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstPropagation.cpp,1,['queue'],['queue']
Performance,"// Do not promote if the operand has been added by codegenprepare.; // Otherwise, it means we are undoing an optimization that is likely to be; // redone, thus causing potential infinite loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:109,optimiz,optimization,109,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimization']
Performance,"// Do not reassociate boolean (i1) expressions. We want to preserve the; // original order of evaluation for short-circuited comparisons that; // SimplifyCFG has folded to AND/OR expressions. If the expression; // is not further optimized, it is likely to be transformed back to a; // short-circuited form for code gen, and the source order may have been; // optimized for the most likely conditions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp:229,optimiz,optimized,229,interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,2,['optimiz'],['optimized']
Performance,// Do not rely on real path names when executing the crash reproducer scripts; // since we only want to actually use the files we have on the VFS cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ModuleDependencyCollector.cpp:146,cache,cache,146,interpreter/llvm-project/clang/lib/Frontend/ModuleDependencyCollector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ModuleDependencyCollector.cpp,1,['cache'],['cache']
Performance,"// Do not reorder! The getMostNullable method relies on the order.; // Optimization: Most pointers expected to be unspecified. When a symbol has an; // unspecified or nonnull type non of the rules would indicate any problem for; // that symbol. For this reason only nullable and contradicted nullability are; // stored for a symbol. When a symbol is already contradicted, it can not be; // casted back to nullable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/CheckerHelpers.h:71,Optimiz,Optimization,71,interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/CheckerHelpers.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/CheckerHelpers.h,1,['Optimiz'],['Optimization']
Performance,// Do not save AGPRs prior to GFX90A because there was no easy way to do so.; // In gfx908 there was do AGPR loads and stores and thus spilling also; // require a temporary VGPR.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:109,load,loads,109,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,1,['load'],['loads']
Performance,// Do not serialize (non-volatile) loads of constant memory with anything.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,2,['load'],['loads']
Performance,// Do not serialize masked loads of constant memory with anything.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:27,load,loads,27,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['load'],['loads']
Performance,// Do not serialize non-volatile loads against each other.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:33,load,loads,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,2,['load'],['loads']
Performance,// Do not serialize variable-length loads of constant memory with; // anything.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:36,load,loads,36,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['load'],['loads']
Performance,"// Do not set CacheAndTrack on constant expressions",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsOptTestStatistic.cxx:14,Cache,CacheAndTrack,14,roofit/roofitcore/src/RooAbsOptTestStatistic.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsOptTestStatistic.cxx,2,['Cache'],['CacheAndTrack']
Performance,// Do not shrink an aligned scalar load to sub-dword.; // Scalar engine cannot do sub-dword loads.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,3,['load'],"['load', 'loads']"
Performance,// Do not sibcall optimize vararg calls unless all arguments are passed via; // registers.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:18,optimiz,optimize,18,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,2,['optimiz'],['optimize']
Performance,// Do not simplify loads that are only used in llvm.assume if we cannot also; // remove all stores that may feed into the load. The reason is that the; // assume is probably worth something as long as the stores are around.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,2,['load'],"['load', 'loads']"
Performance,// Do not sink MI if it might be used to optimize a redundant compare.; // We heuristically only look at the instruction immediately following MI to; // avoid potentially searching the entire basic block.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:41,optimiz,optimize,41,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['optimiz'],['optimize']
Performance,// Do not tail call optimize vararg calls.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:20,optimiz,optimize,20,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['optimiz'],['optimize']
Performance,// Do not use 'gnu' hash style for Mips targets because .gnu.hash; // and the MIPS ABI require .dynsym to be sorted in different ways.; // .gnu.hash needs symbols to be grouped by hash code whereas the MIPS; // ABI requires a mapping between the GOT and the symbol table.; // Android loader does not support .gnu.hash until API 23.; // Hexagon linker/loader does not support .gnu.hash,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Linux.cpp:284,load,loader,284,interpreter/llvm-project/clang/lib/Driver/ToolChains/Linux.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Linux.cpp,2,['load'],['loader']
Performance,// Do not use builder here: CreateICmp may simplify this into a constant and; // unswitching will break. Better optimize it away later.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SimpleLoopUnswitch.cpp:112,optimiz,optimize,112,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SimpleLoopUnswitch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SimpleLoopUnswitch.cpp,1,['optimiz'],['optimize']
Performance,"// Do not use f64 to lower memcpy if source is string constant. It's; // better to use i32 to avoid the loads.; // Also, do not use f64 to lower memset unless this is a memset of zeros.; // The gymnastics of splatting a byte value into an XMM register and then; // only using 8-byte stores (because this is a CPU with slow unaligned; // 16-byte accesses) makes that a loser.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:104,load,loads,104,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['load'],['loads']
Performance,"// Do not use the cached DataLayout because some client use it without a Module; // (dsymutil, llvm-dwarfdump).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp:18,cache,cached,18,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,1,['cache'],['cached']
Performance,// Do not want to hoist if we're not optimizing for size.; // TODO: We'd like to remove this restriction.; // See the comment in X86InstrInfo.td for more info.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:37,optimiz,optimizing,37,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['optimiz'],['optimizing']
Performance,// Do not warn about not applied transformations if optimizations are; // disabled.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/WarnMissedTransforms.cpp:52,optimiz,optimizations,52,interpreter/llvm-project/llvm/lib/Transforms/Scalar/WarnMissedTransforms.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/WarnMissedTransforms.cpp,1,['optimiz'],['optimizations']
Performance,// Do not widen if it would introduce a slow unaligned load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:55,load,load,55,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,// Do not widen load if atomic/volatile or under asan/hwasan/memtag/tsan.; // The widened load may load data from dirty regions or create data races; // non-existent in the source.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,3,['load'],['load']
Performance,// Do nothing for (preserve.static.offset (load/store ..)) or for; // GEPs with zero indices. Such constructs lead to zero offset and; // are simplified by other passes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFPreserveStaticOffset.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/Target/BPF/BPFPreserveStaticOffset.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFPreserveStaticOffset.cpp,1,['load'],['load']
Performance,"// Do really simple store-to-load forwarding and load CSE, to catch cases; // where there are several consecutive memory accesses to the same location,; // separated by a few arithmetic operations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,2,['load'],['load']
Performance,// Do repeated 4-byte loads and stores. To be improved.; // This requires 4-byte alignment.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp:22,load,loads,22,interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp,1,['load'],['loads']
Performance,"// Do simple ""peephole"" optimizations and bit-twiddling optzns.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ExceptionDemo/ExceptionDemo.cpp:24,optimiz,optimizations,24,interpreter/llvm-project/llvm/examples/ExceptionDemo/ExceptionDemo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/ExceptionDemo/ExceptionDemo.cpp,7,['optimiz'],['optimizations']
Performance,// Do some basic validation checking on our liveness results before; // performing relocation. Relocation can and will turn mistakes in liveness; // results into non-sensical code which is must harder to debug.; // TODO: It would be nice to test consistency as well,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp:72,perform,performing,72,interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,1,['perform'],['performing']
Performance,"// Do struct initialization; this code just sets each individual member; // to the approprate value. This makes bitfield support automatic;; // the disadvantage is that the generated code is more difficult for; // the optimizer, especially with bitfields.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:218,optimiz,optimizer,218,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['optimiz'],['optimizer']
Performance,// Do target-specific constant optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:31,optimiz,optimization,31,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['optimiz'],['optimization']
Performance,// Do the atomic load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp:17,load,load,17,interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,2,['load'],['load']
Performance,// Do the remaining work on a scalar since it allows the code generator to; // combine the shift and bitwise operation into one instruction and since; // integer instructions can have higher throughput than vector instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:191,throughput,throughput,191,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['throughput'],['throughput']
Performance,"// Do the transformation (For 32-bit type):; // -> (and (load arr[idx]), inp); // <- (and (srl 0xFFFFFFFF, (sub 32, idx))); // that will be replaced with one bzhi instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// Do various transformations for exception handling.; // Every CFG-changing optimizations should come before this.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyTargetMachine.cpp:77,optimiz,optimizations,77,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyTargetMachine.cpp,1,['optimiz'],['optimizations']
Performance,// Do we already know cached result?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp:22,cache,cached,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp,1,['cache'],['cached']
Performance,// Do we have the sorted successors in cache ?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp:39,cache,cache,39,interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,1,['cache'],['cache']
Performance,"// Do we need to perform FI resolution for this function. Normally, this is; // required only when the function has any stack objects. However, FI; // resolution actually has another job, not apparent from the title - it; // resolves callframe setup/destroy that were not simplified earlier.; //; // So, this is required for M68k functions that have push sequences even; // when there are no stack objects.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.h:17,perform,perform,17,interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.h,1,['perform'],['perform']
Performance,// Does baseline recommend not to perform the fold by default?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:34,perform,perform,34,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['perform'],['perform']
Performance,"// Does the ADD_TLS node of the load/store use the thread pointer?; // If the thread pointer is not used as one of the operands of ADD_TLS,; // then this optimization is not valid.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"// Does the initial scan of the directory - directly calling Receiver,; // bypassing the Queue. Both InitialScan and EventReceivingLoop use Receiver; // which isn't necessarily thread-safe.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp:89,Queue,Queue,89,interpreter/llvm-project/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/linux/DirectoryWatcher-linux.cpp,1,['Queue'],['Queue']
Performance,// Does the load have an offset?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:12,load,load,12,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,2,['load'],['load']
Performance,"// Does the packet contain a load. Used to restrict another load, if possible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonHazardRecognizer.h:29,load,load,29,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonHazardRecognizer.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonHazardRecognizer.h,2,['load'],['load']
Performance,// Does these two blocks pair be queried before and have a definite cached; // result?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp:68,cache,cached,68,interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,1,['cache'],['cached']
Performance,"// Dom: LHS+RHS; // I: sext(LHS)+sext(RHS); // If Dom can't sign overflow and Dom dominates I, optimize I to sext(Dom).; // TODO: handle zext",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp:95,optimiz,optimize,95,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,1,['optimiz'],['optimize']
Performance,// Don't CSE load that is volatile or anything stronger than unordered.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,1,['load'],['load']
Performance,// Don't access the slot unless we're trying to cache the result.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp:48,cache,cache,48,interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,1,['cache'],['cache']
Performance,// Don't actually mark a global constant if it's atomic because atomic loads; // are implemented by a trivial cmpxchg in some edge-cases and that usually; // requires write access to the variable even if it's not actually changed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:71,load,loads,71,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['loads']
Performance,"// Don't add a zero-latency instruction to the Ready queue.; // A zero-latency instruction doesn't consume any scheduler resources. That is; // because it doesn't need to be executed, and it is often removed at register; // renaming stage. For example, register-register moves are often optimized at; // register renaming stage by simply updating register aliases. On some; // targets, zero-idiom instructions (for example: a xor that clears the value; // of a register) are treated specially, and are often eliminated at register; // renaming stage.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/HardwareUnits/Scheduler.cpp:20,latency,latency,20,interpreter/llvm-project/llvm/lib/MCA/HardwareUnits/Scheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/HardwareUnits/Scheduler.cpp,4,"['latency', 'optimiz', 'queue']","['latency', 'optimized', 'queue']"
Performance,"// Don't allow the instruction defining AntiDepReg to earlyclobber its; // operands, in case they may be assigned to NewReg. In this case antidep; // breaking must fail, but it's too rare to bother optimizing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CriticalAntiDepBreaker.cpp:198,optimiz,optimizing,198,interpreter/llvm-project/llvm/lib/CodeGen/CriticalAntiDepBreaker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CriticalAntiDepBreaker.cpp,1,['optimiz'],['optimizing']
Performance,// Don't analyze if the user explicitly asked for no checks to be performed; // on this file.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Frontend/AnalysisConsumer.cpp:66,perform,performed,66,interpreter/llvm-project/clang/lib/StaticAnalyzer/Frontend/AnalysisConsumer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Frontend/AnalysisConsumer.cpp,1,['perform'],['performed']
Performance,// Don't ask a Mach-O STAB symbol for its section unless you know that; // STAB symbol's section field refers to a valid section index. Otherwise; // the symbol may error trying to load a section that does not exist.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/llvm-objdump.cpp:181,load,load,181,interpreter/llvm-project/llvm/tools/llvm-objdump/llvm-objdump.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/llvm-objdump.cpp,2,['load'],['load']
Performance,// Don't ask a Mach-O STABS symbol for its section unless we know that; // STAB symbol's section field refers to a valid section index. Otherwise; // the symbol may error trying to load a section that does not exist.; // TODO: Add a whitelist of STABS symbol types that contain valid section; // indices.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/MachODumper.cpp:181,load,load,181,interpreter/llvm-project/llvm/tools/llvm-readobj/MachODumper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/MachODumper.cpp,1,['load'],['load']
Performance,// Don't attempt to analyze GEPs if the scalable index is not zero.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp:40,scalab,scalable,40,interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,1,['scalab'],['scalable']
Performance,// Don't attempt to combine non power of 2 loads or unaligned loads when; // subtarget doesn't support them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsPreLegalizerCombiner.cpp:43,load,loads,43,interpreter/llvm-project/llvm/lib/Target/Mips/MipsPreLegalizerCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsPreLegalizerCombiner.cpp,2,['load'],['loads']
Performance,"// Don't bother looking for an integer type if the vector is scalable, skip; // to vector types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:61,scalab,scalable,61,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['scalab'],['scalable']
Performance,// Don't bother performing a no-op shift.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APInt.cpp:16,perform,performing,16,interpreter/llvm-project/llvm/lib/Support/APInt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APInt.cpp,3,['perform'],['performing']
Performance,"// Don't break ""store (load float*)"" pattern, this pattern will be combined; // to ""store (load int32)"" in later InstCombine pass. See function; // combineLoadToOperationType. On PowerPC, loading a float point takes more; // cycles than loading a 32 bit integer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,4,['load'],"['load', 'loading']"
Performance,"// Don't break implicit null checks. This is a performance heuristic, and not; // required for correctness.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp:47,perform,performance,47,interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,1,['perform'],['performance']
Performance,// Don't break things that occupy more than one cacheline.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp:48,cache,cacheline,48,interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp,1,['cache'],['cacheline']
Performance,// Don't cache Argv.size() because it can change.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CommandLine.cpp:9,cache,cache,9,interpreter/llvm-project/llvm/lib/Support/CommandLine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CommandLine.cpp,1,['cache'],['cache']
Performance,// Don't cache constant materializations in the general ValueMap.; // To do so would require tracking what uses they dominate.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp:9,cache,cache,9,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,1,['cache'],['cache']
Performance,// Don't cache results for invariant load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:9,cache,cache,9,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,"['cache', 'load']","['cache', 'load']"
Performance,// Don't cache the result for nodes with different hashes. The hash; // comparison is fast enough.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp:9,cache,cache,9,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonCommonGEP.cpp,1,['cache'],['cache']
Performance,// Don't change point on empty transforms queue,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:42,queue,queue,42,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['queue'],['queue']
Performance,// Don't change the width of a volatile or atomic loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:50,load,loads,50,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,"// Don't combine things like a[i], a[i] -> a bigger load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// Don't continue because there are not enough lifetime markers, or the; // stack is too small, or we are told not to optimize the slots.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/StackColoring.cpp:118,optimiz,optimize,118,interpreter/llvm-project/llvm/lib/CodeGen/StackColoring.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/StackColoring.cpp,1,['optimiz'],['optimize']
Performance,// Don't convert with stack if the load/store is expensive.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,// Don't create 256-bit non-temporal aligned loads without AVX2 as these; // will lower to regular temporal loads and use the cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,"['cache', 'load']","['cache', 'loads']"
Performance,// Don't create a PCH if there were fatal failures during module loading.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/GeneratePCH.cpp:65,load,loading,65,interpreter/llvm-project/clang/lib/Serialization/GeneratePCH.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/GeneratePCH.cpp,1,['load'],['loading']
Performance,// Don't create a indexed load / store with zero offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// Don't create a loadext if we can fold the extension into a wide/long; // instruction.; // If there's more than one user instruction, the loadext is desirable no; // matter what. There can be two uses by the same instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:18,load,loadext,18,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['load'],['loadext']
Performance,"// Don't do PRE on GEPs. The inserted PHI would prevent CodeGenPrepare from; // sinking the addressing mode computation back to its uses. Extending the; // GEP's live range increases the register pressure, and therefore it can; // introduce unnecessary spills.; //; // This doesn't prevent Load PRE. PHI translation will make the GEP available; // to the load by moving it to the predecessor block if necessary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:290,Load,Load,290,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,"['Load', 'load']","['Load', 'load']"
Performance,"// Don't do dynamic alloca or stack malloc if:; // 1) There is inline asm: too often it makes assumptions on which registers; // are available.; // 2) There is a returns_twice call (typically setjmp), which is; // optimization-hostile, and doesn't play well with introduced indirect; // register-relative calculation of local variable addresses.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp:214,optimiz,optimization-hostile,214,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,1,['optimiz'],['optimization-hostile']
Performance,"// Don't do if we could do an indexed load on the original type, but not on; // the new one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,1,['load'],['load']
Performance,"// Don't do reclaims on performSelector calls; despite their; // return type, the invoked method doesn't necessarily actually; // return an object.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:24,perform,performSelector,24,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['perform'],['performSelector']
Performance,"// Don't do this early, since it may interfere with adjacent load merging for; // illegal types. We can avoid losing alignment information for exotic types; // pre-legalize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:61,load,load,61,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['load'],['load']
Performance,// Don't do this if PACKSS/PACKUS could perform it cheaper.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:40,perform,perform,40,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,// Don't duplicate a load with other uses.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:21,load,load,21,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['load'],['load']
Performance,// Don't expand Op into scalar loads/stores in these cases:,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:31,load,loads,31,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['loads']
Performance,// Don't expand if optimizing for size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:19,optimiz,optimizing,19,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['optimiz'],['optimizing']
Performance,// Don't expand if this will require more loads than desired by the target.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:42,load,loads,42,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['load'],['loads']
Performance,"// Don't fold a load into a shift by immediate. The BMI2 instructions; // support folding a load, but not an immediate. The legacy instructions; // support folding an immediate, but can't fold a load. Folding an; // immediate is preferable to folding a load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,4,['load'],['load']
Performance,"// Don't fold load if this matches the BTS/BTR/BTC patterns.; // BTS: (or X, (shl 1, n)); // BTR: (and X, (rotl -2, n)); // BTC: (xor X, (shl 1, n))",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,// Don't fold loads into indirect calls that need a KCFI check as we'll; // have to unfold these in X86TargetLowering::EmitKCFICheck anyway.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['loads']
Performance,// Don't fold non-temporal loads if we have an instruction for them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:27,load,loads,27,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['loads']
Performance,// Don't forward from (non-atomic) memset to atomic load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,1,['load'],['load']
Performance,// Don't generate the same instruction as the one being optimized.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp:56,optimiz,optimized,56,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,1,['optimiz'],['optimized']
Performance,"// Don't generate unaligned loads. If either source is constant data,; // alignment doesn't matter for that source because there is no load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,2,['load'],"['load', 'loads']"
Performance,// Don't hack on volatile loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/GlobalStatus.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Transforms/Utils/GlobalStatus.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/GlobalStatus.cpp,1,['load'],['loads']
Performance,// Don't hack volatile and ordered loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['loads']
Performance,// Don't handle scalable vectors,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:16,scalab,scalable,16,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['scalab'],['scalable']
Performance,"// Don't include sm_30_intrinsics.h and sm_32_intrinsics.h. These define the; // __shfl and __ldg intrinsics using inline (volatile) asm, but we want to; // define them using builtins so that the optimizer can reason about and across; // these instructions. In particular, using intrinsics for ldg gets us the; // [addr+imm] addressing mode, which, although it doesn't actually exist in the; // hardware, seems to generate faster machine code because ptxas can more easily; // reason about our code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Headers/__clang_cuda_runtime_wrapper.h:196,optimiz,optimizer,196,interpreter/llvm-project/clang/lib/Headers/__clang_cuda_runtime_wrapper.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Headers/__clang_cuda_runtime_wrapper.h,1,['optimiz'],['optimizer']
Performance,"// Don't initialize Python from two concurrent threads",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/tpython/src/TPython.cxx:36,concurren,concurrent,36,bindings/tpython/src/TPython.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/tpython/src/TPython.cxx,1,['concurren'],['concurrent']
Performance,"// Don't interfere with something that can be handled by extracting AH.; // TODO: If we are able to fold a load, BEXTR might still be better than AH.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:107,load,load,107,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,// Don't interleave if the loop has been vectorized with scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.h:57,scalab,scalable,57,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.h,1,['scalab'],['scalable']
Performance,// Don't issue movs with shifter operand for some CPUs unless we; // are optimizing for size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp:73,optimiz,optimizing,73,interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,2,['optimiz'],['optimizing']
Performance,// Don't load the size if it's a lower bound.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:9,load,load,9,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,// Don't make these simplifications in 64-bit mode; other assemblers don't; // perform them because they make the code larger.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86EncodingOptimization.cpp:79,perform,perform,79,interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86EncodingOptimization.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86EncodingOptimization.cpp,1,['perform'],['perform']
Performance,"// Don't make work for ourselves. If we know the loaded type is legal,; // don't add it to the worklist.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:49,load,loaded,49,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,1,['load'],['loaded']
Performance,// Don't mix temporal loads with non-temporal loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:22,load,loads,22,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['loads']
Performance,"// Don't modify the function attributes if it's ""on"". ""on"" resets the; // optimizations to the ones listed on the command line",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaAttr.cpp:74,optimiz,optimizations,74,interpreter/llvm-project/clang/lib/Sema/SemaAttr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaAttr.cpp,1,['optimiz'],['optimizations']
Performance,"// Don't move these; the RV optimization depends on the autoreleaseRV; // being tail called, and the retainRV being immediately after a call; // (which might still happen if we get lucky with codegen layout, but; // it's not worth taking the chance).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:28,optimiz,optimization,28,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['optimiz'],['optimization']
Performance,// Don't need to load VGPR in.; // Unpack lanes,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp:17,load,load,17,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIRegisterInfo.cpp,1,['load'],['load']
Performance,// Don't optimize GEPs that don't have indices.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimize']
Performance,// Don't optimize barriers or block placement at -O0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetMachine.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetMachine.cpp,1,['optimiz'],['optimize']
Performance,// Don't optimize before the condition has been transformed to a legal type; // and don't ever optimize vector selects that map to AVX512 mask-registers.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['optimiz'],['optimize']
Performance,// Don't optimize calls that require strict floating point semantics.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,1,['optimiz'],['optimize']
Performance,// Don't optimize floating-point instructions unless they have the; // appropriate FastMathFlags for reassociation enabled.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,1,['optimiz'],['optimize']
Performance,"// Don't optimize for atomic/volatile load or store. Ensure memory is not; // modified between, vector type matches store size, and index is inbounds.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,// Don't optimize if there are ADD_TLS users that aren't load/stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,// Don't optimize loads of the in-scope locals across this point.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp:9,optimiz,optimize,9,interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,2,"['load', 'optimiz']","['loads', 'optimize']"
Performance,"// Don't perform CSE if the result of the new instruction cannot exist; // within the constraints (register class, bank, or low-level type) of; // the old instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp,1,['perform'],['perform']
Performance,// Don't perform PRE on an EH pad.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['perform'],['perform']
Performance,"// Don't perform SRA if we would have to split into many globals. Ignore; // parts that are either only loaded or only stored, because we expect them; // to be optimized away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,3,"['load', 'optimiz', 'perform']","['loaded', 'optimized', 'perform']"
Performance,"// Don't perform a slow TLI lookup, if this function doesn't return a pointer; // and thus can't be an allocation function.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,1,['perform'],['perform']
Performance,// Don't perform any constexpr calls (other than the call we're checking); // when checking a potential constant expression.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:9,perform,perform,9,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['perform'],['perform']
Performance,// Don't perform argument promotion for naked functions; otherwise we can end; // up removing parameters that are seemingly 'not used' as they are referred; // to in the assembly.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,1,['perform'],['perform']
Performance,// Don't perform global opt pass on naked functions; we don't want fast; // calling conventions for naked functions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['perform'],['perform']
Performance,// Don't perform if there is only one cluster or optimizing for size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,2,"['optimiz', 'perform']","['optimizing', 'perform']"
Performance,"// Don't perform more than a certain amount of comparisons.; // This should limit the cost of grouping the pointers to something; // reasonable. If we do end up hitting this threshold, the algorithm; // will create separate groups for all remaining pointers.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,1,['perform'],['perform']
Performance,// Don't perform the following transforms if the AND has multiple uses,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,1,['perform'],['perform']
Performance,// Don't perform this combine if constructing the vector will be expensive.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// Don't perform this optimization when optimizing for size, since; // materializing elements and inserting them tends to cause code bloat.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,3,"['optimiz', 'perform']","['optimization', 'optimizing', 'perform']"
Performance,// Don't populate the cache for the matching node!,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/ASTMatchers/ASTMatchFinder.cpp:22,cache,cache,22,interpreter/llvm-project/clang/lib/ASTMatchers/ASTMatchFinder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/ASTMatchers/ASTMatchFinder.cpp,1,['cache'],['cache']
Performance,"// Don't produce extloads from sub 32-bit types. SI doesn't have scalar; // extloads, so doing one requires using a buffer_load. In cases where we; // still couldn't use a scalar load, using the wider load shouldn't really; // hurt anything.; // If the old size already had to be an extload, there's no harm in continuing; // to reduce the width.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:179,load,load,179,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,2,['load'],['load']
Performance,// Don't promote to an alignment that would require dynamic stack; // realignment which may conflict with optimizations such as tail call; // optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:106,optimiz,optimizations,106,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,6,['optimiz'],"['optimization', 'optimizations']"
Performance,"// Don't record failed searches (a library might be loaded between now and the next search).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TIsAProxy.cxx:52,load,loaded,52,core/meta/src/TIsAProxy.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TIsAProxy.cxx,1,['load'],['loaded']
Performance,// Don't reduce load width if it would prevent us from combining a shift into; // the offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['load']
Performance,// Don't remove a load folding opportunity for the add. That would neutralize; // any improvements from removing constant materializations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// Don't reorder a store over a load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,1,['load'],['load']
Performance,// Don't reorder the loads if there is an order dependence. This would; // occur if the first instruction must go in slot0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp:21,load,loads,21,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,1,['load'],['loads']
Performance,"// Don't report module maps describing eagerly-loaded dependency. This; // information will be deserialized from the PCM.; // TODO: Verify this works fine when modulemap for module A is eagerly; // loaded from A.pcm, and module map passed on the command line contains; // definition of a submodule: ""explicit module A.Private { ... }"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp:47,load,loaded,47,interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp,2,['load'],['loaded']
Performance,// Don't reuse CacheIt since it may be invalid at this point.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp:15,Cache,CacheIt,15,interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,1,['Cache'],['CacheIt']
Performance,// Don't risk duplicating unordered loads; // This checks for an invariant.start dominating the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:36,load,loads,36,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,2,['load'],"['load', 'loads']"
Performance,"// Don't set an explicit alignment on regular load/stores that we want; // to transform to VLD/VST 1_UPD nodes.; // This matches the behavior of regular load/stores, which only get an; // explicit alignment if the MMO alignment is larger than the standard; // alignment of the memory type.; // Intrinsics, however, always get an explicit alignment, set to the; // alignment of the MMO.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:46,load,load,46,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['load'],['load']
Performance,"// Don't sink stores from loops with coroutine suspend instructions.; // LICM would sink instructions into the default destination of; // the coroutine switch. The default destination of the switch is to; // handle the case where the coroutine is suspended, by which point the; // coroutine frame may have been destroyed. No instruction can be sunk there.; // FIXME: This would unfortunately hurt the performance of coroutines, however; // there is currently no general solution for this. Similar issues could also; // potentially happen in other passes where instructions are being moved; // across that edge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:401,perform,performance,401,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['perform'],['performance']
Performance,"// Don't sink/hoist volatile or ordered atomic loads!; // Loads from constant memory are always safe to move, even if they end up; // in the same alias set as something that ends up being modified.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:47,load,loads,47,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,2,"['Load', 'load']","['Loads', 'loads']"
Performance,"// Don't sink/hoist volatile or ordered atomic store!; // We can only hoist a store that we can prove writes a value which is not; // read or overwritten within the loop. For those cases, we fallback to; // load store promotion instead. TODO: We can extend this to cases where; // there is exactly one write to the location and that write dominates an; // arbitrary number of reads in the loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:207,load,load,207,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// Don't skip over loads, throws or things that can modify memory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['loads']
Performance,"// Don't speculate loads. Note that it may be possible and desirable to; // speculate GOT or constant pool loads that are guaranteed not to trap,; // but we don't support that for now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/EarlyIfConversion.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/CodeGen/EarlyIfConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/EarlyIfConversion.cpp,4,['load'],['loads']
Performance,// Don't split v2i64 vectors. Memcpy lowering produces those and splitting; // those up regresses performance on micro-benchmarks and olden/bh.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:98,perform,performance,98,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['performance']
Performance,"// Don't tranform ""load <256 x i32>, <256 x i32>*"" to; // ""load x86_amx, x86_amx*"", because x86_amx* is invalid.; // TODO: Remove this check when bitcast between vector and x86_amx; // is replaced with a specific intrinsic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,2,['load'],['load']
Performance,"// Don't transform one with multiple uses, this would require adding a new; // load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:79,load,load,79,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// Don't try to do this optimization when the setcc itself has i1 operands.; // There are no legal vectors of i1, so this would be pointless. v1f16 is; // ruled out to prevent the creation of setcc that need to be scalarized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,optimiz,optimization,24,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['optimiz'],['optimization']
Performance,// Don't try to find the property if the ivar was not loaded from the; // given instance.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CheckObjCDealloc.cpp:54,load,loaded,54,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CheckObjCDealloc.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CheckObjCDealloc.cpp,1,['load'],['loaded']
Performance,// Don't try to fold volatile loads. Target has to deal with alignment; // constraints.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp:30,load,loads,30,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,1,['load'],['loads']
Performance,// Don't try to handle bitcasting vector ext loads for now.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['loads']
Performance,// Don't try to optimize volatile.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:16,optimiz,optimize,16,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['optimiz'],['optimize']
Performance,// Don't try to promote scalable types.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp:24,scalab,scalable,24,interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,1,['scalab'],['scalable']
Performance,// Don't unfold simple loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,1,['load'],['loads']
Performance,// Don't unpack for structure with scalable vector.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:35,scalab,scalable,35,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,2,['scalab'],['scalable']
Performance,// Don't use a frame pointer on linux if optimizing for certain targets.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/CommonArgs.cpp:41,optimiz,optimizing,41,interpreter/llvm-project/clang/lib/Driver/ToolChains/CommonArgs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/CommonArgs.cpp,1,['optimiz'],['optimizing']
Performance,"// Don't use gROOT so that this routine does not trigger TROOT's initialization; // This is essential since this routine is called via operator delete; // which is used during RegisterModule (i.e. during library loading, including the initial; // start up). Using gROOT leads to recursive call to RegisterModule and initialization of; // the interpreter in the middle of the execution of RegisterModule (i.e. undefined behavior).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TMapFile.cxx:212,load,loading,212,io/io/src/TMapFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TMapFile.cxx,1,['load'],['loading']
Performance,// Don't use scalar loads for volatile accesses to non-constant address; // spaces.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:20,load,loads,20,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,1,['load'],['loads']
Performance,// Don't use the cached binary holder because we have no thread-safety; // guarantee and the lifetime is limited.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Classic/DWARFLinker.cpp:17,cache,cached,17,interpreter/llvm-project/llvm/lib/DWARFLinker/Classic/DWARFLinker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DWARFLinker/Classic/DWARFLinker.cpp,2,['cache'],['cached']
Performance,"// Don't warn about /Oy- in x86-64 builds (where; // SupportsForcingFramePointer is false). The flag having no effect; // there is a compiler-internal optimization, and people shouldn't have; // to special-case their build files for x86-64 clang-cl.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/MSVC.cpp:151,optimiz,optimization,151,interpreter/llvm-project/clang/lib/Driver/ToolChains/MSVC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/MSVC.cpp,1,['optimiz'],['optimization']
Performance,"// Don't warn if the user explicitly requested re-optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/main/src/hadd.cxx:50,optimiz,optimization,50,main/src/hadd.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/main/src/hadd.cxx,1,['optimiz'],['optimization']
Performance,// Done with the inference if the calle is reachable via a single callsite.; // This may not be accurate but it improves the search throughput.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp:132,throughput,throughput,132,interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp,1,['throughput'],['throughput']
Performance,// Double MMA loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:14,load,loads,14,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['loads']
Performance,// Double MMA loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:14,load,loads,14,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['loads']
Performance,// Double-check that we're not trying to optimize an instruction that was; // already optimized by some other part of this pass.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:41,optimiz,optimize,41,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,2,['optimiz'],"['optimize', 'optimized']"
Performance,"// Draw three axis rings where permitted; // Not drawing will prevent interaction; // GL name loading for hit testing - 0 reserved for no selection",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLRotateManip.cxx:94,load,loading,94,graf3d/gl/src/TGLRotateManip.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLRotateManip.cxx,1,['load'],['loading']
Performance,"// Draw three axis widgets out of bounding box where permitted; // Not drawing will prevent interaction; // GL name loading for hit testing - 0 reserved for no selection",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScaleManip.cxx:116,load,loading,116,graf3d/gl/src/TGLScaleManip.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScaleManip.cxx,2,['load'],['loading']
Performance,// DriverKit always uses the build version load command.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCStreamer.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/MC/MCStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCStreamer.cpp,1,['load'],['load']
Performance,"// Drop Load, but keep its chain. No cycle check necessary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp:8,Load,Load,8,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,2,['Load'],['Load']
Performance,// Drop the current token and bring the first cached one. It's the same token; // as when we entered this function.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExprCXX.cpp:46,cache,cached,46,interpreter/llvm-project/clang/lib/Parse/ParseExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExprCXX.cpp,1,['cache'],['cached']
Performance,// Drop the event if its image is loaded at the same address,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/PerfReader.cpp:34,load,loaded,34,interpreter/llvm-project/llvm/tools/llvm-profgen/PerfReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/PerfReader.cpp,1,['load'],['loaded']
Performance,"// Droppable users, e.g., llvm::assume does not actually perform any action.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:57,perform,perform,57,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,1,['perform'],['perform']
Performance,"// Due to VADD, VSUB, VMUL can executed on more ports than VINSERT and; // their latency are short, so here we don't replace them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:81,latency,latency,81,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['latency'],['latency']
Performance,"// Due to a bug in TailMerging/CFG Optimization, we need to add a; // special case handling of a predicated jump followed by an; // unconditional jump. If not, Tail Merging and CFG Optimization go; // into an infinite loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp:35,Optimiz,Optimization,35,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp,2,['Optimiz'],['Optimization']
Performance,"// Due to a parsing error, we either went over the cached tokens or; // there are still cached tokens left, so we skip the leftover tokens.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseCXXInlineMethods.cpp:51,cache,cached,51,interpreter/llvm-project/clang/lib/Parse/ParseCXXInlineMethods.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseCXXInlineMethods.cpp,2,['cache'],['cached']
Performance,"// Due to call graph mutations, we may have invalid SCCs or SCCs from; // other RefSCCs in the worklist. The invalid ones are dead and the; // other RefSCCs should be queued above, so we just need to skip both; // scenarios here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp:167,queue,queued,167,interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp,1,['queue'],['queued']
Performance,"// Due to hash collisions, it can happen that we load another template; // specialization with the same hash. This is fine, as long as the next; // call to findSpecializationImpl does not find a matching Decl for the; // template arguments.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclTemplate.cpp:49,load,load,49,interpreter/llvm-project/clang/lib/AST/DeclTemplate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclTemplate.cpp,1,['load'],['load']
Performance,"// Due to isTypeDesirableForOp, we won't always shrink a load truncated to; // i16. So shrink it ourselves if we can make a broadcast_load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// Due to parsing error, we either went over the cached tokens or; // there are still cached tokens left. If it's the latter case skip the; // leftover tokens.; // Since this is an uncommon situation that should be avoided, use the; // expensive isBeforeInTranslationUnit call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseObjc.cpp:49,cache,cached,49,interpreter/llvm-project/clang/lib/Parse/ParseObjc.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseObjc.cpp,2,['cache'],['cached']
Performance,"// Due to the ISEL shortcoming noted above, be conservative if this op is; // likely to be selected as part of a load-modify-store instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:113,load,load-modify-store,113,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,1,['load'],['load-modify-store']
Performance,"// Dummy for backward compatability.; // Loading of libraries and initialization of graphics objects is; // done in TApplication::LoadGraphicsLibs().",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/gui/src/InitGui.cxx:41,Load,Loading,41,core/gui/src/InitGui.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/gui/src/InitGui.cxx,2,['Load'],"['LoadGraphicsLibs', 'Loading']"
Performance,// Dump loaded SLocEntries.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:8,load,loaded,8,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['load'],['loaded']
Performance,// Dump the coverage mapping data for this function by decoding the; // encoded data. This allows us to dump the mapping regions which were; // also processed by the CoverageMappingWriter which performs; // additional minimization operations such as reducing the number of; // expressions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CoverageMappingGen.cpp:194,perform,performs,194,interpreter/llvm-project/clang/lib/CodeGen/CoverageMappingGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CoverageMappingGen.cpp,1,['perform'],['performs']
Performance,"// During a tail call, stores to the argument area must happen after all of; // the function's incoming arguments have been loaded because they may alias.; // This is done by folding in a TokenFactor from LowerFormalArguments, but; // there's no point in doing so repeatedly so this tracks whether that's; // happened yet.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:124,load,loaded,124,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['load'],['loaded']
Performance,"// During context-sensitive analysis, a struct may be allocated in one; // function, but its field accessed in a function lower in the stack than; // the allocation. Since we only collect fields used in the function where; // the allocation occurs, we can't apply that filter when performing; // context-sensitive analysis. But, this only applies to storage locations,; // since field access it not allowed to fail. In contrast, field *values*; // don't need this allowance, since the API allows for uninitialized fields.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/FlowSensitive/DataflowAnalysisContext.cpp:281,perform,performing,281,interpreter/llvm-project/clang/lib/Analysis/FlowSensitive/DataflowAnalysisContext.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/FlowSensitive/DataflowAnalysisContext.cpp,1,['perform'],['performing']
Performance,"// During deserialization, we might compare variables before we load; // their types. Assume the types will end up being the same.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ASTContext.cpp:64,load,load,64,interpreter/llvm-project/clang/lib/AST/ASTContext.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ASTContext.cpp,1,['load'],['load']
Performance,"// During the ThinLTO backend phase we perform early indirect call promotion; // here, before globalopt. Otherwise imported available_externally functions; // look unreferenced and are removed. If we are going to load the sample; // profile then defer until later.; // TODO: See if we can move later and consolidate with the location where; // we perform ICP when we are loading a sample profile.; // TODO: We pass HasSampleProfile (whether there was a sample profile file; // passed to the compile) to the SamplePGO flag of ICP. This is used to; // determine whether the new direct calls are annotated with prof metadata.; // Ideally this should be determined from whether the IR is annotated with; // sample profile, and not whether the a sample profile was provided on the; // command line. E.g. for flattened profiles where we will not be reloading; // the sample profile in the ThinLTO backend, we ideally shouldn't have to; // provide the sample profile file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:39,perform,perform,39,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,4,"['load', 'perform']","['load', 'loading', 'perform']"
Performance,"// During this forward scan, at some point it needs to answer the question; // ""given a pointer to an MI in the current BB, is it located before or; // after the current instruction"".; // To perform this, the following set keeps track of the MIs already seen; // during the scan, if a MI is not in the set, it is assumed to be located; // after. Newly created MIs have to be inserted in the set as well.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp:191,perform,perform,191,interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,1,['perform'],['perform']
Performance,"// Dynamic information is required to be stripped for comparisons,; // because it could leak the dynamic information. Based on comparisons; // of pointers to dynamic objects, the optimizer can replace one pointer; // with another, which might be incorrect in presence of invariant; // groups. Comparison with null is safe because null does not carry any; // dynamic information.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:179,optimiz,optimizer,179,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['optimiz'],['optimizer']
Performance,"// Dynamic load path for rootmap files.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.h:11,load,load,11,core/metacling/src/TCling.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.h,1,['load'],['load']
Performance,"// E.g. ""PT_LOAD"" -> ""LOAD"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/ELFDumper.cpp:22,LOAD,LOAD,22,interpreter/llvm-project/llvm/tools/llvm-readobj/ELFDumper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/ELFDumper.cpp,1,['LOAD'],['LOAD']
Performance,"// EAR can only load the low subregister so us a shift for %a0 to produce; // the GR containing %a0 and %a1.; // ear <reg>, %a0",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,1,['load'],['load']
Performance,// ELF header for loadable partition.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/ELF.h:18,load,loadable,18,interpreter/llvm-project/llvm/include/llvm/BinaryFormat/ELF.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/ELF.h,1,['load'],['loadable']
Performance,"// END ROOT PCMS; // The rest of the lines are not changed to leave in place the system which; // works with bulk header parsing on library load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/rootcling_impl.cxx:140,load,load,140,core/dictgen/src/rootcling_impl.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/rootcling_impl.cxx,1,['load'],['load']
Performance,"// EVT workaround end; // Perform PCA and put it into PCAed events tree",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/VariablePCATransform.cxx:26,Perform,Perform,26,tmva/tmva/src/VariablePCATransform.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/VariablePCATransform.cxx,1,['Perform'],['Perform']
Performance,"// EWOULDBLOCK; //; // The OOB data has not yet arrived: flush the input stream; //; // In some systems (Solaris) regular recv() does not return upon; // receipt of the oob byte, which makes the below call to recv(); // block indefinitely if there are no other data in the queue.; // FIONREAD ioctl can be used to check if there are actually any; // data to be flushed. If not, wait for a while for the oob byte; // to arrive and try to read it again.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationRemote.cxx:273,queue,queue,273,net/net/src/TApplicationRemote.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationRemote.cxx,4,['queue'],['queue']
Performance,"// EXTLOAD:i24 -> ZEXTLOAD:i16 | (shl EXTLOAD@+2:i8, 16); // Load the bottom RoundWidth bits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:61,Load,Load,61,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['Load'],['Load']
Performance,// EXTRACT_SUBVECTOR can be used to extract a fixed-width vector from; // a scalable vector. But we don't want to match the case.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:76,scalab,scalable,76,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['scalab'],['scalable']
Performance,// EXTRACT_VECTOR_ELT of out-of-bounds element is an UNDEF for fixed length; // vectors. For scalable vectors we will provide appropriate support for; // dealing with arbitrary indices.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:93,scalab,scalable,93,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// EXTRACT_VECTOR_ELT of v1iX EXTRACT_SUBVECTOR could be formed; // when vector types are scalarized and v1iX is legal.; // vextract (v1iX extract_subvector(vNiX, Idx)) -> vextract(vNiX,Idx).; // Here we are completely ignoring the extract element index (N2),; // which is fine for fixed width vectors, since any index other than 0; // is undefined anyway. However, this cannot be ignored for scalable; // vectors - in theory we could support this, but we don't want to do this; // without a profitability check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:393,scalab,scalable,393,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// EXTRACT_VECTOR_ELT performs an implicit any_ext; BUILD_VECTOR an implicit; // trunc. So only std::min(SrcBits, DestBits) actually get defined in this; // segment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,perform,performs,22,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['perform'],['performs']
Performance,// Each argument needs to either be loaded into a register or onto the stack.; // Some arguments will only be loaded into the stack once the argument; // registers are filled.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp:36,load,loaded,36,interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,2,['load'],['loaded']
Performance,// Each irreducible loop within the unloop induces a round of iteration using; // the DFS result cached by Traversal.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopInfo.cpp:97,cache,cached,97,interpreter/llvm-project/llvm/lib/Analysis/LoopInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopInfo.cpp,1,['cache'],['cached']
Performance,// Each load found for the pattern. There should be one for each RegsToVisit.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,// Each load/store unit costs 1.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['load'],['load']
Performance,// Each loaded element must be the correct fractional portion of the; // requested vector load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:8,load,loaded,8,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],"['load', 'loaded']"
Performance,"// Each output incurs a load after the call, so we add that to the cost.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,1,['load'],['load']
Performance,"// Each processor has a SchedClassDesc table with an entry for each SchedClass.; // The SchedClassDesc table indexes into a global write resource table, write; // latency table, and read advance table.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SubtargetEmitter.cpp:163,latency,latency,163,interpreter/llvm-project/llvm/utils/TableGen/SubtargetEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SubtargetEmitter.cpp,1,['latency'],['latency']
Performance,// Each thread gets an independent copy of a VFS to allow different; // concurrent working directories.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/AllTUsExecution.cpp:72,concurren,concurrent,72,interpreter/llvm-project/clang/lib/Tooling/AllTUsExecution.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/AllTUsExecution.cpp,1,['concurren'],['concurrent']
Performance,// Earliest instruction-order load in the pattern.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,// Early check if we directly have ext(load).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['load'],['load']
Performance,// Early checks for performance reason.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngine.cpp:20,perform,performance,20,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngine.cpp,1,['perform'],['performance']
Performance,"// Early exit branch if difference found to EndBlock. Otherwise, continue to; // next LoadCmpBlock,",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:86,Load,LoadCmpBlock,86,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['Load'],['LoadCmpBlock']
Performance,"// Early exit branch if difference found to ResultBlock. Otherwise, continue; // to next LoadCmpBlock or EndBlock.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:89,Load,LoadCmpBlock,89,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['Load'],['LoadCmpBlock']
Performance,"// Early exit branch if difference found to ResultBlock. Otherwise,; // continue to next LoadCmpBlock or EndBlock.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:89,Load,LoadCmpBlock,89,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['Load'],['LoadCmpBlock']
Performance,// Early exit if no bottlenecks were found during the simulation.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/BottleneckAnalysis.cpp:20,bottleneck,bottlenecks,20,interpreter/llvm-project/llvm/tools/llvm-mca/Views/BottleneckAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/BottleneckAnalysis.cpp,1,['bottleneck'],['bottlenecks']
Performance,// Early exit if we found an invalid latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCSchedule.cpp:37,latency,latency,37,interpreter/llvm-project/llvm/lib/MC/MCSchedule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCSchedule.cpp,1,['latency'],['latency']
Performance,// Early failures in LoadFromCommandLine may return with ErrUnit unset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp:21,Load,LoadFromCommandLine,21,interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,1,['Load'],['LoadFromCommandLine']
Performance,"// Eat the identifier.; // In this context, we convert the register operand into; // a plain ""%asi"" token since the register access is already; // implicit in the instruction definition and encoding.; // See LoadASI/StoreASI in SparcInstrInfo.td.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp:208,Load,LoadASI,208,interpreter/llvm-project/llvm/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp,1,['Load'],['LoadASI']
Performance,// Either a load from immediate instruction or X0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,1,['load'],['load']
Performance,"// Either apply -debugify/-check-debugify before/after each pass and collect; // debug info loss statistics, or collect and check original debug info in; // the optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/Debugify.h:161,optimiz,optimizations,161,interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/Debugify.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/Debugify.h,1,['optimiz'],['optimizations']
Performance,// Either lookup a split load or create one.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,"// Either the class is not loaded or the data member is gone",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx:27,load,loaded,27,io/io/src/TStreamerInfo.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx,2,['load'],['loaded']
Performance,// Element swapping load/store. Same operands as regular load/store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.h:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.h,2,['load'],['load']
Performance,// Elide the copying store if the target loaded this argument from a; // suitable fixed stack object.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:41,load,loaded,41,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['load'],['loaded']
Performance,// Eliminate CleanupDestSlot alloca by replacing it with SSA values and; // PHIs if the current function is a coroutine. We don't do it for all; // functions as it may result in slight increase in numbers of instructions; // if compiled with no optimizations. We do it for coroutine as the lifetime; // of CleanupDestSlot alloca make correct coroutine frame building very; // difficult.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp:245,optimiz,optimizations,245,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,1,['optimiz'],['optimizations']
Performance,// Eliminate loads by forwarding stores from the previous iteration to loads; // of the current iteration.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:13,load,loads,13,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,2,['load'],['loads']
Performance,"// Eliminate redundant instructions; //; // This transformation will identify instructions where the output register; // is the same as one of its input registers. This only works on instructions; // that define a single register (unlike post-increment loads, for example).; // The equality check is actually more detailed: the code calculates which; // bits of the output are used, and only compares these bits with the input; // registers.; // If the output matches an input, the instruction is replaced with COPY.; // The copies will be removed by another transformation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp:253,load,loads,253,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,1,['load'],['loads']
Performance,"// Eliminating the dirty entry from 'Cache', so update the reverse info.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:37,Cache,Cache,37,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,['Cache'],['Cache']
Performance,// Else it's a plain old load/store with no offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyISelDAGToDAG.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// Elt has been copied in case it's an internal reference, side-stepping; // reference invalidation problems without losing the realloc optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/SmallVector.h:136,optimiz,optimization,136,interpreter/llvm-project/llvm/include/llvm/ADT/SmallVector.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/SmallVector.h,1,['optimiz'],['optimization']
Performance,"// Emit ""default queue"" and ""completion action"" arguments if enqueue kernel is; // used, otherwise emit dummy ""none"" arguments.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUHSAMetadataStreamer.cpp:17,queue,queue,17,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUHSAMetadataStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUHSAMetadataStreamer.cpp,1,['queue'],['queue']
Performance,// Emit a combined atomicrmw load/store operation for the interlocked; // intrinsics.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:29,load,load,29,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['load']
Performance,// Emit a load instruction and replace the use of the output value; // with it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,1,['load'],['load']
Performance,// Emit a load instruction and replace uses of the output value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,1,['load'],['load']
Performance,"// Emit a load instruction if possible, returning true if we succeeded,; // otherwise false. See commentary below for how the register class of; // the load is determined.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,2,['load'],['load']
Performance,// Emit a load of a LValue of matrix type. This may require casting the pointer; // to memory address (ArrayType) to a pointer to the value type (VectorType).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:10,load,load,10,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,// Emit a maximum of 4 loads in Thumb1 since we have fewer registers,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSelectionDAGInfo.cpp,1,['load'],['loads']
Performance,"// Emit a package option.; //; // CHECKER_OPTION(OPTIONTYPE, CHECKERNAME, OPTIONNAME, DESCRIPTION, DEFAULT); // - OPTIONTYPE: Type of the option, whether it's integer or boolean etc.; // This is important for validating user input. Note that; // it's a string, rather than an actual type: since we can; // load checkers runtime, we can't use template hackery for; // sorting this out compile-time.; // - CHECKERNAME: Name of the package.; // - OPTIONNAME: Name of the option.; // - DESCRIPTION; // - DEFAULT: The default value for this option.; //; // The full option can be specified in the command like this:; // -analyzer-config CHECKERNAME:OPTIONNAME=VALUE",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/TableGen/ClangSACheckersEmitter.cpp:306,load,load,306,interpreter/llvm-project/clang/utils/TableGen/ClangSACheckersEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/TableGen/ClangSACheckersEmitter.cpp,1,['load'],['load']
Performance,"// Emit a package option.; //; // PACKAGE_OPTION(OPTIONTYPE, PACKAGENAME, OPTIONNAME, DESCRIPTION, DEFAULT); // - OPTIONTYPE: Type of the option, whether it's integer or boolean etc.; // This is important for validating user input. Note that; // it's a string, rather than an actual type: since we can; // load checkers runtime, we can't use template hackery for; // sorting this out compile-time.; // - PACKAGENAME: Name of the package.; // - OPTIONNAME: Name of the option.; // - DESCRIPTION; // - DEFAULT: The default value for this option.; //; // The full option can be specified in the command like this:; // -analyzer-config PACKAGENAME:OPTIONNAME=VALUE",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/TableGen/ClangSACheckersEmitter.cpp:306,load,load,306,interpreter/llvm-project/clang/utils/TableGen/ClangSACheckersEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/utils/TableGen/ClangSACheckersEmitter.cpp,1,['load'],['load']
Performance,// Emit a plain load for the non-interlocked intrinsics.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:16,load,load,16,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['load']
Performance,// Emit a reg-reg copy so we don't propagate cached known bits information; // with the wrong VT if we fall out of fast isel after selecting this.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp:45,cache,cached,45,interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,1,['cache'],['cached']
Performance,// Emit a remark if there are stores to floats that required a floating point; // extension. If the vectorized loop was generated with floating point there; // will be a performance penalty from the conversion overhead and the change in; // the vector width.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:170,perform,performance,170,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['perform'],['performance']
Performance,// Emit a sequence of copyto/copyfrom virtual registers for arguments that; // might overwrite each other in case of tail call optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:127,optimiz,optimization,127,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['optimiz'],['optimization']
Performance,// Emit a store of the return value through the virtual register.; // Leave Outs empty so that LowerReturn won't try to load return; // registers the usual way.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:120,load,load,120,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['load'],['load']
Performance,"// Emit additional code that is required to explicitly handle the stack in; // HiPE native code (if needed) when loaded in the Erlang/OTP runtime. The; // approach is rather similar to that of Segmented Stacks, but it uses a; // different conditional check and another BIF for allocating more stack; // space.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp:113,load,loaded,113,interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp,1,['load'],['loaded']
Performance,"// Emit all abbrevs upfront, so that the reader can jump in the middle of the; // block and load any metadata.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Writer/BitcodeWriter.cpp:92,load,load,92,interpreter/llvm-project/llvm/lib/Bitcode/Writer/BitcodeWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Writer/BitcodeWriter.cpp,2,['load'],['load']
Performance,// Emit branch probability as optimization remarks.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/PGOInstrumentation.cpp:30,optimiz,optimization,30,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/PGOInstrumentation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/PGOInstrumentation.cpp,1,['optimiz'],['optimization']
Performance,// Emit code before MBBI to load immediate value into physical register Reg.; // Returns an iterator to the new instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARC/ARCInstrInfo.h,2,['load'],['load']
Performance,// Emit code to load the value if it was passed in memory.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/SystemZ.cpp:16,load,load,16,interpreter/llvm-project/clang/lib/CodeGen/Targets/SystemZ.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/SystemZ.cpp,2,['load'],['load']
Performance,// Emit code to load the value if it was passed in registers.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/SystemZ.cpp:16,load,load,16,interpreter/llvm-project/clang/lib/CodeGen/Targets/SystemZ.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/SystemZ.cpp,2,['load'],['load']
Performance,// Emit equivalent scalable vector gather.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,scalab,scalable,19,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,// Emit equivalent scalable vector scatter.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:19,scalab,scalable,19,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// Emit following codes. It is not possible to insert multiple; // BasicBlocks in PEI pass, so we emit two pseudo instructions here.; //; // EXTEND_STACK // pseudo instrcution; // EXTEND_STACK_GUARD // pseudo instrcution; //; // EXTEND_STACK pseudo will be converted by ExpandPostRA pass into; // following instructions with multiple basic blocks later.; //; // thisBB:; // brge.l.t %sp, %sl, sinkBB; // syscallBB:; // ld %s61, 0x18(, %tp) // load param area; // or %s62, 0, %s0 // spill the value of %s0; // lea %s63, 0x13b // syscall # of grow; // shm.l %s63, 0x0(%s61) // store syscall # at addr:0; // shm.l %sl, 0x8(%s61) // store old limit at addr:8; // shm.l %sp, 0x10(%s61) // store new limit at addr:16; // monc // call monitor; // or %s0, 0, %s62 // restore the value of %s0; // sinkBB:; //; // EXTEND_STACK_GUARD pseudo will be simply eliminated by ExpandPostRA; // pass. This pseudo is required to be at the next of EXTEND_STACK; // pseudo in order to protect iteration loop in ExpandPostRA.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEFrameLowering.cpp:443,load,load,443,interpreter/llvm-project/llvm/lib/Target/VE/VEFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEFrameLowering.cpp,1,['load'],['load']
Performance,// Emit intrinsic responsible for updating the global bitmap corresponding to; // a boolean expression. The index being set is based on the value loaded; // from a pointer to a dedicated temporary value on the stack that is itself; // updated via emitMCDCCondBitmapReset() and emitMCDCCondBitmapUpdate(). The; // index represents an executed test vector.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenPGO.cpp:146,load,loaded,146,interpreter/llvm-project/clang/lib/CodeGen/CodeGenPGO.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenPGO.cpp,1,['load'],['loaded']
Performance,// Emit output using the new pass manager for the optimization pipeline.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp:50,optimiz,optimization,50,interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,1,['optimiz'],['optimization']
Performance,"// Emit store for the initial gc value. Store must be inserted after load,; // otherwise store will be in alloca's use list and an extra load will be; // inserted before it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp:69,load,load,69,interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,2,['load'],['load']
Performance,"// Emit subscript expressions in rvalue context's. For most cases, this just; // loads the lvalue formed by the subscript expr. However, we have to be; // careful, because the base of a vector subscript is occasionally an rvalue,; // so we can't get it as an lvalue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:81,load,loads,81,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['load'],['loads']
Performance,"// Emit the 5-insn large address load sequence with the `%got_pc` family; // of relocs, loading the result from GOT with `ldx.d` in the end.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,2,['load'],"['load', 'loading']"
Performance,"// Emit the 5-insn large address load sequence with the `%got_pc` family; // of relocs, with the `pcalau12i` insn relocated with `%gd_pc_hi20`.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,1,['load'],['load']
Performance,"// Emit the 5-insn large address load sequence with the `%got_pc` family; // of relocs, with the `pcalau12i` insn relocated with `%ld_pc_hi20`.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,1,['load'],['load']
Performance,"// Emit the 5-insn large address load sequence with the `%ie_pc` family; // of relocs, loading the result with `ldx.d` in the end.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,2,['load'],"['load', 'loading']"
Performance,// Emit the 5-insn large address load sequence with the `%pc` family of; // relocs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,1,['load'],['load']
Performance,"// Emit the 5-insn large address load sequence, either directly or; // indirectly in case of going through the GOT, then JIRL_TAIL or; // JIRL_CALL to $addr.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchExpandPseudoInsts.cpp,1,['load'],['load']
Performance,"// Emit the COPY_FD pseudo instruction.; //; // copy_fd_pseudo $fd, $ws, n; // =>; // splati.d $wt, $ws, $n; // copy $fd, $wt:sub_64; //; // When n is zero, the equivalent operation can be performed with (potentially); // zero instructions due to register overlaps. This optimization is always; // valid because FR=1 mode which is the only supported mode in MSA.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp:189,perform,performed,189,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"// Emit the COPY_FW pseudo instruction.; //; // copy_fw_pseudo $fd, $ws, n; // =>; // copy_u_w $rt, $ws, $n; // mtc1 $rt, $fd; //; // When n is zero, the equivalent operation can be performed with (potentially); // zero instructions due to register overlaps. This optimization is never valid; // for lane 1 because it would require FR=0 mode which isn't supported by MSA.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp:182,perform,performed,182,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"// Emit the ID's for all the instructions that are matched by this rule.; // TODO: Limit this to matched instructions that mayLoad/mayStore or have; // some other means of having a memoperand. Also limit this to; // emitted instructions that expect to have a memoperand too. For; // example, (G_SEXT (G_LOAD x)) that results in separate load and; // sign-extend instructions shouldn't put the memoperand on the; // sign-extend since it has no effect there.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelMatchTable.cpp:337,load,load,337,interpreter/llvm-project/llvm/utils/TableGen/GlobalISelMatchTable.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelMatchTable.cpp,1,['load'],['load']
Performance,"// Emit the LD_F16_PSEDUO instruction to load a f16 value into an MSA register.; //; // LD_F16 MSA128F16:$wd, mem_simm10:$addr; // =>; // lh $rtemp, $addr; // fill.h $wd, $rtemp; //; // Safety: We can't use ld.h & co as they over-read from the source.; // Additionally, if the address is not modulo 16, 2 cases can occur:; // a) Segmentation fault as the load instruction reads from a memory page; // memory it's not supposed to.; // b) The load crosses an implementation specific boundary, requiring OS; // intervention.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,3,['load'],['load']
Performance,// Emit the LHS and perform the store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:20,perform,perform,20,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['perform'],['perform']
Performance,"// Emit the code; // If all cases cover a contiguous range, it is not necessary to jump to; // the default block after the last bit test fails. This is because the; // range check during bit test header creation has guaranteed that every; // case here doesn't go outside the range. In this case, there is no need; // to perform the last bit test, as it will always be true. Instead, make; // the second-to-last bit-test fall through to the target of the last bit; // test, and delete the last bit test.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp:320,perform,perform,320,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,1,['perform'],['perform']
Performance,// Emit the declarations for the functions that will perform lookup.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SearchableTableEmitter.cpp:53,perform,perform,53,interpreter/llvm-project/llvm/utils/TableGen/SearchableTableEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SearchableTableEmitter.cpp,1,['perform'],['perform']
Performance,// Emit the label that immediately follows the PLDpc for a link time GOT PC Rel; // optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCELFStreamer.cpp:84,optimiz,optimization,84,interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCELFStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCELFStreamer.cpp,1,['optimiz'],['optimization']
Performance,// Emit the load instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,// Emit the load or broadcast instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,// Emit the load or store with the adjusted base and offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,1,['load'],['load']
Performance,// Emit the load with the adjusted base and offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MCTargetDesc/MipsTargetStreamer.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/Mips/MCTargetDesc/MipsTargetStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MCTargetDesc/MipsTargetStreamer.cpp,1,['load'],['load']
Performance,"// Empty base optimization is required for standard layout types (since C++11)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_types.cxx:14,optimiz,optimization,14,tree/ntuple/v7/test/ntuple_types.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_types.cxx,1,['optimiz'],['optimization']
Performance,"// Empty file to test TProof::Load in runProof.C and StressProof.cxx in; // conjunction with ProcFileElements and ProofEventProc .",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/proof/EmptyInclude.h:30,Load,Load,30,tutorials/proof/EmptyInclude.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/proof/EmptyInclude.h,1,['Load'],['Load']
Performance,"// Empty list of cached functions",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx:17,cache,cached,17,roofit/roofitcore/src/RooTreeDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx,1,['cache'],['cached']
Performance,"// Empty struct for cuda architecture; // void * cudnnWorkspace = nullptr); // Remains nullptr for cuda architecture; /** @name Backward Propagation in Convolutional Layer; */; ///@{; /** Perform the complete backward propagation step in a Convolutional Layer.; * If the provided \p activationGradientsBackward matrix is not empty, compute the; * gradients of the objective function with respect to the activations; * of the previous layer (backward direction).; * Also compute the weight and the bias gradients. Modifies the values; * in \p df and thus produces only a valid result, if it is applied the; * first time after the corresponding forward propagation has been per-; * formed. */",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/DNN/Architectures/Cpu.h:188,Perform,Perform,188,tmva/tmva/inc/TMVA/DNN/Architectures/Cpu.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/DNN/Architectures/Cpu.h,1,['Perform'],['Perform']
Performance,"// Empty struct for cuda architecture; //void * cudnnWorkspace = nullptr); // Remains nullptr for cuda architecture; /** @name Backward Propagation in Convolutional Layer; */; ///@{; /** Perform the complete backward propagation step in a Convolutional Layer.; * If the provided \p activationGradientsBackward matrix is not empty, compute the; * gradients of the objective function with respect to the activations; * of the previous layer (backward direction).; * Also compute the weight and the bias gradients. Modifies the values; * in \p df and thus produces only a valid result, if it is applied the; * first time after the corresponding forward propagation has been per-; * formed. */",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/DNN/Architectures/Cuda.h:187,Perform,Perform,187,tmva/tmva/inc/TMVA/DNN/Architectures/Cuda.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/DNN/Architectures/Cuda.h,1,['Perform'],['Perform']
Performance,// Enable Mode register optimization,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:24,optimiz,optimization,24,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,1,['optimiz'],['optimization']
Performance,// Enable SubRegLiveness for MVE to better optimize s subregs for mqpr regs; // and q subregs for qqqqpr regs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSubtarget.cpp:43,optimiz,optimize,43,interpreter/llvm-project/llvm/lib/Target/ARM/ARMSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSubtarget.cpp,1,['optimiz'],['optimize']
Performance,"// Enable dynamic opt level switching.; // Set up inlining, even if we switch to O0 later: some transactions' code; // might pass `#pragma cling optimize` levels that require it. This is; // adjusted per transaction in IncrementalParser::codeGenTransaction().",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp:145,optimiz,optimize,145,interpreter/cling/lib/Interpreter/CIFactory.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp,1,['optimiz'],['optimize']
Performance,"// Enable optimized sending of streamer infos to use embedded backward/forward; // compatibility support between different ROOT versions and different versions of; // users classes",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProof.cxx:10,optimiz,optimized,10,proof/proof/src/TProof.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProof.cxx,2,['optimiz'],['optimized']
Performance,// Enable testing of whole program devirtualization on this module by invoking; // the facility for updating public visibility to linkage unit visibility when; // specified by an internal option. This is normally done during LTO which is; // not performed via opt.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/opt.cpp:246,perform,performed,246,interpreter/llvm-project/llvm/tools/opt/opt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/opt.cpp,1,['perform'],['performed']
Performance,"// Enable the binned likelihood optimization to avoid integrals; // (like in HistFactory).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/testTestStatistics.cxx:32,optimiz,optimization,32,roofit/roofitcore/test/testTestStatistics.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/testTestStatistics.cxx,1,['optimiz'],['optimization']
Performance,"// Enable the performance tree",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProof.h:14,perform,performance,14,proof/proof/inc/TProof.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProof.h,1,['perform'],['performance']
Performance,// Enable the skip-parsed-bodies optimization only for C++; this may be; // revisited.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/Indexing.cpp:33,optimiz,optimization,33,interpreter/llvm-project/clang/tools/libclang/Indexing.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/Indexing.cpp,1,['optimiz'],['optimization']
Performance,// Enable vectorization per default according to the optimization level; // selected. For optimization levels that want vectorization we use the alias; // option to simplify the hasFlag logic.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp:53,optimiz,optimization,53,interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,2,['optimiz'],['optimization']
Performance,"// Enabling or Disabling the latency heuristic is a close call: It seems to; // help nearly no benchmark on out-of-order architectures, on the other hand; // it regresses register pressure on a few benchmarking.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:29,latency,latency,29,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,1,['latency'],['latency']
Performance,// Encode imm for the hash load/store to stack for the ROP Protection; // instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCMCCodeEmitter.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCMCCodeEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCMCCodeEmitter.cpp,1,['load'],['load']
Performance,// Encoded stack offset of load/store instruction; decoding varies by mode.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/ARMWinEHPrinter.cpp:27,load,load,27,interpreter/llvm-project/llvm/tools/llvm-readobj/ARMWinEHPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/ARMWinEHPrinter.cpp,1,['load'],['load']
Performance,// End the current packet and reset the state of the packetizer.; // Overriding this function allows the target-specific packetizer; // to perform custom finalization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/DFAPacketizer.h:139,perform,perform,139,interpreter/llvm-project/llvm/include/llvm/CodeGen/DFAPacketizer.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/DFAPacketizer.h,1,['perform'],['perform']
Performance,"// End: Loop over channels; // Successfully initialized the cache; // Printing some info; /*; std::map< std::string, std::vector< BarlowCache > >::iterator iter_cache;; for( iter_cache = _barlowCache.begin(); iter_cache != _barlowCache.end(); ++iter_cache ) {. std::string channel_name = (*iter_cache).first;; std::vector< BarlowCache >& channel_cache = (*iter_cache).second;. for( unsigned int i = 0; i < channel_cache.size(); ++i ) {; BarlowCache& bin_cache = channel_cache.at(i);. RooRealVar* gamma = bin_cache.gamma;; RooRealVar* tau = bin_cache.tau;; RooAbsReal* pois_mean = bin_cache.nom_pois_mean;; RooAbsPdf* sum_pdf = (RooAbsPdf*) bin_cache.sumPdf;; double binVolume = bin_cache.binVolume;. if( !bin_cache.hasStatUncert ) {; std::cout << ""Barlow Cache for Channel: "" << channel_name; << "" Bin: "" << i; << "" NO STAT UNCERT""; << std::endl;; }; else {; std::cout << ""Barlow Cache for Channel: "" << channel_name; << "" Bin: "" << i; << "" gamma: "" << gamma->GetName(); << "" tau: "" << tau->GetName(); << "" pois_mean: "" << pois_mean->GetName(); << "" sum_pdf: "" << sum_pdf->GetName(); << "" binVolume: "" << binVolume; << std::endl;; }. }; }; */",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/RooBarlowBeestonLL.cxx:60,cache,cache,60,roofit/histfactory/src/RooBarlowBeestonLL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/RooBarlowBeestonLL.cxx,3,"['Cache', 'cache']","['Cache', 'cache']"
Performance,// Ensure IRB insertion point is after loads for shadow and origin.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:39,load,loads,39,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['load'],['loads']
Performance,// Ensure all SLocEntries are loaded from the external source.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:30,load,loaded,30,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['load'],['loaded']
Performance,// Ensure both input are loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:25,load,loads,25,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loads']
Performance,// Ensure each load is in the same MBB.; // TODO: Support multiple MachineBasicBlocks.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// Ensure relocation global won't appear in PHI node; // This may happen if the compiler generated the following code:; // B1:; // g1 = @llvm.skb_buff:0:1...; // ...; // goto B_COMMON; // B2:; // g2 = @llvm.skb_buff:0:2...; // ...; // goto B_COMMON; // B_COMMON:; // g = PHI(g1, g2); // x = load g; // ...; // If anything likes the above ""g = PHI(g1, g2)"", issue a fatal error.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFCheckAndAdjustIR.cpp:291,load,load,291,interpreter/llvm-project/llvm/lib/Target/BPF/BPFCheckAndAdjustIR.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFCheckAndAdjustIR.cpp,1,['load'],['load']
Performance,// Ensure that LoadBasePtr is after StoreBasePtr or before StoreBasePtr; // for negative stride.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:15,Load,LoadBasePtr,15,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,1,['Load'],['LoadBasePtr']
Performance,// Ensure that LoadBasePtr is after StoreBasePtr or before StoreBasePtr; // for negative stride. LoadBasePtr shouldn't overlap with StoreBasePtr.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:15,Load,LoadBasePtr,15,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,2,['Load'],['LoadBasePtr']
Performance,// Ensure that an auto decl is deduced otherwise the checks below might cache; // the wrong linkage.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:72,cache,cache,72,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['cache'],['cache']
Performance,"// Ensure that cluster prefetching and smearing of read requests works; // Note that ""nClusterLoaded"" is the number of _partial_ clusters preloaded from storage. Because we read; // from the first cluster first px and then py, we'll call two times `LoadCluster()` on the first cluster.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_view.cxx:249,Load,LoadCluster,249,tree/ntuple/v7/test/ntuple_view.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_view.cxx,1,['Load'],['LoadCluster']
Performance,// Ensure that if any location op is undef that the dbg.vlue is not; // cached.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp:72,cache,cached,72,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,1,['cache'],['cached']
Performance,// Ensure that the HWS has not stored this instruction in its queues.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/Stages/ExecuteStage.cpp:62,queue,queues,62,interpreter/llvm-project/llvm/lib/MCA/Stages/ExecuteStage.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/Stages/ExecuteStage.cpp,1,['queue'],['queues']
Performance,// Ensure that the first byte is loaded from zero offset of the first load.; // So the combined value can be loaded from the first load address.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:33,load,loaded,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],"['load', 'loaded']"
Performance,"// Ensure that the function adheres to the forward progress guarantee, which; // is required by certain optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp:104,optimiz,optimizations,104,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,1,['optimiz'],['optimizations']
Performance,// Ensure that the load from the narrow width is being zero extended to i128.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// Ensure that the pointer is non-null before loading it. If there is no; // compile-time guarantee, reuse the run-time null check or emit a new one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:46,load,loading,46,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['loading']
Performance,// Ensure that there are no instructions between the PHI and the load that; // could store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:65,load,load,65,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,// Ensure that we've loaded all potentially-interesting declarations; // that need to be eagerly loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:21,load,loaded,21,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,2,['load'],['loaded']
Performance,// Ensure the entire stack is aligned to at least the RVV requirement: some; // scalable-vector object alignments are not considered by the; // target-independent code.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp:80,scalab,scalable-vector,80,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,1,['scalab'],['scalable-vector']
Performance,"// Ensure the register format is LOAD <reg>, <reg>, 0",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFMISimplifyPatchable.cpp:33,LOAD,LOAD,33,interpreter/llvm-project/llvm/lib/Target/BPF/BPFMISimplifyPatchable.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFMISimplifyPatchable.cpp,1,['LOAD'],['LOAD']
Performance,"// Ensure we generate all loads for each tuple part, whilst updating the; // pointer after each load correctly using vscale.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,// Ensures that following loads will not see stale remote VMEM data or; // stale local VMEM data with MTYPE NC. Local VMEM data with MTYPE RW and; // CC will never be stale due to the local memory probes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,2,['load'],['loads']
Performance,// Ensures that following loads will not see stale remote date or local; // MTYPE NC global data. Local MTYPE RW and CC memory will never be stale; // due to the memory probes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,1,['load'],['loads']
Performance,// Enter a normal cleanup which will perform the @finally block.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGException.cpp:37,perform,perform,37,interpreter/llvm-project/clang/lib/CodeGen/CGException.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGException.cpp,1,['perform'],['perform']
Performance,"// Enter the CFG for Decl D, and perform any initial setup operations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Analysis/Analyses/ThreadSafetyCommon.h:33,perform,perform,33,interpreter/llvm-project/clang/include/clang/Analysis/Analyses/ThreadSafetyCommon.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Analysis/Analyses/ThreadSafetyCommon.h,1,['perform'],['perform']
Performance,// Enter the header-name token into the token stream; a Lex action cannot; // both return a token and cache tokens (doing so would corrupt the token; // cache if the call to Lex comes from CachingLex / PeekAhead).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp:102,cache,cache,102,interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,2,['cache'],['cache']
Performance,// Entries for NEON load/store information table. The table is sorted by; // PseudoOpc for fast binary-search lookups.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,1,['load'],['load']
Performance,// Enums from <mach-o/loader.h>,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/MachO.h:22,load,loader,22,interpreter/llvm-project/llvm/include/llvm/BinaryFormat/MachO.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/BinaryFormat/MachO.h,1,['load'],['loader']
Performance,// Epilogue vectorization is usually unprofitable - tail folding or; // a smaller VF would have been better. This a blunt hammer - we; // should re-examine this once vectorization is better tuned.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.h:190,tune,tuned,190,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.h,1,['tune'],['tuned']
Performance,"// Equivalence class key, the initial tuple by which we group loads/stores.; // Loads/stores with different EqClassKeys are never merged.; //; // (We could in theory remove element-size from the this tuple. We'd just need; // to fix up the vector packing/unpacking code.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp:62,load,loads,62,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,2,"['Load', 'load']","['Loads', 'loads']"
Performance,// Erase HVX groups on targets < HvxV62 (due to lack of predicated loads).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:67,load,loads,67,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,2,['load'],['loads']
Performance,"// Erase VarLocs which reside in one of the dead registers. For performance; // reasons, it's critical to not iterate over the full set of open VarLocs.; // Iterate over the set of dying/used regs instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/VarLocBasedImpl.cpp:64,perform,performance,64,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/VarLocBasedImpl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/VarLocBasedImpl.cpp,1,['perform'],['performance']
Performance,"// Erase everything that was computed in this iteration from the cache, so; // that no dangling references are left behind. We could be a bit smarter if; // we kept a dependency graph. It's probably not worth the complexity.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp:65,cache,cache,65,interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,1,['cache'],['cache']
Performance,// Erase old load/store instruction,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp,1,['load'],['load']
Performance,// Erase the object from LoadedObjectMap,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/IntelJITEventListener.cpp:25,Load,LoadedObjectMap,25,interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/IntelJITEventListener.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/IntelJITEventListener.cpp,1,['Load'],['LoadedObjectMap']
Performance,// Error if no more slots available for loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonShuffler.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonShuffler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonShuffler.cpp,1,['load'],['loads']
Performance,// Essentially boils down to performing an unaligned VMX load sequence so; // as to avoid crossing a page boundary and then shuffling the elements; // into the right side of the vector register.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:29,perform,performing,29,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,2,"['load', 'perform']","['load', 'performing']"
Performance,// Establish the correct scalable-vector types for any fixed-length type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp:25,scalab,scalable-vector,25,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,2,['scalab'],['scalable-vector']
Performance,"// Estimate register pressure to determine whether to LICM the instruction.; // In low register pressure situation, we can be more aggressive about; // hoisting. Also, favors hoisting long latency instructions even in; // moderately high pressure situation.; // Cheap instructions will only be hoisted if they don't increase register; // pressure at all.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:189,latency,latency,189,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,1,['latency'],['latency']
Performance,"// Estimate that each loaded source vector containing this Index; // requires one operation, except that vperm can handle two input; // registers first time for each dst vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:22,load,loaded,22,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,1,['load'],['loaded']
Performance,"// Estimate the number of operations the below transform will turn a; // constant multiply into. The number is approximately equal to the minimal; // number of powers of two that constant can be broken down to by adding; // or subtracting them.; //; // If we have taken more than 12[1] / 8[2] steps to attempt the; // optimization for a native sized value, it is more than likely that this; // optimization will make things worse.; //; // [1] MIPS64 requires 6 instructions at most to materialize any constant,; // multiplication requires at least 4 cycles, but another cycle (or two); // to retrieve the result from the HI/LO registers.; //; // [2] For MIPS32, more than 8 steps is expensive as the constant could be; // materialized in 2 instructions, multiplication requires at least 4; // cycles, but another cycle (or two) to retrieve the result from the; // HI/LO registers.; //; // TODO:; // - MaxSteps needs to consider the `VT` of the constant for the current; // target.; // - Consider to perform this optimization after type legalization.; // That allows to remove a workaround for types not supported natively.; // - Take in account `-Os, -Oz` flags because this optimization; // increases code size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp:318,optimiz,optimization,318,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,5,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,// Estimated cost of a load-hit-store delay. This was obtained; // experimentally as a minimum needed to prevent unprofitable; // vectorization for the paq8p benchmark. It may need to be; // raised further if other unprofitable cases remain.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp:23,load,load-hit-store,23,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,1,['load'],['load-hit-store']
Performance,"// EvalInstance(0) always needs to be called so that; // the proper branches are loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/splot/src/TSPlot.cxx:81,load,loaded,81,math/splot/src/TSPlot.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/splot/src/TSPlot.cxx,7,['load'],['loaded']
Performance,"// Evaluate and cache the common expression. We treat it as a temporary,; // even though it's not quite the same thing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:16,cache,cache,16,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['cache'],['cache']
Performance,"// Evaluate the cached variables and store the results",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx:16,cache,cached,16,roofit/roofitcore/src/RooTreeDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx,1,['cache'],['cached']
Performance,// Evaluate the expression representing the load address.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldChecker.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldChecker.cpp,1,['load'],['load']
Performance,"// Evaluates an XRay record and performs accounting on it.; //; // If the record is an ENTER record it pushes the FuncID and TSC onto a; // structure representing the call stack for that function.; // If the record is an EXIT record it checks computes computes the ammount of; // time the function took to complete and then stores that information in an; // edge of the graph. If there is no matching ENTER record the function tries; // to recover by assuming that there were EXIT records which were missed, for; // example caused by tail call elimination and if the option is enabled then; // then tries to recover from this.; //; // This function will also error if the records are out of order, as the trace; // is expected to be sorted.; //; // The graph generated has an immaginary root for functions called by no-one at; // FuncId 0.; //; // FIXME: Refactor this and account subcommand to reduce code duplication.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-xray/xray-graph.cpp:32,perform,performs,32,interpreter/llvm-project/llvm/tools/llvm-xray/xray-graph.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-xray/xray-graph.cpp,1,['perform'],['performs']
Performance,"// Evaluating in the trip count's type can not overflow here as the overflow; // checks are performed in checkOverflow, but are first tried to avoid by; // widening the IV.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFlatten.cpp:92,perform,performed,92,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFlatten.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFlatten.cpp,1,['perform'],['performed']
Performance,"// Even if the instruction is not marked as uniform, there are certain; // intrinsic calls that can be effectively treated as such, so we check for; // them here. Conservatively, we only do this for scalable vectors, since; // for fixed-width VFs we can always fall back on full scalarization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:199,scalab,scalable,199,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['scalab'],['scalable']
Performance,"// Even if the total offset is inbounds, we may end up representing it; // by first performing a larger negative offset, and then a smaller; // positive one. The large negative offset might go out of bounds. Only; // preserve inbounds if all signs are the same.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:84,perform,performing,84,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['perform'],['performing']
Performance,"// Even if there are multiple definitions of the vtable, they are required; // by the ABI to use the same symbol name, so should be merged at load; // time. However, if the class has hidden visibility, there can be; // different versions of the class in different modules, and the ABI; // library might treat them as being the same.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp:142,load,load,142,interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,1,['load'],['load']
Performance,"// Even if we didn't load the section, we need to record an entry for it; // to handle later processing (and by 'handle' I mean don't do anything; // with these sections).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:21,load,load,21,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['load']
Performance,"// Even though KnownVal is only used in the else branch of the next; // conditional, tryEvaluateBool performs additional checking on the; // Expr, so it should be called unconditionally.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CFG.cpp:101,perform,performs,101,interpreter/llvm-project/clang/lib/Analysis/CFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CFG.cpp,1,['perform'],['performs']
Performance,"// Even though it is technically only required when the computation graph; // is changed because global observables are taken from data, it is safer; // to clone the constraint model in general to reset the normalization; // integral caches and avoid ASAN build failures (the PDF of the main; // measurement is cloned too anyway, so not much overhead). This can be; // reconsidered after the caching of normalization sets by pointer is changed; // to a more memory-safe solution.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/FitHelpers.cxx:234,cache,caches,234,roofit/roofitcore/src/FitHelpers.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/FitHelpers.cxx,1,['cache'],['caches']
Performance,"// Even though this modifies IvarList, it's conceptually const:; // the ivar chain is essentially a cached property of ObjCInterfaceDecl.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/DeclObjC.h:100,cache,cached,100,interpreter/llvm-project/clang/include/clang/AST/DeclObjC.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/DeclObjC.h,1,['cache'],['cached']
Performance,"// Even though we have identified a concrete base (or a conflict) for all live; // pointers at this point, there are cases where the base is of an; // incompatible type compared to the original instruction. We conservatively; // mark those as conflicts to ensure that corresponding BDVs will be generated; // in the next steps.; // this is a rather explicit check for all cases where we should mark the; // state as a conflict to force the latter stages of the algorithm to emit; // the BDVs.; // TODO: in many cases the instructions emited for the conflicting states; // will be identical to the I itself (if the I's operate on their BDVs; // themselves). We should exploit this, but can't do it here since it would; // break the invariant about the BDVs not being known to be a base.; // TODO: the code also does not handle constants at all - the algorithm relies; // on all constants having the same BDV and therefore constant-only insns; // will never be in conflict, but this check is ignored here. If the; // constant conflicts will be to BDVs themselves, they will be identical; // instructions and will get optimized away (as in the above TODO)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp:1115,optimiz,optimized,1115,interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,1,['optimiz'],['optimized']
Performance,"// Even though we're optimising for size at the expense of performance,; // avoid creating really long predicated blocks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp:59,perform,performance,59,interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp,1,['perform'],['performance']
Performance,"// Every load must share the same base pointer; don't combine things like:; //; // a[i], b[i + 1] -> a bigger load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,2,['load'],['load']
Performance,"// Every other architecture would use shouldAssumeDSOLocal in here, but; // mips is special.; // * In PIC code mips requires got loads even for local statics!; // * To save on got entries, for local statics the got entry contains the; // page and an additional add instruction takes care of the low bits.; // * It is legal to access a hidden symbol with a non hidden undefined,; // so one cannot guarantee that all access to a hidden symbol will know; // it is hidden.; // * Mips linkers don't support creating a page and a full got entry for; // the same symbol.; // * Given all that, we have to use a full got entry for hidden symbols :-(",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:129,load,loads,129,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['loads']
Performance,"// Every thread loads/unloads a different library",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/test/TClingLoadUnloadFileTests.cxx:16,load,loads,16,core/metacling/test/TClingLoadUnloadFileTests.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/test/TClingLoadUnloadFileTests.cxx,1,['load'],['loads']
Performance,"// Everybody expects std::iostream to be available, so load it...",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationServer.cxx:55,load,load,55,net/net/src/TApplicationServer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationServer.cxx,3,['load'],['load']
Performance,"// Everything checks out. Create a fake body that just calls the block.; // This is basically just an AST dump of:; //; // void dispatch_sync(dispatch_queue_t queue, void (^block)(void)) {; // block();; // }; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/BodyFarm.cpp:159,queue,queue,159,interpreter/llvm-project/clang/lib/Analysis/BodyFarm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/BodyFarm.cpp,1,['queue'],['queue']
Performance,// Everything matched - assume that we can fold the whole sequence using; // load combining.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:77,load,load,77,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['load']
Performance,// Everything needs a load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:22,load,load,22,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,"// Everything under here is trying to match an extract of a loaded value.; // If the result of load has to be truncated, then it's not necessarily; // profitable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:60,load,loaded,60,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],"['load', 'loaded']"
Performance,// Evict entries from the binary cache until it is under the maximum size; // given in the options. Calling this invalidates references in the DI...; // objects returned by the methods above.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/Symbolize/Symbolize.h:33,cache,cache,33,interpreter/llvm-project/llvm/include/llvm/DebugInfo/Symbolize/Symbolize.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/Symbolize/Symbolize.h,1,['cache'],['cache']
Performance,// Evict the LRU binary until the max cache size is reached or there's <= 1; // item in the cache. The MRU binary is always kept to avoid thrashing if it's; // larger than the cache size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/Symbolize/Symbolize.cpp:38,cache,cache,38,interpreter/llvm-project/llvm/lib/DebugInfo/Symbolize/Symbolize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/Symbolize/Symbolize.cpp,3,['cache'],['cache']
Performance,"// Examine debug-info attached to the consecutive select instructions. They; // won't be individually optimised by optimizeInst, so we need to perform; // DPValue maintenence here instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:115,optimiz,optimizeInst,115,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,2,"['optimiz', 'perform']","['optimizeInst', 'perform']"
Performance,"// Examines the loop nesting of the Src and Dst; // instructions and establishes their shared loops. Sets the variables; // CommonLevels, SrcLevels, and MaxLevels.; // The source and destination instructions needn't be contained in the same; // loop. The routine establishNestingLevels finds the level of most deeply; // nested loop that contains them both, CommonLevels. An instruction that's; // not contained in a loop is at level = 0. MaxLevels is equal to the level; // of the source plus the level of the destination, minus CommonLevels.; // This lets us allocate vectors MaxLevels in length, with room for every; // distinct loop referenced in both the source and destination subscripts.; // The variable SrcLevels is the nesting depth of the source instruction.; // It's used to help calculate distinct loops referenced by the destination.; // Here's the map from loops to levels:; // 0 - unused; // 1 - outermost common loop; // ... - other common loops; // CommonLevels - innermost common loop; // ... - loops containing Src but not Dst; // SrcLevels - innermost loop containing Src but not Dst; // ... - loops containing Dst but not Src; // MaxLevels - innermost loops containing Dst but not Src; // Consider the follow code fragment:; // for (a = ...) {; // for (b = ...) {; // for (c = ...) {; // for (d = ...) {; // A[] = ...;; // }; // }; // for (e = ...) {; // for (f = ...) {; // for (g = ...) {; // ... = A[];; // }; // }; // }; // }; // }; // If we're looking at the possibility of a dependence between the store; // to A (the Src) and the load from A (the Dst), we'll note that they; // have 2 loops in common, so CommonLevels will equal 2 and the direction; // vector for Result will have 2 entries. SrcLevels = 4 and MaxLevels = 7.; // A map from loop names to loop numbers would look like; // a - 1; // b - 2 = CommonLevels; // c - 3; // d - 4 = SrcLevels; // e - 5; // f - 6; // g - 7 = MaxLevels",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/DependenceAnalysis.cpp:1559,load,load,1559,interpreter/llvm-project/llvm/lib/Analysis/DependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/DependenceAnalysis.cpp,1,['load'],['load']
Performance,"// Example IR module.; //; // This IR contains a recursive definition of the factorial function:; //; // fac(n) | n == 0 = 1; // | otherwise = n * fac(n - 1); //; // It also contains an entry function which calls the factorial function with; // an input value of 5.; //; // We expect the IR optimization transform that we build below to transform; // this into a non-recursive factorial function and an entry function that; // returns a constant value of 5!, or 120.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/OrcV2Examples/LLJITWithOptimizingIRTransform/LLJITWithOptimizingIRTransform.cpp:291,optimiz,optimization,291,interpreter/llvm-project/llvm/examples/OrcV2Examples/LLJITWithOptimizingIRTransform/LLJITWithOptimizingIRTransform.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/OrcV2Examples/LLJITWithOptimizingIRTransform/LLJITWithOptimizingIRTransform.cpp,1,['optimiz'],['optimization']
Performance,"// Example IR modules.; //; // Note that in the conditionally compiled modules, FooMod and BarMod, functions; // have been given an _body suffix. This is to ensure that their names do not; // clash with their lazy-reexports.; // For clients who do not wish to rename function bodies (e.g. because they want; // to re-use cached objects between static and JIT compiles) techniques exist to; // avoid renaming. See the lazy-reexports section of the ORCv2 design doc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/OrcV2Examples/LLJITWithExecutorProcessControl/LLJITWithExecutorProcessControl.cpp:321,cache,cached,321,interpreter/llvm-project/llvm/examples/OrcV2Examples/LLJITWithExecutorProcessControl/LLJITWithExecutorProcessControl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/OrcV2Examples/LLJITWithExecutorProcessControl/LLJITWithExecutorProcessControl.cpp,4,['cache'],['cached']
Performance,"// Example of inputs are; // vector<int> (*); // vector<Int_t>; // vector<long long>; // vector<Long_64_t> (*); // vector<int, allocator<int> >; // vector<Int_t, allocator<int> >; //; // One of the possibly expensive operation is the resolving of the typedef; // which can provoke the parsing of the header files (and/or the loading; // of clang pcms information).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx:325,load,loading,325,core/base/src/TROOT.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx,1,['load'],['loading']
Performance,"// Example sequence:; // li a0, 90; // vsetivli zero, 8, e8, mf2, ta, ma (ignored); // vmv.s.x v0, a0; // vmerge.vvm v8, v9, v8, v0; // We use 2 for the cost of the mask materialization as this is the true; // cost for small masks and most shuffles are small. At worst, this cost; // should be a very small constant for the constant pool load. As such,; // we may bias towards large selects slightly more than truely warranted.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:338,load,load,338,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['load'],['load']
Performance,// Exclude INT64_MIN to avoid passing it to std::abs. We won't optimize it; // anyway as the shift of 63 won't fit in uimm5.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:63,optimiz,optimize,63,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['optimiz'],['optimize']
Performance,// Exclude indirect inputs while they are unsupported because the code; // to perform the load is missing and thus OpInfo.CallOperand still; // refers to the input address rather than the pointed-to value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:78,perform,perform,78,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,2,"['load', 'perform']","['load', 'perform']"
Performance,// Exclude potentially vectorized loads from list of gathered; // scalars.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:34,load,loads,34,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['loads']
Performance,"// Exclude somes case where LD_SPLAT is worse than scalar_to_vector:; // Below cases should also happen for ""lfiwzx/lfiwax + LE target + index; // 1"" and ""lxvrhx + BE target + index 7"" and ""lxvrbx + BE target + index; // 15"", but function IsValidSplatLoad() now will only return true when; // the data at index 0 is not nullptr. So we will not get into trouble for; // these cases.; //; // case 1 - lfiwzx/lfiwax; // 1.1: load result is i32 and is sign/zero extend to i64;; // 1.2: build a v2i64 vector type with above loaded value;; // 1.3: the vector has only one value at index 0, others are all undef;; // 1.4: on BE target, so that lfiwzx/lfiwax does not need any permute.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:422,load,load,422,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loaded']"
Performance,"// Execute code on library loading by running code in the constructor of a; // class that is a static class member of another class.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/hs3/src/static_execute.h:27,load,loading,27,roofit/hs3/src/static_execute.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/hs3/src/static_execute.h,1,['load'],['loading']
Performance,"// Executor for multi-thread or serial execution",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/Config.h:16,multi-thread,multi-thread,16,tmva/tmva/inc/TMVA/Config.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/inc/TMVA/Config.h,1,['multi-thread'],['multi-thread']
Performance,"// Existing Phi blocks may need renaming too, if an access was previously; // optimized and the inserted Defs ""covers"" the Optimized value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp:78,optimiz,optimized,78,interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,2,"['Optimiz', 'optimiz']","['Optimized', 'optimized']"
Performance,"// Exit early if DstTy is not a vector type whose elements are one of [i16,; // i32, i64]. SVE doesn't generally have the same set of instructions to; // perform an extend with the add/sub/mul. There are SMULLB style; // instructions, but they operate on top/bottom, requiring some sort of lane; // interleaving to be used with zext/sext.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:154,perform,perform,154,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['perform'],['perform']
Performance,// Exit early if the loads are neither consecutive nor reverse consecutive.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:21,load,loads,21,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loads']
Performance,"// Expand FP32 immediates into loads from the stack, save special cases.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:31,load,loads,31,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loads']
Performance,"// Expand FP64 immediates into loads from the stack, save special cases.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:31,load,loads,31,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loads']
Performance,// Expand VSX Memory Pseudo instruction to either a VSX or a FP instruction.; // The VSX versions have the advantage of a full 64-register target whereas; // the FP ones have the advantage of lower latency and higher throughput. So; // what we are after is using the faster instructions in low register pressure; // situations and using the larger register file in high register pressure; // situations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:198,latency,latency,198,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,2,"['latency', 'throughput']","['latency', 'throughput']"
Performance,// Expand all extending loads and truncating stores:,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:24,load,loads,24,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['load'],['loads']
Performance,"// Expand all extending loads to types larger than this, and truncating; // stores from types larger than this.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:24,load,loads,24,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['loads']
Performance,// Expand all truncating stores and extending loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp:46,load,loads,46,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp,2,['load'],['loads']
Performance,// Expand an unaligned 32 or 64-bit integer load node.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['load']
Performance,"// Expand and promote recursively.; // TODO: This is non-optimal, but dealing with the concurrently happening; // vector-legalization is non-trivial. We could do something similar to; // PromoteFloatRes_EXTRACT_VECTOR_ELT here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeFloatTypes.cpp:87,concurren,concurrently,87,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeFloatTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeFloatTypes.cpp,1,['concurren'],['concurrently']
Performance,"// Expand is only ever created as a masked instruction. It is not safe to; // unfold a masked expand because we don't know if it came from an expand load; // intrinsic or folding a plain load. If it is from a expand load intrinsic,; // Unfolding to plain load would read more elements and could trigger a fault.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/X86FoldTablesEmitter.cpp:149,load,load,149,interpreter/llvm-project/llvm/utils/TableGen/X86FoldTablesEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/X86FoldTablesEmitter.cpp,4,['load'],['load']
Performance,"// Expand memcpy to a series of load and store ops if the size operand falls; // below a certain threshold.; // TODO: In the AlwaysInline case, if the size is big then generate a loop; // rather than maybe a humongous number of loads and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,2,['load'],"['load', 'loads']"
Performance,// Expand memmove to a series of load and store ops if the size operand falls; // below a certain threshold.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['load'],['load']
Performance,// Expand memset to a series of load/store ops if the size operand; // falls below a certain threshold.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['load'],['load']
Performance,"// Expand pseudo instructions which load, store or copy accumulators.; // Add an emergency spill slot if a pseudo was expanded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEFrameLowering.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEFrameLowering.cpp,1,['load'],['load']
Performance,// Expand slow SHLD/SHRD cases if we are not optimizing for size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:45,optimiz,optimizing,45,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimizing']
Performance,"// Expand small (<= 128-bit) record types when we know that the stack layout; // of those arguments will match the struct. This is important because the; // LLVM backend isn't smart enough to remove byval, which inhibits many; // optimizations.; // Don't do this for the MCU if there are still free integer registers; // (see X86_64 ABI for full explanation).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/X86.cpp:230,optimiz,optimizations,230,interpreter/llvm-project/clang/lib/CodeGen/Targets/X86.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/X86.cpp,1,['optimiz'],['optimizations']
Performance,"// Expand the (load) instruction into just a load-linked, which has; // greater atomic guarantees than a normal load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h:15,load,load,15,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,3,['load'],"['load', 'load-linked']"
Performance,// Expand the fpsosisat if it is scalable to prevent it from unrolling below.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorOps.cpp:33,scalab,scalable,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorOps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorOps.cpp,1,['scalab'],['scalable']
Performance,// Expand the instruction into loadlinked/storeconditional; used; // by ARM/AArch64.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h:31,load,loadlinked,31,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,1,['load'],['loadlinked']
Performance,"// Expand through memory thusly:; // Alloca CONCAT_VECTORS_TYPES(V1, V2) Ptr; // Store V1, Ptr; // Store V2, Ptr + sizeof(V1); // If (Imm < 0); // TrailingElts = -Imm; // Ptr = Ptr + sizeof(V1) - (TrailingElts * sizeof(VT.Elt)); // else; // Ptr = Ptr + (Imm * sizeof(VT.Elt)); // Res = Load Ptr",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:286,Load,Load,286,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['Load'],['Load']
Performance,"// Expand to a (misaligned) integer load of the same size,; // then bitconvert to floating point or vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['load'],['load']
Performance,"// Expand unaligned loads earlier than legalization. Due to visitation order; // problems during legalization, the emitted instructions to pack and unpack; // the bytes again are not eliminated in the case of an unaligned copy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:20,load,loads,20,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,1,['load'],['loads']
Performance,"// Expand various CCs to best match the RVV ISA, which natively supports UNE; // but no other unordered comparisons, and supports all ordered comparisons; // except ONE. Additionally, we expand GT,OGT,GE,OGE for optimization; // purposes; they are expanded to their swapped-operand CCs (LT,OLT,LE,OLE),; // and we pattern-match those back to the ""original"", swapping operands once; // more. This way we catch both operations and both ""vf"" and ""fv"" forms with; // fewer patterns.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:212,optimiz,optimization,212,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// Expand; // (set dst, (i32 (load baseptr))) or; // (set dst, (i64 (sextload baseptr))) or; // (set dst, (i64 (extload baseptr))); // to; // (set tmp, (lwl (add baseptr, 3), undef)); // (set dst, (lwr baseptr, tmp))",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['load']
Performance,"// Expand; // (set dst, (i64 (load baseptr))); // to; // (set tmp, (ldl (add baseptr, 7), undef)); // (set dst, (ldr baseptr, tmp))",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['load']
Performance,// Expanding cbz/tbz requires an extra cycle of latency on the condition.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:48,latency,latency,48,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['latency'],['latency']
Performance,// Expanding to the strong ATOMIC_CMP_SWAP node means we can determine; // success simply by comparing the loaded value against the ingoing; // comparison.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp:107,load,loaded,107,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,1,['load'],['loaded']
Performance,"// Expected a cache should exist; perhaps the user moved it; // Do nothing more here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx:14,cache,cache,14,tree/tree/src/TTree.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx,1,['cache'],['cache']
Performance,"// Expected latency is the max of InstrLatency and DefaultDefLatency, if we; // didn't find an operand latency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp:12,latency,latency,12,interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp,2,['latency'],['latency']
Performance,"// Expected number of parameters that we actually track is 1.; //; // Also, the maximum number of declared parameters could not be on a scale; // of hundreds of thousands.; //; // In this setting, linear search seems reasonable and even performs better; // than bisection.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CalledOnceCheck.cpp:237,perform,performs,237,interpreter/llvm-project/clang/lib/Analysis/CalledOnceCheck.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CalledOnceCheck.cpp,1,['perform'],['performs']
Performance,"// Experimental feature that inserts callbacks for certain data events.; // Currently callbacks are only inserted for loads, stores, memory transfers; // (i.e. memcpy and memmove), and comparisons.; //; // If this flag is set to true, the user must provide definitions for the; // following callback functions:; // void __dfsan_load_callback(dfsan_label Label, void* addr);; // void __dfsan_store_callback(dfsan_label Label, void* addr);; // void __dfsan_mem_transfer_callback(dfsan_label *Start, size_t Len);; // void __dfsan_cmp_callback(dfsan_label CombinedLabel);",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:118,load,loads,118,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['load'],['loads']
Performance,"// Experimental feature that inserts callbacks for data reaching a function,; // either via function arguments and loads.; // This must be true for dfsan_set_reaches_function_callback() to have effect.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:115,load,loads,115,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['load'],['loads']
Performance,// Experimental option that will only be fully functional when the cost-model; // and code-generator have been changed to avoid using scalable vector; // instructions that are not legal in streaming SVE mode.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:134,scalab,scalable,134,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['scalab'],['scalable']
Performance,"// Experimental option to allow imprecision in LICM in pathological cases, in; // exchange for faster compile. This is to be removed if MemorySSA starts to; // address the same issue. LICM calls MemorySSAWalker's; // getClobberingMemoryAccess, up to the value of the Cap, getting perfect; // accuracy. Afterwards, LICM will call into MemorySSA's getDefiningAccess,; // which may not be precise, since optimizeUses is capped. The result is; // correct, but we may not get as ""far up"" as possible to get which access is; // clobbering the one queried.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:401,optimiz,optimizeUses,401,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['optimiz'],['optimizeUses']
Performance,"// Experimental option to eagerly invalidate more analyses. This has the; // potential to decrease max memory usage in exchange for more compile time.; // This may affect codegen due to either passes using analyses only when; // cached, or invalidating and recalculating an analysis that was; // stale/imprecise but still valid. Currently this invalidates all function; // analyses after various module->function or cgscc->function adaptors in the; // default pipelines.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Passes/PassBuilder.h:229,cache,cached,229,interpreter/llvm-project/llvm/include/llvm/Passes/PassBuilder.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Passes/PassBuilder.h,1,['cache'],['cached']
Performance,// Experiments of different caps with Chromium on both x64 and ARM64; // have shown that the 32-byte cap generates the smallest binary on; // both platforms while different caps yield similar performance.; // (see https://lists.llvm.org/pipermail/llvm-dev/2018-July/124694.html),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp:192,perform,performance,192,interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,1,['perform'],['performance']
Performance,"// Explicit add hot edges to enforce importing for designated GUIDs for; // sample PGO, to enable the same inlines as the profiled optimized binary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ModuleSummaryAnalysis.cpp:131,optimiz,optimized,131,interpreter/llvm-project/llvm/lib/Analysis/ModuleSummaryAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ModuleSummaryAnalysis.cpp,1,['optimiz'],['optimized']
Performance,// Explicit inlining flags can disable some or all inlining even at; // optimization levels above zero.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInvocation.cpp:72,optimiz,optimization,72,interpreter/llvm-project/clang/lib/Frontend/CompilerInvocation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInvocation.cpp,1,['optimiz'],['optimization']
Performance,"// Explicit template member lookup/instantiation; works by re-bounding. This method can; // not cache overloads as instantiations need not be unique for the argument types due; // to template specializations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/src/TemplateProxy.cxx:96,cache,cache,96,bindings/pyroot/cppyy/CPyCppyy/src/TemplateProxy.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/src/TemplateProxy.cxx,1,['cache'],['cache']
Performance,"// Explicitly load libMathCore it cannot be auto-loaded it when using one; // of its freestanding functions. Once functions can trigger autoloading we; // can get rid of this.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/rint/src/TRint.cxx:14,load,load,14,core/rint/src/TRint.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/rint/src/TRint.cxx,2,['load'],"['load', 'loaded']"
Performance,// Explicitly perform load combine to make sure no opposing transform; // can remove the bitcast in the meantime and trigger an infinite loop.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:14,perform,perform,14,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,2,"['load', 'perform']","['load', 'perform']"
Performance,// ExtSymbol being loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h:19,load,loaded,19,interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMConstantPoolValue.h,2,['load'],['loaded']
Performance,"// Extend inputs to XLen, and shift by 32. This will add 64 trailing zeros; // to the full 128-bit clmul result of multiplying two xlen values.; // Perform clmulr or clmulh on the shifted values. Finally, extract the; // upper 32 bits.; //; // The alternative is to mask the inputs to 32 bits and use clmul, but; // that requires two shifts to mask each input without zext.w.; // FIXME: If the inputs are known zero extended or could be freely; // zero extended, the mask form would be better.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:148,Perform,Perform,148,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['Perform'],['Perform']
Performance,// Extend loaded pointer if necessary (i.e. if ILP32) to DAG pointer.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:10,load,loaded,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loaded']
Performance,// Extend the element if necessary (e.g. an i8 is loaded; // into an i16 register),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:50,load,loaded,50,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['loaded']
Performance,// Extend the latency if needed. Equivalent to; // removePred(PredDep) + addPred(D).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAG.cpp:14,latency,latency,14,interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAG.cpp,1,['latency'],['latency']
Performance,"// Extend to 64-bits, then perform a 64-bit multiply.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,perform,perform,27,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// ExtendUsesToFormExtLoad - Trying to extend uses of a load to enable this:; // ""fold ({s|z|a}ext (load x)) -> ({s|z|a}ext (truncate ({s|z|a}extload x)))""; // transformation. Returns true if extension are possible and the above; // mentioned transformation is profitable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:56,load,load,56,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,// Extended load operations for i1 types must be promoted,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFISelLowering.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/BPF/BPFISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFISelLowering.cpp,2,['load'],['load']
Performance,// Extending load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['load'],['load']
Performance,// Extending loads from (native) vectors of i8 into (native) vectors of i16; // are legal.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:13,load,loads,13,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['load'],['loads']
Performance,// Extending masked load/Truncating masked stores is expensive because we; // currently don't split them. This means that we'll likely end up; // loading/storing each element individually (hence the high cost).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,2,['load'],"['load', 'loading']"
Performance,// Extending v8i1/v16i1 to 512-bit get better performance on KNL; // than extending to 128/256bit.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:46,perform,performance,46,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performance']
Performance,// Extends have to be extending-loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:32,load,loads,32,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['load'],['loads']
Performance,// Extension mode for integer loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h:30,load,loads,30,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h,1,['load'],['loads']
Performance,"// Extensions from GPR to i128 (in VR) typically costs two instructions,; // but a zero-extending load would be just one extra instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:98,load,load,98,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,1,['load'],['load']
Performance,"// Extern weak functions can sometimes be null at execution time.; // Code will sometimes check if an extern weak function is null.; // This could look something like:; // declare extern_weak i8 @my_func(i8); // br i1 icmp ne (i8 (i8)* @my_func, i8 (i8)* null), label %use_my_func,; // label %avoid_my_func; // The @""dfsw$my_func"" wrapper is never null, so if we replace this use; // in the comparison, the icmp will simplify to false and we have; // accidentally optimized away a null check that is necessary.; // This can lead to a crash when the null extern_weak my_func is called.; //; // To prevent (the most common pattern of) this problem,; // do not replace uses in comparisons with the wrapper.; // We definitely want to replace uses in call instructions.; // Other uses (e.g. store the function address somewhere) might be; // called or compared or both - this case may not be handled correctly.; // We will default to replacing with wrapper in cases we are unsure.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:464,optimiz,optimized,464,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['optimiz'],['optimized']
Performance,"// External and linkonce definitions are converted to available_externally; // definitions upon import, so that they are available for inlining; // and/or optimization, but are turned into declarations later; // during the EliminateAvailableExternally pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/FunctionImportUtils.cpp:155,optimiz,optimization,155,interpreter/llvm-project/llvm/lib/Transforms/Utils/FunctionImportUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/FunctionImportUtils.cpp,1,['optimiz'],['optimization']
Performance,// External variable reference. Try to use the dynamic loader to; // get a pointer to it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/ExecutionEngine.cpp:55,load,loader,55,interpreter/llvm-project/llvm/lib/ExecutionEngine/ExecutionEngine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/ExecutionEngine.cpp,1,['load'],['loader']
Performance,// Extra structures used by load commands,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ObjectYAML/MachOYAML.h:28,load,load,28,interpreter/llvm-project/llvm/include/llvm/ObjectYAML/MachOYAML.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ObjectYAML/MachOYAML.h,1,['load'],['load']
Performance,"// Extra threads can be used to send data to different clients via websocket (default no); ///; /// WebGui.SenderThrds: no; ///; /// If required, one could change websocket timeouts (default is 10000 ms); ///; /// WebGui.HttpWSTmout: 10000; ///; /// By default, THttpServer created in restricted mode which only allows websocket handlers; /// and processes only very few other related http requests. For security reasons such mode; /// should be always enabled. Only if it is really necessary to process all other kinds; /// of HTTP requests, one could specify no for following parameter (default yes):; ///; /// WebGui.WSOnly: yes; ///; /// In some applications one may need to force longpoll websocket emulations from the beginning,; /// for instance when clients connected via proxys. Although JSROOT should automatically fallback; /// to longpoll engine, one can configure this directly (default no); ///; /// WebGui.WSLongpoll: no; ///; /// Following parameter controls browser max-age caching parameter for files (default 3600); /// When 0 is specified, browser cache will be disabled; ///; /// WebGui.HttpMaxAge: 3600; ///; /// Also one can provide extra URL options for, see TCivetweb::Create for list of supported options; ///; /// WebGui.HttpExtraArgs: winsymlinks=no; ///; /// One also can configure usage of FastCGI server for web windows:; ///; /// WebGui.FastCgiPort: 4000; /// WebGui.FastCgiThreads: 10; ///; /// To be able start web browser for such windows, one can provide real URL of the; /// web server which will connect with that FastCGI instance:; ///; /// WebGui.FastCgiServer: https://your_apache_server.com/root_cgi_path; ///; /// For some custom applications one requires to load JavaScript modules or other files.; /// For such applications one may require to load files from other locations which can be configured; /// with AddServerLocation() method or directly via:; ///; /// WebGui.ServerLocations: location1:/file/path/to/location1;location2:/file/path/to/location2",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/webdisplay/src/RWebWindowsManager.cxx:2928,cache,cache,2928,gui/webdisplay/src/RWebWindowsManager.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/webdisplay/src/RWebWindowsManager.cxx,3,"['cache', 'load']","['cache', 'load']"
Performance,"// Extract - store vector to stack, load scalar.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['load'],['load']
Performance,// Extract and store the sub-vectors returned by the load intrinsic.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['load']
Performance,// Extract tag from X5 and compare it with loaded tag from shadow,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp:43,load,loaded,43,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVAsmPrinter.cpp,1,['load'],['loaded']
Performance,// Extract the subvector by loading the correct part.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:28,load,loading,28,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['loading']
Performance,// Extract y component. Upper half of LoadZU should be zero already.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:38,Load,LoadZU,38,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,1,['Load'],['LoadZU']
Performance,"// Extract/InsertElement with non-constant index is very costly when; // scalarized; estimate cost of loads/stores sequence via the stack:; // ExtractElement cost: store vector to stack, load scalar;; // InsertElement cost: store vector to stack, store scalar, load vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:102,load,loads,102,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,3,['load'],"['load', 'loads']"
Performance,// Extracts from consecutive indexes of the same vector better score as; // the extracts could be optimized away.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:98,optimiz,optimized,98,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['optimiz'],['optimized']
Performance,"// F i t p d f t o d a t a , s a v e f i t r e s u l t; // -------------------------------------------------------------; // Perform fit and save result",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/stressRooFit_tests.h:125,Perform,Perform,125,roofit/roofitcore/test/stressRooFit_tests.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/stressRooFit_tests.h,1,['Perform'],['Perform']
Performance,// F16 - Load/Store Actions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:9,Load,Load,9,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['Load'],['Load']
Performance,// FD is only needed to avoid race conditions. Close it right away.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Path.cpp:30,race condition,race conditions,30,interpreter/llvm-project/llvm/lib/Support/Path.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Path.cpp,2,['race condition'],['race conditions']
Performance,"// FDIV is always expensive, even if it has a very low uop count.; // TODO: Still necessary for recent CPUs with low latency/throughput fdiv?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:117,latency,latency,117,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,2,"['latency', 'throughput']","['latency', 'throughput']"
Performance,// FIXME - If finding successor is compile time expensive then cache results.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp:63,cache,cache,63,interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,1,['cache'],['cache']
Performance,"// FIXME This causes a redundant load/store if the SSE-class value is already; // in memory, such as if it is on the callstack.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// FIXME dl should come from parent load or store, not from address",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// FIXME threads are hard coded to 1, no use of slave threads or multi-threading; // After the processing of the options, initialize the master deep net",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/MethodDL.cxx:65,multi-thread,multi-threading,65,tmva/tmva/src/MethodDL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/MethodDL.cxx,1,['multi-thread'],['multi-threading']
Performance,// FIXME!! -- get Load working properly; // return new (Arena) til::Load(E0);,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/ThreadSafetyCommon.cpp:18,Load,Load,18,interpreter/llvm-project/clang/lib/Analysis/ThreadSafetyCommon.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/ThreadSafetyCommon.cpp,2,['Load'],['Load']
Performance,// FIXME: Add a flag to the ScopeInfo to indicate whether we're performing; // deduction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp:64,perform,performing,64,interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,1,['perform'],['performing']
Performance,// FIXME: Add loop interchange.; // Unroll small loops and perform peeling.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:59,perform,perform,59,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['perform'],['perform']
Performance,// FIXME: Adding this to every 'CallStackFrame' may have a nontrivial impact; // on the overall stack usage of deeply-recursing constexpr evaluations.; // (We should cache this map rather than recomputing it repeatedly.); // But let's try this and see how it goes; we can look into caching the map; // as a later change.; /// LambdaCaptureFields - Mapping from captured variables/this to; /// corresponding data members in the closure class.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:166,cache,cache,166,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['cache'],['cache']
Performance,"// FIXME: After copying the source-location information, should we free; // our (temporary) buffer and adopt the ASTContext-allocated memory?; // Doing so would optimize repeated calls to getWithLocInContext().",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/NestedNameSpecifier.cpp:161,optimiz,optimize,161,interpreter/llvm-project/clang/lib/AST/NestedNameSpecifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/NestedNameSpecifier.cpp,1,['optimiz'],['optimize']
Performance,// FIXME: Allow unaligned atomic load/store on x86. (It is not; // currently supported by the backend.),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:33,load,load,33,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['load'],['load']
Performance,// FIXME: Also update cached pressure for where the def was sinked from.; // Update RP for all regions that has this reg as a live-in and remove; // the reg from all regions as a live-in.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:22,cache,cached,22,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,1,['cache'],['cached']
Performance,"// FIXME: As a speculative fix to a defect introduced by CWG2352, we rank; // a reference binding that performs a non-top-level qualification; // conversion as a qualification conversion, not as an identity conversion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:103,perform,performs,103,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,1,['perform'],['performs']
Performance,"// FIXME: Assess perf here? Figure out what cases are worth optimizing here; // (Length of globals? Chunks of zeroed-out space?).; //; // If we can, prefer a copy from a global; this is a lot less code for long; // globals, and it's easier for the current optimizers to analyze.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:60,optimiz,optimizing,60,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,2,['optimiz'],"['optimizers', 'optimizing']"
Performance,// FIXME: Automatically giving all implicit defs defaultDefLatency is; // undesirable. We should only do it for defs that are known to the MC; // desc like flags. Truly implicit defs should get 1 cycle latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp:202,latency,latency,202,interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp,1,['latency'],['latency']
Performance,"// FIXME: Both positive and negative access functions will be placed; // into the same reference group, resulting in a bi-directional array; // access such as:; // for (i = N; i > 0; i--); // A[i] = A[N - i];; // having the same cost calculation as a single dimention access pattern; // for (i = 0; i < N; i++); // A[i] = A[i];; // when in actuality, depending on the array size, the first example; // should have a cost closer to 2x the second due to the two cache; // access per iteration from opposite ends of the array",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopCacheAnalysis.cpp:460,cache,cache,460,interpreter/llvm-project/llvm/lib/Analysis/LoopCacheAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopCacheAnalysis.cpp,1,['cache'],['cache']
Performance,// FIXME: Cache the number on the AL object?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclAttr.cpp:10,Cache,Cache,10,interpreter/llvm-project/clang/lib/Sema/SemaDeclAttr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclAttr.cpp,1,['Cache'],['Cache']
Performance,"// FIXME: Can we somehow regenerate the stat cache here, or do we need to; // unconditionally create a stat cache when we parse the file?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:45,cache,cache,45,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,2,['cache'],['cache']
Performance,// FIXME: Check for missing '()' if T is a function type?; // We can only perform contextual implicit conversions on objects of class; // type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:74,perform,perform,74,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,1,['perform'],['perform']
Performance,// FIXME: Clean this up after splitting each Thumb load / store opcode; // into multiple ones.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp:51,load,load,51,interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,1,['load'],['load']
Performance,"// FIXME: Compare performance against using RPO to consider nodes, rather than; // following successors.; //; // Elements must not be null. Duplicates are prevented using `Workset`, below.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/IntervalPartition.cpp:18,perform,performance,18,interpreter/llvm-project/clang/lib/Analysis/IntervalPartition.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/IntervalPartition.cpp,1,['perform'],['performance']
Performance,// FIXME: Constant loads should all be marked invariant.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['load'],['loads']
Performance,"// FIXME: Copied from PPC; // First, load into 32 bits, then truncate to 1 bit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['load'],['load']
Performance,// FIXME: Could we fold this with the load? It would require careful EFLAGS; // management.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['load']
Performance,"// FIXME: Currently the DataLayout string carries a ""preferred alignment""; // for types. As the DataLayout is module/global, this should likely be; // sunk down to an FTTI element that is queried rather than a global; // preference.; /// Layout alignment element.; ///; /// Stores the alignment data associated with a given type bit width.; ///; /// \note The unusual order of elements in the structure attempts to reduce; /// padding and make the structure slightly more cache friendly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/DataLayout.h:472,cache,cache,472,interpreter/llvm-project/llvm/include/llvm/IR/DataLayout.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/DataLayout.h,1,['cache'],['cache']
Performance,"// FIXME: Currently the calls which may access the thread id may; // be considered as not accessing the memory. But this is; // problematic for coroutines, since coroutines may resume in a; // different thread. So we disable the optimization here for the; // correctness. However, it may block many other correct; // optimizations. Revert this one when we detect the memory; // accessing kind more precisely.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:229,optimiz,optimization,229,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,8,['optimiz'],"['optimization', 'optimizations']"
Performance,// FIXME: Currently we disable this transformation in big-endian targets as; // performance and correctness are verified only in little-endian.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:80,perform,performance,80,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['perform'],['performance']
Performance,"// FIXME: Debug values referencing 64+ unique machine locations are rare and; // currently unsupported for performance reasons. If we can verify that; // performance is acceptable for such debug values, we can increase the; // bit-width of LocNoCount to 14 to enable up to 16384 unique machine; // locations. We will also need to verify that this does not cause issues; // with LiveDebugVariables' use of IntervalMap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp:107,perform,performance,107,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp,2,['perform'],['performance']
Performance,// FIXME: Delete loop from pass manager's queue?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp:42,queue,queue,42,interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,1,['queue'],['queue']
Performance,"// FIXME: Do we need/want a pre-alloc pass like ARM has to try to keep loads and; // stores near one another? Note: The pre-RA instruction scheduler already has; // hooks to try and schedule pairable loads/stores together to improve pairing; // opportunities. Thus, pre-RA pairing pass may not be worth the effort.; // FIXME: When pairing store instructions it's very possible for this pass to; // hoist a store with a KILL marker above another use (without a KILL marker).; // The resulting IR is invalid, but nothing uses the KILL markers after this; // pass, so it's never caused a problem in practice.; /// createAArch64LoadStoreOptimizationPass - returns an instance of the; /// load / store optimization pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:71,load,loads,71,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,4,"['load', 'optimiz']","['load', 'loads', 'optimization']"
Performance,"// FIXME: Do we want to add a mapping for FLAT load, or should we just; // handle that during instruction selection?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,1,['load'],['load']
Performance,"// FIXME: Emit checks to determine it's _actually_ safe to fold and/or; // account for unsafe cases.; //; // Example:; // MI1--> %0 = ...; // %1 = ... %0; // MI0--> %2 = ... %0; // It's not safe to erase MI1. We currently handle this by not; // erasing %0 (even when it's dead).; //; // Example:; // MI1--> %0 = load volatile @a; // %1 = load volatile @a; // MI0--> %2 = ... %0; // It's not safe to sink %0's def past %1. We currently handle; // this by rejecting all loads.; //; // Example:; // MI1--> %0 = load @a; // %1 = store @a; // MI0--> %2 = ... %0; // It's not safe to sink %0's def past %1. We currently handle; // this by rejecting all loads.; //; // Example:; // G_CONDBR %cond, @BB1; // BB0:; // MI1--> %0 = load @a; // G_BR @BB1; // BB1:; // MI0--> %2 = ... %0; // It's not always safe to sink %0 across control flow. In this; // case it may introduce a memory fault. We currentl handle; // this by rejecting all loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelMatchTable.cpp:312,load,load,312,interpreter/llvm-project/llvm/utils/TableGen/GlobalISelMatchTable.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelMatchTable.cpp,7,['load'],"['load', 'loads']"
Performance,// FIXME: Emit optimized urem by constant instead of letting it expand later.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:15,optimiz,optimized,15,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['optimiz'],['optimized']
Performance,// FIXME: Enable subregister liveness by default for RVV to better handle; // LMUL>1 and segment load/store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVSubtarget.cpp:97,load,load,97,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVSubtarget.cpp,1,['load'],['load']
Performance,// FIXME: Entirely reasonable to perform folding of other unary; // operations here as the need arises.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:33,perform,perform,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['perform'],['perform']
Performance,"// FIXME: Figure out better way to handle:; // C++ [basic.lookup.classref]p1:; // In a class member access expression (5.2.5), if the . or -> token is; // immediately followed by an identifier followed by a <, the; // identifier must be looked up to determine whether the < is the; // beginning of a template argument list (14.2) or a less-than operator.; // The identifier is first looked up in the class of the object; // expression. If the identifier is not found, it is then looked up in; // the context of the entire postfix-expression and shall name a class; // or function template.; //; // We want to ignore object(.|->)member<template>; //if (R.getSema().PP.LookAhead(0).getKind() == tok::less); // TODO: check for . or -> in the cached token stream; // return false;",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx:739,cache,cached,739,core/metacling/src/TClingCallbacks.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx,1,['cache'],['cached']
Performance,"// FIXME: Find a better way to deal with collisions between these; // built-in types. Right now, we just ignore the problem.; // Load the special types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:129,Load,Load,129,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['Load'],['Load']
Performance,"// FIXME: For now we conservatively ignore out of bound accesses, but; // we're allowed to perform the optimization in this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopUnrollAnalyzer.cpp:91,perform,perform,91,interpreter/llvm-project/llvm/lib/Analysis/LoopUnrollAnalyzer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopUnrollAnalyzer.cpp,4,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"// FIXME: For now, all byval parameter objects are marked mutable. This can be; // changed with more analysis.; // In case of tail call optimization mark all arguments mutable. Since they; // could be overwritten by lowering of arguments in case of a tail call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:136,optimiz,optimization,136,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['optimiz'],['optimization']
Performance,"// FIXME: For now, all byval parameter objects are marked mutable.; // This can be changed with more analysis.; // In case of tail call optimization mark all arguments mutable.; // Since they could be overwritten by lowering of arguments in case of; // a tail call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:136,optimiz,optimization,136,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// FIXME: For now, we always block the optimization over SP in windows; // targets as it requires to adjust the unwind/debug info, messing up; // the unwind info can actually cause a miscompile.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:39,optimiz,optimization,39,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,2,['optimiz'],['optimization']
Performance,"// FIXME: For pre/post-increment addressing modes, the base register; // used in address calculation is also defined by this instruction.; // It might be a worthwhile optimization to not harden that; // base register increment/decrement when the increment/decrement is; // an immediate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:167,optimiz,optimization,167,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,1,['optimiz'],['optimization']
Performance,"// FIXME: For scalable vectors, assume vscale=1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp:14,scalab,scalable,14,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp,1,['scalab'],['scalable']
Performance,"// FIXME: Fully computing the resource structures requires analyzing the IR; // because some flags are set based on what operations are performed on the; // resource. This partial patch handles some of the leg work, but not all of it.; // See issue https://github.com/llvm/llvm-project/issues/57936.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/DirectX/DXILResource.h:136,perform,performed,136,interpreter/llvm-project/llvm/lib/Target/DirectX/DXILResource.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/DirectX/DXILResource.h,1,['perform'],['performed']
Performance,"// FIXME: Handle aggregate types; // Since we don't have sub-dword scalar loads, avoid doing an extload by; // loading earlier than the argument address, and extracting the relevant; // bits.; // TODO: Update this for GFX12 which does have scalar sub-dword loads.; //; // Additionally widen any sub-dword load to i32 even if suitably aligned,; // so that CSE between different argument loads works easily.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:74,load,loads,74,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,5,['load'],"['load', 'loading', 'loads']"
Performance,// FIXME: Handle case where a scalable vector is extracted from a scalable; // vector,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h:30,scalab,scalable,30,interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,2,['scalab'],['scalable']
Performance,// FIXME: Handle case where a scalable vector is inserted into a scalable; // vector,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h:30,scalab,scalable,30,interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,2,['scalab'],['scalable']
Performance,"// FIXME: Handle loads from strings where the literal is treated as; // an integer, e.g., *((unsigned int*)""hello""). Such loads are UB according; // to C++20 7.2.1.11 [basic.lval].",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:17,load,loads,17,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,2,['load'],['loads']
Performance,// FIXME: Handle scalable vectors instead of ignoring them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp:17,scalab,scalable,17,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,1,['scalab'],['scalable']
Performance,"// FIXME: Here we actually perform an implicit conversion from the loaded; // value to the element type. Eventually we want to compose these values; // more intelligently. For example, an 'element' can encompass multiple; // bound regions (e.g., several bound bytes), or could be a subset of; // a larger value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:27,perform,perform,27,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,2,"['load', 'perform']","['loaded', 'perform']"
Performance,"// FIXME: Here we actually perform an implicit conversion from the loaded; // value to the ivar type. What we should model is stores to ivars; // that blow past the extent of the ivar. If the address of the ivar is; // reinterpretted, it is possible we stored a different value that could; // fit within the ivar. Either we need to cast these when storing them; // or reinterpret them lazily (as we do here).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:27,perform,perform,27,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,2,"['load', 'perform']","['loaded', 'perform']"
Performance,"// FIXME: Here we actually perform an implicit conversion from the loaded; // value to the variable type. What we should model is stores to variables; // that blow past the extent of the variable. If the address of the; // variable is reinterpretted, it is possible we stored a different value; // that could fit within the variable. Either we need to cast these when; // storing them or reinterpret them lazily (as we do here).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:27,perform,perform,27,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,2,"['load', 'perform']","['loaded', 'perform']"
Performance,// FIXME: How do we load the 'use'd modules? They may not be submodules.; // Might be unnecessary as use declarations are only used to build the; // module itself.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:20,load,load,20,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['load']
Performance,"// FIXME: I think for bottom up scheduling, the register pressure is cached; // and can be retrieved by DAG->getPressureDif(SU).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:69,cache,cached,69,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,1,['cache'],['cached']
Performance,"// FIXME: I would like to be convinced that this code is worth being as; // complicated as it is, binary search isn't that slow.; //; // If it is worth being optimized, then in my opinion it could be more; // performant, simpler, and more obviously correct by just ""galloping"" outward; // from the queried file position. In fact, this could be incorporated into a; // generic algorithm such as lower_bound_with_hint.; //; // If someone gives me a test case where this matters, and I will do it! - DWD; // If the previous query was to the same file, we know both the file pos from; // that query and the line number returned. This allows us to narrow the; // search space from the entire file to something near the match.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:158,optimiz,optimized,158,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,2,"['optimiz', 'perform']","['optimized', 'performant']"
Performance,// FIXME: Ideally the cached value should be cleaned up later.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp:22,cache,cached,22,interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp,1,['cache'],['cached']
Performance,"// FIXME: If both LHS and RHS are SCALAR_TO_VECTOR, but are not the; // same type and have differing element sizes, then do not perform; // the following transformation. The current transformation for; // SCALAR_TO_VECTOR assumes that both input vectors have the same; // element size. This will be updated in the future to account for; // differing sizes of the LHS and RHS.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:128,perform,perform,128,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['perform'],['perform']
Performance,"// FIXME: If either is a pre/post inc/dec load,; // we'd need to split out the address adjustment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// FIXME: If the offset won't fit in 24-bits, compute the offset into a; // scratch register. If DestReg is a virtual register, use it as the; // scratch register; otherwise, create a new virtual register (to be; // replaced by the scavenger at the end of PEI). That case can be optimized; // slightly if DestReg is SP which is always 16-byte aligned, so the scratch; // register can be loaded with offset%8 and the add/sub can use an extending; // instruction with LSL#3.; // Currently the function handles any offsets but generates a poor sequence; // of code.; // assert(Offset < (1 << 24) && ""unimplemented reg plus immediate"");",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:279,optimiz,optimized,279,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,2,"['load', 'optimiz']","['loaded', 'optimized']"
Performance,"// FIXME: If there are long latency loop-invariant instructions inside the; // loop at this point, why didn't the optimizer's LICM hoist them?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:28,latency,latency,28,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,2,"['latency', 'optimiz']","['latency', 'optimizer']"
Performance,"// FIXME: If there's no umbrella header, we could probably scan the; // framework to load *everything*. But, it's not clear that this is a good; // idea.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/ModuleMap.cpp:85,load,load,85,interpreter/llvm-project/clang/lib/Lex/ModuleMap.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/ModuleMap.cpp,1,['load'],['load']
Performance,"// FIXME: If this depth limit is hit, then we may cache sub-optimal results; // for recursive queries. For this reason, this limit is chosen to be large; // enough to be very rarely hit, while still being small enough to avoid; // stack overflows.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp:50,cache,cache,50,interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,1,['cache'],['cache']
Performance,"// FIXME: If we error out because an outer lambda can not implicitly; // capture a variable that an inner lambda explicitly captures, we; // should have the inner lambda do the explicit capture - because; // it makes for cleaner diagnostics later. This would purely be done; // so that the diagnostic does not misleadingly claim that a variable; // can not be captured by a lambda implicitly even though it is captured; // explicitly. Suggestion:; // - create const bool VariableCaptureWasInitiallyExplicit = Explicit; // at the function head; // - cache the StartingDeclContext - this must be a lambda; // - captureInLambda in the innermost lambda the variable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp:549,cache,cache,549,interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,1,['cache'],['cache']
Performance,"// FIXME: If we have a volatile struct, the optimizer can remove what might; // appear to be `extra' memory ops:; //; // volatile struct { int i; } a, b;; //; // int main() {; // a = b;; // a = b;; // }; //; // we need to use a different call here. We use isVolatile to indicate when; // either the source or the destination is volatile.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:44,optimiz,optimizer,44,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['optimiz'],['optimizer']
Performance,"// FIXME: If we split the loaded libraries into a separate JITDylib we should; // be able to delete this code and use something like:; // if (IncludeHostSymbols) {; // if (auto Sym = lookup(<HostSymbolsJD>, Name)) {; // fromJIT = false;; // return Sym;; // }; // }; // if (auto Sym = lookup(<REPLJD>, Name)) {; // fromJIT = true;; // return Sym;; // }; // fromJIT = false;; // return nullptr;",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/IncrementalExecutor.cpp:26,load,loaded,26,interpreter/cling/lib/Interpreter/IncrementalExecutor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/IncrementalExecutor.cpp,1,['load'],['loaded']
Performance,// FIXME: If/when .dump and .load are implemented they will be done in the; // the assembly parser and not have any need for an MCStreamer API.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCParser/DarwinAsmParser.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/MC/MCParser/DarwinAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCParser/DarwinAsmParser.cpp,1,['load'],['load']
Performance,// FIXME: Improve support for scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:30,scalab,scalable,30,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['scalab'],['scalable']
Performance,"// FIXME: In case of N32 / N64 ABI and emabled XGOT, local addresses; // loaded using R_MIPS_GOT_PAGE / R_MIPS_GOT_OFST pair of relocations.; // FIXME: Implement XGOT for microMIPS.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:73,load,loaded,73,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,1,['load'],['loaded']
Performance,"// FIXME: In case when the modulemap is not yet loaded we will return the; // wrong result. Consider a call to HasPCMForLibrary(../test/libEvent.so); // We will only load the modulemap for libEvent.so after we dlopen libEvent; // which may happen after calling this interface. Maybe we should also check; // if there is a Event.pcm file and a module.modulemap, load it and return; // true.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:48,load,loaded,48,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,3,['load'],"['load', 'loaded']"
Performance,"// FIXME: In some cases, VLDRS can be changed to a VLD1DUPd32 which defines; // the full D-register by loading the same value to both lanes. The; // instruction is micro-coded with 2 uops, so don't do this until we can; // properly schedule micro-coded instructions. The dispatcher stalls cause; // too big regressions.; // Insert the dependency-breaking FCONSTD before MI.; // 96 is the encoding of 0.5, but the actual value doesn't matter here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:103,load,loading,103,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['load'],['loading']
Performance,"// FIXME: In some cases, it may be interesting to promote in memory; // a zero initialized constant.; // E.g., when the type of Cst require more instructions than the; // adrp/add/load sequence or when this sequence can be shared by several; // instances of Cst.; // Ideally, we could promote this into a global and rematerialize the constant; // when it was a bad idea.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp:180,load,load,180,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64PromoteConstant.cpp,1,['load'],['load']
Performance,// FIXME: In the long term the verifier should not be controllable with a; // flag. We should either fix all passes to correctly update the assumption; // cache and enable the verifier unconditionally or somehow arrange for the; // assumption list to be updated automatically by passes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/AssumptionCache.cpp:155,cache,cache,155,interpreter/llvm-project/llvm/lib/Analysis/AssumptionCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/AssumptionCache.cpp,1,['cache'],['cache']
Performance,// FIXME: Is this correct?; // We have post-incremented loads / stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430ISelLowering.cpp:56,load,loads,56,interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430ISelLowering.cpp,1,['load'],['loads']
Performance,// FIXME: Issue a diagnostic if FirstFriend doesn't match when we come to; // lazily load it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:85,load,load,85,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,1,['load'],['load']
Performance,// FIXME: Issue a diagnostic if the base classes don't match when we come; // to lazily load them.; // FIXME: Issue a diagnostic if the list of conversion functions doesn't; // match when we come to lazily load them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:88,load,load,88,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,2,['load'],['load']
Performance,// FIXME: It probably would be possible to filter which thunks to produce; // based on which registers are actually used in BLR instructions in this; // function. But would that be a worthwhile optimization?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp:194,optimiz,optimization,194,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SLSHardening.cpp,1,['optimiz'],['optimization']
Performance,// FIXME: It probably would be possible to filter which thunks to produce; // based on which registers are actually used in indirect calls in this; // function. But would that be a worthwhile optimization?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSLSHardening.cpp:192,optimiz,optimization,192,interpreter/llvm-project/llvm/lib/Target/ARM/ARMSLSHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMSLSHardening.cpp,1,['optimiz'],['optimization']
Performance,"// FIXME: It's not clear how deduction of a parameter of reference; // type from an argument (of non-reference type) should be performed.; // For now, we just remove reference types from both sides and let; // the final check for matching types sort out the mess.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp:127,perform,performed,127,interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,1,['perform'],['performed']
Performance,"// FIXME: LLVM lacks nuanced semantics to differentiate between; // memory and direct locations at the IR level. The backend will; // turn a dbg.declare(alloca, ..., DIExpression()) into a memory; // location. Thus, if there are deref and offset operations in the; // expression, we need to add a DW_OP_deref at the *start* of the; // expression to first load the contents of the alloca before; // adjusting it with the expression.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp:355,load,load,355,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,1,['load'],['load']
Performance,// FIXME: Load additional unused private field candidates from the external; // source.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp:10,Load,Load,10,interpreter/llvm-project/clang/lib/Sema/Sema.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp,1,['Load'],['Load']
Performance,"// FIXME: Load constants into registers (e.g. with fld1) to not break; // instructions like x87.; // Ideally we would like the only limitation on executing instructions to be the; // availability of the CPU resources (e.g. execution ports) needed to execute; // them, instead of the availability of their data dependencies.; // To achieve that, one approach is to generate instructions that do not have; // data dependencies between them.; //; // For some instructions, this is trivial:; // mov rax, qword ptr [rsi]; // mov rax, qword ptr [rsi]; // mov rax, qword ptr [rsi]; // mov rax, qword ptr [rsi]; // For the above snippet, haswell just renames rax four times and executes the; // four instructions two at a time on P23 and P0126.; //; // For some instructions, we just need to make sure that the source is; // different from the destination. For example, IDIV8r reads from GPR and; // writes to AX. We just need to ensure that the Var is assigned a; // register which is different from AX:; // idiv bx; // idiv bx; // idiv bx; // idiv bx; // The above snippet will be able to fully saturate the ports, while the same; // with ax would issue one uop every `latency(IDIV8r)` cycles.; //; // Some instructions make this harder because they both read and write from; // the same register:; // inc rax; // inc rax; // inc rax; // inc rax; // This has a data dependency from each instruction to the next, limit the; // number of instructions that can be issued in parallel.; // It turns out that this is not a big issue on recent Intel CPUs because they; // have heuristics to balance port pressure. In the snippet above, subsequent; // instructions will end up evenly distributed on {P0,P1,P5,P6}, but some CPUs; // might end up executing them all on P0 (just because they can), or try; // avoiding P5 because it's usually under high pressure from vector; // instructions.; // This issue is even more important for high-latency instructions because; // they increase the idle time of the CPU, e.g. :",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/ParallelSnippetGenerator.cpp:10,Load,Load,10,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/ParallelSnippetGenerator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/ParallelSnippetGenerator.cpp,1,['Load'],['Load']
Performance,// FIXME: Merge points from several runs (latency and uops).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/llvm-exegesis.cpp:42,latency,latency,42,interpreter/llvm-project/llvm/tools/llvm-exegesis/llvm-exegesis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/llvm-exegesis.cpp,1,['latency'],['latency']
Performance,"// FIXME: Model these properly. LOAD and STORE are complicated, and; // STORE expects the unlegalized operand in some cases.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:32,LOAD,LOAD,32,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['LOAD'],['LOAD']
Performance,// FIXME: Need to fix extra SGPR->VGPRcopies inserted; // FIXME: Don't know this was defined by operand 0; //; // TODO: Remove this when we have copy folding optimizations after; // RegBankSelect.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:158,optimiz,optimizations,158,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,1,['optimiz'],['optimizations']
Performance,// FIXME: Need to split the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['load'],['load']
Performance,// FIXME: No atomic loads are supported.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:20,load,loads,20,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,1,['load'],['loads']
Performance,// FIXME: No atomics loads are supported.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:21,load,loads,21,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,1,['load'],['loads']
Performance,"// FIXME: Nodes such as CopyFromReg probably should not advance the current; // cycle. Otherwise, we can wrongly mask real stalls. If the non-machine node; // has predecessors the cycle will be advanced when they are scheduled.; // But given the crude nature of modeling latency though such nodes, we; // currently need to treat these nodes like real instructions.; // if (!SU->getNode() || !SU->getNode()->isMachineOpcode()) return;",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:271,latency,latency,271,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,1,['latency'],['latency']
Performance,// FIXME: Not currently possible to express a shuffle mask for a scalable; // vector for this case.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Instructions.cpp:65,scalab,scalable,65,interpreter/llvm-project/llvm/lib/IR/Instructions.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Instructions.cpp,3,['scalab'],['scalable']
Performance,"// FIXME: Not quite happy with the statistics here. We probably should; // disable this tracking when called via LoadSelector.; // Also, should entries without methods count as misses?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:113,Load,LoadSelector,113,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['Load'],['LoadSelector']
Performance,"// FIXME: Once this is working, then enable flag will change to a target; // check for whether the frame is large enough to want to use virtual; // frame index registers. Functions which don't want/need this optimization; // will continue to use the existing code path.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp:208,optimiz,optimization,208,interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp,1,['optimiz'],['optimization']
Performance,// FIXME: Optimizations need to be implemented here.; // Otherwise unroll into some nasty scalar code and rebuild the vector.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:10,Optimiz,Optimizations,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['Optimiz'],['Optimizations']
Performance,// FIXME: Optimize away range check based on pivot comparisons.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:10,Optimiz,Optimize,10,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,4,['Optimiz'],['Optimize']
Performance,// FIXME: Optimize the efficiency for cloned value replacement. The current; // implementation is O(SortedBBsToSinkInto.size() * I.num_uses()).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopSink.cpp:10,Optimiz,Optimize,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopSink.cpp,1,['Optimiz'],['Optimize']
Performance,// FIXME: Optimize to allow individual templates to be deserialized.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:10,Optimiz,Optimize,10,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,1,['Optimiz'],['Optimize']
Performance,"// FIXME: Perform ""exact type"" matching first, per CWG discussion?; // Or implement this via an implied 'T(T) -> T' deduction guide?; // FIXME: Do we need/want a std::initializer_list<T> special case?; // Look up deduction guides, including those synthesized from constructors.; //; // C++1z [over.match.class.deduct]p1:; // A set of functions and function templates is formed comprising:; // - For each constructor of the class template designated by the; // template-name, a function template [...]; // - For each deduction-guide, a function or function template [...]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:10,Perform,Perform,10,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['Perform'],['Perform']
Performance,// FIXME: Perform error recovery.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp:10,Perform,Perform,10,interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp,2,['Perform'],['Perform']
Performance,// FIXME: Perform the checks on the field types in SemaInit.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:10,Perform,Perform,10,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,2,['Perform'],['Perform']
Performance,// FIXME: PerformContextualImplicitConversion doesn't always tell us if it; // failed and produced a diagnostic.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp:10,Perform,PerformContextualImplicitConversion,10,interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,1,['Perform'],['PerformContextualImplicitConversion']
Performance,// FIXME: PerformContextualImplicitConversion should return ExprError; // itself in this case.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:10,Perform,PerformContextualImplicitConversion,10,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['Perform'],['PerformContextualImplicitConversion']
Performance,"// FIXME: Precompute which substatement of a compound statement we; // would jump to, and go straight there rather than performing a; // linear scan each time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:120,perform,performing,120,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['perform'],['performing']
Performance,// FIXME: Properly handle all of the latency adjustments for address; // writeback.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,latency,latency,37,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,2,['latency'],['latency']
Performance,"// FIXME: Rather than performing a copy here, we should really be; // initializing the field in place. This would require us to propagate the; // storage location of the field to the AST node that creates the; // `RecordValue`.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/FlowSensitive/TypeErasedDataflowAnalysis.cpp:22,perform,performing,22,interpreter/llvm-project/clang/lib/Analysis/FlowSensitive/TypeErasedDataflowAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/FlowSensitive/TypeErasedDataflowAnalysis.cpp,1,['perform'],['performing']
Performance,"// FIXME: Really we would like to issue multiple 128-bit loads and stores per; // iteration. Should we report a larger size and let it legalize?; //; // FIXME: Should we use narrower types for local/region, or account for when; // unaligned access is legal?; //; // FIXME: This could use fine tuning and microbenchmarks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:57,load,loads,57,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,1,['load'],['loads']
Performance,// FIXME: Return false for x87 stack register classes for now. We can't; // allow any loads of these registers before FpGet_ST0_80.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:86,load,loads,86,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['loads']
Performance,// FIXME: Rewrite the extracted code performing any required adjustments.; // FIXME: Capture any field if necessary (method -> function extraction).; // FIXME: Sort captured variables by name.; // FIXME: Capture 'this' / 'self' if necessary.; // FIXME: Compute the actual parameter types.; // Compute the location of the extracted declaration.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/Refactoring/Extract/Extract.cpp:37,perform,performing,37,interpreter/llvm-project/clang/lib/Tooling/Refactoring/Extract/Extract.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/Refactoring/Extract/Extract.cpp,1,['perform'],['performing']
Performance,// FIXME: Scalable offsets are not yet handled in the offset code below.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp:10,Scalab,Scalable,10,interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,1,['Scalab'],['Scalable']
Performance,"// FIXME: Should also use this for OpenCL, but it requires addressing the; // problem of kernels being called.; //; // FIXME: This doesn't apply the optimization of coercing pointers in structs; // to global address space when using byref. This would require implementing a; // new kind of coercion of the in-memory type when for indirect arguments.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/AMDGPU.cpp:149,optimiz,optimization,149,interpreter/llvm-project/clang/lib/CodeGen/Targets/AMDGPU.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/AMDGPU.cpp,1,['optimiz'],['optimization']
Performance,"// FIXME: Should atomic loads be IsLoad, IsAtomic, or both?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenDAGPatterns.cpp:24,load,loads,24,interpreter/llvm-project/llvm/utils/TableGen/CodeGenDAGPatterns.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenDAGPatterns.cpp,1,['load'],['loads']
Performance,// FIXME: Should be ok if they addresses are identical. But earlier; // optimizations really should have eliminated one of the loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp:72,optimiz,optimizations,72,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,2,"['load', 'optimiz']","['loads', 'optimizations']"
Performance,"// FIXME: Should not be relying on memoperands.; // Look at the source operands of every instruction to see if; // any of them results from a previous memory operation that affects; // its current usage. If so, an s_waitcnt instruction needs to be; // emitted.; // If the source operand was defined by a load, add the s_waitcnt; // instruction.; //; // Two cases are handled for destination operands:; // 1) If the destination operand was defined by a load, add the s_waitcnt; // instruction to guarantee the right WAW order.; // 2) If a destination operand that was used by a recent export/store ins,; // add s_waitcnt on exp_cnt to guarantee the WAR order.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:304,load,load,304,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,2,['load'],['load']
Performance,"// FIXME: Should perform GVN again after PRE does something. PRE can move; // computations into blocks where they become fully redundant. Note that; // we can't do this until PRE's critical edge splitting updates memdep.; // Actually, when this happens, we should just fully integrate PRE into GVN.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:17,perform,perform,17,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['perform'],['perform']
Performance,"// FIXME: Should the Dyld be retaining module information? Probably not.; //; // This is the accessor for the target address, so make sure to check the; // load address of the symbol, not the local address.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp:156,load,load,156,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,1,['load'],['load']
Performance,// FIXME: Should we cache this at all?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/BasicObjCFoundationChecks.cpp:20,cache,cache,20,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/BasicObjCFoundationChecks.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/BasicObjCFoundationChecks.cpp,1,['cache'],['cache']
Performance,// FIXME: Should we keep track within VCtx that we did or didnot; // encounter pasting - and only then perform this loop.; // Perform token pasting (concatenation) prior to stringization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/TokenLexer.cpp:103,perform,perform,103,interpreter/llvm-project/clang/lib/Lex/TokenLexer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/TokenLexer.cpp,2,"['Perform', 'perform']","['Perform', 'perform']"
Performance,"// FIXME: Sink this logic into the module, similar to the handling of; // globals. This will make moving to a concurrent model much easier.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/ModuleUtils.cpp:110,concurren,concurrent,110,interpreter/llvm-project/llvm/lib/Transforms/Utils/ModuleUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/ModuleUtils.cpp,1,['concurren'],['concurrent']
Performance,// FIXME: Textual headers are not marked in the HeaderInfo table. Load; // them here.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:66,Load,Load,66,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['Load'],['Load']
Performance,"// FIXME: The ASTReader still calls loadGlobalIndex and loads the file; // We should investigate how to suppress it completely.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:36,load,loadGlobalIndex,36,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,2,['load'],"['loadGlobalIndex', 'loads']"
Performance,"// FIXME: The ScheduleDAG currently loses information about which of a; // node's values is consumed by each dependence. Consequently, if the node; // defines multiple register classes, we don't know which to pressurize; // here. Instead the following loop consumes the register defs in an; // arbitrary order. At least it handles the common case of clustered loads; // to the same class. For precise liveness, each SDep needs to indicate the; // result number. But that tightly couples the ScheduleDAG with the; // SelectionDAG making updates tricky. A simpler hack would be to attach a; // value type or register class to SDep.; //; // The most important aspect of register tracking is balancing the increase; // here with the reduction further below. Note that this SU may use multiple; // defs in PredSU. The can't be determined here, but we've already; // compensated by reducing NumRegDefsLeft in PredSU during; // ScheduleDAGSDNodes::AddSchedEdges.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:360,load,loads,360,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,1,['load'],['loads']
Performance,"// FIXME: The below just finds *a* unused register. Maybe code could be; // optimized more if this looks for the register that isn't used for the; // longest time around this place, to enable more scheduling freedom. Not; // sure if that would actually result in a big performance difference; // though. Maybe RegisterScavenger::findSurvivorBackwards has some logic; // already to do this - but it's unclear if that could easily be used here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:76,optimiz,optimized,76,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,2,"['optimiz', 'perform']","['optimized', 'performance']"
Performance,// FIXME: The current implementation only supports loading decls with; // a lookup name from a single translation unit. If multiple; // translation units contains decls with the same lookup name an; // error will be returned.; // Try to get the value from the heavily cached storage.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp:51,load,loading,51,interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp,2,"['cache', 'load']","['cached', 'loading']"
Performance,"// FIXME: The hint threshold has the same value used by the regular inliner; // when not optimzing for size. This should probably be lowered after; // performance testing.; // FIXME: this comment is cargo culted from the old pass manager, revisit).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:151,perform,performance,151,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['perform'],['performance']
Performance,// FIXME: The loop above only tries to split in halves. But if the input; // vector for example is <3 x i16> it wouldn't be able to detect a; // SplatBitSize of 16. No idea if that is a design flaw currently limiting; // optimizations. I guess that back in the days when this helper was created; // vectors normally was power-of-2 sized.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:221,optimiz,optimizations,221,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['optimiz'],['optimizations']
Performance,"// FIXME: The offsets of empty bases can be tricky because of; // of the so called ""empty base class optimization"".; // If a base class has been optimized out; // we should not try to create a binding, otherwise we should.; // Unfortunately, at the moment ASTRecordLayout doesn't expose; // the actual sizes of the empty bases; // and trying to infer them from offsets/alignments; // seems to be error-prone and non-trivial because of the trailing padding.; // As a temporary mitigation we don't create bindings for empty bases.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:101,optimiz,optimization,101,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,2,['optimiz'],"['optimization', 'optimized']"
Performance,"// FIXME: The receiver could be a reference to a class, meaning that; // we should use the class method.; // id x = [NSObject class];; // [x performSelector:... withObject:... afterDelay:...];",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/RetainSummaryManager.cpp:141,perform,performSelector,141,interpreter/llvm-project/clang/lib/Analysis/RetainSummaryManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/RetainSummaryManager.cpp,1,['perform'],['performSelector']
Performance,"// FIXME: The scheduler currently can't handle values larger than 16. But; // the values can actually go up to 32 for floating-point load/store; // multiple (VLDMIA etc.). Also, the way this code is reasoning about memory; // operations isn't right; we could end up with ""extra"" memory operands for; // various reasons, like tail merge merging two memory operations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:133,load,load,133,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['load'],['load']
Performance,"// FIXME: The special namespace treatment (not sending itself to; // CodeGen, but only its content - if the contained decl should be; // emitted) works around issue with the static initialization when; // having a PCH and loading a library. We don't want to generate; // code for the static that will come through the library.; //; // This will be fixed with the clang::Modules. Make sure we remember.; // assert(!getCI()->getLangOpts().Modules && ""Please revisit!"");",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DeclCollector.cpp:222,load,loading,222,interpreter/cling/lib/Interpreter/DeclCollector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DeclCollector.cpp,1,['load'],['loading']
Performance,"// FIXME: The vectorizer is highly sensistive to the cost of these; // instructions, which suggests that it may be using the costs incorrectly.; // But, for now, just make them free to avoid performance regressions for; // vector targets.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:191,perform,performance,191,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['perform'],['performance']
Performance,// FIXME: There are no Thumb1 load/store instructions with negative; // offsets. So the Base != ARM::SP might be unnecessary.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// FIXME: There's a potential issue lurking here if a single instance of; // RuntimeDyld is used to load multiple objects. The current implementation; // associates a single memory manager with a RuntimeDyld instance. Even; // though the public class spawns a new 'impl' instance for each load,; // they share a single memory manager. This can become a problem when page; // permissions are applied.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:100,load,load,100,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,2,['load'],['load']
Performance,"// FIXME: These numbers are for the A2, how well they work for other cores is; // an open question. On the A2, the isel instruction has a 2-cycle latency; // but single-cycle throughput. These numbers are used in combination with; // the MispredictPenalty setting from the active SchedMachineModel.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:146,latency,latency,146,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,2,"['latency', 'throughput']","['latency', 'throughput']"
Performance,"// FIXME: This assumes that all values are signed integers which may; // be incorrect in unusual codes and incorrectly use sext instead of zext.; // for (uint32_t i = 0; i < 512; ++i) {; // uint8_t trunc = i;; // A[trunc] = 42;; // }; // This consecutively iterates twice over A. If `trunc` is sign-extended,; // we would conclude that this may iterate backwards over the array.; // However, LoopCacheAnalysis is heuristic anyway and transformations must; // not result in wrong optimizations if the heuristic was incorrect.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopCacheAnalysis.cpp:479,optimiz,optimizations,479,interpreter/llvm-project/llvm/lib/Analysis/LoopCacheAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopCacheAnalysis.cpp,1,['optimiz'],['optimizations']
Performance,"// FIXME: This behaves strangely. If, for example, you have 32 load + stores,; // the first 16 loads will be interleaved with the stores, and the next 16 will; // be clustered as expected. It should really split into 2 16 store batches.; //; // Loads are clustered until this returns false, rather than trying to schedule; // groups of stores. This also means we have to deal with saying different; // address space loads should be clustered, and ones which might cause bank; // conflicts.; //; // This might be deprecated so it might not be worth that much effort to fix.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:63,load,load,63,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,4,"['Load', 'load']","['Loads', 'load', 'loads']"
Performance,"// FIXME: This can easily happen, if we have a reference to a submodule that; // did not result in us loading a module file for that submodule. For; // instance, a cross-top-level-module 'conflict' declaration will hit this.; // assert((ID || !Mod) &&; // ""asked for module ID for non-local, non-imported module"");",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp:102,load,loading,102,interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,1,['load'],['loading']
Performance,"// FIXME: This can perform qualified lookups into function contexts,; // which are meaningless.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:19,perform,perform,19,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['perform'],['perform']
Performance,// FIXME: This causes each file to be re-parsed and syntax-highlighted; // and macro-expanded separately for each report. We could cache such rewrites; // across all reports and only re-do the part that's actually different:; // the warning/note bubbles.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/HTMLDiagnostics.cpp:131,cache,cache,131,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/HTMLDiagnostics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/HTMLDiagnostics.cpp,1,['cache'],['cache']
Performance,// FIXME: This check is dubious. It's used to get around a problem where; // people incorrectly expect inline asm directives to remain in the same; // relative order. This is untenable because normal compiler; // optimizations (like this one) may reorder and/or merge these; // directives.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp:213,optimiz,optimizations,213,interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,1,['optimiz'],['optimizations']
Performance,"// FIXME: This does not work for vectors on most targets. Sign-; // and zero-extend operations are currently folded into extending; // loads, whether they are legal or not, and then we end up here; // without any support for legalizing them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:135,load,loads,135,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['loads']
Performance,// FIXME: This duplicates the check performed for warn_not_in_enum below.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp:36,perform,performed,36,interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,1,['perform'],['performed']
Performance,"// FIXME: This function check the maximum table size and density, but the; // minimum size is not checked. It would be nice if the minimum size is; // also combined within this function. Currently, the minimum size check is; // performed in findJumpTable() in SelectionDAGBuiler and; // getEstimatedNumberOfCaseClusters() in BasicTTIImpl.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp:228,perform,performed,228,interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetLoweringBase.cpp,1,['perform'],['performed']
Performance,"// FIXME: This implementation works under the assumption that load/store queue; // entries are reserved at 'instruction dispatched' stage, and released at; // 'instruction executed' stage. This currently matches the behavior of LSUnit.; //; // The current design minimizes the number of events generated by the; // Dispatch/Execute stages, at the cost of doing extra bookkeeping in method; // `onEvent`. However, it introduces a subtle dependency between this view and; // how the LSUnit works.; //; // In future we should add a new ""memory queue"" event type, so that we stop; // making assumptions on how LSUnit internally works (See PR39828).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.cpp:62,load,load,62,interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.cpp,3,"['load', 'queue']","['load', 'queue']"
Performance,// FIXME: This is a heuristic that works around the fact that; // LLVM IR debug intrinsics cannot yet distinguish between; // memory and value locations: Because a dbg.declare(alloca) is; // implicitly a memory location no DW_OP_deref operation for the; // last direct load from an alloca is necessary. This condition; // effectively drops the *last* DW_OP_deref in the expression.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp:269,load,load,269,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,1,['load'],['load']
Performance,"// FIXME: This is a temporary fix for the ROOT module preloading mechanism.; // When we preload modules we would like to enable a module as if we called; // clang::Sema::ActOnModuleImport (which does not call HandleTopLevelDecl).; // However, we need a valid source locations as modules are very sensitive; // to them. In order to have a valid source location,; // Interpreter::loadModule calls '#pragma clang module import ""A""', which; // calls HandleTopLevelDecl which causes CodeGen to run the module; // initializers eagerly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DeclCollector.cpp:378,load,loadModule,378,interpreter/cling/lib/Interpreter/DeclCollector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DeclCollector.cpp,1,['load'],['loadModule']
Performance,"// FIXME: This is a workaround for poor cost modelling. Min/Max intrinsics are; // not currently canonical, but soon will be. Code without them uses icmp, and; // so is not tail predicated as per the condition above. In order to get the; // same performance we treat min and max the same as an icmp for tailpred; // purposes for the moment (we often rely on non-tailpred and higher VF's to; // pick more optimial instructions like VQDMULH. They need to be recognized; // directly by the vectorizer).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:246,perform,performance,246,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['perform'],['performance']
Performance,"// FIXME: This is untested and unused anywhere in the LLVM project, it's; // used/needed by Julia (an external project). It should have some coverage; // (at least tests, but ideally example functionality).; /// Obtain a copy of this LoadedObjectInfo.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/DIContext.h:234,Load,LoadedObjectInfo,234,interpreter/llvm-project/llvm/include/llvm/DebugInfo/DIContext.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/DIContext.h,1,['Load'],['LoadedObjectInfo']
Performance,"// FIXME: This method is too general. In principle we should compute the number; // of instructions required to synthesize the immediate inline compared to; // synthesizing the address inline and relying on non .text sections.; // For static O32 and N32 this may yield a small benefit, for static N64 this is; // likely to yield a much larger benefit as we have to synthesize a 64bit; // address to load a 64 bit value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:399,load,load,399,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,1,['load'],['load']
Performance,// FIXME: This multiplier was not really tuned up.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionImport.cpp:41,tune,tuned,41,interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionImport.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionImport.cpp,1,['tune'],['tuned']
Performance,"// FIXME: This pass is really intended to be invoked during IR optimization,; // but it's very NVPTX-specific.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp:63,optimiz,optimization,63,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXTargetMachine.cpp,1,['optimiz'],['optimization']
Performance,// FIXME: This pass will invalidate cached MBBLiveIns for regions; // inbetween the defs and region we sinked the def to. Cached pressure; // for regions where a def is sinked from will also be invalidated. Will; // need to be fixed if there is another pass after this pass.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp:36,cache,cached,36,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNSchedStrategy.cpp,2,"['Cache', 'cache']","['Cached', 'cached']"
Performance,"// FIXME: This should not be treated as a SFINAE context, because; // we will cache an incorrect exception specification. However, clang; // bootstrap relies this! See PR31692.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp:78,cache,cache,78,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,1,['cache'],['cache']
Performance,// FIXME: Tune this more.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/Localizer.cpp:10,Tune,Tune,10,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/Localizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/Localizer.cpp,1,['Tune'],['Tune']
Performance,// FIXME: Tune this.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:10,Tune,Tune,10,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['Tune'],['Tune']
Performance,// FIXME: Unaligned loads need special handling. Doublewords require; // word-alignment.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:20,load,loads,20,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,1,['load'],['loads']
Performance,// FIXME: Use VLDM / VSTM to emulate indexed FP load / store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:48,load,load,48,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['load'],['load']
Performance,"// FIXME: Use function attribute ""OptimizeForSize"" and/or CodeGenOptLevel to; // decide if we should generate a 16-byte constant mask when we only need 4 or; // 8 bytes for the scalar case.; // There are no scalar bitwise logical SSE/AVX instructions, so we; // generate a 16-byte vector constant and logic op even for the scalar case.; // Using a 16-byte mask allows folding the load of the mask with; // the logic op, so it can save (~4 bytes) on code size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:34,Optimiz,OptimizeForSize,34,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,"['Optimiz', 'load']","['OptimizeForSize', 'load']"
Performance,"// FIXME: Using the instsimplify logic directly for this is inefficient; // because we have to continually rebuild the argument list even when no; // simplifications can be performed. Until that is fixed with remapping; // inside of instsimplify, directly constant fold calls here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp:173,perform,performed,173,interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,1,['perform'],['performed']
Performance,// FIXME: VNEG FP and -0? I think we'll need to handle this once we allow; // fp16 -> fp32 vector conversions.; // Instructions that perform a NOT will generate 1s from 0s.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp:133,perform,perform,133,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,1,['perform'],['perform']
Performance,"// FIXME: We are ignoring the suggestions currently, because they are; // only 50% accurate (even if the second suggestion is unavailable),; // which may confuse the user.; // Think how to perform more accurate suggestions?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CloneChecker.cpp:189,perform,perform,189,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CloneChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CloneChecker.cpp,1,['perform'],['perform']
Performance,// FIXME: We can cache this if needed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/MemRegion.h:17,cache,cache,17,interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/MemRegion.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/MemRegion.h,3,['cache'],['cache']
Performance,// FIXME: We can extract pow of 2 of splat constant for scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Constants.cpp:56,scalab,scalable,56,interpreter/llvm-project/llvm/lib/IR/Constants.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Constants.cpp,1,['scalab'],['scalable']
Performance,// FIXME: We can move load/store/call/free instructions above the call if the; // call does not mod/ref the memory location being processed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/TailRecursionElimination.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/TailRecursionElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/TailRecursionElimination.cpp,1,['load'],['load']
Performance,// FIXME: We can possibly optimize this later to cache this value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/MemRegion.h:26,optimiz,optimize,26,interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/MemRegion.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Core/PathSensitive/MemRegion.h,2,"['cache', 'optimiz']","['cache', 'optimize']"
Performance,"// FIXME: We cannot use stdin for an update because stdin will be; // consumed by the BinaryHolder during the debugmap parsing, and; // then we will want to consume it again in DwarfLinker. If we; // used a unique BinaryHolder object that could cache multiple; // binaries this restriction would go away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/dsymutil.cpp:245,cache,cache,245,interpreter/llvm-project/llvm/tools/dsymutil/dsymutil.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/dsymutil.cpp,1,['cache'],['cache']
Performance,"// FIXME: We could perform more analysis here to determine whether a; // particular class type has any conversions to Objective-C types. For now,; // just accept all class types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp:19,perform,perform,19,interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,1,['perform'],['perform']
Performance,// FIXME: We could probably handle weird extending loads better.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['loads']
Performance,// FIXME: We could probably with some care handle both volatile and ordered; // atomic loads here but it isn't clear that this is important.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:87,load,loads,87,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['loads']
Performance,"// FIXME: We currently only set the profile data when it is missing.; // With PGO, this can be used to refine even existing profile data with; // context information. This needs to be done after more performance; // testing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:200,perform,performance,200,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['perform'],['performance']
Performance,"// FIXME: We don't cache the result of getFileInfo across the call to; // getFileAndSuggestModule, because it's a reference to an element of; // a container that could be reallocated across this call.; //; // If we have no includer, that means we're processing a #include; // from a module build. We should treat this as a system header if we're; // building a [system] module.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:19,cache,cache,19,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['cache'],['cache']
Performance,"// FIXME: We don't currently have the ability to accurately; // compute the length of an initializer list without; // performing full type-checking of the initializer list; // (since we have to determine where braces are implicitly; // introduced and such). So, we fall back to making the array; // type a dependently-sized array type with no specified; // bound.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:118,perform,performing,118,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['perform'],['performing']
Performance,// FIXME: We don't currently support creating pre-indexed loads/stores when; // the load or store is the unscaled version. If we decide to perform such an; // optimization in the future the cases for the unscaled loads/stores will; // need to be added here.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:58,load,loads,58,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,5,"['load', 'optimiz', 'perform']","['load', 'loads', 'optimization', 'perform']"
Performance,"// FIXME: We inherit EnableImplicitIMT from TBufferMerger tests (we are sharing the same executable) where we call; // EnableThreadSafety(). Here, we hit a race condition in TBranch::FlushBaskets. Once we get that fixed we probably; // should re-enable implicit MT.; //; // In general, we should probably have a way to conditionally enable/disable thread safety.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/test/TFileMergerTests.cxx:156,race condition,race condition,156,io/io/test/TFileMergerTests.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/test/TFileMergerTests.cxx,1,['race condition'],['race condition']
Performance,"// FIXME: We may be able to remove those checks as cling::loadModule; // checks if a module was alredy loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:58,load,loadModule,58,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,2,['load'],"['loadModule', 'loaded']"
Performance,// FIXME: We may possibly optimize the COPY once we find ways to make LLVM; // optimizations (mainly Register Coalescer) aware of WWM register liveness.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:26,optimiz,optimize,26,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,2,['optimiz'],"['optimizations', 'optimize']"
Performance,"// FIXME: We need to run some loop optimizations to re-rotate loops after; // simplifycfg and others undo their rotation.; // Optimize the loop execution. These passes operate on entire loop nests; // rather than on each loop in an inside-out manner, and so they are actually; // function passes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:35,optimiz,optimizations,35,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,2,"['Optimiz', 'optimiz']","['Optimize', 'optimizations']"
Performance,"// FIXME: We should add the pcm to the InMemoryModuleCache if it could be; // read again later, but we do not have the context here to determine if it; // is safe to change the result of InMemoryModuleCache::getPCMState().; // FIXME: This allows use of the VFS; we do not allow use of the; // VFS when actually loading a module.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:311,load,loading,311,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loading']
Performance,// FIXME: We should also canonicalize loads of vectors when their elements are; // cast to other types.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:38,load,loads,38,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['loads']
Performance,"// FIXME: We should load only the first available and rely on other callbacks; // such as RequireCompleteType and LookupUnqualified to load all.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx:20,load,load,20,core/metacling/src/TClingCallbacks.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx,2,['load'],['load']
Performance,"// FIXME: We should optimize this routine instead making it do the wrong thing; // returning an empty comment if the decl came from the AST.; // In order to do that we need to: check if the decl has an attribute and; // return the attribute content (including walking the redecl chain) and if; // this is not the case we should try finding it in the header file.; // This will allow us to move the implementation of TCling*Info::Title() in; // TClingDeclInfo.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/clingutils/src/TClingUtils.cxx:20,optimiz,optimize,20,core/clingutils/src/TClingUtils.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/clingutils/src/TClingUtils.cxx,1,['optimiz'],['optimize']
Performance,"// FIXME: We temporarily disable post increment load from program memory,; // due to bug https://github.com/llvm/llvm-project/issues/59914.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp:48,load,load,48,interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelLowering.cpp,1,['load'],['load']
Performance,// FIXME: We're generating redundant loads and stores here!,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp:37,load,loads,37,interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,1,['load'],['loads']
Performance,"// FIXME: Without optimizations, the temporary result from `await_suspend()`; // may be put on the coroutine frame since the coroutine frame constructor; // will think the temporary variable will escape from the; // `coroutine_handle<>::address()` call. This is problematic since the; // coroutine should be considered to be suspended after it enters; // `await_suspend` so it shouldn't access/update the coroutine frame after; // that.; //; // See https://github.com/llvm/llvm-project/issues/65054 for the report.; //; // The long term solution may wrap the whole logic about `await-suspend`; // into a standalone function. This is similar to the proposed solution; // in tryMarkAwaitSuspendNoInline. See the comments there for details.; //; // The short term solution here is to mark `coroutine_handle<>::address()`; // function as always-inline so that the coroutine frame constructor won't; // think the temporary result is escaped incorrectly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCoroutine.cpp:18,optimiz,optimizations,18,interpreter/llvm-project/clang/lib/Sema/SemaCoroutine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCoroutine.cpp,1,['optimiz'],['optimizations']
Performance,// FIXME: check for the largest legal type we can load/store to.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:50,load,load,50,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['load'],['load']
Performance,"// FIXME: could perform constant-folding.; // If there was a truncation, and we have a right-shift, we can only fold if; // we are left with the original sign bit. Likewise, if we were just checking; // that this is a sighbit extraction, this is the place to check it.; // FIXME: zero shift amount is also legal here, but we can't *easily* check; // more than one predicate so it's not really worth it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp:16,perform,perform,16,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,1,['perform'],['perform']
Performance,// FIXME: extern global merging is only enabled when we optimise for size; // because there are some regressions with it also enabled for performance.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:138,perform,performance,138,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,1,['perform'],['performance']
Performance,// FIXME: getIndexedOffsetInType() does not handled scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:52,scalab,scalable,52,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['scalab'],['scalable']
Performance,"// FIXME: implement a more clever base choosing policy.; // Currently we always choose an exist load/store offset. This maybe lead to; // suboptimal code sequences. For example, for one DS chain with offsets; // {-32769, 2003, 2007, 2011}, we choose -32769 as base offset, and left disp; // for load/stores are {0, 34772, 34776, 34780}. Though each offset now is a; // multipler of 4, it cannot be represented by sint16.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:96,load,load,96,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,2,['load'],['load']
Performance,"// FIXME: it is entirely possible that PHI translating will end up with; // the same value. Consider PHI translating something like:; // X = phi [x, bb1], [y, bb2]. PHI translating for bb1 doesn't *need*; // to recurse here, pedantically speaking.; // If getNonLocalPointerDepFromBB fails here, that means the cached; // result conflicted with the Visited list; we have to conservatively; // assume it is unknown, but this also does not block PRE of the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:310,cache,cached,310,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,"['cache', 'load']","['cached', 'load']"
Performance,"// FIXME: it might be a worthwhile optimization to not mask loaded; // values if all the registers involved in address calculation are already; // hardened, leading to this load not able to execute on a miss-speculated; // path.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:35,optimiz,optimization,35,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,3,"['load', 'optimiz']","['load', 'loaded', 'optimization']"
Performance,"// FIXME: it might make sense to use a locked operation here but on a; // different cache-line to prevent cache-line bouncing. In practice it; // is probably a small win, and x86 processors without mfence are rare; // enough that we do not bother.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:84,cache,cache-line,84,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['cache'],['cache-line']
Performance,"// FIXME: lli aims to provide both, RuntimeDyld and JITLink, as the dynamic; // loaders for it's JIT implementations. And they both offer debugging via the; // GDB JIT interface, which builds on the two well-known symbol names below.; // As these symbols must be unique accross the linked executable, we can only; // define them in one of the libraries and make the other depend on it.; // OrcTargetProcess is a minimal stub for embedding a JIT client in remote; // executors. For the moment it seems reasonable to have the definition there; // and let ExecutionEngine depend on it, until we find a better solution.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/GDBRegistrationListener.cpp:80,load,loaders,80,interpreter/llvm-project/llvm/lib/ExecutionEngine/GDBRegistrationListener.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/GDBRegistrationListener.cpp,1,['load'],['loaders']
Performance,// FIXME: load/store narrowing should be moved to lower action,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,"// FIXME: loads from the stack with an immediate offset from the stack; // pointer probably shouldn't be hardened, which could result in a; // significant optimization. See section ""Dont check loads from; // compile-time constant stack offsets"", in; // https://llvm.org/docs/SpeculativeLoadHardening.html",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:10,load,loads,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,3,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"// FIXME: move this information to an HTML file in docs/.; // At the very least, a checker plugin is a dynamic library that exports; // clang_analyzerAPIVersionString. This should be defined as follows:; //; // extern ""C""; // const char clang_analyzerAPIVersionString[] =; // CLANG_ANALYZER_API_VERSION_STRING;; //; // This is used to check whether the current version of the analyzer is known to; // be incompatible with a plugin. Plugins with incompatible version strings,; // or without a version string at all, will not be loaded.; //; // To add a custom checker to the analyzer, the plugin must also define the; // function clang_registerCheckers. For example:; //; // extern ""C""; // void clang_registerCheckers (CheckerRegistry &registry) {; // registry.addChecker<MainCallChecker>(""example.MainCallChecker"",; // ""Disallows calls to functions called main"");; // }; //; // The first method argument is the full name of the checker, including its; // enclosing package. By convention, the registered name of a checker is the; // name of the associated class (the template argument).; // The second method argument is a short human-readable description of the; // checker.; //; // The clang_registerCheckers function may add any number of checkers to the; // registry. If any checkers require additional initialization, use the three-; // argument form of CheckerRegistry::addChecker.; //; // To load a checker plugin, specify the full path to the dynamic library as; // the argument to the -load option in the cc1 frontend. You can then enable; // your custom checker using the -analyzer-checker:; //; // clang -cc1 -load </path/to/plugin.dylib> -analyze; // -analyzer-checker=<example.MainCallChecker>; //; // For a complete working example, see examples/analyzer-plugin.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Frontend/CheckerRegistry.h:527,load,loaded,527,interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Frontend/CheckerRegistry.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/StaticAnalyzer/Frontend/CheckerRegistry.h,4,['load'],"['load', 'loaded']"
Performance,// FIXME: optimize the case where the src/dest is a load or store?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:10,optimiz,optimize,10,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,"// FIXME: optimize the case where the src/dest is a load or store?; //Since the operation is StrictFP, use the preexisting chain.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:10,optimiz,optimize,10,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,// FIXME: overly conservative?; // Four loads in a row should be sufficient.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['load'],['loads']
Performance,// FIXME: probably we'll need to cache the results here somehow?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerInfo.cpp:33,cache,cache,33,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerInfo.cpp,1,['cache'],['cache']
Performance,// FIXME: remove this to enable 64-bit SLP if performance looks good.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp:46,perform,performance,46,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64Subtarget.cpp,6,['perform'],['performance']
Performance,// FIXME: share CBufferDataLayout with CBuffer load lowering.; // See https://github.com/llvm/llvm-project/issues/58381,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/DirectX/DXILResource.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Target/DirectX/DXILResource.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/DirectX/DXILResource.cpp,1,['load'],['load']
Performance,"// FIXME: speed up this search, e.g. by using a results cache for repeated; // queries?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegacyLegalizerInfo.cpp:56,cache,cache,56,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegacyLegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegacyLegalizerInfo.cpp,1,['cache'],['cache']
Performance,// FIXME: there's probably *something* we can do with scalable vectors,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp:54,scalab,scalable,54,interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,1,['scalab'],['scalable']
Performance,"// FIXME: this could be a transitional option, and we probably need to remove; // it if only we are sure this optimization could always benefit all targets.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalMerge.cpp:110,optimiz,optimization,110,interpreter/llvm-project/llvm/lib/CodeGen/GlobalMerge.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalMerge.cpp,1,['optimiz'],['optimization']
Performance,// FIXME: this is an inefficient way to handle this. We should computed the; // necessary section allocation size in loadObject by walking all the sections; // once.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:117,load,loadObject,117,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['loadObject']
Performance,// FIXME: this should be able to do something for scalable vectors,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/PatternMatch.h:50,scalab,scalable,50,interpreter/llvm-project/llvm/include/llvm/IR/PatternMatch.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/PatternMatch.h,1,['scalab'],['scalable']
Performance,"// FIXME: this works around module+PCH performance issue.; // Rather than erase the result from the map, which is O(n), just clear; // the vector of NamedDecls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:39,perform,performance,39,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['perform'],['performance']
Performance,// FIXME: we should be able to hoist loads with no other side effects if; // there are no other instructions which can change memory in this loop.; // This is a trivial form of alias analysis.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp:37,load,loads,37,interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp,1,['load'],['loads']
Performance,"// FIXME:; // globalptr->load()->fGDirectories will still contain globalptr, but we cannot; // know whether globalptr->load() has been deleted by another thread in the meantime.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TDirectory.cxx:25,load,load,25,core/base/src/TDirectory.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TDirectory.cxx,2,['load'],['load']
Performance,// FIXME? We could make this more precise by looking at the CFG and; // e.g. not counting loads in each side of an if-then-else diamond.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:90,load,loads,90,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['load'],['loads']
Performance,"// FIXME? We could take pairing of unrolled load copies into account; // by looking at the AddRec, but we would probably have to limit this; // to loops with no stores or other memory optimization barriers.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"// FIXME? We really want the register size rather than the spill size; // since the spill size may be bigger on some targets with; // limited load/store instructions. However, we don't store the; // register size anywhere (we could sum the sizes of the subregisters; // but there may be additional bits too) and we can't derive it from; // the VT's reliably due to Untyped.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/RegisterBankEmitter.cpp:142,load,load,142,interpreter/llvm-project/llvm/utils/TableGen/RegisterBankEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/RegisterBankEmitter.cpp,1,['load'],['load']
Performance,// FIXME?: Should we cache this? We call it twice when inferring immediates.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp:21,cache,cache,21,interpreter/llvm-project/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelCombinerEmitter.cpp,1,['cache'],['cache']
Performance,"// FNSTENV changes the exception mask, so load back the stored environment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// FP MMA loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:10,load,loads,10,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,2,['load'],['loads']
Performance,// FP control word may be set only from data in memory. So we need to allocate; // stack space to save/load FP control word.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:103,load,load,103,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// FSEventStreamStop and Invalidate must be called after Start and; // SetDispatchQueue to follow FSEvents API contract. The call to Receiver; // also uses Queue to not race with the initial scan.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp:156,Queue,Queue,156,interpreter/llvm-project/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/mac/DirectoryWatcher-mac.cpp,1,['Queue'],['Queue']
Performance,// Fabricate val-num for dead-code in order to suppress assertion in; // performPRE().,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:73,perform,performPRE,73,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['perform'],['performPRE']
Performance,"// Factor which can be used to tune the smoothing.; // It is used as multiplicative factor for the fixed and adaptive bandwidth.; // A value < 1 will reproduce better the tails but oversmooth the peak; // while a factor > 1 will overestimate the tail",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/hist/src/TKDE.cxx:31,tune,tune,31,hist/hist/src/TKDE.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/hist/src/TKDE.cxx,1,['tune'],['tune']
Performance,// Fail out if we encounter an operand that is not available in; // the PRE predecessor. This is typically because of loads which; // are not value numbered precisely.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:118,load,loads,118,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['load'],['loads']
Performance,// Fail to combine if we have encountered anything but a LOAD after handling; // an ExtractVectorElement.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:57,LOAD,LOAD,57,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['LOAD'],['LOAD']
Performance,// Failed to load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PreprocessingRecord.cpp:13,load,load,13,interpreter/llvm-project/clang/lib/Lex/PreprocessingRecord.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PreprocessingRecord.cpp,1,['load'],['load']
Performance,"// Failure to load a dictionary is not (quite) a failure load; // the top-level library. If we return false here, then; // we would end up in a situation where the library and thus; // the dictionary is loaded for ""cls"" but the TClass is; // not created and/or marked as unavailable (in case where; // AutoLoad is called from TClass::GetClass).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:14,load,load,14,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,3,['load'],"['load', 'loaded']"
Performance,"// Failure to load a dictionary is not (quite) a failure load; // the top-level library. See detailed comment in the TProtoClass; // branch (above).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:14,load,load,14,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,2,['load'],['load']
Performance,"// Fake request the condition, otherwise the intrinsic might be completely; // optimized away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:79,optimiz,optimized,79,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,3,['optimiz'],['optimized']
Performance,// Fall back to a stack store and stride x0 vector load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:51,load,load,51,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,// Fall back to loading it from memory.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:16,load,loading,16,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['loading']
Performance,// Fall through to handling below to perform the recording of the; // function for this callsite clone. This enables handling of cases; // where the callers were assigned to different clones of a function.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp:37,perform,perform,37,interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp,1,['perform'],['perform']
Performance,// Fall through to optimize Cmp if Cmp is CMPrr or CMPri.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:19,optimiz,optimize,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['optimiz'],['optimize']
Performance,// Fall through to the next check; see if we can optimize further.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:49,optimiz,optimize,49,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['optimiz'],['optimize']
Performance,"// Fallback to runtime check, which still can be optimized out later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp:49,optimiz,optimized,49,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp,2,['optimiz'],['optimized']
Performance,"// Fast exit: the cluster happens to be already present in the cache pool",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RClusterPool.cxx:63,cache,cache,63,tree/ntuple/v7/src/RClusterPool.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RClusterPool.cxx,1,['cache'],['cache']
Performance,// Fast path for the overly-common case (no crazy phi optimization; // necessary),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:54,optimiz,optimization,54,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['optimiz'],['optimization']
Performance,"// Fast path, no lock? Classes load at creation time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:31,load,load,31,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,2,['load'],['load']
Performance,"// Fast path: if the cache is valid, just use it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ConstantInitBuilder.cpp:21,cache,cache,21,interpreter/llvm-project/clang/lib/CodeGen/ConstantInitBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ConstantInitBuilder.cpp,1,['cache'],['cache']
Performance,"// Fast path: if we have a cached state, use it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ModuleManager.cpp:27,cache,cached,27,interpreter/llvm-project/clang/lib/Serialization/ModuleManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ModuleManager.cpp,1,['cache'],['cached']
Performance,"// Fast-isel and the optimizer generally like scalar values better than; // FCAs, so we flatten them if this is safe to do for this argument.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:21,optimiz,optimizer,21,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,3,['optimiz'],['optimizer']
Performance,"// FastCC has less than 1% performance improvement for some particular; // benchmark. But theoretically, it may has benenfit for some cases.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:27,perform,performance,27,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['perform'],['performance']
Performance,// Feature ELPM is needed for loading from extended program memory.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelDAGToDAG.cpp:30,load,loading,30,interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRISelDAGToDAG.cpp,1,['load'],['loading']
Performance,"// Fetch all the blocks in DLs scope. Because the range / block list also; // contain any subscopes, any instruction that DL dominates can be found in; // the block set.; //; // Cache the set of fetched blocks to avoid repeatedly recomputing the set in; // the LiveDebugValues pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LexicalScopes.cpp:178,Cache,Cache,178,interpreter/llvm-project/llvm/lib/CodeGen/LexicalScopes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LexicalScopes.cpp,1,['Cache'],['Cache']
Performance,"// Fetch the buffer out of the enumeration state.; // TODO: this pointer should actually be invariant between; // refreshes, which would help us do certain loop optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:161,optimiz,optimizations,161,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['optimiz'],['optimizations']
Performance,// Fetch the memoperand of the load/store that is a candidate for; // combination.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,// Fetch the next correction by erasing the typo from the cache and calling; // `TryTransform` which will iterate through corrections in; // `TransformTypoExpr`.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:58,cache,cache,58,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['cache'],['cache']
Performance,// Figure out if we are running the optimized func or the unoptimized func,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp:36,optimiz,optimized,36,interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,1,['optimiz'],['optimized']
Performance,"// Figure out the maximum size that a field can be, and ignore this; // queue if there's nothing in it that small.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp:72,queue,queue,72,interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,1,['queue'],['queue']
Performance,"// Figure out the number of lanes in vectors for this function variant. This; // is easy for fixed length, as the vlen encoding just gives us the value; // directly. However, if the vlen mangling indicated that this function; // variant expects scalable vectors we need to work it out based on the; // demangled parameter types and the scalar function signature.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/VFABIDemangler.cpp:245,scalab,scalable,245,interpreter/llvm-project/llvm/lib/IR/VFABIDemangler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/VFABIDemangler.cpp,1,['scalab'],['scalable']
Performance,"// Figure out which arguments are going to go in registers, and which in; // memory. Also, if this is a vararg function, floating point operations; // must be stored to our stack, and loaded into integer regs as well, if; // any integer regs are available for argument passing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:184,load,loaded,184,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loaded']
Performance,// Figure out which vreg this is going into. If there is no assigned vreg yet; // then there actually was no reference to it. Perhaps the load is referenced; // by a dead instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp:138,load,load,138,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,1,['load'],['load']
Performance,"// File in cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationRemote.cxx:11,cache,cache,11,net/net/src/TApplicationRemote.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationRemote.cxx,1,['cache'],['cache']
Performance,"// FileInfos are stored as a mapping, and invalidating the cache; // can change iteration order.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/ClangInternalState.cpp:59,cache,cache,59,interpreter/cling/lib/Interpreter/ClangInternalState.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/ClangInternalState.cpp,1,['cache'],['cache']
Performance,"// Files && Trees currently open; // Auxilliary class to keep track open files and loaded trees",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/inc/TEventIter.h:83,load,loaded,83,proof/proofplayer/inc/TEventIter.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/inc/TEventIter.h,1,['load'],['loaded']
Performance,"// Fill Cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddModel.cxx:8,Cache,Cache,8,roofit/roofitcore/src/RooAddModel.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddModel.cxx,1,['Cache'],['Cache']
Performance,"// Fill all the tuning parameters that should be optimized into a map",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/MethodSVM.cxx:49,optimiz,optimized,49,tmva/tmva/src/MethodSVM.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/src/MethodSVM.cxx,1,['optimiz'],['optimized']
Performance,"// Fill cache data",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMultiVarGaussian.cxx:8,cache,cache,8,roofit/roofitcore/src/RooMultiVarGaussian.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMultiVarGaussian.cxx,2,['cache'],['cache']
Performance,"// Fill new baskets into cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCacheUnzip.cxx:25,cache,cache,25,tree/tree/src/TTreeCacheUnzip.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCacheUnzip.cxx,1,['cache'],['cache']
Performance,"// Fill performance profiles using tree 't'(PROOF_PerfStats).; // Input parameters; // t: Proof output tree (PROOF_PerfStat) containing performance statistics.; // profile: Profile to be filled up with information from tree 't'.; // nactive: Number of active workers processed the query.; // Return; // Nothing; // find perfstat profile",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchRunCPU.cxx:8,perform,performance,8,proof/proofbench/src/TProofBenchRunCPU.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchRunCPU.cxx,2,['perform'],['performance']
Performance,"// Fill performance profiles using tree 't'(PROOF_PerfStats).; // Input parameters; // t: Proof output tree (PROOF_PerfStat) containing performance; // statistics.; // nactive: Number of active workers processed the query.; // Return; // Nothing; // extract timing information",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchRunDataRead.cxx:8,perform,performance,8,proof/proofbench/src/TProofBenchRunDataRead.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchRunDataRead.cxx,2,['perform'],['performance']
Performance,"// Fill the ""else"" block, created in the previous iteration; //; // %Mask1 = and i16 %scalar_mask, i32 1 << Idx; // %cond = icmp ne i16 %mask_1, 0; // br i1 %Mask1, label %cond.load, label %else; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp:177,load,load,177,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,1,['load'],['load']
Performance,"// Fill the ""else"" block, created in the previous iteration; //; // %res.phi.else3 = phi <16 x i32> [ %11, %cond.load1 ], [ %res.phi.else, %else ]; // %mask_1 = and i16 %scalar_mask, i32 1 << Idx; // %cond = icmp ne i16 %mask_1, 0; // br i1 %mask_1, label %cond.load, label %else; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp:262,load,load,262,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,1,['load'],['load']
Performance,"// Fill the ""else"" block, created in the previous iteration; //; // %res.phi.else3 = phi <16 x i32> [ %11, %cond.load1 ], [ %res.phi.else, %else ]; // %mask_1 = extractelement <16 x i1> %mask, i32 Idx; // br i1 %mask_1, label %cond.load, label %else; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp:232,load,load,232,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ScalarizeMaskedMemIntrin.cpp,1,['load'],['load']
Performance,"// Fill the cache buffer with the branches in the cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCacheUnzip.cxx:12,cache,cache,12,tree/tree/src/TTreeCacheUnzip.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCacheUnzip.cxx,2,['cache'],['cache']
Performance,"// Fill the cache for performant attribute retrival",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/res/BaseSelectionRule.h:12,cache,cache,12,core/dictgen/res/BaseSelectionRule.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/res/BaseSelectionRule.h,2,"['cache', 'perform']","['cache', 'performant']"
Performance,"// Fill the cache of all selection rules",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/res/SelectionRules.h:12,cache,cache,12,core/dictgen/res/SelectionRules.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/res/SelectionRules.h,1,['cache'],['cache']
Performance,"// Fill the cache of every selection rule",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/SelectionRules.cxx:12,cache,cache,12,core/dictgen/src/SelectionRules.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/SelectionRules.cxx,1,['cache'],['cache']
Performance,// Fill the placeholder with the new load from constant pool.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,// Filter out flags that don't apply to the first file we load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp:58,load,load,58,interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp,1,['load'],['load']
Performance,"// Filter out useless results (non-locals, etc). Keep track of the blocks; // where we have a value available in repl, also keep track of whether we see; // dependencies that produce an unknown value for the load (such as a call; // that could potentially clobber the load).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:208,load,load,208,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],['load']
Performance,"// Filtering is not implemented a very efficient way to keep it simple, but the; // implementation ensures that the performance drop is opt-in. Only when filters are; // used there is a performance loss.; // The intended use case of filtering is for debugging, when highest performance; // does not matter. Filtering is only every attempted if the message passes the; // threshold level.; // Filtering is very fast when the filter is empty.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/src/MnPrint.cxx:116,perform,performance,116,math/minuit2/src/MnPrint.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/minuit2/src/MnPrint.cxx,3,['perform'],['performance']
Performance,"// Final bailout: if the mask is simple, we are better off using an extract; // and a simple narrow shuffle. Prefer extract+unpack(h/l)ps to vpermps; // because that avoids a constant load from memory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:184,load,load,184,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// FinalReg now holds final stack pointer value, or zero if; // allocation would overflow. Compare against the current stack; // limit from the thread environment block. Note this limit is the; // lowest touched page on the stack, not the point at which the OS; // will cause an overflow exception, so this is just an optimization; // to avoid unnecessarily touching pages that are below the current; // SP but already committed to the stack by the OS.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FrameLowering.cpp:318,optimiz,optimization,318,interpreter/llvm-project/llvm/lib/Target/X86/X86FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FrameLowering.cpp,1,['optimiz'],['optimization']
Performance,// Finally load the value back from the stack temporary and return it.; // This load is not atomic and doesn't need to be.; // This load will be further type legalized.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,['load'],['load']
Performance,// Finally perform an implicit address space cast,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp:11,perform,perform,11,interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,1,['perform'],['perform']
Performance,// Finally we can emit the atomic load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// Finally we know we can profitably perform the optimisation so go; // ahead: strip all existing registers off and add them back again; // in the right order.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:37,perform,perform,37,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['perform'],['perform']
Performance,"// Finally, bypass two innermost shifts, and perform the outermost shift on; // the operands of the innermost shift.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp:45,perform,perform,45,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,1,['perform'],['perform']
Performance,"// Finally, load the updated vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,"// Finally, perform the original load only redirected to the stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:12,perform,perform,12,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,2,"['load', 'perform']","['load', 'perform']"
Performance,"// Finally, perform the post-condition check of the CallExpr and store; // the created nodes in 'Dst'.; // Note that if the call was inlined, dstCallEvaluated will be empty.; // The post-CallExpr check will occur in processCallExit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCallAndReturn.cpp:12,perform,perform,12,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCallAndReturn.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCallAndReturn.cpp,1,['perform'],['perform']
Performance,"// Finally, perform the post-condition check of the ObjCMessageExpr and store; // the created nodes in 'Dst'.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineObjC.cpp:12,perform,perform,12,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineObjC.cpp,1,['perform'],['perform']
Performance,"// Finally, perform the replacements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedAccessPass.cpp:12,perform,perform,12,interpreter/llvm-project/llvm/lib/CodeGen/InterleavedAccessPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedAccessPass.cpp,1,['perform'],['perform']
Performance,"// Finally, store the address point. Use the same LLVM types as the field to; // support optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp:89,optimiz,optimization,89,interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,1,['optimiz'],['optimization']
Performance,"// Finally, try to load the archives.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp:19,load,load,19,interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,1,['load'],['load']
Performance,"// Finally, we have control-flow based knowledge of whether the cmpxchg; // succeeded or not. We expose this to later passes by converting any; // subsequent ""icmp eq/ne %loaded, %oldval"" into a use of an appropriate; // PHI.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp:171,load,loaded,171,interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,1,['load'],['loaded']
Performance,"// Finally, we need to reorder the instrs in the BB so that the (transitive); // operands of VecInst appear before it. To see why, suppose we have; // vectorized the following code:; //; // ptr1 = gep a, 1; // load1 = load i32 ptr1; // ptr0 = gep a, 0; // load0 = load i32 ptr0; //; // We will put the vectorized load at the location of the earliest load in; // the BB, i.e. load1. We get:; //; // ptr1 = gep a, 1; // loadv = load <2 x i32> ptr0; // load0 = extractelement loadv, 0; // load1 = extractelement loadv, 1; // ptr0 = gep a, 0; //; // Notice that loadv uses ptr0, which is defined *after* it!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp:218,load,load,218,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,9,['load'],"['load', 'loadv']"
Performance,"// Find MSSA insertion point. Normally P will always have a corresponding; // memory access before which we can insert. However, with non-standard AA; // pipelines, there may be a mismatch between AA and MSSA, in which case we; // will scan for a memory access before P. In either case, we know for sure; // that at least the load will have a memory access.; // TODO: Simplify this once P will be determined by MSSA, in which case the; // discrepancy can no longer occur.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:326,load,load,326,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['load'],['load']
Performance,"// Find VMEM loads that may be executed before long-enough sequences of; // VALU instructions. We currently assume that backedges/loops, branch; // probabilities and other details can be ignored, so we essentially; // determine the largest number of VALU instructions along every; // possible path from the start of the function that may potentially be; // executed provided no backedge is ever taken.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp:13,load,loads,13,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUSetWavePriority.cpp,1,['load'],['loads']
Performance,// Find a larger type to do a load / store of a vector with.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,1,['load'],['load']
Performance,"// Find a load or store from corresponding pattern root.; // Roots may be build_vector, bitconvert or their combinations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// Find a single register and/or large offset which, if compressible, would; // allow the given instruction to be compressed.; //; // Possible return values:; //; // {Reg, 0} - Uncompressed Reg needs replacing with a compressed; // register.; // {Reg, N} - Reg needs replacing with a compressed register and; // N needs adding to the new register. (Reg may be; // compressed or uncompressed).; // {RISCV::NoRegister, 0} - No suitable optimization found for this; // instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp:434,optimiz,optimization,434,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,1,['optimiz'],['optimization']
Performance,"// Find a task to perform",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/thread/inc/TThreadPool.h:18,perform,perform,18,core/thread/inc/TThreadPool.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/thread/inc/TThreadPool.h,1,['perform'],['perform']
Performance,"// Find acceptable loads. Loads need to have the same chain (token factor),; // must not be zext, volatile, indexed, and they must be consecutive.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,"['Load', 'load']","['Loads', 'loads']"
Performance,"// Find all reorderable leaf nodes with the given VF.; // Currently the are vectorized loads,extracts without alternate operands +; // some gathering of extracts.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:87,load,loads,87,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['loads']
Performance,"// Find all reorderable nodes with the given VF.; // Currently the are vectorized stores,loads,extracts + some gathering of; // extracts.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:89,load,loads,89,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['loads']
Performance,"// Find all the potential call graph edges in this function. We track both; // actual call edges and indirect references to functions. The direct calls; // are trivially added, but to accumulate the latter we walk the instructions; // and add every operand which is a constant to the worklist to process; // afterward.; //; // Note that we consider *any* function with a definition to be a viable; // edge. Even if the function's definition is subject to replacement by; // some other module (say, a weak definition) there may still be; // optimizations which essentially speculate based on the definition and; // a way to check that the specific definition is in fact the one being; // used. For example, this could be done by moving the weak definition to; // a strong (internal) definition and making the weak definition be an; // alias. Then a test of the address of the weak function against the new; // strong definition's address would be an effective way to determine the; // safety of optimizing a direct call edge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyCallGraph.cpp:540,optimiz,optimizations,540,interpreter/llvm-project/llvm/lib/Analysis/LazyCallGraph.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyCallGraph.cpp,2,['optimiz'],"['optimizations', 'optimizing']"
Performance,"// Find and cache cling::runtime on first request.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/ValuePrinterSynthesizer.h:12,cache,cache,12,interpreter/cling/lib/Interpreter/ValuePrinterSynthesizer.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/ValuePrinterSynthesizer.h,1,['cache'],['cache']
Performance,"// Find and cache cling::runtime::gCling, setValueNoAlloc,; // setValueWithAlloc on first request.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/ValueExtractionSynthesizer.h:12,cache,cache,12,interpreter/cling/lib/Interpreter/ValueExtractionSynthesizer.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/ValueExtractionSynthesizer.h,1,['cache'],['cache']
Performance,// Find and promote load instructions which read directly from store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// Find dead PHI cycles and PHI cycles that can be replaced by a single; // value. InstCombine does these optimizations, but DAG legalization may; // introduce new opportunities, e.g., when i64 values are split up for; // 32-bit targets.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/OptimizePHIs.cpp:106,optimiz,optimizations,106,interpreter/llvm-project/llvm/lib/CodeGen/OptimizePHIs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/OptimizePHIs.cpp,1,['optimiz'],['optimizations']
Performance,// Find if it is better to use vectors or integers to load and store; // to memory.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:54,load,load,54,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,// Find loads and stores that can be merged into a single load or store pair; // instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:8,load,loads,8,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,2,['load'],"['load', 'loads']"
Performance,"// Find loads whose uses only use some of the loaded value's bits. Add an ""and""; // just after the load if the target can fold this into one extload instruction,; // with the hope of eliminating some of the other later ""and"" instructions using; // the loaded value. ""and""s that are made trivially redundant by the insertion; // of the new ""and"" are removed by this function, while others (e.g. those whose; // path from the load goes through a phi) are left for isel to potentially; // remove.; //; // For example:; //; // b0:; // x = load i32; // ...; // b1:; // y = and x, 0xff; // z = use y; //; // becomes:; //; // b0:; // x = load i32; // x' = and x, 0xff; // ...; // b1:; // z = use x'; //; // whereas:; //; // b0:; // x1 = load i32; // ...; // b1:; // x2 = load i32; // ...; // b2:; // x = phi x1, x2; // y = and x, 0xff; //; // becomes (after a call to optimizeLoadExt for each load):; //; // b0:; // x1 = load i32; // x1' = and x1, 0xff; // ...; // b1:; // x2 = load i32; // x2' = and x2, 0xff; // ...; // b2:; // x = phi x1', x2'; // y = and x, 0xff",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:8,load,loads,8,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,13,"['load', 'optimiz']","['load', 'loaded', 'loads', 'optimizeLoadExt']"
Performance,// Find out what the base pointer and index for the load is.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// Find out why we have a vector result - these are a few examples:; // 1. We have a scalar pointer and a vector of indices, or; // 2. We have a vector of pointers and a scalar index, or; // 3. We have a vector of pointers and a vector of indices, etc.; // Here we only consider combining when there is exactly one vector; // operand, since the optimization is less obviously a win due to; // needing more than one extractelements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp:345,optimiz,optimization,345,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp,1,['optimiz'],['optimization']
Performance,// Find store->load dependences (consequently true dep). Both lexically; // forward and backward dependences qualify. Disqualify loads that have; // other unknown dependences.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,2,['load'],"['load', 'loads']"
Performance,// Find stream with that name and read its data.; // FIXME: Consider validating (or even loading) all this in; // InjectedSourceStream so that no error can happen here.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/NativeEnumInjectedSources.cpp:89,load,loading,89,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/NativeEnumInjectedSources.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/NativeEnumInjectedSources.cpp,1,['load'],['loading']
Performance,"// Find the best (FI, Tag) pair to pin to offset 0.; // Looking at the possible uses of a tagged address, the advantage of pinning; // is:; // - COPY to physical register.; // Does not matter, this would trade a MOV instruction for an ADDG.; // - ST*G matter, but those mostly appear near the function prologue where all; // the tagged addresses need to be materialized anyway; also, counting ST*G; // uses would overweight large allocas that require more than one ST*G; // instruction.; // - Load/Store instructions in the address operand do not require a tagged; // pointer, so they also do not benefit. These operands have already been; // eliminated (see uncheckLoadsAndStores) so all remaining load/store; // instructions count.; // - Any other instruction may benefit from being pinned to offset 0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp:493,Load,Load,493,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64StackTaggingPreRA.cpp,2,"['Load', 'load']","['Load', 'load']"
Performance,// Find the checkers that should run for this Decl and cache them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/CheckerManager.cpp:55,cache,cache,55,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/CheckerManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/CheckerManager.cpp,1,['cache'],['cache']
Performance,// Find the checkers that should run for this Stmt and cache them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/CheckerManager.cpp:55,cache,cache,55,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/CheckerManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/CheckerManager.cpp,1,['cache'],['cache']
Performance,// Find the cost of an intrinsic; some targets may have instructions that; // perform the operation without needing an actual call.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:78,perform,perform,78,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['perform'],['perform']
Performance,// Find the latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/SchedClassResolution.cpp:12,latency,latency,12,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/SchedClassResolution.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/SchedClassResolution.cpp,1,['latency'],['latency']
Performance,"// Find the load, and find the position that it will end up in (e.g. a; // shifted) value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,// Find the module we're entering. We require that a module map for it; // be loaded or implicitly loadable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Pragma.cpp:78,load,loaded,78,interpreter/llvm-project/clang/lib/Lex/Pragma.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Pragma.cpp,2,['load'],"['loadable', 'loaded']"
Performance,// Find the nearest store that has a lower index than this load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:59,load,load,59,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,1,['load'],['load']
Performance,// Find the new opcode for the updating load/store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,['load'],['load']
Performance,// Find the optimization level from the command line args,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp:12,optimiz,optimization,12,interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,1,['optimiz'],['optimization']
Performance,// Find the preferred load address for text sections.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.cpp:22,load,load,22,interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.cpp,1,['load'],['load']
Performance,"// Find the preferred type aside from the any-extends (unless it's the only; // one) and non-extending ops. We'll emit an extending load to that type and; // and emit a variant of (extend (trunc X)) for the others according to the; // relative type sizes. At the same time, pick an extend to use based on the; // extend involved in the chosen type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:132,load,load,132,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,// Find the segment the event falls in. A large segment could be loaded; // via multiple mmap calls with consecutive memory addresses.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/PerfReader.cpp:65,load,loaded,65,interpreter/llvm-project/llvm/tools/llvm-profgen/PerfReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/PerfReader.cpp,1,['load'],['loaded']
Performance,"// Find the single reaching def, or determine if Use is jointly dominated by; // multiple values, and we may need to create even more phi-defs to preserve; // VNInfo SSA form. Perform a search for all predecessor blocks where we; // know the dominating VNInfo.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveRangeCalc.cpp:176,Perform,Perform,176,interpreter/llvm-project/llvm/lib/CodeGen/LiveRangeCalc.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveRangeCalc.cpp,1,['Perform'],['Perform']
Performance,// Find the size of memory referenced by the load/store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:45,load,load,45,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['load'],['load']
Performance,// Find the switch case corresponding to the value of the condition.; // FIXME: Cache this lookup.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:80,Cache,Cache,80,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['Cache'],['Cache']
Performance,// Find the type to narrow it the load / op / store to.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,// Find the unscheduled node in ReadySUs with the highest latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:58,latency,latency,58,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,1,['latency'],['latency']
Performance,// Find the vector type that can load from.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['load']
Performance,// Find unused reg to load flat scratch init into,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFrameLowering.cpp,1,['load'],['load']
Performance,// Finding an edge is expensive in the worst case (O(max_clique(G))). So; // cache locally edges we have already seen.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocPBQP.cpp:77,cache,cache,77,interpreter/llvm-project/llvm/lib/CodeGen/RegAllocPBQP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocPBQP.cpp,1,['cache'],['cache']
Performance,// Finding the appropriate PtrInfo if offset is a known constant.; // This is required to create the memory operand for the narrowed load.; // This machine memory operand object helps us infer about legality; // before we proceed to combine the instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:133,load,load,133,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,// Finds or allocates the provided BitVector in the cache and retrieves it's; // unique instance.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/MCInstrDescView.h:52,cache,cache,52,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/MCInstrDescView.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/MCInstrDescView.h,1,['cache'],['cache']
Performance,"// Fine-tune stat printing",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofPerfAnalysis.cxx:8,tune,tune,8,proof/proofbench/src/TProofPerfAnalysis.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofPerfAnalysis.cxx,1,['tune'],['tune']
Performance,"// Finish any concurrent I/O operations before we close the file handles.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFile.cxx:14,concurren,concurrent,14,io/io/src/TFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFile.cxx,1,['concurren'],['concurrent']
Performance,// Finish scanning because Reg is overwritten by a non-load; // instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCPreEmitPeephole.cpp:55,load,load,55,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCPreEmitPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCPreEmitPeephole.cpp,1,['load'],['load']
Performance,// First (resp. Second) is the first (resp. Second) potentially candidate; // to be placed in a paired load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:103,load,load,103,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// First a side track to insure proper end of process behavior.; // Register for each loaded dictionary (and thus for each library),; // that we need to Close the ROOT files as soon as this library; // might start being unloaded after main.; //; // By calling atexit here (rather than directly from within the; // library) we make sure that this is not called if the library is; // 'only' dlclosed.; // On Ubuntu the linker strips the unused libraries. Eventhough; // stressHistogram is explicitly linked against libNet, it is not; // retained and thus is loaded only as needed in the middle part of; // the execution. Concretely this also means that it is loaded; // *after* the construction of the TApplication object and thus; // after the registration (atexit) of the EndOfProcessCleanups; // routine. Consequently, after the end of main, libNet is; // unloaded before EndOfProcessCleanups is called. When; // EndOfProcessCleanups is executed it indirectly needs the TClass; // for TSocket and its search will use resources that have already; // been unloaded (technically the function static in TUnixSystem's; // DynamicPath and the dictionary from libNet).; // Similarly, the ordering (before this commit) was broken in the; // following case:; // TApplication creation (EndOfProcessCleanups registration); // load UserLibrary; // create TFile; // Append UserObject to TFile; // and after the end of main the order of execution was; // unload UserLibrary; // call EndOfProcessCleanups; // Write the TFile; // attempt to write the user object.; // ....; // where what we need is to have the files closen/written before; // the unloading of the library.; // To solve the problem we now register an atexit function for; // every dictionary thus making sure there is at least one executed; // before the first library tear down after main.; // If atexit is called directly within a library's code, the; // function will called *either* when the library is 'dlclose'd or; // after then end of main (w",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx:86,load,loaded,86,core/base/src/TROOT.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx,3,['load'],['loaded']
Performance,// First check if the result is already in the cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:47,cache,cache,47,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['cache'],['cache']
Performance,// First check if there are type tests / type checked loads in the; // merged regular LTO module IR.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp:54,load,loads,54,interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,1,['load'],['loads']
Performance,"// First check if we have any cluster that is currently in the; // cache but was not used and would be reloaded in the next; // cluster.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx:67,cache,cache,67,tree/tree/src/TTreeCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx,1,['cache'],['cache']
Performance,"// First check if we're extending the result of a load which has a dest type; // smaller than 32 bits, then this zext is redundant. GPR32 is the smallest; // GPR register on AArch64 and all loads which are smaller automatically; // zero-extend the upper bits. E.g.; // %v(s8) = G_LOAD %p, :: (load 1); // %v2(s32) = G_ZEXT %v(s8)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:50,load,load,50,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,3,['load'],"['load', 'loads']"
Performance,// First check that all the vldN-lane uses are VDUPLANEs and that the lane; // numbers match the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:97,load,load,97,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['load'],['load']
Performance,"// First check whether this sub-branch is part of the 'cache' (because the data member it; // represents is no longer in the current class layout.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBranchElement.cxx:55,cache,cache,55,tree/tree/src/TBranchElement.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBranchElement.cxx,1,['cache'],['cache']
Performance,// First create a temporary that is aligned for both the load and store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,// First create the loads to the guard/stack slot for the comparison.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:20,load,loads,20,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,2,['load'],['loads']
Performance,"// First determine if it is required or is profitable to flip the operands.; // If LHS is a foldable load, but RHS is not, flip the condition.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:101,load,load,101,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,2,['load'],['load']
Performance,// First element comes from the first element of second source.; // Remaining elements: Load zero extends / Move copies from first source.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86ShuffleDecode.cpp:88,Load,Load,88,interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86ShuffleDecode.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86ShuffleDecode.cpp,1,['Load'],['Load']
Performance,"// First emit non-scalable frame offsets, or a simple 'mov'.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:18,scalab,scalable,18,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['scalab'],['scalable']
Performance,"// First handle load and store instructions with postinc or predec; // of the form ""ld reg, X+"".; // TODO: We should be able to rewrite this using TableGen data.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/MCTargetDesc/AVRInstPrinter.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/AVR/MCTargetDesc/AVRInstPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/MCTargetDesc/AVRInstPrinter.cpp,1,['load'],['load']
Performance,// First incremement past the first load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['load']
Performance,"// First initialisation of the pointers. When implementations of the batch compute library; // are loaded, they will overwrite the pointers.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/batchcompute/src/Initialisation.cxx:99,load,loaded,99,roofit/batchcompute/src/Initialisation.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/batchcompute/src/Initialisation.cxx,1,['load'],['loaded']
Performance,// First let the preprocessor process the entire file and call callbacks.; // Callbacks will record which #include's were actually performed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/Rewrite/InclusionRewriter.cpp:131,perform,performed,131,interpreter/llvm-project/clang/lib/Frontend/Rewrite/InclusionRewriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/Rewrite/InclusionRewriter.cpp,1,['perform'],['performed']
Performance,// First load is always LI1. This is where we put the new load.; // Use the merged load size available from LI1 for forward loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,4,['load'],"['load', 'loads']"
Performance,// First load the argument into the next available FPR.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,// First load this into an 80-bit X87 register using a stack temporary.; // This will put the whole integer into the significand.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// First load this into an 80-bit X87 register. This will put the whole; // integer into the significand.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// First make a pass through all stack ids that correspond to a call,; // as identified in the above loop. Compute the context ids corresponding to; // each of these calls when they correspond to multiple stack ids due to; // due to inlining. Perform any duplication of context ids required when; // there is more than one call with the same stack ids. Their (possibly newly; // duplicated) context ids are saved in the StackIdToMatchingCalls map.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp:243,Perform,Perform,243,interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp,1,['Perform'],['Perform']
Performance,// First pass. Try to remove or optimize existing LEAs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FixupLEAs.cpp:32,optimiz,optimize,32,interpreter/llvm-project/llvm/lib/Target/X86/X86FixupLEAs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FixupLEAs.cpp,1,['optimiz'],['optimize']
Performance,// First perform a quick check if Class can not touch ref counts.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/DependencyAnalysis.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/DependencyAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/DependencyAnalysis.cpp,1,['perform'],['perform']
Performance,"// First perform a vector comparison, where lane 0 is the one we're interested; // in.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:9,perform,perform,9,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// First scan down the BB from Load, looking for a store of the RCIdentityRoot; // of Load's",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp:31,Load,Load,31,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,2,['Load'],['Load']
Performance,// First see if it's already in our cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/SymbolCache.cpp:36,cache,cache,36,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/SymbolCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/SymbolCache.cpp,1,['cache'],['cache']
Performance,"// First time we loop over proxies: cache the results to avoid future; // costly dynamic_casts",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx:36,cache,cache,36,roofit/roofitcore/src/RooAbsArg.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx,1,['cache'],['cache']
Performance,"// First try to load configuration from the default files, return on error.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp:16,load,load,16,interpreter/llvm-project/clang/lib/Driver/Driver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp,1,['load'],['load']
Performance,// First try to optimize away the conversion entirely when it's; // conditionally from a constant. Vectors only.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:16,optimiz,optimize,16,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimize']
Performance,// First try to optimize away the conversion when it's conditionally from; // a constant. Vectors only.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,optimiz,optimize,16,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['optimiz'],['optimize']
Performance,// First try to pre-split loads and stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// First try to simplify using SimplifyMultipleUseDemandedBits which allows; // the operands to have other uses, but will only perform simplifications that; // involve bypassing some nodes for this user.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:127,perform,perform,127,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,1,['perform'],['perform']
Performance,// First we load the records into memory.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/XRay/Trace.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/XRay/Trace.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/XRay/Trace.cpp,1,['load'],['load']
Performance,"// First we need to insert an explicit load on the false path for any memory; // operand. We also need to potentially do register rewriting here, but it is; // simpler as the memory operands are always on the false path so we can; // simply take that input, whatever it is.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp,1,['load'],['load']
Performance,// First we optimize the IR by running a loop vectorizer pass,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp:12,optimiz,optimize,12,interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,1,['optimiz'],['optimize']
Performance,"// First, add the def for loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstExtenders.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstExtenders.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstExtenders.cpp,1,['load'],['loads']
Performance,"// First, attempt to evaluate each operand.; // Avoid performing the look-up in the common case where the specified; // expression has no loop-variant portions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp:54,perform,performing,54,interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp,1,['perform'],['performing']
Performance,"// First, check the cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:20,cache,cache,20,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,1,['cache'],['cache']
Performance,"// First, combine the VECTOR_SHUFFLE away. This makes the value produced; // by the load dead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:84,load,load,84,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,"// First, combine the bswap away. This makes the value produced by the; // load dead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:75,load,load,75,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,"// First, combine the scalar loads to a vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:29,load,loads,29,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['loads']
Performance,"// First, compute the minimum iteration count required so that the vector; // loop outperforms the scalar loop.; // The total cost of the scalar loop is; // ScalarC * TC; // where; // * TC is the actual trip count of the loop.; // * ScalarC is the cost of a single scalar iteration.; //; // The total cost of the vector loop is; // RtC + VecC * (TC / VF) + EpiC; // where; // * RtC is the cost of the generated runtime checks; // * VecC is the cost of a single vector iteration.; // * TC is the actual trip count of the loop; // * VF is the vectorization factor; // * EpiCost is the cost of the generated epilogue, including the cost; // of the remaining scalar operations.; //; // Vectorization is profitable once the total vector cost is less than the; // total scalar cost:; // RtC + VecC * (TC / VF) + EpiC < ScalarC * TC; //; // Now we can compute the minimum required trip count TC as; // (RtC + EpiC) / (ScalarC - (VecC / VF)) < TC; //; // For now we assume the epilogue cost EpiC = 0 for simplicity. Note that; // the computations are performed on doubles, not integers and the result; // is rounded up, hence we get an upper estimate of the TC.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:1043,perform,performed,1043,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['perform'],['performed']
Performance,"// First, do a cache lookup. Without this cache, certain CFG structures; // (like a series of if statements) take exponential time to visit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp:15,cache,cache,15,interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,2,['cache'],['cache']
Performance,"// First, do memdep-style RLE and S2L optimizations. We can't use memdep; // itself because it uses AliasAnalysis and we need to do provenance; // queries instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:38,optimiz,optimizations,38,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['optimiz'],['optimizations']
Performance,"// First, handle cases where having a fixed length vector enables us to; // give a more accurate cost than falling back to generic scalable codegen.; // TODO: Each of these cases hints at a modeling gap around scalable vectors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:131,scalab,scalable,131,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,2,['scalab'],['scalable']
Performance,"// First, load 8 bits into 32 bits, then truncate to 1 bit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// First, load all the temporaries. This can add new placeholders or; // forward references.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,1,['load'],['load']
Performance,"// First, perform some low-level loop optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp:10,perform,perform,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"// First, perform the cheaper check that compares the base register.; // If they are the same and the load offset is less than the store; // offset, then mark the dependence as loop carried potentially.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachinePipeliner.cpp:10,perform,perform,10,interpreter/llvm-project/llvm/lib/CodeGen/MachinePipeliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachinePipeliner.cpp,2,"['load', 'perform']","['load', 'perform']"
Performance,"// First, search up the chain, branching to follow all token-factor operands.; // If we find a consecutive load, then we're done, otherwise, record all; // nodes just above the top-level loads and token factors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:107,load,load,107,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,"// First, see if the module uses either of the llvm.type.test or; // llvm.type.checked.load intrinsics, which indicates that splitting globals; // may be beneficial.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalSplit.cpp:87,load,load,87,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalSplit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalSplit.cpp,1,['load'],['load']
Performance,"// First, see if we have a cache hit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Caching.cpp:27,cache,cache,27,interpreter/llvm-project/llvm/lib/Support/Caching.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Caching.cpp,1,['cache'],['cache']
Performance,"// First, split any FCA loads and stores touching this alloca to promote; // better splitting and promotion opportunities.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:24,load,loads,24,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// First, try cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/src/clingwrapper.cxx:14,cache,cache,14,bindings/pyroot/cppyy/cppyy-backend/clingwrapper/src/clingwrapper.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/src/clingwrapper.cxx,1,['cache'],['cache']
Performance,"// First, we want to figure out all of the sets with which we definitely; // don't alias. Iterate over all noalias set, and add those for which:; // 1. The noalias argument is not in the set of objects from which we; // definitely derive.; // 2. The noalias argument has not yet been captured.; // An arbitrary function that might load pointers could see captured; // noalias arguments via other noalias arguments or globals, and so we; // must always check for prior capture.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:331,load,load,331,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['load'],['load']
Performance,"// FirstSPAdjustAmount is chosen as (2048 - StackAlign) because 2048 will; // cause sp = sp + 2048 in the epilogue to be split into multiple; // instructions. Offsets smaller than 2048 can fit in a single load/store; // instruction, and we have to stick with the stack alignment.; // So (2048 - StackAlign) will satisfy the stack alignment.; //; // FIXME: This place may seem odd. When using multiple ADDI instructions to; // adjust the stack in Prologue, and there are no callee-saved registers, we; // can take advantage of the logic of split sp ajustment to reduce code; // changes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchFrameLowering.cpp:205,load,load,205,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchFrameLowering.cpp,1,['load'],['load']
Performance,"// FirstSPAdjustAmount is chosen at most as (2048 - StackAlign) because; // 2048 will cause sp = sp + 2048 in the epilogue to be split into multiple; // instructions. Offsets smaller than 2048 can fit in a single load/store; // instruction, and we have to stick with the stack alignment. 2048 has; // 16-byte alignment. The stack alignment for RV32 and RV64 is 16 and for; // RV32E it is 4. So (2048 - StackAlign) will satisfy the stack alignment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp:213,load,load,213,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,1,['load'],['load']
Performance,"// Firstly, exclude all scalable vector extending loads/truncating stores,; // include both integer and floating scalable vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:24,scalab,scalable,24,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,"['load', 'scalab']","['loads', 'scalable']"
Performance,"// Firstly, the cost of load/store operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h:24,load,load,24,interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,2,['load'],['load']
Performance,"// Fix C++20 builds caused by commit:; // llvm-project/commit/574ee1c02ef73b66c5957cf93888234b0471695f; // We are loading clang modules here and not C++20 modules",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/Interpreter.cpp:114,load,loading,114,interpreter/cling/lib/Interpreter/Interpreter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/Interpreter.cpp,1,['load'],['loading']
Performance,// Fix the alignment of any loads or stores using this PHI node.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,// Fix the alignment of any loads or stores using this select.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,// Fixed-length RVV vectors are represented as scalable vectors in function; // args/return and must be coerced from fixed vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/RISCV.cpp:47,scalab,scalable,47,interpreter/llvm-project/clang/lib/CodeGen/Targets/RISCV.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/RISCV.cpp,1,['scalab'],['scalable']
Performance,// Fixed-length vectors are located in the corresponding scalable-vector; // container types.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:57,scalab,scalable-vector,57,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,['scalab'],['scalable-vector']
Performance,"// Fixed-point conversion of reg/reg instructions fed by load-immediate; // into reg/imm instructions. FIXME: This is expensive, control it with; // an option.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:57,load,load-immediate,57,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,1,['load'],['load-immediate']
Performance,"// Fixes for known features, binned likelihood optimization",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/TestStatistics/testLikelihoodGradientJob.cxx:47,optimiz,optimization,47,roofit/roofitcore/test/TestStatistics/testLikelihoodGradientJob.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/TestStatistics/testLikelihoodGradientJob.cxx,1,['optimiz'],['optimization']
Performance,// Fixup for Thumb load/store from constant pool instrs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/MCTargetDesc/ARMFixupKinds.h:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/ARM/MCTargetDesc/ARMFixupKinds.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/MCTargetDesc/ARMFixupKinds.h,1,['load'],['load']
Performance,"// Flag bits ""flags"" field of the Html widget:; //; // REDRAW_PENDING An idle handler has already been queued to; // call the TGHtml::Redraw() method.; //; // GOT_FOCUS This widget currently has input focus.; //; // HSCROLL Horizontal scrollbar position needs to be; // recomputed.; //; // VSCROLL Vertical scrollbar position needs to be; // recomputed.; //; // RELAYOUT We need to reposition every element on the; // virtual canvas. (This happens, for example,; // when the size of the widget changes and we; // need to recompute the line breaks.); //; // RESIZE_ELEMENTS We need to recompute the size of every element.; // This happens, for example, when the fonts; // change.; //; // REDRAW_FOCUS We need to repaint the focus highlight border.; //; // REDRAW_TEXT Everything in the clipping window needs to be redrawn.; //; // STYLER_RUNNING There is a call to HtmlAddStyle() in process.; // Used to prevent a recursive call to HtmlAddStyle().; //; // INSERT_FLASHING True if there is a timer scheduled that will toggle; // the state of the insertion cursor.; //; // REDRAW_IMAGES One or more TGHtmlImageMarkup objects have their; // redrawNeeded flag set.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/guihtml/inc/TGHtml.h:103,queue,queued,103,gui/guihtml/inc/TGHtml.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/guihtml/inc/TGHtml.h,1,['queue'],['queued']
Performance,// Flag whether the insn is a load or a store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonShuffler.h:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonShuffler.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/MCTargetDesc/HexagonShuffler.h,1,['load'],['load']
Performance,// Flags for computing the optimal addressing mode for loads and stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h:55,load,loads,55,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.h,1,['load'],['loads']
Performance,// Flags for tracking per-element vectorization state of loads/stores; // of a flattened function parameter or return value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:57,load,loads,57,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['loads']
Performance,// Flatten queue; //,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:11,queue,queue,11,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['queue'],['queue']
Performance,// Float32Array caches used for uploading Matrix uniforms,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:16,cache,caches,16,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['cache'],['caches']
Performance,// Floating point constant that can be loaded into a; // register with one instruction per word,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/Targets/PPC.h:39,load,loaded,39,interpreter/llvm-project/clang/lib/Basic/Targets/PPC.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/Targets/PPC.h,1,['load'],['loaded']
Performance,"// Floating point number that occupies 32 bits or less of storage, providing; // improved range compared to half (16-bit) formats, at (potentially); // greater throughput than single precision (32-bit) formats.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/APFloat.h:160,throughput,throughput,160,interpreter/llvm-project/llvm/include/llvm/ADT/APFloat.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/APFloat.h,1,['throughput'],['throughput']
Performance,// Flush load info for the pointer.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['load'],['load']
Performance,"// Fold and/or of setcc's to double CMOV:; // (CMOV F, T, ((cc1 | cc2) != 0)) -> (CMOV (CMOV F, T, cc1), T, cc2); // (CMOV F, T, ((cc1 & cc2) != 0)) -> (CMOV (CMOV T, F, !cc1), F, !cc2); //; // This combine lets us generate:; // cmovcc1 (jcc1 if we don't have CMOV); // cmovcc2 (same); // instead of:; // setcc1; // setcc2; // and/or; // cmovne (jne if we don't have CMOV); // When we can't use the CMOV instruction, it might increase branch; // mispredicts.; // When we can use CMOV, or when there is no mispredict, this improves; // throughput and reduces register pressure.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:535,throughput,throughput,535,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['throughput'],['throughput']
Performance,"// Fold away bit casts of the loaded value by loading the desired type.; // Note that we should not do this for pointer<->integer casts,; // because that would result in type punning.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:30,load,loaded,30,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,2,['load'],"['loaded', 'loading']"
Performance,"// Fold extract-and-trunc into a narrow extract. For example:; // i64 x = EXTRACT_VECTOR_ELT(v2i64 val, i32 1); // i32 y = TRUNCATE(i64 x); // -- becomes --; // v16i8 b = BITCAST (v2i64 val); // i8 x = EXTRACT_VECTOR_ELT(v16i8 b, i32 8); //; // Note: We only run this optimization after type legalization (which often; // creates this pattern) and before operation legalization after which; // we need to be more careful about the vector instructions that we generate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:268,optimiz,optimization,268,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['optimiz'],['optimization']
Performance,"// Fold if there is at least one specific constant value in phi0 or phi1's; // incoming values that comes from the same block and this specific constant; // value can be used to do optimization for specific binary operator.; // For example:; // %phi0 = phi i32 [0, %bb0], [%i, %bb1]; // %phi1 = phi i32 [%j, %bb0], [0, %bb1]; // %add = add i32 %phi0, %phi1; // ==>; // %add = phi i32 [%j, %bb0], [%i, %bb1]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:181,optimiz,optimization,181,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['optimiz'],['optimization']
Performance,// Fold load from stack followed by sext.b/sext.h/sext.w/zext.b/zext.h/zext.w.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,1,['load'],['load']
Performance,"// Fold loads into extends when possible.; // FIXME: We can have multiple redundant extend/trunc instructions; // following a load. The folding only picks up one. Extend this; // to check subsequent instructions for the same pattern and remove; // them. Thus ResultReg should be the def reg for the last redundant; // instruction in a chain, and all intervening instructions can be; // removed from parent. Change test/CodeGen/PowerPC/fast-isel-fold.ll; // to add ELF64-NOT: rldicl to the appropriate tests when this works.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:8,load,loads,8,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,2,['load'],"['load', 'loads']"
Performance,// Fold out compares that express a class test.; //; // FIXME: Should be able to perform folds without context; // instruction. Always pass in the context function?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp:81,perform,perform,81,interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,1,['perform'],['perform']
Performance,// Fold sign-extensions into MipsISD::VEXTRACT_[SZ]EXT_ELT for MSA and fold; // constant splats into MipsISD::SHRA_DSP for DSPr2.; //; // Performs the following transformations:; // - Changes MipsISD::VEXTRACT_[SZ]EXT_ELT to sign extension if its; // sign/zero-extension is completely overwritten by the new one performed by; // the ISD::SRA and ISD::SHL nodes.; // - Removes redundant sign extensions performed by an ISD::SRA and ISD::SHL; // sequence.; //; // See performDSPShiftCombine for more information about the transformation; // used for DSPr2.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp:138,Perform,Performs,138,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,4,"['Perform', 'perform']","['Performs', 'performDSPShiftCombine', 'performed']"
Performance,"// Fold subvector loads into one.; // If needed, look through bitcasts to get to the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:18,load,loads,18,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,// Fold the following sign-/zero-extend into the load instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:49,load,load,49,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,1,['load'],['load']
Performance,"// Fold xor(zext(xor(x,c1)),c2) -> xor(zext(x),xor(zext(c1),c2)); // Fold xor(truncate(xor(x,c1)),c2) -> xor(truncate(x),xor(truncate(c1),c2)); // TODO: Under what circumstances could this be performed in DAGCombine?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:192,perform,performed,192,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performed']
Performance,// Fold zero extensions into MipsISD::VEXTRACT_[SZ]EXT_ELT; //; // Performs the following transformations:; // - Changes MipsISD::VEXTRACT_[SZ]EXT_ELT to zero extension if its; // sign/zero-extension is completely overwritten by the new one performed by; // the ISD::AND.; // - Removes redundant zero extensions performed by an ISD::AND.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp:67,Perform,Performs,67,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelLowering.cpp,3,"['Perform', 'perform']","['Performs', 'performed']"
Performance,"// Folding a V_SET0 or V_SETALLONES as a load, to ease register pressure.; // Create a constant-pool entry and operands to load from it.; // Large code model can't fold loads this way.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,3,['load'],"['load', 'loads']"
Performance,// Folding a normal load. Just copy the load's address operands.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,2,['load'],['load']
Performance,"// Folding fadd (fmul x, y), (fmul x, y) -> fma x, y, (fmul x, y) is never; // beneficial. It does not reduce latency. It increases register pressure. It; // replaces an fadd with an fma which is a more complex instruction, so is; // likely to have a larger encoding, use more functional units, etc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:110,latency,latency,110,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['latency'],['latency']
Performance,// Folds zero into instructions which have a load immediate zero as an operand; // but also recognize zero as immediate zero. If the definition of the load; // has no more users it is deleted.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:45,load,load,45,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,2,['load'],['load']
Performance,// Follow gcc implementation of CC_PRINT_OPTIONS; we could also cache the; // output stream.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Compilation.cpp:64,cache,cache,64,interpreter/llvm-project/clang/lib/Driver/Compilation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Compilation.cpp,1,['cache'],['cache']
Performance,// Follow the load with a store to the stack slot. Remember the store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['load'],['load']
Performance,// Follow the load with a store to the stack slot. Remember the store.; // On big-endian machines this requires a truncating store to ensure; // that the bits end up in the right place.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['load'],['load']
Performance,"// For 16, 64, and 128b values, emit a constant pool load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['load'],['load']
Performance,"// For 32-bit values, we need to add an FP_ROUND node (if we made it; // here, we know that all inputs are extending loads so this is safe).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:117,load,loads,117,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loads']
Performance,"// For 4-byte load-and-splat, we need Power9.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:14,load,load-and-splat,14,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load-and-splat']
Performance,"// For 8-bit load/store instructions with a register offset, both the; // ""DoShift"" and ""NoShift"" variants have a shift of 0. Because of this,; // they're disambiguated by whether the shift was explicit or implicit rather; // than its size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AsmParser/AArch64AsmParser.cpp,1,['load'],['load']
Performance,"// For ARM mode, we have different pseudoinstructions for direct accesses; // and indirect accesses, and the ones for indirect accesses include the; // load from GOT. For Thumb mode, we use the same pseudoinstruction for both; // direct and indirect accesses, and we need to manually generate the load; // from GOT.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp:152,load,load,152,interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMInstructionSelector.cpp,2,['load'],['load']
Performance,"// For AVX1 only, if we are extracting from a 256-bit and+not (which will; // eventually get combined/lowered into ANDNP) with a concatenated operand,; // split the 'and' into 128-bit ops to avoid the concatenate and extract.; // We let generic combining take over from there to simplify the; // insert/extract and 'not'.; // This pattern emerges during AVX1 legalization. We handle it before lowering; // to avoid complications like splitting constant vector loads.; // Capture the original wide type in the likely case that we need to bitcast; // back to this type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:460,load,loads,460,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loads']
Performance,"// For AVX2+ targets, with the upper bits known zero, we can perform MULHU on; // the (bitcasted) inputs directly, and then cheaply pack/truncate the result; // (upper elts will be zero). Don't attempt this with just AVX512F as MULHU; // will have to split anyway.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:61,perform,perform,61,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// For AsynchEH, insert a Nop if followed by a trap inst; // Or the exception won't be caught.; // (see MCConstantExpr::create(1,..) in WinException.cpp); // Ignore SDiv/UDiv because a DIV with Const-0 divisor; // must have being turned into an UndefValue.; // Div with variable opnds won't be the first instruction in; // an EH region as it must be led by at least a Load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp:368,Load,Load,368,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,1,['Load'],['Load']
Performance,"// For C++11 Lambdas a Field will be the same as a Capture, but the Capture; // has the name and the location of the variable so we should iterate over; // both concurrently.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp:161,concurren,concurrently,161,interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,1,['concurren'],['concurrently']
Performance,"// For CMOV groups which we can rewrite and which contain a memory load,; // always rewrite them. On x86, a CMOV will dramatically amplify any; // memory latency by blocking speculative execution.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp:67,load,load,67,interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp,2,"['latency', 'load']","['latency', 'load']"
Performance,"// For CPUs that favor the register form of a call or push,; // do not fold loads into calls or pushes, unless optimizing for size; // aggressively.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:76,load,loads,76,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,2,"['load', 'optimiz']","['loads', 'optimizing']"
Performance,"// For CUDA, create a string literal containing the fat binary loaded from; // the given file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp:63,load,loaded,63,interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp,1,['load'],['loaded']
Performance,"// For CUDA, we preserve the param loads coming from function arguments",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXReplaceImageHandles.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXReplaceImageHandles.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXReplaceImageHandles.cpp,1,['load'],['loads']
Performance,"// For Falkor, we want to avoid having too many strided loads in a loop since; // that can exhaust the HW prefetcher resources. We adjust the unroller; // MaxCount preference below to attempt to ensure unrolling doesn't create too; // many strided loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:56,load,loads,56,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,2,['load'],['loads']
Performance,"// For GEPs with identical offsets, we can preserve the size and AAInfo; // when performing the alias check on the underlying objects.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp:81,perform,performing,81,interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,1,['perform'],['performing']
Performance,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Only min(max(QNaN, 0.0), 1.0) evaluates; // to 0.0 requires dx10_clamp = true.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:18,perform,perform,18,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,1,['perform'],['perform']
Performance,// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. Requires dx10_clamp = true. Safe to fold; // when Val could be QNaN. If Val can also be SNaN third input should be 0.0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:18,perform,perform,18,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,1,['perform'],['perform']
Performance,"// For IEEE=false perform combine only when it's safe to assume that there are; // no NaN inputs. Most often MI is marked with nnan fast math flag.; // For IEEE=true consider NaN inputs. fmed3(NaN, K0, K1) is equivalent to; // min(min(NaN, K0), K1). Safe to fold for min(max(Val, K0), K1) since inner; // nodes(max/min) have same behavior when one input is NaN and other isn't.; // Don't consider max(min(SNaN, K1), K0) since there is no isKnownNeverQNaN,; // also post-legalizer inputs to min/max are fcanonicalized (never SNaN).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp:18,perform,perform,18,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegBankCombiner.cpp,1,['perform'],['perform']
Performance,"// For LSE2, loads/stores should have been converted to monotonic and had; // a fence inserted after them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:13,load,loads,13,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,1,['load'],['loads']
Performance,"// For Mac Os X only: do not OS cache the files read",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchRunDataRead.cxx:32,cache,cache,32,proof/proofbench/src/TProofBenchRunDataRead.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofbench/src/TProofBenchRunDataRead.cxx,1,['cache'],['cache']
Performance,// For OpenCL even if RV is located in default or alloca address space; // we don't want to perform address space cast for it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:92,perform,perform,92,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['perform'],['perform']
Performance,"// For Order dependences:; // 1. Volatile loads/stores can be packetized together, unless other; // rules prevent is.; // 2. Store followed by a load is not allowed.; // 3. Store followed by a store is valid.; // 4. Load followed by any memory operation is allowed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp:42,load,loads,42,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,3,"['Load', 'load']","['Load', 'load', 'loads']"
Performance,"// For P7 and P8, floating-point instructions have a 6-cycle latency and; // there are two execution units, so unroll by 12x for latency hiding.; // FIXME: the same for P9 as previous gen until POWER9 scheduling is ready; // FIXME: the same for P10 as previous gen until POWER10 scheduling is ready; // Assume that future is the same as the others.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp:61,latency,latency,61,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,2,['latency'],['latency']
Performance,"// For PGO use pipeline, try to optimize memory intrinsics such as memcpy; // using the size value profile. Don't perform this when optimizing for size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:32,optimiz,optimize,32,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,3,"['optimiz', 'perform']","['optimize', 'optimizing', 'perform']"
Performance,"// For PIC, the sequence is:; // BRIND(load(Jumptable + index) + RelocBase); // RelocBase can be JumpTable, GOT or some sort of global base.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,"// For PTEST(PG, PTEST_LIKE(PG, ...)), the PTEST is redundant since the; // flags are set based on the same mask 'PG', but PTEST_LIKE must operate; // on 8-bit predicates like the PTEST. Otherwise, for instructions like; // compare that also support 16/32/64-bit predicates, the implicit PTEST; // performed by the compare could consider fewer lanes for these element; // sizes.; //; // For example, consider; //; // ptrue p0.b ; P0=1111-1111-1111-1111; // index z0.s, #0, #1 ; Z0=<0,1,2,3>; // index z1.s, #1, #1 ; Z1=<1,2,3,4>; // cmphi p1.s, p0/z, z1.s, z0.s ; P1=0001-0001-0001-0001; // ; ^ last active; // ptest p0, p1.b ; P1=0001-0001-0001-0001; // ; ^ last active; //; // where the compare generates a canonical all active 32-bit predicate; // (equivalent to 'ptrue p1.s, all'). The implicit PTEST sets the last; // active flag, whereas the PTEST instruction with the same mask doesn't.; // For PTEST_ANY this doesn't apply as the flags in this case would be; // identical regardless of element size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:298,perform,performed,298,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['perform'],['performed']
Performance,"// For PTEST(PTRUE_ALL, WHILE), if the element size matches, the PTEST is; // redundant since WHILE performs an implicit PTEST with an all active; // mask. Must be an all active predicate of matching element size.; // For PTEST(PTRUE_ALL, PTEST_LIKE), the PTEST is redundant if the; // PTEST_LIKE instruction uses the same all active mask and the element; // size matches. If the PTEST has a condition of any then it is always; // redundant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:100,perform,performs,100,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['perform'],['performs']
Performance,// For PredBB in CriticalEdgePredAndLoad we need to replace the uses of old; // load instruction with the new created load instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:80,load,load,80,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],['load']
Performance,"// For RISC-V, The machine instructions that include a FrameIndex operand; // are load/store, ADDI instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp:82,load,load,82,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp,1,['load'],['load']
Performance,"// For ROOT-6704: use normalised name for matching if the class is in stl; // The reason for this check is that we have rules like std::map<*, int>; // We do not know how the internal representation of the innocuous ""map""; // is. We therefore have to act on a nicer name, obtained with TClassEdit; // The check ROOT::TMetaUtils::IsStdDropDefaultClass is there to call; // TClassEdit only when necessary as it can be expensive, a performance; // optimisation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/BaseSelectionRule.cxx:429,perform,performance,429,core/dictgen/src/BaseSelectionRule.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/BaseSelectionRule.cxx,1,['perform'],['performance']
Performance,"// For RVV spill, non-scalable stack offsets computing requires up to one; // scratch register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp:22,scalab,scalable,22,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,1,['scalab'],['scalable']
Performance,"// For RVV spill, scalable stack offsets computing requires up to two scratch; // registers",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp:18,scalab,scalable,18,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVFrameLowering.cpp,1,['scalab'],['scalable']
Performance,"// For Scalable Matrix Extension (SME) instructions that have an implicit; // operand for the accumulator (ZA) or implicit immediate zero which isn't; // encoded, manually insert operand.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp:7,Scalab,Scalable,7,interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/Disassembler/AArch64Disassembler.cpp,1,['Scalab'],['Scalable']
Performance,"// For ThinLTO importing, we need to remove the type test assumes if this is; // an MDString type id without a corresponding TypeIdSummary. Any; // non-MDString type ids are ignored and treated as Unknown by LTT, so their; // type test assumes can be kept. If the MDString type id is missing a; // TypeIdSummary (e.g. because there was no use on a vcall, preventing the; // exporting phase of WPD from analyzing it), then it would be treated as; // Unsat by LTT and we need to remove its type test assumes here. If not; // used on a vcall we don't need them for later optimization use in any; // case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp:568,optimiz,optimization,568,interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp,1,['optimiz'],['optimization']
Performance,"// For V64 types, we perform extraction by expanding the value; // to a V128 type and perform the extraction on that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:21,perform,perform,21,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['perform'],['perform']
Performance,"// For VFP / NEON load/store multiples, the registers must be; // consecutive and within the limit on the number of registers per; // instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['load']
Performance,// For Val that has zero Lo12 (implies Val equals to Hi52) should has; // already been processed to LUI+SH*ADD by previous optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp:123,optimiz,optimization,123,interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp,1,['optimiz'],['optimization']
Performance,"// For WinCFI, if optimizing for size, prefer to not combine the stack bump; // (to force a stp with predecrement) to match the packed unwind format,; // provided that there actually are any callee saved registers to merge the; // decrement with.; // This is potentially marginally slower, but allows using the packed; // unwind format for functions that both have a local area and callee saved; // registers. Using the packed unwind format notably reduces the size of; // the unwind info.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:18,optimiz,optimizing,18,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,1,['optimiz'],['optimizing']
Performance,"// For X86-64, if there are vararg parameters that are passed via; // registers, then we must store them to their spots on the stack so; // they may be loaded by dereferencing the result of va_next.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:152,load,loaded,152,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['load'],['loaded']
Performance,"// For Z/Sext, the context is the operand, which must be a LoadInst.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:59,Load,LoadInst,59,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['Load'],['LoadInst']
Performance,"// For __atomic_*_fetch operations, perform the operation again to; // determine the value which was written.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp:36,perform,perform,36,interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,1,['perform'],['perform']
Performance,"// For `call @llvm.eh.sjlj.longjmp(buf)`, we generate following instructions.; //; // ThisMBB:; // %fp = load buf[0]; // %jmp = load buf[1]; // %s10 = buf ; Store an address of buf to SX10 for RestoreMBB; // %sp = load buf[2] ; generated by llvm.eh.sjlj.setjmp.; // jmp %jmp; // Reload FP.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp:105,load,load,105,interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,3,['load'],['load']
Performance,"// For a POD type, just emit a load of the lvalue + a copy, because our; // compound literal might alias the destination.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:31,load,load,31,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['load'],['load']
Performance,"// For a block which requires predication, a address may be safe to access; // in the loop w/o predication if we can prove dereferenceability facts; // sufficient to ensure it'll never fault within the loop. For the moment,; // we restrict this to loads; stores are more complicated due to; // concurrency restrictions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp:248,load,loads,248,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,2,"['concurren', 'load']","['concurrency', 'loads']"
Performance,"// For a byte position in the result of an Or, traverse the tree and find the; // node (and the byte of the node) which ultimately provides this {Or,; // BytePosition}. \p Op is the operand we are currently examining. \p Index is; // the byte position of the Op that corresponds with the originally requested; // byte of the Or \p Depth tracks how many recursive iterations we have; // performed. \p StartingIndex is the originally requested byte of the Or",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:386,perform,performed,386,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['perform'],['performed']
Performance,"// For a file representing a module, use the submodule ID of the top-level; // module as the file ID. For any other kind of file, the number of such; // files loaded beforehand will be the same on reload.; // FIXME: Is this true even if we have an explicit module file and a PCH?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:159,load,loaded,159,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loaded']
Performance,// For a fixed or scalable vector get N from <{vscale x} N x iM>,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp:18,scalab,scalable,18,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp,1,['scalab'],['scalable']
Performance,"// For a fixed or scalable vector, get the size in bits of N x iM; for a; // scalar this is just M.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp:18,scalab,scalable,18,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAndOrXor.cpp,1,['scalab'],['scalable']
Performance,"// For a generic pointer loaded from the constant memory, it could be assumed; // as a global pointer since the constant memory is only populated on the; // host side. As implied by the offload programming model, only global; // pointers could be referenced on the host side.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp:25,load,loaded,25,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetMachine.cpp,1,['load'],['loaded']
Performance,"// For a given BB instruction, evaluate all loads in the chain that form a; // pattern which suggests that the loads can be combined. The one and only use; // of the loads is to form a wider load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp:44,load,loads,44,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,4,['load'],"['load', 'loads']"
Performance,"// For a global variable with one store, if the store dominates any loads,; // those loads will always load the stored value (as opposed to the; // initializer), even in the presence of recursion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:68,load,loads,68,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,3,['load'],"['load', 'loads']"
Performance,"// For a list of ordered instruction-vf pairs:; // [(load, vf1), (load, vf2), (store, vf1)]; // Group the instructions together to emit separate remarks for:; // load (vf1, vf2); // store (vf1)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,3,['load'],['load']
Performance,"// For a memory-to-memory move, we need to check if the frame; // index is used for storing or loading, by inspecting the; // memory operands.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp:95,load,loading,95,interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp,1,['load'],['loading']
Performance,"// For a nested statement, write out the substatements in reverse order (so; // that a simple stack machine can be used when loading), and don't emit a; // STMT_STOP after each one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriterStmt.cpp:125,load,loading,125,interpreter/llvm-project/clang/lib/Serialization/ASTWriterStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriterStmt.cpp,1,['load'],['loading']
Performance,"// For a normal instance message, we should extend unless the; // receiver is loaded from a variable with precise lifetime.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:78,load,loaded,78,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['load'],['loaded']
Performance,"// For a reassociatable opcode perform:; // op x, (op y, z) -> op (op x, z), y, if x and z are uniform",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:31,perform,perform,31,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['perform'],['perform']
Performance,"// For a rewritten candidate, we've already reversed the arguments; // if needed. Perform the rest of the rewrite now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:82,Perform,Perform,82,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,1,['Perform'],['Perform']
Performance,// For a single use integer load:,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,1,['load'],['load']
Performance,"// For a splat, perform a scalar truncate before creating the wider; // vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:16,perform,perform,16,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['perform'],['perform']
Performance,"// For a surface load of vector size N, the Nth operand will be the surfref",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp:17,load,load,17,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXAsmPrinter.cpp,2,['load'],['load']
Performance,"// For a trivial copy or move assignment, perform an APValue copy. This is; // essential for unions, where the operations performed by the assignment; // operator cannot be represented as statements.; //; // Skip this for non-union classes with no fields; in that case, the defaulted; // copy/move does not actually read the object.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:42,perform,perform,42,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,2,['perform'],"['perform', 'performed']"
Performance,"// For a trivial copy or move constructor, perform an APValue copy. This is; // essential for unions (or classes with anonymous union members), where the; // operations performed by the constructor cannot be represented by; // ctor-initializers.; //; // Skip this for empty non-union classes; we should not perform an; // lvalue-to-rvalue conversion on them because their copy constructor does not; // actually read them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:43,perform,perform,43,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,3,['perform'],"['perform', 'performed']"
Performance,"// For a vector type, there is also scalarization overhead (only for; // stores, loads are expanded using the vector-load + permutation sequence,; // which is much less expensive).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp:81,load,loads,81,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,2,['load'],"['load', 'loads']"
Performance,// For actions that have to be performed before a label is emitted,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/MC/MCParser/MCTargetAsmParser.h:31,perform,performed,31,interpreter/llvm-project/llvm/include/llvm/MC/MCParser/MCTargetAsmParser.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/MC/MCParser/MCTargetAsmParser.h,1,['perform'],['performed']
Performance,"// For aggressive optimization, we can adjust some thresholds to be less; // conservative.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp:18,optimiz,optimization,18,interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp,1,['optimiz'],['optimization']
Performance,"// For all (Reg, SubReg) pair that are used more than once, cache the value in; // a VGPR.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp:60,cache,cache,60,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFoldOperands.cpp,1,['cache'],['cache']
Performance,"// For all vectors, but vXi8 we can just emit a sign_extend and a shift. This; // avoids a constant pool load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:105,load,load,105,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// For allocas, we consider only static ones (dynamic; // allocas might be transformed into calls to malloc not simultaneously; // live with the compared-to allocation). For globals, we exclude symbols; // that might be resolve lazily to symbols in another dynamically-loaded; // library (and, thus, could be malloc'ed by the implementation).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp:269,load,loaded,269,interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,1,['load'],['loaded']
Performance,"// For allocating stack space when using stack clash protector.; // Allocation is performed by block, and each block is probed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.h:82,perform,performed,82,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.h,2,['perform'],['performed']
Performance,"// For an anonymous union member, our overload resolution will perform; // overload resolution for its members.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp:63,perform,perform,63,interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp,1,['perform'],['perform']
Performance,"// For an exact VLEN value, scalable offsets become constant and thus; // can be converted entirely into fixed offsets.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp:28,scalab,scalable,28,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp,1,['scalab'],['scalable']
Performance,"// For an interleaving store of 2 vectors, we perform one large interleaving; // shuffle that goes into the wide store",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:46,perform,perform,46,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['perform'],['perform']
Performance,"// For an operand generating multiple values, one of the values may; // become dead allowing further simplification (e.g. split index; // arithmetic from an indexed load).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:165,load,load,165,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// For analyzeBranch.; // Now that all the basic blocks in the chain have the proper layout,; // make a final call to analyzeBranch with AllowModify set.; // Indeed, the target may be able to optimize the branches in a way we; // cannot because all branches may not be analyzable.; // E.g., the target may be able to remove an unconditional branch to; // a fallthrough when it occurs after predicated terminators.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp:192,optimiz,optimize,192,interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp,1,['optimiz'],['optimize']
Performance,// For and load instructions following the call,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,1,['load'],['load']
Performance,"// For any Objective-C class definitions we have already loaded, make sure; // that we load any additional categories.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:57,load,loaded,57,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,2,['load'],"['load', 'loaded']"
Performance,// For any extends we can cheat for larger element sizes and use shuffle; // instructions that can fold with a load and/or copy.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:111,load,load,111,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// For any loaded plugins, let them register pass builder callbacks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/NewPMDriver.cpp:11,load,loaded,11,interpreter/llvm-project/llvm/tools/opt/NewPMDriver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/opt/NewPMDriver.cpp,1,['load'],['loaded']
Performance,"// For any original GEP Call and Base %2 like; // %4 = bitcast %struct.net_device** %dev1 to i64*; // it is transformed to:; // %6 = load llvm.sk_buff:0:50$0:0:0:2:0; // %7 = bitcast %struct.sk_buff* %2 to i8*; // %8 = getelementptr i8, i8* %7, %6; // %9 = bitcast i8* %8 to i64*; // using %9 instead of %4; // The original Call inst is removed.; // Load the global variable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp:133,load,load,133,interpreter/llvm-project/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp,2,"['Load', 'load']","['Load', 'load']"
Performance,// For arrays with complex element types perform element by element; // copying.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp:41,perform,perform,41,interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,1,['perform'],['perform']
Performance,"// For atomic sub, perform scan with add operation and allow one lane to; // subtract the reduced value later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:19,perform,perform,19,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,1,['perform'],['perform']
Performance,"// For automatic generation of LDG (through SelectLoad[Vector], not the; // intrinsics), we may have an extending load like:; //; // i32,ch = load<LD1[%data1(addrspace=1)], zext from i8> t0, t7, undef:i64; //; // In this case, the matching logic above will select a load for the original; // memory type (in this case, i8) and our types will not match (the node needs; // to return an i32 in this case). Our LDG/LDU nodes do not support the; // concept of sign-/zero-extension, so emulate it here by adding an explicit; // CVT instruction. Ptxas should clean up any redundancies here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp:114,load,load,114,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,3,['load'],['load']
Performance,"// For backward compatibility the last Cache set is the default cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFile.cxx:39,Cache,Cache,39,io/io/src/TFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFile.cxx,2,"['Cache', 'cache']","['Cache', 'cache']"
Performance,// For backward merging load:; // Make sure the register being renamed is not used between the; // paired instructions. That would trash the content after the new; // paired instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// For big endian targets, we need to adjust the offset to the pointer to; // load the correct bytes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:78,load,load,78,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,// For big-endian targets we need to load the two subregisters of Reg; // manually because VLDRD would load them in wrong order,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMExpandPseudoInsts.cpp,2,['load'],['load']
Performance,"// For blocks with CFG violations, we may be able to lay them out anyway with; // tail-duplication. We keep this vector so we can perform the probability; // calculations the minimum number of times.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp:130,perform,perform,130,interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBlockPlacement.cpp,1,['perform'],['perform']
Performance,"// For both RIP-relative addressed loads or absolute loads, we cannot; // meaningfully harden them because the address being loaded has no; // dynamic component.; //; // FIXME: When using a segment base (like TLS does) we end up with the; // dynamic address being the base plus -1 because we can't mutate the; // segment register here. This allows the signed 32-bit offset to point at; // valid segment-relative addresses and load them successfully.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,4,['load'],"['load', 'loaded', 'loads']"
Performance,"// For broadcast loads from a constant pool to a vector register, repeatedly; // print the constant loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp:17,load,loads,17,interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// For case HasBP && MaxAlign > 1, we have to realign the SP by performing; // SP = SP - SP % MaxAlign, thus make the probe more like dynamic probe since; // the offset subtracted from SP is determined by SP's runtime value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp:64,perform,performing,64,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp,1,['perform'],['performing']
Performance,"// For case in LLVM IR; // entry:; // %iconv = sext i32 %index to i64; // br i1 undef label %true, label %false; // true:; // %ptr = getelementptr inbounds i32, i32* null, i64 %iconv; // ...; // PPCISelLowering::combineSHL fails to combine, because sext and shl are in; // different BBs when conducting instruction selection. We can do a peephole; // optimization to combine these two instructions into extswsli after; // instruction selection.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:351,optimiz,optimization,351,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,1,['optimiz'],['optimization']
Performance,"// For chips with slow 32-byte unaligned loads, break the 32-byte operation; // into two 16-byte operations. Also split non-temporal aligned loads on; // pre-AVX2 targets as 32-byte loads will lower to regular temporal loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:41,load,loads,41,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,4,['load'],['loads']
Performance,"// For code object version 5, QueuePtr is passed through implicit kernarg.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:30,Queue,QueuePtr,30,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['Queue'],['QueuePtr']
Performance,"// For compatibility, ignore this directive.; // (It's supposed to be an ""optimization"" in the Sun assembler)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp:74,optimiz,optimization,74,interpreter/llvm-project/llvm/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/AsmParser/SparcAsmParser.cpp,1,['optimiz'],['optimization']
Performance,"// For conditional branches, we can perform simple conditional propagation on; // the condition value itself.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/SafepointIRVerifier.cpp:36,perform,perform,36,interpreter/llvm-project/llvm/lib/IR/SafepointIRVerifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/SafepointIRVerifier.cpp,2,['perform'],['perform']
Performance,"// For consistency (and performance), we reset the MustCleanup be also for those; // 'key' retrieved indirectly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFileMerger.cxx:24,perform,performance,24,io/io/src/TFileMerger.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TFileMerger.cxx,1,['perform'],['performance']
Performance,"// For constructors, the access check is performed against the underlying; // declaration, not the found declaration.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Overload.h:41,perform,performed,41,interpreter/llvm-project/clang/include/clang/Sema/Overload.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/Overload.h,1,['perform'],['performed']
Performance,"// For convenience, we store the block array contiguously. This is because; // if someone calls setStreamMap(), it is more convenient to be able to call; // it with an ArrayRef instead of setting up a StreamRef. Since the; // DirectoryStream is cached in the class and thus lives for the life of the; // class, we can be guaranteed that readArray() will return a stable; // reference, even if it has to allocate from its internal pool.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/PDBFile.cpp:245,cache,cached,245,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/PDBFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/Native/PDBFile.cpp,1,['cache'],['cached']
Performance,"// For copy or move constructors, we need to perform overload resolution.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:45,perform,perform,45,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['perform']
Performance,"// For debug-info, in instruction referencing mode, we need to perform some; // post-isel maintenence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp:63,perform,perform,63,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,1,['perform'],['perform']
Performance,"// For declarators, there are some additional syntactic-ish checks we need; // to perform.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:82,perform,perform,82,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['perform'],['perform']
Performance,"// For declrefs and variable length array need to load the pointer for; // correct mapping, since the pointer to the data was passed to the runtime.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp:50,load,load,50,interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,1,['load'],['load']
Performance,"// For dependently-typed local extern declarations and friends, we can't; // perform a correct type check in general until instantiation:; //; // int f();; // template<typename T> void g() { T f(); }; //; // (valid if g() is only instantiated with T = int).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:77,perform,perform,77,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['perform'],['perform']
Performance,"// For each NSObject descendant having a +load method, this method is invoked; // by the ObjC runtime before any of the static constructors is called.; // Therefore we need to instrument such methods with a call to __asan_init; // at the beginning in order to initialize our runtime before any access to; // the shadow memory.; // We cannot just ignore these methods, because they may call other; // instrumented functions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,1,['load'],['load']
Performance,"// For each NSObject descendant having a +load method, this method is invoked; // by the ObjC runtime before any of the static constructors is called.; // Therefore we need to instrument such methods with a call to __memprof_init; // at the beginning in order to initialize our runtime before any access to; // the shadow memory.; // We cannot just ignore these methods, because they may call other; // instrumented functions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemProfiler.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemProfiler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemProfiler.cpp,1,['load'],['load']
Performance,"// For each argument, we must add an instruction for loading the argument; // out of the register and into a value inside of the newly outlined function.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp:53,load,loading,53,interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/IROutliner.cpp,1,['load'],['loading']
Performance,"// For each element in the initializer, see if we've found a load, zero or an; // undef.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:61,load,load,61,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// For each element, we need to ensure we have an odd element from one vector; // multiplied by the odd element of another vector and the even element from; // one of the same vectors being multiplied by the even element from the; // other vector. So we need to make sure for each element i, this operator; // is being performed:; // A[2 * i] * B[2 * i] + A[2 * i + 1] * B[2 * i + 1]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:319,perform,performed,319,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performed']
Performance,"// For each event, write temporary values into our servers' caches, and run a single-value computation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsReal.cxx:60,cache,caches,60,roofit/roofitcore/src/RooAbsReal.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsReal.cxx,1,['cache'],['caches']
Performance,"// For each function in current module, load all context profiles for; // the function as well as their callee contexts which can help profile; // guided importing for ThinLTO. This can be achieved by walking; // through an ordered context container, where contexts are laid out; // as if they were walked in preorder of a context trie. While; // traversing the trie, a link to the highest common ancestor node is; // kept so that all of its decendants will be loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp,2,['load'],"['load', 'loaded']"
Performance,"// For each identifier token, insert into the token stream a; // annot_pragma_unused token followed by the identifier token.; // This allows us to cache a ""#pragma unused"" that occurs inside an inline; // C++ member function.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParsePragma.cpp:147,cache,cache,147,interpreter/llvm-project/clang/lib/Parse/ParsePragma.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParsePragma.cpp,1,['cache'],['cache']
Performance,"// For each interesting use I of PN, find an Instruction BEUser that; // performs the same operation as I on BEInst and whose other operands,; // if any, can also be rematerialized in OtherBB. We stop when we find the; // first such Instruction BEUser. This is because once BEUser is; // rematerialized in OtherBB, we may find more such ""fixup"" opportunities; // in this block. So, we'll start over again.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorLoopCarriedReuse.cpp:73,perform,performs,73,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorLoopCarriedReuse.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorLoopCarriedReuse.cpp,1,['perform'],['performs']
Performance,"// For each invariant address, check its last stored value is the result; // of one of our reductions.; //; // We do not check if dependence with loads exists because they are; // currently rejected earlier in LoopAccessInfo::analyzeLoop. In case this; // behaviour changes we have to modify this code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp:146,load,loads,146,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,1,['load'],['loads']
Performance,"// For each member in the group, shuffle out the appropriate data from the; // wide loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:84,load,loads,84,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['load'],['loads']
Performance,"// For each operand, create a latency entry.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SubtargetEmitter.cpp:30,latency,latency,30,interpreter/llvm-project/llvm/utils/TableGen/SubtargetEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SubtargetEmitter.cpp,1,['latency'],['latency']
Performance,"// For each output value, apply the manual sign/zero-extension and make sure; // all users of the load go through that CVT.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp:98,load,load,98,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// For each select group in an inner-most loop,; // a branch is more preferable than a select/conditional-move if:; // i) conversion to branches for all the select groups of the loop satisfies; // loop-level heuristics including reducing the loop's critical path by; // some threshold (see SelectOptimizeImpl::checkLoopHeuristics); and; // ii) the total cost of the select group is cheaper with a branch compared; // to its predicated version. The cost is in terms of latency and the cost; // of a select group is the cost of its most expensive select instruction; // (assuming infinite resources and thus fully leveraging available ILP).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp:468,latency,latency,468,interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,1,['latency'],['latency']
Performance,"// For each unroll part, create a wide load for the group.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['load'],['load']
Performance,"// For every other atomic operation, we need to emit a load-op-cmpxchg loop",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:55,load,load-op-cmpxchg,55,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['load'],['load-op-cmpxchg']
Performance,"// For every pointer, there must be exactly two stores, one coming from; // PTB or PFB, and the other from QTB or QFB. We don't support more than one; // store (to any address) in PTB,PFB or QTB,QFB.; // FIXME: We could relax this restriction with a bit more work and performance; // testing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:268,perform,performance,268,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,1,['perform'],['performance']
Performance,"// For example, ""testl %eax, $32776"" to ""testw %ax, $32776"".; // NOTE: We only want to form TESTW instructions if optimizing for; // min size. Otherwise we only save one byte and possibly get a length; // changing prefix penalty in the decoders.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:114,optimiz,optimizing,114,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['optimiz'],['optimizing']
Performance,"// For extension loads, it may not be more efficient to chop up the vector; // and then extend it. Instead, we unroll the load and build a new vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:17,load,loads,17,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,2,['load'],"['load', 'loads']"
Performance,// For fp16 we need to extract the upper lane elements. MVE can add a; // VREV+FMIN/MAX to perform another vector step instead.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:91,perform,perform,91,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,2,['perform'],['perform']
Performance,"// For fully redundant case, we select two basic blocks MBB1 and MBB2; // as an optimization target if; // - both MBBs end with a conditional branch,; // - MBB1 is the only predecessor of MBB2, and; // - compare does not take a physical register as a operand in both MBBs.; // In this case, eligibleForCompareElimination sets MBBtoMoveCmp nullptr.; //; // As partially redundant case, we additionally handle if MBB2 has one; // additional predecessor, which has only one successor (MBB2).; // In this case, we move the compare instruction originally in MBB2 into; // MBBtoMoveCmp. This partially redundant case is typically appear by; // compiling a while loop; here, MBBtoMoveCmp is the loop preheader.; //; // Overview of CFG of related basic blocks; // Fully redundant case Partially redundant case; // -------- ---------------- --------; // | MBB1 | (w/ 2 succ) | MBBtoMoveCmp | | MBB1 | (w/ 2 succ); // -------- ---------------- --------; // | \ (w/ 1 succ) \ | \; // | \ \ | \; // | \ |; // -------- --------; // | MBB2 | (w/ 1 pred | MBB2 | (w/ 2 pred; // -------- and 2 succ) -------- and 2 succ); // | \ | \; // | \ | \; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:80,optimiz,optimization,80,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,1,['optimiz'],['optimization']
Performance,"// For function in the current module, keep its farthest ancestor; // context. This can be used to load itself and its child and; // sibling contexts.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp:99,load,load,99,interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp,1,['load'],['load']
Performance,"// For general purpose register loads, harden the registers loaded into.; // For other loads, harden the address loaded from.; // Masking the loaded value is expected to result in less performance; // overhead, as the load can still execute speculatively in comparison to; // when the address loaded from gets masked. However, masking is only; // easy to do efficiently on GPR registers, so for loads into non-GPR; // registers (e.g. floating point loads), mask the address loaded from.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp:32,load,loads,32,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SpeculationHardening.cpp,11,"['load', 'perform']","['load', 'loaded', 'loads', 'performance']"
Performance,"// For generic targets, we default to bottom-up, because it's simpler and more; // compile-time optimizations have been implemented in that direction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:96,optimiz,optimizations,96,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,1,['optimiz'],['optimizations']
Performance,"// For globals that require a load from a stub to get the address, emit the; // load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,4,['load'],['load']
Performance,// For huge function we tend to quickly go though the inner optmization; // opportunities in the BB. So we go back to the BB head to re-optimize; // each instruction instead of go back to the function head.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:136,optimiz,optimize,136,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimize']
Performance,"// For i16/i8, MVE will perform a VREV + VORR/VAND/VEOR for the 64bit vector; // step.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:24,perform,perform,24,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['perform'],['perform']
Performance,"// For in-loop reductions, no element types are added to ElementTypesInLoop; // if there are no loads/stores in the loop. In this case, check through the; // reduction variables to determine the maximum width.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:96,load,loads,96,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['load'],['loads']
Performance,"// For inlinees that are full optimized away, we can establish zero size using; // their remaining probes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h:30,optimiz,optimized,30,interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h,1,['optimiz'],['optimized']
Performance,"// For instructions with variable uops, use uops as latency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:52,latency,latency,52,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['latency'],['latency']
Performance,"// For integer types, check if we can shorten the entire input expression to; // DestWidth * 2, which won't allow removing the truncate, but reducing the; // width may enable further optimizations, e.g. allowing for larger; // vectorization factors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:183,optimiz,optimizations,183,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,1,['optimiz'],['optimizations']
Performance,"// For integer types, we can't handle any bit-width differences. This would; // break both vector conversions with extension and introduce endianness; // issues when in conjunction with loads and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:186,load,loads,186,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// For integers, no extension is the same as zero extension.; // We set the extension mode to zero extension so we don't have; // to add separate entries in AddrModesMap for loads and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:174,load,loads,174,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loads']
Performance,// For internal functions we ignore `argmemonly` and; // `inaccessiblememorargmemonly` as we might break it via interprocedural; // constant propagation. It is unclear if this is the best way but it is; // unlikely this will cause real performance problems. If we are deriving; // attributes for the anchor function we even remove the attribute in; // addition to ignoring it.; // TODO: A better way to handle this would be to add ~NO_GLOBAL_MEM /; // MemoryEffects::Other as a possible location.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:236,perform,performance,236,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,1,['perform'],['performance']
Performance,"// For kernels we perform more initialization work, first we find the init; // and deinit calls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp:18,perform,perform,18,interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,1,['perform'],['perform']
Performance,"// For larger immediates, we might be able to save one instruction from; // constant materialization by folding the Lo12 bits of the immediate into; // the address. We should only do this if the ADD is only used by loads and; // stores that can fold the lo12 bits. Otherwise, the ADD will get iseled; // separately with the full materialized immediate creating extra; // instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp:215,load,loads,215,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,1,['load'],['loads']
Performance,"// For left qualifiers preceeded by nothing, a template declaration, or *,&,&&; // we only perform sorting.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Format/QualifierAlignmentFixer.cpp:91,perform,perform,91,interpreter/llvm-project/clang/lib/Format/QualifierAlignmentFixer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Format/QualifierAlignmentFixer.cpp,1,['perform'],['perform']
Performance,"// For little endian, VSX loads require generating lxvd2x/xxswapd.; // Not needed on ISA 3.0 based CPUs since we have a non-permuting load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,4,['load'],"['load', 'loads']"
Performance,"// For loaded class, CallShowMember will (especially for TObject); // call the virtual ShowMember rather than the class specific version; // resulting in an infinite recursion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:7,load,loaded,7,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,1,['load'],['loaded']
Performance,"// For loads from a constant pool to a vector register, print the constant; // loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp:7,load,loads,7,interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86MCInstLower.cpp,2,['load'],"['loaded', 'loads']"
Performance,// For loads setting SLC configures L0 and L1 cache policy to HIT_EVICT; // and L2 cache policy to STREAM.; // For stores setting both GLC and SLC configures L0 and L1 cache policy; // to MISS_EVICT and the L2 cache policy to STREAM.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:7,load,loads,7,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,10,"['cache', 'load']","['cache', 'loads']"
Performance,"// For loads, check that there are no instructions writing to memory in; // between them.; // TODO: we only have to forbid instructions writing to memory that could; // interfere with any of the loads in the bundle",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanSLP.cpp:7,load,loads,7,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanSLP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanSLP.cpp,2,['load'],['loads']
Performance,"// For loads, clobber the base register with the second load instead of the; // first if the BaseReg == FirstReg.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:7,load,loads,7,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,2,['load'],"['load', 'loads']"
Performance,"// For loads, we can only change the base register since dest is defined; // rather than used.; //; // For stores, we can change SrcDest (and Base if SrcDest == Base) but; // cannot resolve an uncompressible offset in this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp:7,load,loads,7,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,1,['load'],['loads']
Performance,"// For local-exec and initial-exec on AIX (32-bit), the sequence generated; // involves loading the variable offset from the TOC, generating a call to; // .__get_tpointer to get the thread pointer (which will be in R3), and; // adding the two together:; // lwz reg1,var[TC](2); // bla .__get_tpointer; // add reg2, reg1, r3",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:88,load,loading,88,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loading']
Performance,"// For local-exec and initial-exec on AIX (64-bit), the sequence generated; // involves a load of the variable offset (from the TOC), followed by an; // add of the loaded variable offset to R13 (the thread pointer).; // This code sequence looks like:; // ld reg1,var[TC](2); // add reg2, reg1, r13 // r13 contains the thread pointer",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:90,load,load,90,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loaded']"
Performance,"// For loops that are acyclic path limited, aggressively schedule for; // latency. Within an single cycle, whenever CurrMOps > 0, allow normal; // heuristics to take precedence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:74,latency,latency,74,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,2,['latency'],['latency']
Performance,"// For masked load/store intrinsics, the local_dep may actually be; // a normal load or store instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],['load']
Performance,"// For memcmp expansion when the memcmp result is only compared equal or; // not-equal to 0, allow up to this number of load pairs per block. As an; // example, this may allow 'memcmp(a, b, 3) == 0' in a single block:; // a0 = load2bytes &a[0]; // b0 = load2bytes &b[0]; // a2 = load1byte &a[2]; // b2 = load1byte &b[2]; // r = cmp eq (a0 ^ b0 | a2 ^ b2), 0",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h:120,load,load,120,interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h,1,['load'],['load']
Performance,"// For most operations returning SDValue() will result in the node being; // expanded by the DAG Legalizer. This is not the case for ISD::LOAD, so we; // need to manually expand loads that may be legal in some address spaces and; // illegal in others. SEXT loads from CONSTANT_BUFFER_0 are supported for; // compute shaders, since the data is sign extended when it is uploaded to the; // buffer. However SEXT loads from other address spaces are not supported, so; // we need to expand them here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:138,LOAD,LOAD,138,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,4,"['LOAD', 'load']","['LOAD', 'loads']"
Performance,"// For multiple uses, if the Latency is different across uses, reset; // DLatency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp:29,Latency,Latency,29,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,1,['Latency'],['Latency']
Performance,"// For multiplication, the infinitely precise result has at most; // LHSWidth + RHSWidth significant bits; if OpWidth is sufficient; // that such a value can be exactly represented, then no double; // rounding can possibly occur; we can safely perform the operation; // in the destination format if it can represent both sources.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:244,perform,perform,244,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,1,['perform'],['perform']
Performance,"// For narrowing to be valid, it must be the case that the load the; // immediately preceding memory operation before the store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:59,load,load,59,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// For non power-of-2 types, they will very likely be legalized into multiple; // loads. Don't bother trying to match them into extending loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:82,load,loads,82,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,2,['load'],['loads']
Performance,"// For non-composite shapes we are the main paint method & perform the negotiation; // with the viewer here",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geompainter/src/TGeoPainter.cxx:59,perform,perform,59,geom/geompainter/src/TGeoPainter.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geompainter/src/TGeoPainter.cxx,1,['perform'],['perform']
Performance,"// For non-fragile ivars, set the instance size to 0 - {the size of just this; // class}. The runtime will then set this to the correct value on load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp:145,load,load,145,interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,1,['load'],['load']
Performance,"// For non-optimized library calls, the size is the first parameter",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp:11,optimiz,optimized,11,interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,1,['optimiz'],['optimized']
Performance,// For non-simple lvalues perform compare-and-swap procedure.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp:26,perform,perform,26,interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGAtomic.cpp,2,['perform'],['perform']
Performance,"// For nontemporal loads, check that a nontemporal vector version is; // supported on the target (arbitrarily try a vector of 2 elements).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,1,['load'],['loads']
Performance,"// For now check if the header files (or the module containing them); // has been loaded in Cling.; // This is required because the dictionaries must be initialized; // __before__ the TGClient creation which will induce the creation; // of a TClass object which will need the dictionary for TGClient!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGClient.cxx:82,load,loaded,82,gui/gui/src/TGClient.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGClient.cxx,1,['load'],['loaded']
Performance,"// For now this code do the conservative optimization, only work for; // the header block. Later we can hoist the IVInc to the block post; // dominate all users.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp:41,optimiz,optimization,41,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,1,['optimiz'],['optimization']
Performance,// For now we only allow loads in the same block as the PHI. This is; // a common case that happens when instcombine merges two loads through; // a PHI.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:25,load,loads,25,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,2,['load'],['loads']
Performance,"// For now, always emit the memory import, since loads and stores are not; // valid without it. In the future, we could perhaps be more clever and omit; // it if there are no loads or stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/WasmObjectWriter.cpp:49,load,loads,49,interpreter/llvm-project/llvm/lib/MC/WasmObjectWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/WasmObjectWriter.cpp,2,['load'],['loads']
Performance,"// For now, any undef indexes we'll just assume to be 0. This should be; // optimized in future, e.g. to select DUP etc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:76,optimiz,optimized,76,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['optimiz'],['optimized']
Performance,"// For now, disable spanning tree optimization when fork or exec* is; // used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/GCOVProfiling.cpp:34,optimiz,optimization,34,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/GCOVProfiling.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/GCOVProfiling.cpp,1,['optimiz'],['optimization']
Performance,"// For now, it is assumed that for the MVE gather instructions the loads are; // all effectively serialised. This means the cost is the scalar cost; // multiplied by the number of elements being loaded. This is possibly very; // conservative, but even so we still end up vectorising loops because the; // cost per iteration for many loops is lower than for scalar loops.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:67,load,loads,67,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// For now, only in optimized builds.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp:20,optimiz,optimized,20,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,1,['optimiz'],['optimized']
Performance,"// For now, only performCSELCombine and performBRCONDCombine call this; // function. And both of them pass 2 for CCIndex, 3 for CmpIndex with 4; // operands. So just init the ops direct to simplify the code. If we have some; // other case with different CCIndex, CmpIndex, we need to use for loop to; // rewrite the code here.; // TODO: Do we need to assert number of operand is 4 here?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:17,perform,performCSELCombine,17,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['perform'],"['performBRCONDCombine', 'performCSELCombine']"
Performance,"// For now, only use non-vector load / store's for the left-over pieces.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,2,['load'],['load']
Performance,"// For now, perform default visitation for Decls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp:12,perform,perform,12,interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,1,['perform'],['perform']
Performance,"// For now, perform erase, followed by insert.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CSEInfo.cpp:12,perform,perform,12,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CSEInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CSEInfo.cpp,1,['perform'],['perform']
Performance,"// For now, require SSE/SSE2 for performing floating-point operations,; // since x87 requires additional work.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp:33,perform,performing,33,interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,1,['perform'],['performing']
Performance,"// For now, simply use DForm with load/store addr as base and 0 as imm.; // FIXME: optimize load/store with some specific address patterns.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/GISel/PPCInstructionSelector.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/PowerPC/GISel/PPCInstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/GISel/PPCInstructionSelector.cpp,3,"['load', 'optimiz']","['load', 'optimize']"
Performance,"// For now, we can only do this promotion if the load is in the same block; // as the PHI, and if there are no stores between the phi and load.; // TODO: Allow recursive phi users.; // TODO: Allow stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:49,load,load,49,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,2,['load'],['load']
Performance,"// For now, we only handle splats for scalable vectors.; // The DAGCombiner will perform a BUILD_VECTOR -> SPLAT_VECTOR transformation; // for targets that support a SPLAT_VECTOR for non-scalable vector types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:38,scalab,scalable,38,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,3,"['perform', 'scalab']","['perform', 'scalable']"
Performance,"// For now, we only maintain the cache during one request.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/GISelKnownBits.cpp:33,cache,cache,33,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/GISelKnownBits.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/GISelKnownBits.cpp,1,['cache'],['cache']
Performance,"// For now, we only support loading/storing one specific type at a given; // offset.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp:28,load,loading,28,interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,1,['load'],['loading']
Performance,"// For object files containing precompiled types, we need to extract the; // signature, through EndPrecompRecord. This is done here for performance; // reasons, to avoid re-parsing the Types stream.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/CodeView/TypeStreamMerger.cpp:136,perform,performance,136,interpreter/llvm-project/llvm/lib/DebugInfo/CodeView/TypeStreamMerger.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/CodeView/TypeStreamMerger.cpp,1,['perform'],['performance']
Performance,"// For offline mode, one needs a a full URL or the request; // gets forwarded to openi5.hana.ondemand.com.; // This has to be understood and fixed. Loading of shaders; // afterwards fails, too.; // // console.log(window.location.pathname); // where are we loading from?; // import(""https://desire.physics.ucsd.edu/matevz/alja.github.io/rootui5/eve7/rnr_core/RenderCore.js"").then((module) => {",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/ui5/eve7/lib/GlViewerRCore.js:148,Load,Loading,148,ui5/eve7/lib/GlViewerRCore.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/ui5/eve7/lib/GlViewerRCore.js,2,"['Load', 'load']","['Loading', 'loading']"
Performance,"// For optimized builds, lower large range as a balanced binary tree.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:7,optimiz,optimized,7,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,2,['optimiz'],['optimized']
Performance,"// For optimized merging of the ntuple",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/proof/ProofNtuple.h:7,optimiz,optimized,7,tutorials/proof/ProofNtuple.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/proof/ProofNtuple.h,2,['optimiz'],['optimized']
Performance,"// For packed D16 results with TFE enabled, all the data components are; // S32. Cast back to the expected type.; //; // TODO: We don't really need to use load s32 elements. We would only need one; // cast for the TFE result if a multiple of v2s16 was used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:155,load,load,155,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,"// For pairs loading into the same reg, try to find a renaming; // opportunity to allow the renaming of Reg between FirstMI and MI; // and combine MI into FirstMI; otherwise bail and keep looking.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:13,load,loading,13,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['loading']
Performance,// For performance reasons only check non-inlined ones.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Verifier.cpp:7,perform,performance,7,interpreter/llvm-project/llvm/lib/IR/Verifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Verifier.cpp,1,['perform'],['performance']
Performance,// For performance reasons we prefer 16-byte alignment.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:7,perform,performance,7,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['perform'],['performance']
Performance,"// For performance reasons, make the offset at the end different than the; // one used in \ref begin, to optimize the common `It == end()` pattern.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/CoalescingBitVector.h:7,perform,performance,7,interpreter/llvm-project/llvm/include/llvm/ADT/CoalescingBitVector.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/CoalescingBitVector.h,2,"['optimiz', 'perform']","['optimize', 'performance']"
Performance,"// For phis, use the walker, see where we ended up, go there.; // The invariant.group handling in MemorySSA is ad-hoc and doesn't; // support updates, so don't use it to optimize uses.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:170,optimiz,optimize,170,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['optimiz'],['optimize']
Performance,"// For positive shift amounts we can use SHL, as ushl/sshl perform a regular; // left shift for positive shift amounts. For negative shifts we can use a; // VASHR/VLSHR as appropiate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:59,perform,perform,59,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// For register units, the def latency is not included because we don't; // know the def yet.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineTraceMetrics.cpp:31,latency,latency,31,interpreter/llvm-project/llvm/lib/CodeGen/MachineTraceMetrics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineTraceMetrics.cpp,1,['latency'],['latency']
Performance,"// For regular (non-intrinsic) loads/stores, this is set to -1. For; // intrinsic loads/stores, the id is retrieved from the corresponding; // field in the MemIntrinsicInfo structure. That field contains; // non-negative values only.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:31,load,loads,31,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,2,['load'],['loads']
Performance,"// For reinclusion, we want to stop at the reachable successors, who are at; // the beginning of the worklist; but, starting from the callsite bb and; // ending at those successors, we also want to perform a traversal.; // IncludeSuccessorsMark is the index after which we include successors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/FunctionPropertiesAnalysis.cpp:198,perform,perform,198,interpreter/llvm-project/llvm/lib/Analysis/FunctionPropertiesAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/FunctionPropertiesAnalysis.cpp,1,['perform'],['perform']
Performance,// For scalable 'contains' check.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopInfo.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/Analysis/LoopInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopInfo.cpp,1,['scalab'],['scalable']
Performance,"// For scalable and fixed types, mark them as cheap so we can handle it much; // later. This allows us to handle larger than legal types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['scalab'],['scalable']
Performance,// For scalable types the only thing we know about sizeof is; // that this is a multiple of the minimum size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,1,['scalab'],['scalable']
Performance,// For scalable vector types we know we're dealing with SPLAT_VECTORs. We; // only have one operand to check. For fixed-length vector types we may have; // a combination of BUILD_VECTOR and SPLAT_VECTOR.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// For scalable vectors if one of the operands is variant then we still; // want to mark as uniform, which will generate one instruction for just; // the first lane of the vector. We can't scalarize the call in the same; // way as for fixed-width vectors because we don't know how many lanes; // there are.; //; // The reasons for doing it this way for scalable vectors are:; // 1. For the assume intrinsic generating the instruction for the first; // lane is still be better than not generating any at all. For; // example, the input may be a splat across all lanes.; // 2. For the lifetime start/end intrinsics the pointer operand only; // does anything useful when the input comes from a stack object,; // which suggests it should always be uniform. For non-stack objects; // the effect is to poison the object, which still allows us to; // remove the call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,2,['scalab'],['scalable']
Performance,"// For scalable vectors it is safe to use LoVT.getVectorMinNumElements(); // (rather than having to use ElementCount), because EXTRACT_SUBVECTOR scales; // IDX with the runtime scaling factor of the result vector type. For; // fixed-width result vectors, that runtime scaling factor is 1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// For scalable vectors, a uniform memop load is always; // uniform-by-parts and we know how to scalarize that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,2,"['load', 'scalab']","['load', 'scalable']"
Performance,"// For scalable vectors, the only interleave factor currently supported; // is 2 since we require the (de)interleave2 intrinsics instead of; // shufflevectors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['scalab'],['scalable']
Performance,"// For scalable vectors, try to use a SPLAT_VECTOR_PARTS node.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// For scalable vectors, use the minimum size; individual targets; // are responsible for handling scalable vector arguments and; // return values.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:7,scalab,scalable,7,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,2,['scalab'],['scalable']
Performance,"// For scalar loads of pointers, we try to convert the dest type from p0; // to s64 so that our imported patterns can match. Like with the G_PTR_ADD; // conversion, this should be ok because all users should have been; // selected already, so the type doesn't matter for them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['load'],['loads']
Performance,// For scalar loads the extend would be free.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['load'],['loads']
Performance,"// For scalars, its still beneficial to transfer to/from the SIMD unit to; // perform the BITREVERSE.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:78,perform,perform,78,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// For sign-extends, we only need a smov, which performs the extension; // automatically.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:48,perform,performs,48,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['perform'],['performs']
Performance,"// For simplicity, we put the data on both host and device for; // now. This could be optimized by inspecting the clients of the; // variable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFit/Evaluator.cxx:86,optimiz,optimized,86,roofit/roofitcore/src/RooFit/Evaluator.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFit/Evaluator.cxx,1,['optimiz'],['optimized']
Performance,"// For sinking, we'd need to check all Defs below this use. The getClobbering; // call will look on the backedge of the loop, but will check aliasing with; // the instructions on the previous iteration.; // For example:; // for (i ... ); // load a[i] ( Use (LoE); // store a[i] ( 1 = Def (2), with 2 = Phi for the loop.; // i++;; // The load sees no clobbering inside the loop, as the backedge alias check; // does phi translation, and will check aliasing against store a[i-1].; // However sinking the load outside the loop, below the store is incorrect.; // For now, only sink if there are no Defs in the loop, and the existing ones; // precede the use and are in the same block.; // FIXME: Increase precision: Safe to sink if Use post dominates the Def;; // needs PostDominatorTreeAnalysis.; // FIXME: More precise: no Defs that alias this Use.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:241,load,load,241,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,3,['load'],['load']
Performance,"// For small code model, generate a simple TOC load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,1,['load'],['load']
Performance,"// For small loops (between 5 and 8 instructions), align to a 32-byte; // boundary so that the entire loop fits in one instruction-cache line.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:131,cache,cache,131,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['cache'],['cache']
Performance,"// For some DS form load/store instructions, it can also be an update form,; // if the stride is constant and is a multipler of 4. Use update form if; // prefer it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,1,['load'],['load']
Performance,"// For some cases, quantity ordering between scalable and fixed quantity types; // cannot be determined at compile time, so such comparisons aren't allowed.; //; // e.g. <vscale x 2 x i16> could be bigger than <4 x i32> with a runtime; // vscale >= 5, equal sized with a vscale of 4, and smaller with; // a vscale <= 3.; //; // All the functions below make use of the fact vscale is always >= 1, which; // means that <vscale x 4 x i32> is guaranteed to be >= <4 x i32>, etc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/TypeSize.h:45,scalab,scalable,45,interpreter/llvm-project/llvm/include/llvm/Support/TypeSize.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/TypeSize.h,1,['scalab'],['scalable']
Performance,"// For some instructions (ex: COPY), we might end up with < 0 latency; // as they don't have any Itinerary class associated with them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp:62,latency,latency,62,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,1,['latency'],['latency']
Performance,"// For some instructions, it is interesting to measure how it's performance; // characteristics differ depending on it's operands.; // This allows us to produce all the interesting variants.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Target.h:64,perform,performance,64,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Target.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Target.h,1,['perform'],['performance']
Performance,"// For some instructions, it is legal to fold ZERO into the RA register field.; // This function performs that fold by replacing the operand with PPC::ZERO,; // it does not consider whether the load immediate zero is no longer in use.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:97,perform,performs,97,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,2,"['load', 'perform']","['load', 'performs']"
Performance,// For store pairs: returns a register from FirstMI to the beginning of the; // block that can be renamed.; // For load pairs: returns a register from FirstMI to MI that can be renamed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:115,load,load,115,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// For structures that are opaque, return false but do not set the; // SCDB_NotContainsScalableVector flag since it may gain scalable vector type; // when it becomes non-opaque.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Type.cpp:125,scalab,scalable,125,interpreter/llvm-project/llvm/lib/IR/Type.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Type.cpp,1,['scalab'],['scalable']
Performance,"// For symbols that require a load from a stub to get the address, emit the; // load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,2,['load'],['load']
Performance,"// For tail calls lower the arguments to the 'real' stack slot.; //; // Force all the incoming stack arguments to be loaded from the stack; // before any new outgoing arguments are stored to the stack, because the; // outgoing stack slots may alias the incoming argument stack slots, and; // the alias isn't otherwise explicit. This is slightly more conservative; // than necessary, because it means that each store effectively depends; // on every argument instead of just those arguments it would clobber.; //; // Do not flag preceding copytoreg stuff together with the following stuff.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:117,load,loaded,117,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['load'],['loaded']
Performance,// For testing purposes we tell it to not use relaxed load forms so that it; // will split blocks.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsConstantIslandPass.cpp:54,load,load,54,interpreter/llvm-project/llvm/lib/Target/Mips/MipsConstantIslandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsConstantIslandPass.cpp,1,['load'],['load']
Performance,"// For testing with a mock opagent implementation, skips the dynamic load and; // the function resolution.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ExecutionEngine/OProfileWrapper.h:69,load,load,69,interpreter/llvm-project/llvm/include/llvm/ExecutionEngine/OProfileWrapper.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ExecutionEngine/OProfileWrapper.h,1,['load'],['load']
Performance,"// For the 4D case, the simplex is a 4D shape I won't even try to describe.; // To find out which of the 24 possible simplices we're in, we need to; // determine the magnitude ordering of x0, y0, z0 and w0.; // The method below is a good way of finding the ordering of x,y,z,w and; // then find the correct traversal order for the simplex were in.; // First, six pair-wise comparisons are performed between each possible pair; // of the four coordinates, and the results are used to add up binary bits; // for an integer index.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/modules/three_addons.mjs:390,perform,performed,390,js/modules/three_addons.mjs,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/modules/three_addons.mjs,1,['perform'],['performed']
Performance,"// For the TOC based ABIs, we have saved the TOC pointer to the linkage area; // on the stack (this would have been done in `LowerCall_64SVR4` or; // `LowerCall_AIX`). The call instruction is a pseudo instruction that; // represents both the indirect branch and a load that restores the TOC; // pointer from the linkage area. The operand for the TOC restore is an add; // of the TOC save offset to the stack pointer. This must be the second; // operand: after the chain input but before any other variadic arguments.; // For 64-bit ELFv2 ABI with PCRel, do not restore the TOC as it is not; // saved or used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:264,load,load,264,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// For the common case, fall back on the itinerary's latency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:53,latency,latency,53,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['latency'],['latency']
Performance,"// For the concurrent server that's not initiated by inetd,; // we have to wait for a connection request to arrive, then; // fork a child to handle the client's request.; // Beware that the accept() can be interrupted, such as by; // a previously spawned child process that has terminated; // (for which we caught the SIGCLD signal).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/rpdutils/src/net.cxx:11,concurren,concurrent,11,net/rpdutils/src/net.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/rpdutils/src/net.cxx,1,['concurren'],['concurrent']
Performance,"// For the following code,; // Block0:; // ...; // if (...) goto Block1 else ...; // Block1:; // %6 = load llvm.sk_buff:0:50$0:0:0:2:0; // %7 = bitcast %struct.sk_buff* %2 to i8*; // %8 = getelementptr i8, i8* %7, %6; // ...; // goto CommonExit; // Block2:; // ...; // if (...) goto Block3 else ...; // Block3:; // %6 = load llvm.bpf_map:0:40$0:0:0:2:0; // %7 = bitcast %struct.sk_buff* %2 to i8*; // %8 = getelementptr i8, i8* %7, %6; // ...; // goto CommonExit; // CommonExit; // SimplifyCFG may generate:; // Block0:; // ...; // if (...) goto Block_Common else ...; // Block2:; // ...; // if (...) goto Block_Common else ...; // Block_Common:; // PHI = [llvm.sk_buff:0:50$0:0:0:2:0, llvm.bpf_map:0:40$0:0:0:2:0]; // %6 = load PHI; // %7 = bitcast %struct.sk_buff* %2 to i8*; // %8 = getelementptr i8, i8* %7, %6; // ...; // goto CommonExit; // For the above code, we cannot perform proper relocation since; // ""load PHI"" has two possible relocations.; //; // To prevent above tail merging, we use __builtin_bpf_passthrough(); // where one of its parameters is a seq_num. Since two; // __builtin_bpf_passthrough() funcs will always have different seq_num,; // tail merging cannot happen. The __builtin_bpf_passthrough() will be; // removed in the beginning of Target IR passes.; //; // This approach is also used in other places when global var; // representing a relocation is used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp:102,load,load,102,interpreter/llvm-project/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/BPF/BPFAbstractMemberAccess.cpp,5,"['load', 'perform']","['load', 'perform']"
Performance,"// For the loads that combineLoadToOperationType does nothing, like; // ordered load, it should be profitable to hoist them.; // For swifterror load, it can only be used for pointer to pointer type, so; // later type check should get rid of this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,3,['load'],"['load', 'loads']"
Performance,"// For the most part, we just need to load the alloca, except that; // aggregate r-values are actually pointers to temporaries.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:38,load,load,38,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['load']
Performance,"// For the new PM, we also can't use OptimizationRemarkEmitter as an analysis; // pass. Function analyses need to be preserved across loop transformations; // but ORE cannot be preserved (see comment before the pass definition).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:37,Optimiz,OptimizationRemarkEmitter,37,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,3,['Optimiz'],['OptimizationRemarkEmitter']
Performance,"// For the new PM, we can't use OptimizationRemarkEmitter as an analysis; // pass. Function analyses need to be preserved across loop transformations; // but ORE cannot be preserved (see comment before the pass definition).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp:32,Optimiz,OptimizationRemarkEmitter,32,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp,2,['Optimiz'],['OptimizationRemarkEmitter']
Performance,"// For the non-writeback version (this one), the base register must be; // one of the registers being loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp:102,load,loaded,102,interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,1,['load'],['loaded']
Performance,"// For the old PM, we can't use OptimizationRemarkEmitter as an analysis; // pass. Function analyses need to be preserved across loop transformations; // but ORE cannot be preserved (see comment before the pass definition).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:32,Optimiz,OptimizationRemarkEmitter,32,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,2,['Optimiz'],['OptimizationRemarkEmitter']
Performance,"// For the optimization to be correct, we need RPOT to have a property that; // each block is processed after all its predecessors, which may only be; // violated for headers of the current loop and all nested loops. Irreducible; // CFG provides multiple ways to break this assumption, so we do not want to; // deal with it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp:11,optimiz,optimization,11,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDeletion.cpp,1,['optimiz'],['optimization']
Performance,"// For the optimized implementation of checking if two collections overlap by name.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooProdPdf.cxx:11,optimiz,optimized,11,roofit/roofitcore/src/RooProdPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooProdPdf.cxx,1,['optimiz'],['optimized']
Performance,"// For the range reasoning, avoid computing SCEVs in the loop to avoid; // poisoning cache with sub-optimal results. For the must-execute case,; // this is a neccessary precondition for correctness.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp:85,cache,cache,85,interpreter/llvm-project/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/IndVarSimplify.cpp,1,['cache'],['cache']
Performance,"// For the remaining optimizations, we need to be able to make a negative; // number through a combination of mask and undemanded bits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:21,optimiz,optimizations,21,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['optimiz'],['optimizations']
Performance,"// For the scalar case extend to a 128-bit vector, perform the logic op,; // and extract the scalar result back out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:51,perform,perform,51,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,// For the time being limit this optimization to occurring once in a; // function since it can change the CFG significantly. This is not a; // strict requirement but it can cause buggy behavior if there is an; // overlap of blocks in different opportunities. There is a lot of room to; // experiment with catching more opportunities here.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DFAJumpThreading.cpp:33,optimiz,optimization,33,interpreter/llvm-project/llvm/lib/Transforms/Scalar/DFAJumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DFAJumpThreading.cpp,1,['optimiz'],['optimization']
Performance,"// For the transform to be legal, the load must produce only two values; // (the value loaded and the chain). Don't transform a pre-increment; // load, for example, which produces an extra value. Otherwise the; // transformation is not equivalent, and the downstream logic to replace; // uses gets things wrong.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['load'],"['load', 'loaded']"
Performance,"// For the users of the source value being used for compare instruction, if; // the number of signed predicate is greater than unsigned predicate, we; // prefer to use SIGN_EXTEND.; //; // With this optimization, we would be able to reduce some redundant sign or; // zero extension instruction, and eventually more machine CSE opportunities; // can be exposed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp:199,optimiz,optimization,199,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FunctionLoweringInfo.cpp,1,['optimiz'],['optimization']
Performance,// For this AND to be a zero extension of the masked load the elements; // of the BuildVec must mask the bottom bits of the extended element; // type,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// For this optimization, check by comparing the latency of a representative; // instruction to that of the replacement instructions.; // TODO: check for all concerned instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:12,optimiz,optimization,12,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,2,"['latency', 'optimiz']","['latency', 'optimization']"
Performance,"// For this optimization, check for all concerned instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp:12,optimiz,optimization,12,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64SIMDInstrOpt.cpp,1,['optimiz'],['optimization']
Performance,"// For this test, use a uniform non-unity weight of 1.5. It was set to 0.1; // in the past, but then there were fourth-digit differences between the; // scalar mode and the batch mode. However, this is most likeliy not; // pointing towards a flaw in the batch mode, which is why a value was; // handpicked for which the differences disappear. Any residual problems are; // most likely caused by the unnecessarily complicated implementation of the; // RooAddPdf extended term in the scalar mode: the coefficients are; // projected to the subrange by cached scale factors, while the batch mode; // just uses the same scaling factor as for the full likelihood.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/testSumW2Error.cxx:549,cache,cached,549,roofit/roofitcore/test/testSumW2Error.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/testSumW2Error.cxx,1,['cache'],['cached']
Performance,"// For two constants C0 and C1 from; // ```; // li Y, C0; // li Z, C1; // ```; // 1. if C1 = C0 + 1; // we can turn:; // (a) blt Y, X -> bge X, Z; // (b) bge Y, X -> blt X, Z; //; // 2. if C1 = C0 - 1; // we can turn:; // (a) blt X, Y -> bge Z, X; // (b) bge X, Y -> blt Z, X; //; // To make sure this optimization is really beneficial, we only; // optimize for cases where Y had only one use (i.e. only used by the branch).; // Right now we only care about LI (i.e. ADDI x0, imm)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:302,optimiz,optimization,302,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,2,['optimiz'],"['optimization', 'optimize']"
Performance,"// For uninteresting identifiers, there's nothing else to do. Just notify; // the reader that we've finished loading this identifier.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:109,load,loading,109,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loading']
Performance,"// For unsized types or scalable vectors we don't know exactly how many bytes; // are dereferenced, so bail out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:24,scalab,scalable,24,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,1,['scalab'],['scalable']
Performance,"// For unsplit intrinsics, we simply modify the source and destination; // pointers in place. This isn't just an optimization, it is a matter of; // correctness. With unsplit intrinsics we may be dealing with transfers; // within a single alloca before SROA ran, or with transfers that have; // a variable length. We may also be dealing with memmove instead of; // memcpy, and so simply updating the pointers is the necessary for us to; // update both source and dest of a single call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:113,optimiz,optimization,113,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['optimiz'],['optimization']
Performance,// For unsupported scalable vector.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:19,scalab,scalable,19,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['scalab'],['scalable']
Performance,"// For us to make any changes, it must a comparison between a single-use; // load and a constant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:77,load,load,77,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,// For use with PostRAScheduling: get the anti-dependence breaking that should; // be performed before post-RA scheduling.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h:86,perform,performed,86,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h,2,['perform'],['performed']
Performance,// For use with PostRAScheduling: get the minimum optimization level needed; // to enable post-RA scheduling.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h:50,optimiz,optimization,50,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetSubtargetInfo.h,1,['optimiz'],['optimization']
Performance,"// For v = setjmp(buf), we generate; //; // thisMBB:; // buf[LabelOffset] = restoreMBB <-- takes address of restoreMBB; // SjLjSetup restoreMBB; //; // mainMBB:; // v_main = 0; //; // sinkMBB:; // v = phi(main, restore); //; // restoreMBB:; // if base pointer being used, load it from frame; // v_restore = 1",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:272,load,load,272,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// For v16i8 cases we need to perform UMIN on pairs of byte elements,; // shuffling each upper element down and insert zeros. This means that the; // v16i8 UMIN will leave the upper element as zero, performing zero-extension; // ready for the PHMINPOS.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:30,perform,perform,30,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['perform'],"['perform', 'performing']"
Performance,"// For v2f64, v4f32 and v4i32 types, we require the load to be non-extending; // as we cannot handle extending loads for these types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,"// For v8i16 and v16i8 types, extending loads can be handled as long as the; // memory VT is the same vector element VT type.; // The loads feeding into the v8i16 and v16i8 types will be extending because; // scalar i8/i16 are not legal types.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['loads']
Performance,"// For vector geps, use the generic demanded vector support.; // Skip if GEP return type is scalable. The number of elements is unknown at; // compile-time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:92,scalab,scalable,92,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['scalab'],['scalable']
Performance,"// For vector loads, the expected load combine pattern will have an; // ExtractElement for each index in the vector. While each of these; // ExtractElements will be accessing the same base address as determined; // by the load instruction, the actual bytes they interact with will differ; // due to different ExtractElement indices. To accurately determine the; // byte position of an ExtractElement, we offset the base load ptr with; // the index multiplied by the byte size of each element in the vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],"['load', 'loads']"
Performance,"// For vectors, we do not canonicalize all truncs to icmp, so optimize; // patterns that would be covered within visitICmpInst.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:62,optimiz,optimize,62,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,1,['optimiz'],['optimize']
Performance,"// For virtual registers, the def latency is included.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineTraceMetrics.cpp:34,latency,latency,34,interpreter/llvm-project/llvm/lib/CodeGen/MachineTraceMetrics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineTraceMetrics.cpp,1,['latency'],['latency']
Performance,// For zero-extending loads to 64bit we emit a 32bit load and then convert; // the 32bit reg to a 64bit reg.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp:22,load,loads,22,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FastISel.cpp,2,['load'],"['load', 'loads']"
Performance,"// For zero-extends, the extend is performed automatically by a umov unless; // the destination type is i64 and the element type is i8 or i16.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:35,perform,performed,35,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['perform'],['performed']
Performance,// Forbidding inlining improves performance by roughly 20%.; // FIXME: Remove once llvm optimizes this to the faster version without hints.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/YAMLParser.cpp:32,perform,performance,32,interpreter/llvm-project/llvm/lib/Support/YAMLParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/YAMLParser.cpp,2,"['optimiz', 'perform']","['optimizes', 'performance']"
Performance,"// Force all the incoming stack arguments to be loaded from the stack; // before any new outgoing arguments are stored to the stack, because the; // outgoing stack slots may alias the incoming argument stack slots, and; // the alias isn't otherwise explicit. This is slightly more conservative; // than necessary, because it means that each store effectively depends; // on every argument instead of just those arguments it would clobber.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:48,load,loaded,48,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,2,['load'],['loaded']
Performance,"// Force creation of TPosixThreadFactory when shared library will be loaded; // (don't explicitly create a TPosixThreadFactory).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/thread/src/TPosixThreadFactory.cxx:69,load,loaded,69,core/thread/src/TPosixThreadFactory.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/thread/src/TPosixThreadFactory.cxx,1,['load'],['loaded']
Performance,"// Force creation of TWin32ThreadFactory when shared library will be loaded; // (don't explicitly create a TWin32ThreadFactory).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/thread/src/TWin32ThreadFactory.cxx:69,load,loaded,69,core/thread/src/TWin32ThreadFactory.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/thread/src/TWin32ThreadFactory.cxx,1,['load'],['loaded']
Performance,// Force external decls in the HLSL namespace to load from the PCH.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/HLSLExternalSemaSource.cpp:49,load,load,49,interpreter/llvm-project/clang/lib/Sema/HLSLExternalSemaSource.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/HLSLExternalSemaSource.cpp,1,['load'],['load']
Performance,"// Force only the learn entries to be cached by temporarily setting min/max; // to the learning phase entry range; // But save all the old values, so we can restore everything to how it was",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx:38,cache,cached,38,tree/tree/src/TTreeCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx,1,['cache'],['cached']
Performance,"// Force the loading of the index.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx:13,load,loading,13,tree/treeplayer/src/TTreeFormula.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx,4,['load'],['loading']
Performance,// Force the offset into the constant pool and load it from there.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,"// Fork a child process to handle the client's request.; // The parent returns the child pid to the caller, which is; // probably a concurrent server that'll call us again, to wait; // for the next client request to this well-known port.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/rpdutils/src/net.cxx:132,concurren,concurrent,132,net/rpdutils/src/net.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/rpdutils/src/net.cxx,1,['concurren'],['concurrent']
Performance,"// Form a sequence of SVE registers for instructions using list of vectors,; // e.g. structured loads and stores (ldN, stN).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp:96,load,loads,96,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelDAGToDAG.cpp,1,['load'],['loads']
Performance,// Form a simple per-variable cache of these values in case we find we; // want to reuse them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp:30,cache,cache,30,interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp,1,['cache'],['cache']
Performance,// Form and emit the load,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp:21,load,load,21,interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMAsmPrinter.cpp,1,['load'],['load']
Performance,"// Form load groups.; // To avoid complications with moving code across basic blocks, only form; // groups that are contained within a single basic block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,1,['load'],['load']
Performance,// Form the list of comparisons we're going to perform.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:47,perform,perform,47,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,1,['perform'],['perform']
Performance,// Formatting and IO Library Call Optimizations,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/SimplifyLibCalls.h:34,Optimiz,Optimizations,34,interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/SimplifyLibCalls.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/SimplifyLibCalls.h,1,['Optimiz'],['Optimizations']
Performance,"// Forward -ObjC when either -ObjC or -ObjC++ is used, to force loading; // members of static archive libraries which implement Objective-C classes or; // categories.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Darwin.cpp:64,load,loading,64,interpreter/llvm-project/clang/lib/Driver/ToolChains/Darwin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Darwin.cpp,1,['load'],['loading']
Performance,"// Forward call to all cached contained in current object",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx:23,cache,cached,23,roofit/roofitcore/src/RooAbsArg.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx,1,['cache'],['cached']
Performance,// Forward trivially copyable types to array_pod_sort. This avoids a large; // amount of code bloat for a minor performance hit.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/STLExtras.h:112,perform,performance,112,interpreter/llvm-project/llvm/include/llvm/ADT/STLExtras.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/STLExtras.h,1,['perform'],['performance']
Performance,"// Forward, but if vectorized, is likely to prevent store-to-load; // forwarding.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/LoopAccessAnalysis.h:61,load,load,61,interpreter/llvm-project/llvm/include/llvm/Analysis/LoopAccessAnalysis.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/LoopAccessAnalysis.h,1,['load'],['load']
Performance,"// Found a previous load, return it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,1,['load'],['load']
Performance,"// Found a transmitting load -- no need to continue; // traversing its defs (i.e., this load will become; // a new gadget source anyways).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86LoadValueInjectionLoadHardening.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/Target/X86/X86LoadValueInjectionLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86LoadValueInjectionLoadHardening.cpp,2,['load'],['load']
Performance,// Found and loaded new dSYM file,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/MachODebugMapParser.cpp:13,load,loaded,13,interpreter/llvm-project/llvm/tools/dsymutil/MachODebugMapParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/MachODebugMapParser.cpp,1,['load'],['loaded']
Performance,"// Found cached result, return it!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CaptureTracking.cpp:9,cache,cached,9,interpreter/llvm-project/llvm/lib/Analysis/CaptureTracking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CaptureTracking.cpp,1,['cache'],['cached']
Performance,// Found in the cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp:16,cache,cache,16,interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CrossTU/CrossTranslationUnit.cpp,2,['cache'],['cache']
Performance,"// Four tranformations to do here:; // 1) Find loads that directly read from stores and promote them by; // replacing with mov instructions. If the store is wider than the load,; // the load will be replaced with a bitfield extract.; // e.g.,; // str w1, [x0, #4]; // ldrh w2, [x0, #6]; // ; becomes; // str w1, [x0, #4]; // lsr w2, w1, #16",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:47,load,loads,47,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,3,['load'],"['load', 'loads']"
Performance,"// Frames that have variable sized objects and scalable SVE objects,; // should always use a basepointer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp:47,scalab,scalable,47,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64RegisterInfo.cpp,1,['scalab'],['scalable']
Performance,// Free any cached MacroArgs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp:12,cache,cached,12,interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,1,['cache'],['cached']
Performance,"// Free any cached macro expanders.; // This populates MacroArgCache, so all TokenLexers need to be destroyed; // before the code below that frees up the MacroArgCache list.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp:12,cache,cached,12,interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,1,['cache'],['cached']
Performance,// Free the buffers associated with remapped files. We are required to; // perform this operation here because we explicitly request that the; // compiler instance *not* free these buffers for each invocation of the; // parser.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:75,perform,perform,75,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['perform'],['perform']
Performance,// Free the scope cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/Parser.cpp:18,cache,cache,18,interpreter/llvm-project/clang/lib/Parse/Parser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/Parser.cpp,1,['cache'],['cache']
Performance,"// From FirstStore to LastLoad neither of the elimination candidate loads; // should overlap with any of the stores.; //; // E.g.:; //; // st1 C[i]; // ld1 B[i] <-------,; // ld0 A[i] <----, | * LastLoad; // ... | |; // st2 E[i] | |; // st3 B[i+1] -- | -' * FirstStore; // st0 A[i+1] ---'; // st4 D[i]; //; // st0 forwards to ld0 if the accesses in st4 and st1 don't overlap with; // ld0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp:68,load,loads,68,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,1,['load'],['loads']
Performance,"// From RISC-V ISA spec, if both the high and low bits of the same product; // are required, then the recommended code sequence is:; //; // MULH[[S]U] rdh, rs1, rs2; // MUL rdl, rs1, rs2; // (source register specifiers must be in same order and rdh cannot be the; // same as rs1 or rs2); //; // Microarchitectures can then fuse these into a single multiply operation; // instead of performing two separate multiplies.; // MachineCombiner may reassociate MUL operands and lose the fusion; // opportunity.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:382,perform,performing,382,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,1,['perform'],['performing']
Performance,// From here we extract the elements and perform the and/or/xor.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:41,perform,perform,41,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['perform'],['perform']
Performance,"// From the cycles, construct the sequence of values that will; // then form the control values for vdealvdd/vshuffvdd, i.e.; // (M a1 a2)(M a3 a4 a5)... -> a1 a2 a3 a4 a5; // This essentially strips the M value from the cycles where; // it's present, and performs the insertion of M (then stripping); // for cycles without M (as described in an earlier comment).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAGHVX.cpp:256,perform,performs,256,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAGHVX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAGHVX.cpp,1,['perform'],['performs']
Performance,"// From the range of values we could use for BaseOff, choose the one that; // is aligned to the highest power of two, to maximise the chance that; // the same offset can be reused for other load/store pairs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:190,load,load,190,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,2,['load'],['load']
Performance,"// From the wide load, create two values that equal the original two loads.; // Loads[0] needs trunc while Loads[1] needs a lshr and trunc.; // TODO: Support big-endian as well.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp:17,load,load,17,interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,4,"['Load', 'load']","['Loads', 'load', 'loads']"
Performance,"// Fully resolve the given name to the final type name.; // try memoized type cache, in case seen before",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/src/clingwrapper.cxx:78,cache,cache,78,bindings/pyroot/cppyy/cppyy-backend/clingwrapper/src/clingwrapper.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/cppyy-backend/clingwrapper/src/clingwrapper.cxx,1,['cache'],['cache']
Performance,// FuncOffsetTable maps function context to its profile offset in; // SecLBRProfile section. It is used to load function profile on demand.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ProfileData/SampleProfWriter.h:107,load,load,107,interpreter/llvm-project/llvm/include/llvm/ProfileData/SampleProfWriter.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ProfileData/SampleProfWriter.h,1,['load'],['load']
Performance,// Function determines if ALU operation (in alu_iter) can be combined with; // a load/store with base and offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp:81,load,load,81,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp,1,['load'],['load']
Performance,"// Function in the symbol list but without sample will be regarded as; // cold. To minimize the potential negative performance impact it could; // have, we want to be a little conservative here saying if a function; // shows up in the profile, no matter as outline function, inline instance; // or call targets, treat the function as not being cold. This will handle; // the cases such as most callsites of a function are inlined in sampled; // binary but not inlined in current build (because of source code drift,; // imprecise debug information, or the callsites are all cold individually; // but not cold accumulatively...), so the outline function showing up as; // cold in sampled binary will actually not be cold after current build.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp:115,perform,performance,115,interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,1,['perform'],['performance']
Performance,"// Function pointers in the 64-bit SVR4 ABI do not point to the function; // entry point, but to the function descriptor (the function entry point; // address is part of the function descriptor though).; // The function descriptor is a three doubleword structure with the; // following fields: function entry point, TOC base address and; // environment pointer.; // Thus for a call through a function pointer, the following actions need; // to be performed:; // 1. Save the TOC of the caller in the TOC save area of its stack; // frame (this is done in LowerCall_Darwin() or LowerCall_64SVR4()).; // 2. Load the address of the function entry point from the function; // descriptor.; // 3. Load the TOC of the callee from the function descriptor into r2.; // 4. Load the environment pointer from the function descriptor into; // r11.; // 5. Branch to the function entry point address.; // 6. On return of the callee, the TOC of the caller needs to be; // restored (this is done in FinishCall()).; //; // The loads are scheduled at the beginning of the call sequence, and the; // register copies are flagged together to ensure that no other; // operations can be scheduled in between. E.g. without flagging the; // copies together, a TOC access in the caller could be scheduled between; // the assignment of the callee TOC and the branch to the callee, which leads; // to incorrect code.; // Start by loading the function address from the descriptor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:447,perform,performed,447,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,6,"['Load', 'load', 'perform']","['Load', 'loading', 'loads', 'performed']"
Performance,// Function should not be optimized as tail call.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetFrameLoweringImpl.cpp:26,optimiz,optimized,26,interpreter/llvm-project/llvm/lib/CodeGen/TargetFrameLoweringImpl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetFrameLoweringImpl.cpp,1,['optimiz'],['optimized']
Performance,"// Function templates always go through overload resolution, at which; // point we'll perform the various checks (e.g., accessibility) we need; // to based on which function we selected.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:86,perform,perform,86,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['perform'],['perform']
Performance,"// Function to load PyObject information from string",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/pymva/inc/TMVA/PyMethodBase.h:15,load,load,15,tmva/pymva/inc/TMVA/PyMethodBase.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/pymva/inc/TMVA/PyMethodBase.h,1,['load'],['load']
Performance,"// Function used to add the attribute. The parameter `VLEN` is; // templated to allow the use of ""x"" when targeting scalable functions; // for SVE.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp:116,scalab,scalable,116,interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp,1,['scalab'],['scalable']
Performance,"// Functions for testing concurrentfill",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/histv7/test/concurrentfill.cxx:25,concurren,concurrentfill,25,hist/histv7/test/concurrentfill.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/histv7/test/concurrentfill.cxx,1,['concurren'],['concurrentfill']
Performance,"// Functions like isZIPMask return true when a ISD::VECTOR_SHUFFLE's mask; // represents the same logical operation as performed by a ZIP instruction. In; // isolation these functions do not mean the ISD::VECTOR_SHUFFLE is exactly; // equivalent to an AArch64 instruction. There's the extra component of; // ISD::VECTOR_SHUFFLE's value type to consider. Prior to SVE these functions; // only operated on 64/128bit vector types that have a direct mapping to a; // target register and so an exact mapping is implied.; // However, when using SVE for fixed length vectors, most legal vector types; // are actually sub-vectors of a larger SVE register. When mapping; // ISD::VECTOR_SHUFFLE to an SVE instruction care must be taken to consider; // how the mask's indices translate. Specifically, when the mapping requires; // an exact meaning for a specific vector index (e.g. Index X is the last; // vector element in the register) then such mappings are often only safe when; // the exact SVE register size is know. The main exception to this is when; // indices are logically relative to the first element of either; // ISD::VECTOR_SHUFFLE operand because these relative indices don't change; // when converting from fixed-length to scalable vector types (i.e. the start; // of a fixed length vector is always the start of a scalable vector).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:119,perform,performed,119,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,"['perform', 'scalab']","['performed', 'scalable']"
Performance,// Functions needs to be cacheline (256B) aligned.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp:25,cache,cacheline,25,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600AsmPrinter.cpp,1,['cache'],['cacheline']
Performance,"// Funny Darwin hack: This flag tells the linker that no global symbols; // contain code that falls through to other global symbols (e.g. the obvious; // implementation of multiple entry points). If this doesn't occur, the; // linker can safely perform dead code stripping. Since LLVM never; // generates code that does this, it is always safe to set.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp:245,perform,perform,245,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64AsmPrinter.cpp,2,['perform'],['perform']
Performance,// Further compression optimization: For invalid compositions resulting; // in a sequence with 0 entries we can just pick any other. Choose; // Mask 0xffffffff with Rotation 0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenRegisters.cpp:23,optimiz,optimization,23,interpreter/llvm-project/llvm/utils/TableGen/CodeGenRegisters.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenRegisters.cpp,1,['optimiz'],['optimization']
Performance,// GCC and GCC-compatible compilers define __OPTIMIZE__ when optimizations are; // enabled.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CommandLine.cpp:61,optimiz,optimizations,61,interpreter/llvm-project/llvm/lib/Support/CommandLine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CommandLine.cpp,1,['optimiz'],['optimizations']
Performance,"// GCC applies the following optimization to variables and static; // data members, but not to functions:; //; // Modify the variable's LV by the LV of its type unless this is; // C or extern ""C"". This follows from [basic.link]p9:; // A type without linkage shall not be used as the type of a; // variable or function with external linkage unless; // - the entity has C language linkage, or; // - the entity is declared within an unnamed namespace, or; // - the entity is not used or is defined in the same; // translation unit.; // and [basic.link]p10:; // ...the types specified by all declarations referring to a; // given variable or function shall be identical...; // C does not have an equivalent rule.; //; // Ignore this if we've got an explicit attribute; the user; // probably knows what they're doing.; //; // Note that we don't want to make the variable non-external; // because of this, but unique-external linkage suits us.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Decl.cpp:29,optimiz,optimization,29,interpreter/llvm-project/clang/lib/AST/Decl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Decl.cpp,1,['optimiz'],['optimization']
Performance,"// GLD{FF}1_IMM requires that the offset is an immediate that is:; // * a multiple of #SizeInBytes,; // * in the range [0, 31 x #SizeInBytes],; // where #SizeInBytes is the size in bytes of the loaded items. For; // immediates outside that range and non-immediate scalar offsets use; // GLD1_MERGE_ZERO or GLD1_UXTW_MERGE_ZERO instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:194,load,loaded,194,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loaded']
Performance,"// GLOBAL loads and stores are classified as FLAT initially. If both combined; // instructions are FLAT GLOBAL adjust the class to GLOBAL_LOAD or GLOBAL_STORE.; // If either or both instructions are non segment specific FLAT the resulting; // combined operation will be FLAT, potentially promoting one of the GLOBAL; // operations to FLAT.; // For other instructions return the original unmodified class.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp:10,load,loads,10,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILoadStoreOptimizer.cpp,1,['load'],['loads']
Performance,// GMemOperation because we also want to match indexed loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:55,load,loads,55,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,1,['load'],['loads']
Performance,"// GOTTPOFF and TLSDESC relocations require a REX prefix to allow; // linker optimizations: even if the instructions we see may not require; // any prefix, they may be replaced by instructions that do. This is; // handled as a special case here so that it also works for hand-written; // assembly without the user needing to write REX, as with GNU as.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp:77,optimiz,optimizations,77,interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp,1,['optimiz'],['optimizations']
Performance,// GOTTPOFF relocation loads can only be folded into add instructions.; // FIXME: Need to exclude other relocations that only support specific; // instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['loads']
Performance,"// GV is a non-escaping global. V is a pointer address that has been loaded from.; // If we can prove that V must escape, we can conclude that a load from V cannot; // alias GV.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp:69,load,loaded,69,interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,2,['load'],"['load', 'loaded']"
Performance,// G_LOAD is used for both non-extending and any-extending loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp:59,load,loads,59,interpreter/llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/GlobalISelEmitter.cpp,1,['load'],['loads']
Performance,"// Gather if we hit the RecursionMaxDepth, unless this is a load (or z/sext of; // a load), in which case peek through to include it in the tree, without; // ballooning over-budget.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:60,load,load,60,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,2,['load'],['load']
Performance,// Gather statistics after optimization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:27,optimiz,optimization,27,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['optimiz'],['optimization']
Performance,// Gather up the effects that were performed on the object at this; // program point,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountDiagnostics.cpp:35,perform,performed,35,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountDiagnostics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/RetainCountChecker/RetainCountDiagnostics.cpp,1,['perform'],['performed']
Performance,"// Gather/scatters do allow loading from arbitrary strides, at; // least if they are loop invariant.; // TODO: Loop variant strides should in theory work, too, but; // this requires further testing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:28,load,loading,28,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['load'],['loading']
Performance,"// General case - set/clear individual bits in dst based on src.; // TODO - there is scope for optimization here, but at the moment this code; // path is barely used so prefer readability over performance.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APInt.cpp:95,optimiz,optimization,95,interpreter/llvm-project/llvm/lib/Support/APInt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APInt.cpp,2,"['optimiz', 'perform']","['optimization', 'performance']"
Performance,// Generate !llvm.loop.parallel metadata for loads and stores for loops; // with dynamic/guided scheduling and without ordered clause.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp:45,load,loads,45,interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,1,['load'],['loads']
Performance,// Generate Load from constant pool.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:12,Load,Load,12,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['Load'],['Load']
Performance,// Generate N loads of T type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,1,['load'],['loads']
Performance,// Generate a cache of parsed FileEntry pointers for alias lookups.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/VerifyDiagnosticConsumer.cpp:14,cache,cache,14,interpreter/llvm-project/clang/lib/Frontend/VerifyDiagnosticConsumer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/VerifyDiagnosticConsumer.cpp,1,['cache'],['cache']
Performance,// Generate a machine instruction node corresponding to the circ/brev; // load intrinsic.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.h:74,load,load,74,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.h,1,['load'],['load']
Performance,// Generate a misaligned load that is guaranteed to cause a crash.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp,1,['load'],['load']
Performance,// Generate a set of all instructions taking part in load; // interleaved. This list excludes the instructions necessary for the; // polynomial construction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp,1,['load'],['load']
Performance,// Generate a set of all load instructions to be combined,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterleavedLoadCombinePass.cpp,1,['load'],['load']
Performance,"// Generate a vector-predicated load if it is custom/legal on the target. To; // avoid possible recursion, only do this if the widened mask type is legal.; // FIXME: Not all targets may support EVL in VP_LOAD. These will have been; // removed from the IR by the ExpandVectorPredication pass but we're; // reintroducing them here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['load']
Performance,// Generate an error node. Check for a null node in case; // we cache out.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NonNullParamChecker.cpp:64,cache,cache,64,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NonNullParamChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NonNullParamChecker.cpp,1,['cache'],['cache']
Performance,// Generate an index for each load and store according to the original; // program order. This will be used later.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,1,['load'],['load']
Performance,// Generate code to load the content of the guard slot.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:20,load,load,20,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,2,['load'],['load']
Performance,"// Generate equivalence classes for related computations (webs) by; // def-use relationships of virtual registers. Mention of a physical; // register terminates the generation of equivalence classes as this; // indicates a use of a parameter, definition of a return value, use; // of a value returned from a call, or definition of a parameter to a; // call. Computations with physical register mentions are flagged; // as such so their containing webs will not be optimized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp:464,optimiz,optimized,464,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp,1,['optimiz'],['optimized']
Performance,// Generate hi masked load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['load']
Performance,// Generate instructions to load the const fp from constant pool.; // We only support PPC64 and medium code model.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,// Generate load dag and prepare chains.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,2,['load'],['load']
Performance,// Generate loads from param memory/moves from registers for result,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:12,load,loads,12,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['loads']
Performance,// Generate necessary loads at appropriate locations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:22,load,loads,22,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,1,['load'],['loads']
Performance,// Generate new constant pool entry + legalize immediately for the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:67,load,load,67,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// Generate new load node.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,// Generate optimized instructions for pre AVX512 unsigned conversions from; // vXf32 to vXi32.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:12,optimiz,optimized,12,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimized']
Performance,"// Generate the epilogue with inline instrumentation.; // If we do not support SelectionDAG based calls, generate IR level; // calls.; //; // For each block with a return instruction, convert this:; //; // return:; // ...; // ret ...; //; // into this:; //; // return:; // ...; // %1 = <stack guard>; // %2 = load StackGuardSlot; // %3 = icmp ne i1 %1, %2; // br i1 %3, label %CallStackCheckFailBlk, label %SP_return; //; // SP_return:; // ret ...; //; // CallStackCheckFailBlk:; // call void @__stack_chk_fail(); // unreachable; // Create the FailBB. We duplicate the BB every time since the MI tail; // merge pass will merge together all of the various BB into one including; // fail BB generated by the stack protector pseudo instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/StackProtector.cpp:309,load,load,309,interpreter/llvm-project/llvm/lib/CodeGen/StackProtector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/StackProtector.cpp,1,['load'],['load']
Performance,"// Generate the high vp_strided_load.; // To calculate the high base address, we need to sum to the low base; // address stride number of bytes for each element already loaded by low,; // that is: Ptr = Ptr + (LoEVL * Stride)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:169,load,loaded,169,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['loaded']
Performance,// Generate the intrinsic which also performs; // icmp ne zero on the loop counter value and; // produces an i1 to guard the loop entry.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h:37,perform,performs,37,interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfo.h,1,['perform'],['performs']
Performance,"// Generate vtable assumptions if we're constructing a complete object; // with a vtable. We don't do this for base subobjects for two reasons:; // first, it's incorrect for classes with virtual bases, and second, we're; // about to overwrite the vptrs anyway.; // We also have to make sure if we can refer to vtable:; // - Otherwise we can refer to vtable if it's safe to speculatively emit.; // FIXME: If vtable is used by ctor/dtor, or if vtable is external and we are; // sure that definition of vtable is not hidden,; // then we are always safe to refer to it.; // FIXME: It looks like InstCombine is very inefficient on dealing with; // assumes. Make assumption loads require -fstrict-vtable-pointers temporarily.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp:668,load,loads,668,interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,1,['load'],['loads']
Performance,// Generate wider load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,1,['load'],['load']
Performance,"// Generates an error report, indicating that the function whose name is given; // will perform a zero byte allocation.; // Returns false if an error occurred, true otherwise.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/UnixAPIChecker.cpp:88,perform,perform,88,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/UnixAPIChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/UnixAPIChecker.cpp,1,['perform'],['perform']
Performance,// Generates instruction to load an immediate value into a register.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/AArch64/Target.cpp:28,load,load,28,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/AArch64/Target.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/AArch64/Target.cpp,3,['load'],['load']
Performance,"// Generates the 64-bits address loads as exemplified in section; // 4.5.1 in PPC64 ELF ABI. Note that the relocations need to; // apply to the low part of the instructions, so we have to update; // the offset according to the target endianness.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp:33,load,loads,33,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldELF.cpp,1,['load'],['loads']
Performance,// Generic loads and stores must have a single MachineMemOperand; // describing that access.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineVerifier.cpp,1,['load'],['loads']
Performance,"// Get LHS pointer, load its value and cast it to the; // computation type if necessary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp:20,load,load,20,interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp,1,['load'],['load']
Performance,"// Get a Module for \p FileName from the cache, or load it lazily.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp:41,cache,cache,41,interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp,2,"['cache', 'load']","['cache', 'load']"
Performance,"// Get a call instruction from the call sequence chain. Tail calls are not; // allowed. The following code is essentially reverse engineering X86's; // LowerCallTo.; //; // We are expecting DAG to have the following form:; //; // ch = eh_label (only in case of invoke statepoint); // ch, glue = callseq_start ch; // ch, glue = X86::Call ch, glue; // ch, glue = callseq_end ch, glue; // get_return_value ch, glue; //; // get_return_value can either be a sequence of CopyFromReg instructions; // to grab the return value from the return register(s), or it can be a LOAD; // to load a value returned by reference via a stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/StatepointLowering.cpp:563,LOAD,LOAD,563,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/StatepointLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/StatepointLowering.cpp,2,"['LOAD', 'load']","['LOAD', 'load']"
Performance,// Get a list of the features that make up the CPU option for; // cpu_specific/cpu_dispatch so that it can be passed to llvm as optimization; // options.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h:128,optimiz,optimization,128,interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h,1,['optimiz'],['optimization']
Performance,"// Get a sequence of instructions like; // %reg = and* ... // Set EFLAGS; // ... // EFLAGS not changed; // %extended_reg = subreg_to_reg 0, %reg, %subreg.sub_32bit; // test64rr %extended_reg, %extended_reg, implicit-def $eflags; // or; // %reg = and32* ...; // ... // EFLAGS not changed.; // %src_reg = copy %reg.sub_16bit:gr32; // test16rr %src_reg, %src_reg, implicit-def $eflags; //; // If subsequent readers use a subset of bits that don't change; // after `and*` instructions, it's likely that the test64rr could; // be optimized away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:525,optimiz,optimized,525,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['optimiz'],['optimized']
Performance,// Get a thread lock to make sure we aren't trying to load multiple times,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp:54,load,load,54,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,1,['load'],['load']
Performance,"// Get an existing global index. This loads it if not already loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:38,load,loads,38,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,2,['load'],"['loaded', 'loads']"
Performance,// Get an existing global index. This loads it if not already; // loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:38,load,loads,38,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// Get and cache the histograms for this channel:; //collector.CollectHistograms( channel );; // Do I need this...?; // channel.CollectHistograms();; // Make a directory to store the histograms; // for this channel",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/Measurement.cxx:11,cache,cache,11,roofit/histfactory/src/Measurement.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/Measurement.cxx,1,['cache'],['cache']
Performance,"// Get cache size",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/winnt/src/TWinNTSystem.cxx:7,cache,cache,7,core/winnt/src/TWinNTSystem.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/winnt/src/TWinNTSystem.cxx,1,['cache'],['cache']
Performance,"// Get current event and load to python array",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/pymva/src/MethodPyAdaBoost.cxx:25,load,load,25,tmva/pymva/src/MethodPyAdaBoost.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/pymva/src/MethodPyAdaBoost.cxx,6,['load'],['load']
Performance,// Get depth and latency of NewRoot and Root.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCombiner.cpp:17,latency,latency,17,interpreter/llvm-project/llvm/lib/CodeGen/MachineCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCombiner.cpp,1,['latency'],['latency']
Performance,// Get estimation for interleaved load/store operations and strided load.; // \p Indices contains indices for strided load.; // \p Factor - the factor of interleaving.; // AVX-512 provides 3-src shuffles that significantly reduces the cost.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,3,['load'],['load']
Performance,"// Get estimation for interleaved load/store operations for SSE-AVX2.; // As opposed to AVX-512, SSE-AVX2 do not have generic shuffles that allow; // computing the cost using a generic formula as a function of generic; // shuffles. We therefore use a lookup table instead, filled according to; // the instruction sequences that codegen currently generates.; // VecTy for interleave memop is <VF*Factor x Elt>.; // So, for VF=4, Interleave Factor = 3, Element type = i32 we have; // VecTy = <12 x i32>.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['load'],['load']
Performance,// Get length of sub-curve; // Push sums into cached array,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:46,cache,cached,46,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['cache'],['cached']
Performance,// Get load source type if scalarized.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp:7,load,load,7,interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/R600ISelLowering.cpp,1,['load'],['load']
Performance,// Get nearest IDom given a set of blocks.; // TODO: this can be optimized by starting the search at the node with the; // lowest level (highest in the tree).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp:65,optimiz,optimized,65,interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,1,['optimiz'],['optimized']
Performance,// Get opcode and regclass of the output for the given load instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp:55,load,load,55,interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FastISel.cpp,2,['load'],['load']
Performance,// Get or create a virtual register for each value.; // Unless the value is a Constant => loadimm cst?; // or inline constant each time?; // Creation of a virtual register needs to have a size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:90,load,loadimm,90,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,1,['load'],['loadimm']
Performance,"// Get plugin manager to load appropriate TProof from",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProofServ.cxx:25,load,load,25,proof/proof/src/TProofServ.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProofServ.cxx,1,['load'],['load']
Performance,"// Get plugin manager to load the appropriate TDataSetManager",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProofLite.cxx:25,load,load,25,proof/proof/src/TProofLite.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProofLite.cxx,2,['load'],['load']
Performance,"// Get promoted scalable vector VT, i.e. promote nxv4i1 -> nxv4i32.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,scalab,scalable,16,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// Get selector files from cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TProofPlayer.cxx:27,cache,cache,27,proof/proofplayer/src/TProofPlayer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TProofPlayer.cxx,1,['cache'],['cache']
Performance,// Get the AA tags and alignment to use from one of the loads. It does not; // matter which one we get and if any differ.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:56,load,loads,56,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// Get the NLPI for CacheKey, inserting one into the map if it doesn't; // already have one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:20,Cache,CacheKey,20,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['Cache'],['CacheKey']
Performance,"// Get the abbrevs, and preload record positions to make them lazy-loadable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp:67,load,loadable,67,interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,1,['load'],['loadable']
Performance,// Get the action to perform the promotion.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:21,perform,perform,21,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['perform'],['perform']
Performance,"// Get the address of the first item for the PrintDebug.; // (Performance is not essential here since we are going to print to; // the screen anyway).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TBufferFile.cxx:62,Perform,Performance,62,io/io/src/TBufferFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TBufferFile.cxx,2,['Perform'],['Performance']
Performance,"// Get the cache line size of a given cpu. This method switches over; // the given cpu and returns ""std::nullopt"" if the CPU is not found.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h:11,cache,cache,11,interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h,1,['cache'],['cache']
Performance,"// Get the cached objects",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/RooBarlowBeestonLL.cxx:11,cache,cached,11,roofit/histfactory/src/RooBarlowBeestonLL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/RooBarlowBeestonLL.cxx,1,['cache'],['cached']
Performance,// Get the call in its initial state. We use this as a template to perform; // all the checks.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCallAndReturn.cpp:67,perform,perform,67,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCallAndReturn.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCallAndReturn.cpp,1,['perform'],['perform']
Performance,// Get the correct pointer to load the variable argument; // Implement the ContBlock,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/Hexagon.cpp:30,load,load,30,interpreter/llvm-project/clang/lib/CodeGen/Targets/Hexagon.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/Hexagon.cpp,1,['load'],['load']
Performance,// Get the cost for gathered loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:29,load,loads,29,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['loads']
Performance,// Get the cost of all the memory operations.; // FIXME: discount dead loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:71,load,loads,71,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['load'],['loads']
Performance,"// Get the cost of inlining the called function at this call site. Note; // that this is only an estimate. The called function may eventually; // change in a way that leads to it not being inlined here, even though; // inlining looks profitable now. For example, one of its called; // functions may be inlined into it, making the called function too large; // to be inlined into this call site.; //; // We apply a boost for performing indirect call promotion by increasing; // the default threshold by the threshold for indirect calls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionSpecialization.cpp:424,perform,performing,424,interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionSpecialization.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionSpecialization.cpp,1,['perform'],['performing']
Performance,// Get the defining access for the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp,1,['load'],['load']
Performance,"// Get the dependency info for Pointer in BB. If we have cached; // information, we will use it, otherwise we compute it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:57,cache,cached,57,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['cache'],['cached']
Performance,// Get the distance between the first and second loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:49,load,loads,49,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['loads']
Performance,"// Get the file info. This will load info from the external source if; // necessary. Skip emitting this file if we have no information on it; // as a header file (in which case HFI will be null) or if it hasn't; // changed since it was loaded. Also skip it if it's for a modular header; // from a different module; in that case, we rely on the module(s); // containing the header to provide this information.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp:32,load,load,32,interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,2,['load'],"['load', 'loaded']"
Performance,"// Get the index type for this address space, results and intermediate; // computations are performed at that width.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp:92,perform,performed,92,interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,1,['perform'],['performed']
Performance,// Get the instruction that loads the function address from the GOT.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsOptimizePICCall.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Target/Mips/MipsOptimizePICCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsOptimizePICCall.cpp,1,['load'],['loads']
Performance,"// Get the itinerary's latency if possible, and handle variable_ops.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:23,latency,latency,23,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['latency'],['latency']
Performance,"// Get the last to spot the cache readings",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TVirtualPacketizer.cxx:28,cache,cache,28,proof/proof/src/TVirtualPacketizer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TVirtualPacketizer.cxx,1,['cache'],['cache']
Performance,// Get the load and store opcodes for a given register class.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h,1,['load'],['load']
Performance,// Get the load instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachinePipeliner.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/CodeGen/MachinePipeliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachinePipeliner.cpp,1,['load'],['load']
Performance,// Get the location associated with Load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp:36,Load,Load,36,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,1,['Load'],['Load']
Performance,"// Get the necessary information out of the call graph and nuke the; // function there. Also, clear out any cached analyses.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp:108,cache,cached,108,interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Inliner.cpp,1,['cache'],['cached']
Performance,// Get the object's function list from LoadedObjectMap,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/IntelJITEventListener.cpp:39,Load,LoadedObjectMap,39,interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/IntelJITEventListener.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/IntelJITEvents/IntelJITEventListener.cpp,1,['Load'],['LoadedObjectMap']
Performance,// Get the pointer somewhere into the stack slot from which we need to load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp:71,load,load,71,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,1,['load'],['load']
Performance,// Get the register will be loaded or stored.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/Disassembler/AVRDisassembler.cpp:28,load,loaded,28,interpreter/llvm-project/llvm/lib/Target/AVR/Disassembler/AVRDisassembler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/Disassembler/AVRDisassembler.cpp,1,['load'],['loaded']
Performance,"// Get the smaller of the legalized or original pow2-extended number of; // vector elements, which represents the number of unpacks we'll end up; // performing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:149,perform,performing,149,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['perform'],['performing']
Performance,"// Get the start location of the load. NumBytes is basically the; // offset from the stack pointer of previous function, which would be; // the caller in this case, as this function has variable argument; // list.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp,1,['load'],['load']
Performance,"// Get the total bin height for the ith bin (ROOT indexing convention); // in channel 'channel'; // (Could be optimized, it uses an intermediate histogram for now...); // Get the histogram, fetch the bin content, and return",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/HistFactoryNavigation.cxx:110,optimiz,optimized,110,roofit/histfactory/src/HistFactoryNavigation.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/HistFactoryNavigation.cxx,1,['optimiz'],['optimized']
Performance,"// Get the total bin height for the ith bin (ROOT indexing convention); // in channel 'channel'; // (This will be slow if you plan on looping over it.; // Could be optimized, it uses an intermediate histogram for now...); // Get the histogram, fetch the bin content, and return",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/HistFactoryNavigation.cxx:164,optimiz,optimized,164,roofit/histfactory/src/HistFactoryNavigation.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/histfactory/src/HistFactoryNavigation.cxx,1,['optimiz'],['optimized']
Performance,"// Get the total execution count of loops among blocks on the same line.; // Assuming a reducible flow graph, the count is the sum of back edge counts.; // Identifying loops is complex, so we simply find cycles and perform cycle; // cancelling iteratively.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/GCOV.cpp:215,perform,perform,215,interpreter/llvm-project/llvm/lib/ProfileData/GCOV.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/GCOV.cpp,1,['perform'],['perform']
Performance,// Get the unique string identifier for this dependence and cache the; // result.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp:60,cache,cache,60,interpreter/llvm-project/clang/lib/Driver/Driver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp,1,['cache'],['cache']
Performance,// Get the value for the 'tune-cpu' flag for a cpu_specific variant with the; // programmer-specified 'Name'.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h:26,tune,tune-cpu,26,interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/TargetInfo.h,1,['tune'],['tune-cpu']
Performance,"// Get the value in the widest-possible width. What is ""widest"" depends on; // whether the literal is a bit-precise integer or not. For a bit-precise; // integer type, try to scan the source to determine how many bits are; // needed to represent the value. This may seem a bit expensive, but trying; // to get the integer value from an overly-wide APInt is *extremely*; // expensive, so the naive approach of assuming; // llvm::IntegerType::MAX_INT_BITS is a big performance hit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp:463,perform,performance,463,interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,1,['perform'],['performance']
Performance,"// Get the vector type for the strided load, e.g. 4 x v4i8 -> v4i64",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,// Get two load or store instructions. Use the original instruction for; // one of them and create a clone for the other.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,1,['load'],['load']
Performance,"// GetMakeClass is left non-virtual for efficiency reason.; // Making it virtual affects the performance of the I/O",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProofChain.h:93,perform,performance,93,proof/proof/inc/TProofChain.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProofChain.h,2,['perform'],['performance']
Performance,"// Gets the two common ""VL"" operands: an all-ones mask and the vector length.; // VecVT is a vector type, either fixed-length or scalable, and ContainerVT is; // the vector type that the fixed-length vector is contained in. Otherwise if; // VecVT is scalable, then ContainerVT should be the same as VecVT.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:129,scalab,scalable,129,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,['scalab'],['scalable']
Performance,"// GitHub issue 10278: RooDataSet incorrectly loads RooCategory values from TTree branch of type Short_t",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/testRooCategory.cxx:46,load,loads,46,roofit/roofitcore/test/testRooCategory.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/test/testRooCategory.cxx,1,['load'],['loads']
Performance,// Give a Reserved color to every high latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h:39,latency,latency,39,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.h,1,['latency'],['latency']
Performance,// Give preference to a zero latency instruction if the dependent; // instruction is in the current packet.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/VLIWMachineScheduler.cpp:29,latency,latency,29,interpreter/llvm-project/llvm/lib/CodeGen/VLIWMachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/VLIWMachineScheduler.cpp,1,['latency'],['latency']
Performance,// Give store and loads same opcode so they value number together.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp:18,load,loads,18,interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,2,['load'],['loads']
Performance,"// Given a (possibly scalable) vector type, return the ElementCount",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/ValueTypes.h:21,scalab,scalable,21,interpreter/llvm-project/llvm/include/llvm/CodeGen/ValueTypes.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/ValueTypes.h,1,['scalab'],['scalable']
Performance,"// Given a Base Register, optimise the load/store uses to attempt to create more; // post-inc accesses and less register moves. We do this by taking zero offset; // loads/stores with an add, and convert them to a postinc load/store of the; // same type. Any subsequent accesses will be adjusted to use and account for; // the post-inc value.; // For example:; // LDR #0 LDR_POSTINC #16; // LDR #4 LDR #-12; // LDR #8 LDR #-8; // LDR #12 LDR #-4; // ADD #16; //; // At the same time if we do not find an increment but do find an existing; // pre/post inc instruction, we can still adjust the offsets of subsequent; // instructions to save the register move that would otherwise be needed for the; // in-place increment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,3,['load'],"['load', 'loads']"
Performance,"// Given a call to llvm.adjust.trampoline, find and return the corresponding; // call to llvm.init.trampoline if the call to the trampoline can be optimized; // to a direct call to a function. Otherwise return NULL.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp:147,optimiz,optimized,147,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,1,['optimiz'],['optimized']
Performance,"// Given a load or a store instruction, generate an appropriate unwinding SEH; // code on Windows.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FrameLowering.cpp,2,['load'],['load']
Performance,"// Given a scalable vector type and an index into it, returns the type for the; // smallest subvector that the index fits in. This can be used to reduce LMUL; // for operations like vslidedown.; //; // E.g. With Zvl128b, index 3 in a nxv4i32 fits within the first nxv2i32.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:11,scalab,scalable,11,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// Given a select instruction that was understood by analyzeSelect and; // returned Optimizable = true, attempt to optimize MI by merging it with one; // of its operands. Returns NULL on failure.; //; // When successful, returns the new select instruction. The client is; // responsible for deleting MI.; //; // If both sides of the select can be optimized, the TrueOp is modifed.; // PreferFalse is not used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.h:84,Optimiz,Optimizable,84,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.h,3,"['Optimiz', 'optimiz']","['Optimizable', 'optimize', 'optimized']"
Performance,"// Given an SMTExpr, adds/retrives it from the cache and returns; // an SMTExprRef to the SMTExpr in the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Z3Solver.cpp:47,cache,cache,47,interpreter/llvm-project/llvm/lib/Support/Z3Solver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Z3Solver.cpp,2,['cache'],['cache']
Performance,"// Given an SMTSort, adds/retrives it from the cache and returns; // an SMTSortRef to the SMTSort in the cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Z3Solver.cpp:47,cache,cache,47,interpreter/llvm-project/llvm/lib/Support/Z3Solver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/Z3Solver.cpp,2,['cache'],['cache']
Performance,"// Given an instruction Opcode, we can make benchmarks (measurements) of the; // instruction characteristics/performance. Then, to facilitate further analysis; // we group the benchmarks with *similar* characteristics into clusters.; // Now, this is all not entirely deterministic. Some instructions have variable; // characteristics, depending on their arguments. And thus, if we do several; // benchmarks of the same instruction Opcode, we may end up with *different*; // performance characteristics measurements. And when we then do clustering,; // these several benchmarks of the same instruction Opcode may end up being; // clustered into *different* clusters. This is not great for further analysis.; // We shall find every opcode with benchmarks not in just one cluster, and move; // *all* the benchmarks of said Opcode into one new unstable cluster per Opcode.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Clustering.cpp:109,perform,performance,109,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Clustering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Clustering.cpp,2,['perform'],['performance']
Performance,"// Given an iterator (Iter) that points at an instruction with a ""Then""; // predicate, tries to create the largest block of continuous predicated; // instructions possible, and returns the VPT Block Mask of that block.; //; // This will try to perform some minor optimization in order to maximize the; // size of the block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/MVEVPTBlockPass.cpp:244,perform,perform,244,interpreter/llvm-project/llvm/lib/Target/ARM/MVEVPTBlockPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/MVEVPTBlockPass.cpp,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"// Given an offset for a load/store, return the adjustment required to the base; // register such that the address can be accessed with a compressible offset.; // This will return 0 if the offset is already compressible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,1,['load'],['load']
Performance,"// Given an opcode for an instruction with a [Reg, #Imm] addressing mode, return; // the opcode of an instruction performing the same operation, but using the; // [Reg, #Imm] addressing mode with scaled offset.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:114,perform,performing,114,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['perform'],['performing']
Performance,"// Given an opcode for an instruction with a [Reg, #Imm] addressing mode, return; // the opcode of an instruction performing the same operation, but using the; // [Reg, #Imm] addressing mode with unscaled offset.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:114,perform,performing,114,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['perform'],['performing']
Performance,"// Given an opcode for an instruction with a [Reg, #Imm] addressing mode,; // return the opcode of an instruction performing the same operation, but using; // the [Reg, Reg] addressing mode.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:114,perform,performing,114,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['perform'],['performing']
Performance,"// Given that unswitching these terminators will require duplicating parts of; // the loop, so we need to be able to model that cost. Compute the ephemeral; // values and set up a data structure to hold per-BB costs. We cache each; // block's cost so that we don't recompute this when considering different; // subsets of the loop for duplication during unswitching.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SimpleLoopUnswitch.cpp:220,cache,cache,220,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SimpleLoopUnswitch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SimpleLoopUnswitch.cpp,1,['cache'],['cache']
Performance,"// Given the circ/brev load intrinsic and the already generated machine; // instruction, generate the appropriate store (that is a part of the; // intrinsic's functionality).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.h:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.h,1,['load'],['load']
Performance,"// Given the frontend rules for emitting AutoreleaseRV, RetainRV, and; // UnsafeClaimRV, it's probably safe to skip over even opaque function calls; // here since OptimizeInlinedAutoreleaseRVCall will confirm that they; // have the same RCIdentityRoot. However, what really matters is; // skipping instructions or intrinsics that the inliner could leave behind;; // be conservative for now and don't skip over opaque calls, which could; // potentially include other ARC calls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:163,Optimiz,OptimizeInlinedAutoreleaseRVCall,163,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['Optimiz'],['OptimizeInlinedAutoreleaseRVCall']
Performance,"// Given the opcode of a memory load/store instruction, return the opcode of an; // instruction performing the same operation, but using; // the [Reg, Reg, {s,u}xtw #N] addressing mode with sign-/zero-extend of the; // offset register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,2,"['load', 'perform']","['load', 'performing']"
Performance,"// Given: atomicrmw fadd ptr %addr, float %val ordering; //; // With this expansion we produce the following code:; // [...]; // br label %atomicrmw.check.shared; //; // atomicrmw.check.shared:; // %is.shared = call i1 @llvm.amdgcn.is.shared(ptr %addr); // br i1 %is.shared, label %atomicrmw.shared, label %atomicrmw.check.private; //; // atomicrmw.shared:; // %cast.shared = addrspacecast ptr %addr to ptr addrspace(3); // %loaded.shared = atomicrmw fadd ptr addrspace(3) %cast.shared,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.check.private:; // %is.private = call i1 @llvm.amdgcn.is.private(ptr %int8ptr); // br i1 %is.private, label %atomicrmw.private, label %atomicrmw.global; //; // atomicrmw.private:; // %cast.private = addrspacecast ptr %addr to ptr addrspace(5); // %loaded.private = load float, ptr addrspace(5) %cast.private; // %val.new = fadd float %loaded.private, %val; // store float %val.new, ptr addrspace(5) %cast.private; // br label %atomicrmw.phi; //; // atomicrmw.global:; // %cast.global = addrspacecast ptr %addr to ptr addrspace(1); // %loaded.global = atomicrmw fadd ptr addrspace(1) %cast.global,; // float %val ordering; // br label %atomicrmw.phi; //; // atomicrmw.phi:; // %loaded.phi = phi float [ %loaded.shared, %atomicrmw.shared ],; // [ %loaded.private, %atomicrmw.private ],; // [ %loaded.global, %atomicrmw.global ]; // br label %atomicrmw.end; //; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:425,load,loaded,425,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,9,['load'],"['load', 'loaded']"
Performance,"// Given: atomicrmw some_op iN* %addr, iN %incr ordering; //; // The standard expansion we produce is:; // [...]; // %init_loaded = load atomic iN* %addr; // br label %loop; // loop:; // %loaded = phi iN [ %init_loaded, %entry ], [ %new_loaded, %loop ]; // %new = some_op iN %loaded, %incr; // %pair = cmpxchg iN* %addr, iN %loaded, iN %new; // %new_loaded = extractvalue { iN, i1 } %pair, 0; // %success = extractvalue { iN, i1 } %pair, 1; // br i1 %success, label %atomicrmw.end, label %loop; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp:132,load,load,132,interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,4,['load'],"['load', 'loaded']"
Performance,"// Given: atomicrmw some_op iN* %addr, iN %incr ordering; //; // The standard expansion we produce is:; // [...]; // atomicrmw.start:; // %loaded = @load.linked(%addr); // %new = some_op iN %loaded, %incr; // %stored = @store_conditional(%new, %addr); // %try_again = icmp i32 ne %stored, 0; // br i1 %try_again, label %loop, label %atomicrmw.end; // atomicrmw.end:; // [...]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp:139,load,loaded,139,interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,3,['load'],"['load', 'loaded']"
Performance,"// Given: cmpxchg some_op iN* %addr, iN %desired, iN %new success_ord fail_ord; //; // The full expansion we produce is:; // [...]; // %aligned.addr = ...; // cmpxchg.start:; // %unreleasedload = @load.linked(%aligned.addr); // %unreleasedload.extract = extract value from %unreleasedload; // %should_store = icmp eq %unreleasedload.extract, %desired; // br i1 %should_store, label %cmpxchg.releasingstore,; // label %cmpxchg.nostore; // cmpxchg.releasingstore:; // fence?; // br label cmpxchg.trystore; // cmpxchg.trystore:; // %loaded.trystore = phi [%unreleasedload, %cmpxchg.releasingstore],; // [%releasedload, %cmpxchg.releasedload]; // %updated.new = insert %new into %loaded.trystore; // %stored = @store_conditional(%updated.new, %aligned.addr); // %success = icmp eq i32 %stored, 0; // br i1 %success, label %cmpxchg.success,; // label %cmpxchg.releasedload/%cmpxchg.failure; // cmpxchg.releasedload:; // %releasedload = @load.linked(%aligned.addr); // %releasedload.extract = extract value from %releasedload; // %should_store = icmp eq %releasedload.extract, %desired; // br i1 %should_store, label %cmpxchg.trystore,; // label %cmpxchg.failure; // cmpxchg.success:; // fence?; // br label %cmpxchg.end; // cmpxchg.nostore:; // %loaded.nostore = phi [%unreleasedload, %cmpxchg.start],; // [%releasedload,; // %cmpxchg.releasedload/%cmpxchg.trystore]; // @load_linked_fail_balance()?; // br label %cmpxchg.failure; // cmpxchg.failure:; // fence?; // br label %cmpxchg.end; // cmpxchg.end:; // %loaded.exit = phi [%loaded.nostore, %cmpxchg.failure],; // [%loaded.trystore, %cmpxchg.trystore]; // %success = phi i1 [true, %cmpxchg.success], [false, %cmpxchg.failure]; // %loaded = extract value from %loaded.exit; // %restmp = insertvalue { iN, i1 } undef, iN %loaded, 0; // %res = insertvalue { iN, i1 } %restmp, i1 %success, 1; // [...]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp:197,load,load,197,interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,11,['load'],"['load', 'loaded']"
Performance,"// Global logical predicate that acts on all lanes; // of the input and output mask concurrently. For; // example, it is implied by the `M` token in the; // Vector Function ABI mangled name.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/VFABIDemangler.h:84,concurren,concurrently,84,interpreter/llvm-project/llvm/include/llvm/IR/VFABIDemangler.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/VFABIDemangler.h,1,['concurren'],['concurrently']
Performance,// Go ahead and check any implicit conversions we might have skipped.; // The non-canonical typecheck is just an optimization;; // CheckImplicitConversion will filter out dead implicit conversions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp:113,optimiz,optimization,113,interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,1,['optimiz'],['optimization']
Performance,"// Go through all instructions in all blocks, add all calls to @llvm.assume; // to this cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/AssumptionCache.cpp:88,cache,cache,88,interpreter/llvm-project/llvm/lib/Analysis/AssumptionCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/AssumptionCache.cpp,1,['cache'],['cache']
Performance,// Go through all the basic blocks in the current loop and fix any streaming; // loads to avoid collisions with any other loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:81,load,loads,81,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,2,['load'],['loads']
Performance,"// Go through the list of headers that are required by the ModuleGenerator; // and check for each header if it's in one of the modules we loaded.; // If not, make sure we fail at the end and mark the header as missing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/rootcling_impl.cxx:138,load,loaded,138,core/dictgen/src/rootcling_impl.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/src/rootcling_impl.cxx,1,['load'],['loaded']
Performance,// Go through the loads and check that they're strided,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:18,load,loads,18,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['loads']
Performance,// Go to the last node with loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,1,['load'],['loads']
Performance,// Go up the chain of (vector) values to find a scalar load that we can; // combine with the broadcast.; // TODO: Combine this logic with findEltLoadSrc() used by; // EltsFromConsecutiveLoads().,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:55,load,load,55,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// Going from XForm to DForm loads means that the displacement needs to be; // not just an immediate but also a multiple of 4, or 16 depending on the; // load. A DForm load cannot be represented if it is a multiple of say 2.; // XForm loads do not have this restriction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:29,load,loads,29,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,4,['load'],"['load', 'loads']"
Performance,"// Going from the innermost to outermost loops, check if a loop has; // instructions preventing invariant load hoisting. If such instruction is; // found, mark this loop and its parent as non-hoistable and continue; // investigating the next loop.; // Visiting in a reversed pre-ordered DFS manner; // allows us to not process all the instructions of the outer loop if the; // inner loop is proved to be non-load-hoistable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:106,load,load,106,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,2,['load'],"['load', 'load-hoistable']"
Performance,"// Good enough; RequireCompleteType() will tell us if we; // need to auto parse.; // But we might need to auto-load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx:111,load,load,111,core/metacling/src/TClingCallbacks.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx,1,['load'],['load']
Performance,// Grab the most recent declaration to ensure we've loaded any lazy; // redeclarations of this template.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclTemplate.cpp:52,load,loaded,52,interpreter/llvm-project/clang/lib/AST/DeclTemplate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclTemplate.cpp,1,['load'],['loaded']
Performance,// Greedily read and cache the transition table.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/Automaton.h:21,cache,cache,21,interpreter/llvm-project/llvm/include/llvm/Support/Automaton.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Support/Automaton.h,1,['cache'],['cache']
Performance,"// Group page locators by their position in the object store; with caging enabled, this facilitates the; // processing of cages' requests together into a single IOV to be loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RPageStorageDaos.cxx:171,load,loaded,171,tree/ntuple/v7/src/RPageStorageDaos.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RPageStorageDaos.cxx,1,['load'],['loaded']
Performance,// Group scattered pseudo probes in a block to favor SelectionDAG. Scattered; // probes can be chained dependencies of other regular DAG nodes and block DAG; // combine optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:169,optimiz,optimizations,169,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimizations']
Performance,"// Group the chains into disjoint sets based on their liveness range. This is; // a poor-man's version of graph coloring. Ideally we'd create an interference; // graph and perform full-on graph coloring on that, but;; // (a) That's rather heavyweight for only two colors.; // (b) We expect multiple disjoint interference regions - in practice the live; // range of chains is quite small and they are clustered between loads; // and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp:172,perform,perform,172,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64A57FPLoadBalancing.cpp,2,"['load', 'perform']","['loads', 'perform']"
Performance,// Guard against pseudo-instructions like SI_CALL which are marked as; // SALU but with a very high latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp:100,latency,latency,100,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInsertDelayAlu.cpp,1,['latency'],['latency']
Performance,"// HACK: Some functions are not marked noreturn, and don't return.; // Here are a few hardwired ones. If this takes too long, we can; // potentially cache these results.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NoReturnFunctionChecker.cpp:149,cache,cache,149,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NoReturnFunctionChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NoReturnFunctionChecker.cpp,1,['cache'],['cache']
Performance,// HIP function pointer contains kernel handle when it is used in triple; // chevron. The kernel stub needs to be loaded from kernel handle and used; // as callee.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:114,load,loaded,114,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['loaded']
Performance,"// HVX loads are not predicable on v60, but are on v62.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp:7,load,loads,7,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonInstrInfo.cpp,1,['load'],['loads']
Performance,// Hack until load/store selection patterns support any tuple of legal types.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,// Handle 8 bit and 16 bit buffer loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:34,load,loads,34,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,2,['load'],['loads']
Performance,// Handle AVX masked loads which don't support passthru other than 0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:21,load,loads,21,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loads']
Performance,// Handle LOADX separately here. EXTLOAD case will fallthrough.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:10,LOAD,LOADX,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['LOAD'],['LOADX']
Performance,"// Handle OpenACC first, since 'AllowOpenACCArraySections' is only enabled; // when actively parsing a 'var' in a 'var-list' during clause/'cache'; // parsing, so it is the most specific, and best allows us to handle; // OpenACC and OpenMP at the same time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp:140,cache,cache,140,interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp,1,['cache'],['cache']
Performance,"// Handle Optimizer{Early,Last}EPCallbacks added by clang on PreLink. Actual; // optimization is going to be done in PostLink stage, but clang can't add; // callbacks there in case of in-process ThinLTO called by linker.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:10,Optimiz,Optimizer,10,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,2,"['Optimiz', 'optimiz']","['Optimizer', 'optimization']"
Performance,// Handle PIC mode first. SPARC needs a got load for every variable!,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,1,['load'],['load']
Performance,// Handle PIC mode first. VE needs a got load for every variable!,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,1,['load'],['load']
Performance,// Handle PPC32 integer and normal FP loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:38,load,loads,38,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,2,['load'],['loads']
Performance,"// Handle a special-case with a bit-hack instead of cmp+select:; // usubsat X, SMIN --> (X ^ SMIN) & (X s>> BW-1); // If the target can use VPTERNLOG, DAGToDAG will match this as; // ""vpsra + vpternlog"" which is better than ""vpmax + vpsub"" with a; // ""broadcast"" constant load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:272,load,load,272,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// Handle aggregate argument, namely RVV tuple types in segment load/store",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:64,load,load,64,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['load']
Performance,// Handle arrays of trivial type. We can represent this with a; // primitive load/copy from the base array region.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngine.cpp:77,load,load,77,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngine.cpp,1,['load'],['load']
Performance,// Handle base mnemonic for atomic loads where the EH bit is zero.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/PPCAsmParser.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/PPCAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/AsmParser/PPCAsmParser.cpp,1,['load'],['loads']
Performance,// Handle bit-reverse load intrinsics.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// Handle broadcasting a single constant scalar from the constant pool; // into a vector.; // On Sandybridge (no AVX2), it is still better to load a constant vector; // from the constant pool and not to broadcast it from a scalar.; // But override that restriction when optimizing for size.; // TODO: Check if splatting is recommended for other AVX-capable CPUs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:142,load,load,142,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,"['load', 'optimiz']","['load', 'optimizing']"
Performance,"// Handle cases like filling use of:; //; // %0:sub_32<def,read-undef> = COPY %1; GPR64:%0, GPR32:%1; //; // where we can load the full virtual reg source stack slot, into the subreg; // destination, in this case producing:; //; // LDRWui %0:sub_32<def,read-undef>, %stack.0; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:122,load,load,122,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['load'],['load']
Performance,"// Handle cases where there are other cache slices; // Iterator over available slice positions and fill each; // Determine number of bins for each slice position observable",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFFTConvPdf.cxx:38,cache,cache,38,roofit/roofitcore/src/RooFFTConvPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFFTConvPdf.cxx,1,['cache'],['cache']
Performance,// Handle conversion from scalable to fixed when msve-vector-bits is; // specified,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp:26,scalab,scalable,26,interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,1,['scalab'],['scalable']
Performance,// Handle conversion of loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp:24,load,loads,24,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,1,['load'],['loads']
Performance,"// Handle disabling of LLVM optimization, where we want to preserve the; // internal module before any optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/BackendPasses.cpp:28,optimiz,optimization,28,interpreter/cling/lib/Interpreter/BackendPasses.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/BackendPasses.cpp,2,['optimiz'],['optimization']
Performance,// Handle endianness of the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,4,['load'],['load']
Performance,"// Handle immediates in the range [-4096,-2049] or [2048, 4094]. We can use; // an ADDI for part of the offset and fold the rest into the load/store.; // This mirrors the AddiPair PatFrag in RISCVInstrInfo.td.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp:138,load,load,138,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,1,['load'],['load']
Performance,// Handle indexed loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:18,load,loads,18,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['loads']
Performance,// Handle it in the default way if this is an indexed load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:54,load,load,54,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['load'],['load']
Performance,"// Handle load instructions. If the operand is a constant pointer to a constant; // global, we can replace the load with the loaded constant value!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp,3,['load'],"['load', 'loaded']"
Performance,// Handle load-and-splat patterns as we have instructions that will do this; // in one go.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:10,load,load-and-splat,10,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load-and-splat']
Performance,// Handle load.*_pci case which has 6 operands.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// Handle load.*_pcr case which has 5 operands.; // Operands: { Base, Modifier, Start, Chain }.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['load']
Performance,// Handle loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:10,load,loads,10,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['load'],['loads']
Performance,// Handle lowering 256 bit non temporal loads into LDNP for little-endian; // targets.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loads']
Performance,// Handle needing to s.buffer.load() a p8 value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,"// Handle non-optimized IR code like:; // %expval = call i64 @llvm.expect.i64(i64 %conv1, i64 1); // %tobool = icmp ne i64 %expval, 0; // br i1 %tobool, label %if.then, label %if.end; //; // Or the following simpler case:; // %expval = call i1 @llvm.expect.i1(i1 %cmp, i1 1); // br i1 %expval, label %if.then, label %if.end",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerExpectIntrinsic.cpp:14,optimiz,optimized,14,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerExpectIntrinsic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerExpectIntrinsic.cpp,1,['optimiz'],['optimized']
Performance,// Handle only loads/stores with base register followed by immediate offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:15,load,loads,15,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,2,['load'],['loads']
Performance,// Handle only loads/stores with base register followed by immediate offset; // and with add as ALU op.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.cpp:15,load,loads,15,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.cpp,1,['load'],['loads']
Performance,"// Handle partial integrals here; // Retrieve |S22|, S22bar from cache",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMultiVarGaussian.cxx:65,cache,cache,65,roofit/roofitcore/src/RooMultiVarGaussian.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMultiVarGaussian.cxx,1,['cache'],['cache']
Performance,// Handle patterns using circ/brev load intrinsics.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['load']
Performance,// Handle preincrement loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,1,['load'],['loads']
Performance,// Handle scalable splat,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp:10,scalab,scalable,10,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,1,['scalab'],['scalable']
Performance,// Handle scalable vectors (and fixed vectors legalized to scalable vectors).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:10,scalab,scalable,10,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,2,['scalab'],['scalable']
Performance,// Handle scalar UndefValue and scalable vector UndefValue. Fixed-length; // vectors are always evaluated per element.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp:32,scalab,scalable,32,interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,2,['scalab'],['scalable']
Performance,// Handle serialized remarks options: '-fsave-optimization-record'; // and '-foptimization-record-*'.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/CommonArgs.cpp:46,optimiz,optimization-record,46,interpreter/llvm-project/clang/lib/Driver/ToolChains/CommonArgs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/CommonArgs.cpp,1,['optimiz'],['optimization-record']
Performance,"// Handle spill/fill of synthetic register classes for segment operations to; // ensure correctness in the edge case one gets spilled. There are many; // possible optimizations here, but given the extreme rarity of such spills,; // we prefer simplicity of implementation for now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp:163,optimiz,optimizations,163,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVRegisterInfo.cpp,1,['optimiz'],['optimizations']
Performance,"// Handle substitutions (MacOS):; // @rpath - This function does not substitute @rpath, becouse; // this variable is already handled by lookupLibrary where; // @rpath is replaced with all paths from RPATH one by one.; // @executable_path - Main program path.; // @loader_path - Loader library (or main program) path.; //; // Handle substitutions (Linux):; // https://man7.org/linux/man-pages/man8/ld.so.8.html; // $origin - Loader library (or main program) path.; // $lib - lib lib64; // $platform - x86_64 AT_PLATFORM",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManager.cpp:278,Load,Loader,278,interpreter/cling/lib/Interpreter/DynamicLibraryManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManager.cpp,2,['Load'],['Loader']
Performance,"// Handle the case where a load has a vector type, but scalar memory; // with an attached range.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['load'],['load']
Performance,// Handle the case where the truncated result is at least as wide as the; // loaded type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:77,load,loaded,77,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loaded']
Performance,// Handle the cases for vector.reverse with scalable vectors,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:44,scalab,scalable,44,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['scalab'],['scalable']
Performance,// Handle the conversion of bit-reverse load intrinsics to bit code.; // The intrinsic call after this function only reads from memory and the; // write to memory is dealt by the store instruction.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:40,load,load,40,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['load']
Performance,// Handle the rare case of folding multiple loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp:44,load,loads,44,interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,1,['load'],['loads']
Performance,// Handle the remaining sequence of branches. This function will update; // the work queue.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/BitTracker.cpp:85,queue,queue,85,interpreter/llvm-project/llvm/lib/Target/Hexagon/BitTracker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/BitTracker.cpp,1,['queue'],['queue']
Performance,// Handle the simple case where the value is contained in one uint64_t.; // It is wrong to optimize getWord(0) to VAL; there might be more than one word.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APInt.cpp:91,optimiz,optimize,91,interpreter/llvm-project/llvm/lib/Support/APInt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APInt.cpp,1,['optimiz'],['optimize']
Performance,"// Handle the special case of fp16 extloads. EXTLOAD doesn't have the; // normal undefined upper bits behavior to allow using an in-reg extend; // with the illegal FP type, so load as an integer and do the; // from-integer conversion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:176,load,load,176,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,// Handle the very basic case when the two stores are in the same block; // so deciding which one forwards is easy. The later one forwards as; // long as they both have a dependence distance of one to the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp:205,load,load,205,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,1,['load'],['load']
Performance,"// Handle various floating point optimization flags, mapping them to the; // appropriate LLVM code generation flags. This is complicated by several; // ""umbrella"" flags, so we do this by stepping through the flags incrementally; // adjusting what we think is enabled/disabled, then at the end setting the; // LLVM flags based on the final state.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp:33,optimiz,optimization,33,interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,1,['optimiz'],['optimization']
Performance,// Handle vectors of size 3 like size 4 for better performance.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:51,perform,performance,51,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['perform'],['performance']
Performance,"// Handling of indexed loads/stores: default is ""expand"".; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['load'],['loads']
Performance,"// Handling the overflow by a multiplication with 0 or 1 is cheaper; // than branching with an if statement, which the compiler does not; // optimize to this equivalent code. Note that we could do entirely; // without this overflow handling when summing up the intermediate; // products differently as described in the following SO answer:; // https://stackoverflow.com/a/51587262; // However, this approach takes at least the same amount of thinking; // why a) the code gives the same results without b) overflowing due; // to the mixture of 32 bit arithmetic. Moreover, my tests show that; // the scheme implemented here is actually slightly more performant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/math/mathcore/src/ranluxpp/mulmod.h:141,optimiz,optimize,141,math/mathcore/src/ranluxpp/mulmod.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/math/mathcore/src/ranluxpp/mulmod.h,2,"['optimiz', 'perform']","['optimize', 'performant']"
Performance,// Has something earlier tagged that the return type needs adjusting; // This happens if the instruction is a load or has set TexFailCtrl flags,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:110,load,load,110,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['load'],['load']
Performance,"// Hash the module dependencies. These paths may differ even if the invocation; // is identical if they depend on the contents of the files in the TU -- for; // example, case-insensitive paths to modulemap files. Usually such a case; // would indicate a missed optimization to canonicalize, but it may be; // difficult to canonicalize all cases when there is a VFS.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp:261,optimiz,optimization,261,interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp,1,['optimiz'],['optimization']
Performance,"// Have a work queue of defs whose reached uses need to be found.; // For each def, add to the queue all reached (non-phi) defs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RDFLiveness.cpp:15,queue,queue,15,interpreter/llvm-project/llvm/lib/CodeGen/RDFLiveness.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RDFLiveness.cpp,2,['queue'],['queue']
Performance,// Haven't figured out this child pad yet; queue it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:43,queue,queue,43,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['queue'],['queue']
Performance,// Haven't resolved this child yet; queue it and keep searching.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:36,queue,queue,36,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['queue'],['queue']
Performance,"// Having only one phase of training to reduce the test time.; // TString training1(""LearningRate=1e-2,Optimizer="" + optimizerStr +; // "",Momentum=0.9,Repetitions=1,""; // ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,""; // ""WeightDecay=1e-4,Regularization=L2,""; // ""DropConfig=0.0+0.0+0.0+0.0,Multithreading=True"");; // TString training2(""LearningRate=1e-3,Optimizer="" + optimizerStr +; // "",Momentum=0.9,Repetitions=1,""; // ""ConvergenceSteps=20,BatchSize=256,TestRepetitions=10,""; // ""WeightDecay=1e-4,Regularization=L2,""; // ""DropConfig=0.0+0.0+0.0+0.0,Multithreading=True"");",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/TestMethodDLOptimization.h:103,Optimiz,Optimizer,103,tmva/tmva/test/DNN/TestMethodDLOptimization.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/TestMethodDLOptimization.h,4,"['Optimiz', 'optimiz']","['Optimizer', 'optimizerStr']"
Performance,"// Helper classes/structs used for priority queue - BVH traversal; // structure keeping cost (value) for a BVH index",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/geom/geom/src/TGeoParallelWorld.cxx:44,queue,queue,44,geom/geom/src/TGeoParallelWorld.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/geom/geom/src/TGeoParallelWorld.cxx,1,['queue'],['queue']
Performance,// Helper function for performMemPairCombine.; // Try to combine the memory loads/stores LSNode1 and LSNode2; // into a single memory pair operation.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:23,perform,performMemPairCombine,23,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,"['load', 'perform']","['loads', 'performMemPairCombine']"
Performance,"// Helper function of OptimizeXor(). It tries to simplify ""Opnd1 ^ ConstOpnd""; // into ""R ^ C"", where C would be 0, and R is a symbolic value.; //; // If it was successful, true is returned, and the ""R"" and ""C"" is returned; // via ""Res"" and ""ConstOpnd"", respectively; otherwise, false is returned,; // and both ""Res"" and ""ConstOpnd"" remain unchanged.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp:22,Optimiz,OptimizeXor,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,1,['Optimiz'],['OptimizeXor']
Performance,"// Helper function of OptimizeXor(). It tries to simplify; // ""Opnd1 ^ Opnd2 ^ ConstOpnd"" into ""R ^ C"", where C would be 0, and R is a; // symbolic value.; //; // If it was successful, true is returned, and the ""R"" and ""C"" is returned; // via ""Res"" and ""ConstOpnd"", respectively (If the entire expression is; // evaluated to a constant, the Res is set to NULL); otherwise, false is; // returned, and both ""Res"" and ""ConstOpnd"" remain unchanged.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp:22,Optimiz,OptimizeXor,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,1,['Optimiz'],['OptimizeXor']
Performance,// Helper function responsible for increasing the latency only.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.h:50,latency,latency,50,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.h,1,['latency'],['latency']
Performance,"// Helper function to add optimization passes to the TargetMachine at the ; // specified optimization level, OptLevel",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp:26,optimiz,optimization,26,interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,2,['optimiz'],['optimization']
Performance,// Helper function to adjust P and Q bits on load and store instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/MCTargetDesc/LanaiMCCodeEmitter.cpp:45,load,load,45,interpreter/llvm-project/llvm/lib/Target/Lanai/MCTargetDesc/LanaiMCCodeEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/MCTargetDesc/LanaiMCCodeEmitter.cpp,1,['load'],['load']
Performance,// Helper function to get an APInt from an expression. Supports expressions; // which are an IntegerLiteral or a UnaryOperator and returns the value with; // all operations performed on it.; // FIXME: it would be good to unify this function with; // IsIntegerLiteralConstantExpr at some point given the similarity between the; // functions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CFG.cpp:173,perform,performed,173,interpreter/llvm-project/clang/lib/Analysis/CFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Analysis/CFG.cpp,1,['perform'],['performed']
Performance,// Helper function to parse command line args and find the optimization level,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp:59,optimiz,optimization,59,interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/clang-fuzzer/handle-llvm/handle_llvm.cpp,1,['optimiz'],['optimization']
Performance,// Helper function to splice Cur out of the given queue and add it; // to the layout at the given offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp:50,queue,queue,50,interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,1,['queue'],['queue']
Performance,// Helper function to try to find a field in the given queue that'll; // fit starting at StartOffset but before EndOffset (if present).; // Note that this never fails if EndOffset is not provided.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp:55,queue,queue,55,interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,1,['queue'],['queue']
Performance,// Helper functions for ComplexPattern used on LanaiInstrInfo; // Used on Lanai Load/Store instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiISelDAGToDAG.cpp:80,Load,Load,80,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiISelDAGToDAG.cpp,1,['Load'],['Load']
Performance,// Helper functions of redundant load elimination,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Scalar/GVN.h:33,load,load,33,interpreter/llvm-project/llvm/include/llvm/Transforms/Scalar/GVN.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Scalar/GVN.h,1,['load'],['load']
Performance,// Helper lambda to apply additional subregister substitutions to a known; // instruction/operand pair. Adds new (fake) substitutions so that we can; // record the subregister. FIXME: this isn't very space efficient if multiple; // values are tracked back through the same copies; cache something later.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineFunction.cpp:281,cache,cache,281,interpreter/llvm-project/llvm/lib/CodeGen/MachineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineFunction.cpp,1,['cache'],['cache']
Performance,"// Helper struct to hold a cache; // that can accelerate calculation of the RealIndex.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/inc/TTreeFormula.h:27,cache,cache,27,tree/treeplayer/inc/TTreeFormula.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/inc/TTreeFormula.h,1,['cache'],['cache']
Performance,"// Helper to actually emit an instruction to the MCStreamer. Also, when; // possible, compression of the instruction is performed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CSKY/AsmParser/CSKYAsmParser.cpp:120,perform,performed,120,interpreter/llvm-project/llvm/lib/Target/CSKY/AsmParser/CSKYAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/CSKY/AsmParser/CSKYAsmParser.cpp,2,['perform'],['performed']
Performance,// Helper to emit pseudo load/store instruction with a symbol.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/AsmParser/RISCVAsmParser.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/RISCV/AsmParser/RISCVAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/AsmParser/RISCVAsmParser.cpp,1,['load'],['load']
Performance,// Helper to load an API list to preserve from file and expose it as a functor; // for internalization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Internalize.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Transforms/IPO/Internalize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Internalize.cpp,1,['load'],['load']
Performance,// Helper to look for a normal load that can be narrowed into a vzload with the; // specified VT and memory VT. Returns SDValue() on failure.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,// Helper to optimize stpncpy and strncpy.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/SimplifyLibCalls.h:13,optimiz,optimize,13,interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/SimplifyLibCalls.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/SimplifyLibCalls.h,1,['optimiz'],['optimize']
Performance,// Helper to perform binary op on vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:13,perform,perform,13,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,1,['perform'],['perform']
Performance,// Helper to perform unary op on vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:13,perform,perform,13,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,1,['perform'],['perform']
Performance,// Helper to recursively read the module names for all modules we're adding.; // We mark these as known and redirect any attempt to load that module to; // the files we were handed.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:132,load,load,132,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,1,['load'],['load']
Performance,"// Here comes stuff that we only do once the entire chain is loaded. Do *not*; // remove modules from this point. Various fields are updated during reading; // the AST block and removing the modules would result in dangling pointers.; // They are generally only incidentally dereferenced, ie. a binary search; // runs over `GlobalSLocEntryMap`, which could cause an invalid module to; // be dereferenced but it wouldn't actually be used.; // Load the AST blocks of all of the modules that we loaded. We can still; // hit errors parsing the ASTs at this point.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:61,load,loaded,61,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,3,"['Load', 'load']","['Load', 'loaded']"
Performance,"// Here comes the tricky thing: if we need to mangle something like; // void foo(A::X<Y>, B::X<Y>),; // the X<Y> part is aliased. However, if you need to mangle; // void foo(A::X<A::Y>, A::X<B::Y>),; // the A::X<> part is not aliased.; // That is, from the mangler's perspective we have a structure like this:; // namespace[s] -> type[ -> template-parameters]; // but from the Clang perspective we have; // type [ -> template-parameters]; // \-> namespace[s]; // What we do is we create a new mangler, mangle the same type (without; // a namespace suffix) to a string using the extra mangler and then use; // the mangled type name as a key to check the mangling of different types; // for aliasing.; // It's important to key cache reads off ND, not TD -- the same TD can; // be used with different TemplateArgs, but ND uniquely identifies; // TD / TemplateArg pairs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/MicrosoftMangle.cpp:725,cache,cache,725,interpreter/llvm-project/clang/lib/AST/MicrosoftMangle.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/MicrosoftMangle.cpp,1,['cache'],['cache']
Performance,"// Here is a common situation. We want to optimize:; //; // %a = ...; // %b = and i32 %a, 2; // %c = srl i32 %b, 1; // brcond i32 %c ...; //; // into; //; // %a = ...; // %b = and %a, 2; // %c = setcc eq %b, 0; // brcond %c ...; //; // However when after the source operand of SRL is optimized into AND, the SRL; // itself may not be optimized further. Look for it and add the BRCOND into; // the worklist.; //; // The also tends to happen for binary operations when SimplifyDemandedBits; // is involved.; //; // FIXME: This is unecessary if we process the DAG in topological order,; // which we plan to do. This workaround can be removed once the DAG is; // processed in topological order.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:42,optimiz,optimize,42,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['optimiz'],"['optimize', 'optimized']"
Performance,"// Here we ""fuse"" clusters together if the number of clusters is too big with respect to; // the number of slots, otherwise we can incur in an overhead which is big enough; // to make parallelisation detrimental to performance.; // For example, this is the case when, following a merging of many small files, a file; // contains a tree with many entries and with clusters of just a few entries each.; // Another problematic case is a high number of slots (e.g. 256) coupled with a high number; // of files (e.g. 1000 files): the large amount of files might result in a large amount; // of tasks, but the elevated concurrency level makes the little synchronization required by; // task initialization very expensive. In this case it's better to simply process fewer, larger tasks.; // Cluster-merging can help reduce the number of tasks down to a minumum of one task per file.; //; // The criterion according to which we fuse clusters together is to have around; // TTreeProcessorMT::GetTasksPerWorkerHint() clusters per slot.; // Concretely, for each file we will cap the number of tasks to ceil(GetTasksPerWorkerHint() * nWorkers / nFiles).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeProcessorMT.cxx:215,perform,performance,215,tree/treeplayer/src/TTreeProcessorMT.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeProcessorMT.cxx,2,"['concurren', 'perform']","['concurrency', 'performance']"
Performance,"// Here we assume the standard RISC-V ISA, which uses a base+offset; // addressing mode. You'll need to relax these conditions to support custom; // load/store instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp:149,load,load,149,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVInstrInfo.cpp,1,['load'],['load']
Performance,"// Here we could have an FP stack truncation or an FPStack <-> SSE convert.; // FPStack has extload and truncstore. SSE can fold direct loads into other; // operations. Based on this, decide what we want to do.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:136,load,loads,136,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,2,['load'],['loads']
Performance,// Here we differentiate two cases: (1) when Ptrs represent a regular; // vectorization tree node (as they are pointer arguments of scattered; // loads) or (2) when Ptrs are the arguments of loads or stores being; // vectorized as plane wide unit-stride load/store since all the; // loads/stores are known to be from/to adjacent locations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:146,load,loads,146,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,4,['load'],"['load', 'loads']"
Performance,"// Here we just try to handle vector loads/stores where our value type might; // have pointer elements, which the SelectionDAG importer can't handle. To; // allow the existing patterns for s64 to fire for p0, we just try to bitcast; // the value to use s64 types.; // Custom legalization requires the instruction, if not deleted, must be fully; // legalized. In order to allow further legalization of the inst, we create; // a new instruction and erase the existing one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp:37,load,loads,37,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64LegalizerInfo.cpp,1,['load'],['loads']
Performance,"// Here we need to check whether this TVirtualStreamerInfo (which presumably has been; // loaded from a file) is consistent with the definition in the library we just loaded.; // BuildCheck is not appropriate here since it check a streamerinfo against the; // 'current streamerinfo' which, at time point, would be the same as 'info'!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:90,load,loaded,90,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,2,['load'],['loaded']
Performance,"// Here we perform these steps:; // Compile macro which has a linkdef section with extra includes; // The extra includes get parsed. They contain a function, f; // The value returned by f is checked to be correct ",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/test/dictgen_base.cxx:11,perform,perform,11,core/dictgen/test/dictgen_base.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/dictgen/test/dictgen_base.cxx,1,['perform'],['perform']
Performance,"// Here we use the same DebugLoc as the scalar loop latch terminator instead; // of the corresponding compare because they may have ended up with; // different line numbers and we want to avoid awkward line stepping while; // debugging. Eg. if the compare has got a line number inside the loop.; // TODO: At the moment, CreateICmpEQ will simplify conditions with constant; // operands. Perform simplification directly on VPlan once the branch is; // modeled there.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:386,Perform,Perform,386,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['Perform'],['Perform']
Performance,"// Here we want to handle splat load for type v16i8 and v8i16 when there is; // no direct move, we don't need to use stack for this case. If target has; // direct move, we should be able to get the best selection in the .td file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,1,['load'],['load']
Performance,// Here we're relying on the SCEV Expander's cache to only emit code for the; // same bounds once.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp:45,cache,cache,45,interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LoopUtils.cpp,1,['cache'],['cache']
Performance,"// Here, we have a memory allocation that should be avoided when this; // code needs to be optimized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/TestStatistics/RooUnbinnedL.cxx:91,optimiz,optimized,91,roofit/roofitcore/src/TestStatistics/RooUnbinnedL.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/TestStatistics/RooUnbinnedL.cxx,1,['optimiz'],['optimized']
Performance,"// Here, we have a pattern like:; //; // (sra (shl val, NN), MM); // or; // (srl (shl val, NN), MM); //; // If MM >= NN, we can efficiently optimize this with bfe",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp:140,optimiz,optimize,140,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,1,['optimiz'],['optimize']
Performance,"// Heuristics for workaround performance problems:; // (H1) If RPATH and RUNPATH == """" -> skip handling Deps",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp:29,perform,performance,29,interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp,1,['perform'],['performance']
Performance,// Hexagon needs to optimize cases with negative constants.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:20,optimiz,optimize,20,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['optimiz'],['optimize']
Performance,// High latency instructions: already given.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:8,latency,latency,8,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,1,['latency'],['latency']
Performance,"// HighLatency is the expected latency of ""very high latency"" operations.; // See TargetInstrInfo::isHighLatencyDef().; // By default, this is set to an arbitrarily high number of cycles; // likely to have some impact on scheduling heuristics.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/MC/MCSchedule.h:31,latency,latency,31,interpreter/llvm-project/llvm/include/llvm/MC/MCSchedule.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/MC/MCSchedule.h,2,['latency'],['latency']
Performance,"// Hints for optimized brute-force sampling",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooBinSamplingPdf.h:13,optimiz,optimized,13,roofit/roofitcore/inc/RooBinSamplingPdf.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooBinSamplingPdf.h,1,['optimiz'],['optimized']
Performance,"// Historically, PIC code for MIPS was associated with -mabicalls, a.k.a; // SVR4 abicalls. Static code does not use SVR4 calling sequences. An ABI; // extension was developed by Richard Sandiford & Code Sourcery to support; // static code calling PIC code (CPIC). For O32 and N32 this means we have; // several combinations of PIC/static and abicalls. Pure static, static; // with the CPIC extension, and pure PIC code.; // At final link time, O32 and N32 with CPIC will have another section; // added to the binary which contains the stub functions to perform; // any fixups required for PIC code.; // For N64, the situation is more regular: code can either be static; // (non-abicalls) or PIC (abicalls). GCC has traditionally picked PIC code; // code for N64. Since Clang has already built the relocation model portion; // of the commandline, we pick add +noabicalls feature in the N64 static; // case.; // The is another case to be accounted for: -msym32, which enforces that all; // symbols have 32 bits in size. In this case, N64 can in theory use CPIC; // but it is unsupported.; // The combinations for N64 are:; // a) Static without abicalls and 64bit symbols.; // b) Static with abicalls and 32bit symbols.; // c) PIC with abicalls and 64bit symbols.; // For case (a) we need to add +noabicalls for N64.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Arch/Mips.cpp:554,perform,perform,554,interpreter/llvm-project/clang/lib/Driver/ToolChains/Arch/Mips.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Arch/Mips.cpp,1,['perform'],['perform']
Performance,// Hoist VFP / NEON instructions with 4 or higher latency.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:50,latency,latency,50,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['latency'],['latency']
Performance,// Hoist equivalent loads and sink stores; // outside diamonds when possible,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergedLoadStoreMotion.cpp:20,load,loads,20,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergedLoadStoreMotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergedLoadStoreMotion.cpp,1,['load'],['loads']
Performance,// Hoisting of scalars and load expressions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,2,['load'],['load']
Performance,// Hold off inserting this value into the Cache in case we have to return; // false and come back later.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp:42,Cache,Cache,42,interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp,1,['Cache'],['Cache']
Performance,// Holds all interleaved load groups temporarily.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,1,['load'],['load']
Performance,// Holds sub-vectors extracted from the load intrinsic return values. The; // sub-vectors are associated with the shufflevector instructions they will; // replace.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['load']
Performance,// Holds the Load and Store instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:13,Load,Load,13,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,1,['Load'],['Load']
Performance,"// Honor -mllvm.; //; // FIXME: Remove this, one day.; // This should happen AFTER plugins have been loaded!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp:101,load,loaded,101,interpreter/llvm-project/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp,1,['load'],['loaded']
Performance,"// Honor set of `-mllvm` options. This should happen AFTER plugins have been; // loaded!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/Interpreter.cpp:81,load,loaded,81,interpreter/cling/lib/Interpreter/Interpreter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/Interpreter.cpp,1,['load'],['loaded']
Performance,"// Hook to the constructor. This is needed to avoid using the plugin manager; // which may create problems in multi-threaded environments.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProofServ.cxx:110,multi-thread,multi-threaded,110,proof/proof/src/TProofServ.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProofServ.cxx,2,['multi-thread'],['multi-threaded']
Performance,"// How many times to perform a test",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/test/tcollbm.cxx:21,perform,perform,21,test/tcollbm.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/test/tcollbm.cxx,1,['perform'],['perform']
Performance,// How much memory is being used by SourceManager's content cache?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp:60,cache,cache,60,interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,1,['cache'],['cache']
Performance,"// However we loaded the old byte, either by plain load or atomicrmw, shift; // the bit into the low position and mask it to 0 or 1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:14,load,loaded,14,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,2,['load'],"['load', 'loaded']"
Performance,"// However, if the mask contains undef's, we have to enumerate possible tuples; // and pick one. There are bounds on replication factor: [1, mask size]; // (where RF=1 is an identity shuffle, RF=mask size is a broadcast shuffle); // Additionally, mask size is a replication factor multiplied by vector size,; // which further significantly reduces the search space.; // Before doing that, let's perform basic correctness checking first.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Instructions.cpp:395,perform,perform,395,interpreter/llvm-project/llvm/lib/IR/Instructions.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Instructions.cpp,1,['perform'],['perform']
Performance,"// However, load and store *are* legal.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,1,['load'],['load']
Performance,"// However, loading a single sealed page should work",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_storage_daos.cxx:12,load,loading,12,tree/ntuple/v7/test/ntuple_storage_daos.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/test/ntuple_storage_daos.cxx,1,['load'],['loading']
Performance,// I has only one memory operand which is load from constant pool.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,"// I is an operand of U. Check if U is an arithmetic (binary) operation; // usable in a memop, where the other operand is a loaded value, and the; // result of U is stored in the same location.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp:124,load,loaded,124,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelDAGToDAG.cpp,1,['load'],['loaded']
Performance,// ID to use for local font cache (for single equation processing),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:28,cache,cache,28,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['cache'],['cache']
Performance,"// IMAGE_REL_BRANCH20T, IMAGE_REL_ARM_BRANCH24T, IMAGE_REL_ARM_BLX23T all; // perform a 4 byte adjustment to the relocation. Relative branches are; // offset by 4 on ARM, however, because there is no RELA relocations, all; // branches are offset by 4.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/WinCOFFObjectWriter.cpp:78,perform,perform,78,interpreter/llvm-project/llvm/lib/MC/WinCOFFObjectWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/WinCOFFObjectWriter.cpp,1,['perform'],['perform']
Performance,"// INCLUDED BELOW #include ""qpsrt2.c""; /* The smallest interval has the largest error. Before bisecting; decrease the sum of the errors over the larger intervals; (error_over_large_intervals) and perform extrapolation. */",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitmore/src/RooAdaptiveGaussKronrodIntegrator1D.cxx:196,perform,perform,196,roofit/roofitmore/src/RooAdaptiveGaussKronrodIntegrator1D.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitmore/src/RooAdaptiveGaussKronrodIntegrator1D.cxx,1,['perform'],['perform']
Performance,"// INSERT_VECTOR_ELT into out-of-bounds element is an UNDEF, except; // for scalable vectors where we will generate appropriate code to; // deal with out-of-bounds cases correctly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:76,scalab,scalable,76,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['scalab'],['scalable']
Performance,"// ISD::FSHL and ISD::FSHR have defined overflow behavior but ISD::SHL and; // ISD::SRA/L nodes haven't. Insert an AND to be safe, it's usually optimized; // away during isel.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:144,optimiz,optimized,144,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['optimiz'],['optimized']
Performance,"// ISel pattern matching also adds a load memory operand of the same; // address, so take special care to find the storing memory operand.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,"// ISel process runs DAGCombiner after legalization; this step is called; // SelectionDAG optimization phase. This post-legalization combining process; // runs DAGCombiner on each node, and if there was a change to be made,; // re-runs legalization again on it and its user nodes to make sure; // everythiing is in a legalized state.; //; // The legalization calls lowering routines, and we do our custom lowering for; // build_vectors (LowerBUILD_VECTOR), which converts undef vector elements; // into zeros. But there is a set of routines in DAGCombiner that turns unused; // (= not demanded) nodes into undef, among which SimplifyDemandedVectorElts; // turns unused vector elements into undefs. But this routine does not work; // with our custom LowerBUILD_VECTOR, which turns undefs into zeros. This; // combination can result in a infinite loop, in which undefs are converted to; // zeros in legalization and back to undefs in combining.; //; // So after DAG is legalized, we prevent SimplifyDemandedVectorElts from; // running for build_vectors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyISelLowering.cpp:90,optimiz,optimization,90,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// Ideally, if D.AddressOfRawData == 0, we should try to load the payload; // using D.PointerToRawData instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/COFFDumper.cpp:57,load,load,57,interpreter/llvm-project/llvm/tools/llvm-readobj/COFFDumper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/COFFDumper.cpp,1,['load'],['load']
Performance,"// Ideally, operations with undef should be folded before we get here, but we; // can't guarantee it. Bail out because optimizing undefs is a waste of time.; // Without this, we have to forward undef state to new register operands to; // avoid machine verifier errors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:119,optimiz,optimizing,119,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['optimiz'],['optimizing']
Performance,"// Identifies a virtual call made by this function using an; // llvm.type.checked.load intrinsic with all constant integer arguments.; // [typeid, offset, n x arg]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Bitcode/LLVMBitCodes.h:82,load,load,82,interpreter/llvm-project/llvm/include/llvm/Bitcode/LLVMBitCodes.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Bitcode/LLVMBitCodes.h,1,['load'],['load']
Performance,// Identify and Merge consecutive loads recursively which is of the form; // (ZExt(L1) << shift1) | (ZExt(L2) << shift2) -> ZExt(L3) << shift1; // (ZExt(L1) << shift1) | ZExt(L2) -> ZExt(L3),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp:34,load,loads,34,interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/AggressiveInstCombine/AggressiveInstCombine.cpp,1,['load'],['loads']
Performance,"// If 'self' is passed to the call by value, assume that the function; // returns 'self'. So assign the flags, which were set on 'self' to the; // return value.; // EX: self = performMoreInitialization(self)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/ObjCSelfInitChecker.cpp:176,perform,performMoreInitialization,176,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/ObjCSelfInitChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/ObjCSelfInitChecker.cpp,1,['perform'],['performMoreInitialization']
Performance,"// If 'this' is captured, load it into CXXThisValue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp:26,load,load,26,interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,2,['load'],['load']
Performance,"// If -Ofast is the optimization level, then -ffast-math should be enabled",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp:20,optimiz,optimization,20,interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,1,['optimiz'],['optimization']
Performance,"// If -Ofast is the optimization level, then -fstrict-aliasing should be; // enabled. This alias option is being used to simplify the hasFlag logic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp:20,optimiz,optimization,20,interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Clang.cpp,1,['optimiz'],['optimization']
Performance,"// If 2 threads gets here at the same time, the static initialization ""lock""; // will stall one of them until ProcessLine is finished and both will return the; // correct answer.; // Note: if one (or more) thread(s) is suspended right after the 'isInited.load()`; // and restart after this thread has finished the initialization (i.e. a rare case),; // the only penalty we pay is a spurious 2nd lookup for an unknown function.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx:255,load,load,255,core/base/src/TROOT.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx,1,['load'],['load']
Performance,"// If A is a load, dependencies are tolerable, there's nothing to do here.; // If both A and B belong to the same (store) group, they are independent,; // even if dependencies have not been recorded.; // If both GroupA and GroupB are null, there's nothing to do here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,1,['load'],['load']
Performance,"// If B is a load and part of an interleave group, no earlier loads; // can be added to B's interleave group, because this would mean the; // DependentInst would move across store A. Mark the interleave group; // as complete.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,2,['load'],"['load', 'loads']"
Performance,"// If C compares the truncation of an extending load, try to compare; // the untruncated value instead. This exposes more opportunities to; // reuse CC.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:48,load,load,48,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,"// If CPSR is not killed nor re-defined, we should check whether it is; // live-out. If it is live-out, do not optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:111,optimiz,optimize,111,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['optimiz'],['optimize']
Performance,"// If Caller's vararg or byval argument has been split between registers and; // stack, do not perform tail call, since part of the argument is in caller's; // local frame.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:95,perform,perform,95,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['perform'],['perform']
Performance,"// If ClReachesFunctionCallbacks is enabled, insert a callback for each; // argument and load instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:89,load,load,89,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['load'],['load']
Performance,"// If ConstD is not zero, then replace D by ConstD so that division and modulo; // operations by D can be optimized, in case this function is not inlined by the; // compiler.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BranchProbability.cpp:106,optimiz,optimized,106,interpreter/llvm-project/llvm/lib/Support/BranchProbability.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BranchProbability.cpp,1,['optimiz'],['optimized']
Performance,"// If DeadI is an atomic load/store stronger than monotonic, do not try to; // eliminate/reorder it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DeadStoreElimination.cpp,1,['load'],['load']
Performance,"// If DefIdx does not exist in the model (e.g. implicit defs), then return; // unit latency (defaultDefLatency may be too conservative).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp:84,latency,latency,84,interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetSchedule.cpp,1,['latency'],['latency']
Performance,"// If Dest is ignored, then we're evaluating an aggregate expression; // in a context that doesn't care about the result. Note that loads; // from volatile l-values force the existence of a non-ignored; // destination.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:132,load,loads,132,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['load'],['loads']
Performance,"// If DivInst has the exact flag, remove it. Otherwise this optimization; // may replace a well-defined value 'X % Y' with poison.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DivRemPairs.cpp:60,optimiz,optimization,60,interpreter/llvm-project/llvm/lib/Transforms/Scalar/DivRemPairs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/DivRemPairs.cpp,1,['optimiz'],['optimization']
Performance,// If FC0.Latch and FC0.ExitingBlock are the same then we have already; // performed the updates above.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFuse.cpp:75,perform,performed,75,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFuse.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFuse.cpp,2,['perform'],['performed']
Performance,"// If FrameReg is a high register, add the reg values in a separate; // instruction as the load won't be able to access it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ThumbRegisterInfo.cpp:91,load,load,91,interpreter/llvm-project/llvm/lib/Target/ARM/ThumbRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ThumbRegisterInfo.cpp,2,['load'],['load']
Performance,"// If FrameReg is modified, no previous load-address instructions (using; // it) are valid.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLateInstrsCleanup.cpp:40,load,load-address,40,interpreter/llvm-project/llvm/lib/CodeGen/MachineLateInstrsCleanup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLateInstrsCleanup.cpp,1,['load'],['load-address']
Performance,"// If GVElType is already i1, it is already shrunk. If the type of the GV is; // an FP value, pointer or vector, don't do this optimization because a select; // between them is very expensive and unlikely to lead to later; // simplification. In these cases, we typically end up with ""cond ? v1 : v2""; // where v1 and v2 both require constant pool loads, a big loss.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:127,optimiz,optimization,127,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,2,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"// If GroupB is a load group, we have to compare AI against all; // members of GroupB because if any load within GroupB has a dependency; // on AI, we need to mark GroupB as complete and also release the; // store GroupA (if A belongs to one). The former prevents incorrect; // hoisting of load B above store A while the latter prevents incorrect; // sinking of store A below load B.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,4,['load'],['load']
Performance,"// If I and V are pointers in different address space, it is not allowed to; // use replaceAllUsesWith since I and V have different types. A; // non-target-specific transformation should not use addrspacecast on V since; // the two address space may be disjoint depending on target.; //; // This class chases down uses of the old pointer until reaching the load; // instructions, then replaces the old pointer in the load instructions with; // the new pointer. If during the chasing it sees bitcast or GEP, it will; // create new bitcast or GEP with the new pointer and use them in the load; // instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:357,load,load,357,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,3,['load'],['load']
Performance,"// If I is in the chain, we can tell whether it aliases ChainIt by checking; // what offset ChainIt accesses. This may be better than AA is able to do.; //; // We should really only have duplicate offsets for stores (the duplicate; // loads should be CSE'ed), but in case we have a duplicate load, we'll; // split the chain so we don't have to handle this case specially.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp:235,load,loads,235,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,2,['load'],"['load', 'loads']"
Performance,"// If Inst can, then check if Inst is a simple store. If Inst is not a; // store or a store that is not simple, then we have some we do not; // understand writing to this memory implying we can not move the load; // over the write to any subsequent store that we may find.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp:207,load,load,207,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,1,['load'],['load']
Performance,"// If J is neither load nor store, assume a dependency.; // If J is a load, but I is neither, also assume a dependency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,2,['load'],['load']
Performance,"// If K moves, only set the !invariant.load if it is present in both; // instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,1,['load'],['load']
Performance,"// If LHS is a gep based on RHS or RHS is a gep based on LHS, we can optimize; // this.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp:69,optimiz,optimize,69,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp,1,['optimiz'],['optimize']
Performance,"// If LoadSU has already been scheduled, we should clone it but; // this would negate the benefit to unfolding so just return SU.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:6,Load,LoadSU,6,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,1,['Load'],['LoadSU']
Performance,"// If LowerGEP is disabled, before really splitting the GEP, check whether the; // backend supports the addressing mode we are about to produce. If no, this; // splitting probably won't be beneficial.; // If LowerGEP is enabled, even the extracted constant offset can not match; // the addressing mode, we can still do optimizations to other lowered parts; // of variable indices. Therefore, we don't check for addressing modes in that; // case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp:319,optimiz,optimizations,319,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,1,['optimiz'],['optimizations']
Performance,"// If MI defines a formal parameter, but is not a copy (loads are handled; // in evaluateLoad), then it's not clear what to do.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitTracker.cpp:56,load,loads,56,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitTracker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitTracker.cpp,1,['load'],['loads']
Performance,"// If MI is a load instruction, try to convert it into a LOAD AND TEST.; // Return true on success.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp,2,"['LOAD', 'load']","['LOAD', 'load']"
Performance,"// If MI is a simple load or store for a frame object, return the register; // it loads or stores and set FrameIndex to the index of the frame object.; // Return 0 otherwise.; //; // Flag is SimpleBDXLoad for loads and SimpleBDXStore for stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp:21,load,load,21,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,3,['load'],"['load', 'loads']"
Performance,"// If MI is an ordered or volatile memory reference, disallow moving; // subsequent loads and stores to delay slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsDelaySlotFiller.cpp:84,load,loads,84,interpreter/llvm-project/llvm/lib/Target/Mips/MipsDelaySlotFiller.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsDelaySlotFiller.cpp,1,['load'],['loads']
Performance,"// If MainFileID is loaded it means we loaded an AST file, no need to enter; // a main file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp:20,load,loaded,20,interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Preprocessor.cpp,2,['load'],['loaded']
Performance,// If MemCpyInst length is 1/2/4/8 bytes then replace memcpy with; // load/store.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp:70,load,load,70,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,1,['load'],['load']
Performance,"// If MemorySSA has determined that one of EarlierInst or LaterInst does not; // read/write memory, then we can safely return true here.; // FIXME: We could be more aggressive when checking doesNotAccessMemory(),; // onlyReadsMemory(), mayReadFromMemory(), and mayWriteToMemory() in this pass; // by also checking the MemorySSA MemoryAccess on the instruction. Initial; // experiments suggest this isn't worthwhile, at least for C/C++ code compiled; // with the default optimization pipeline.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:470,optimiz,optimization,470,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,1,['optimiz'],['optimization']
Performance,"// If N is a load, do additional profitability checks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If Offset is bigger than TySizeInBytes, it means we are loading all; // zeros. This should have been optimized before in the process.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:59,load,loading,59,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,"['load', 'optimiz']","['loading', 'optimized']"
Performance,"// If Offset is not provably in the range [0, NullTermIdx], we can still; // optimize if we can prove that the program has undefined behavior when; // Offset is outside that range. That is the case when GEP->getOperand(0); // is a pointer to an object whose memory extent is NullTermIdx+1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp:77,optimiz,optimize,77,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,1,['optimiz'],['optimize']
Performance,"// If Op0 is null, then Node is a constant that can be loaded using:; //; // (Opcode UpperVal LowerVal); //; // If Op0 is nonnull, then Node can be implemented using:; //; // (Opcode (Opcode Op0 UpperVal) LowerVal)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp:55,load,loaded,55,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,1,['load'],['loaded']
Performance,"// If Opcode is a LOAD opcode for with an associated LOAD AND TRAP; // operation exists, returh the opcode for the latter, otherwise return 0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h:18,LOAD,LOAD,18,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h,2,['LOAD'],['LOAD']
Performance,"// If Opcode is a load instruction that has a LOAD AND TEST form,; // return the opcode for the testing form, otherwise return 0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.h,2,"['LOAD', 'load']","['LOAD', 'load']"
Performance,"// If PHI translation was unable to find an available pointer in this; // predecessor, then we have to assume that the pointer is clobbered in; // that predecessor. We can still do PRE of the load, which would insert; // a computation of the pointer in this predecessor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:192,load,load,192,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['load'],['load']
Performance,"// If Pred has unreachable predecessors, but has at least a Def, the; // incoming access can be the last Def in Pred, or it could have been; // optimized to LoE. After an update, though, the LoE may have been; // replaced by another access, so IncAcc may be any access.; // If Pred has unreachable predecessors and no Defs, incoming access; // should be LoE; However, after an update, it may be any access.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:144,optimiz,optimized,144,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['optimiz'],['optimized']
Performance,"// If PrefetchDistance is not set, don't run the pass. This gives an; // opportunity for targets to run this pass for selected subtargets only; // (whose TTI sets PrefetchDistance and CacheLineSize).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDataPrefetch.cpp:184,Cache,CacheLineSize,184,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDataPrefetch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDataPrefetch.cpp,1,['Cache'],['CacheLineSize']
Performance,"// If R has been added to the system, add the new variables and queue it for; // removal once it goes out-of-scope.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp:64,queue,queue,64,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp,1,['queue'],['queue']
Performance,"// If RegImm.Reg is modified by this instruction, then we cannot optimize; // past this instruction. If the register is already compressed, then it may; // possible to optimize a large offset in the current instruction - this; // will have been detected by the preceeding call to; // getRegImmPairPreventingCompression.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp:65,optimiz,optimize,65,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,2,['optimiz'],['optimize']
Performance,"// If ResultReg is given, it determines the register class of the load.; // Otherwise, RC is the register class to use. If the result of the; // load isn't anticipated in this block, both may be zero, in which; // case we must make a conservative guess. In particular, don't assign; // R0 or X0 to the result register, as the result may be used in a load,; // store, add-immediate, or isel that won't permit this. (Though; // perhaps the spill and reload of live-exit values would handle this?)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:66,load,load,66,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,3,['load'],['load']
Performance,"// If Root use can somehow reach N through a path that doesn't contain; // U then folding N would create a cycle. e.g. In the following; // diagram, Root can reach N through X. If N is folded into Root, then; // X is both a predecessor and a successor of U.; //; // [N*] //; // ^ ^ //; // / \ //; // [U*] [X]? //; // ^ ^ //; // \ / //; // \ / //; // [Root*] //; //; // * indicates nodes to be folded together.; //; // If Root produces glue, then it gets (even more) interesting. Since it; // will be ""glued"" together with its glue use in the scheduler, we need to; // check if it might reach N.; //; // [N*] //; // ^ ^ //; // / \ //; // [U*] [X]? //; // ^ ^ //; // \ \ //; // \ | //; // [Root*] | //; // ^ | //; // f | //; // | / //; // [Y] / //; // ^ / //; // f / //; // | / //; // [GU] //; //; // If GU (glue use) indirectly reaches N (the load), and Root folds N; // (call it Fold), then X is a predecessor of GU and a successor of; // Fold. But since Fold and GU are glued together, this will create; // a cycle in the scheduling graph.; // If the node has glue, walk down the graph to the ""lowest"" node in the; // glueged set.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp:842,load,load,842,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,1,['load'],['load']
Performance,"// If SET_FPENV is custom or legal, use it. Otherwise use loading; // environment from memory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:58,load,loading,58,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['load'],['loading']
Performance,"// If SF is used, but the instruction doesn't update the SF, then we; // can't do the optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:86,optimiz,optimization,86,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,2,['optimiz'],['optimization']
Performance,"// If SR is not killed nor re-defined, we should check whether it is; // live-out. If it is live-out, do not optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.cpp:109,optimiz,optimize,109,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiInstrInfo.cpp,1,['optimiz'],['optimize']
Performance,"// If Src is a fixed vector and Dst is a scalable vector, and both have the; // same element type, use the llvm.vector.insert intrinsic to perform the; // bitcast.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:41,scalab,scalable,41,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,2,"['perform', 'scalab']","['perform', 'scalable']"
Performance,"// If Src is a scalable vector and Dst is a fixed vector, and both have the; // same element type, use the llvm.vector.extract intrinsic to perform the; // bitcast.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:15,scalab,scalable,15,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,2,"['perform', 'scalab']","['perform', 'scalable']"
Performance,"// If Src is i8, promote it to i32 with any_extend. There is no i8 BT; // instruction. Since the shift amount is in-range-or-undefined, we know; // that doing a bittest on the i32 value is ok. We extend to i32 because; // the encoding for the i16 version is larger than the i32 version.; // Also promote i16 to i32 for performance / code size reason.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:319,perform,performance,319,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performance']
Performance,"// If SrcTy and Ty are the same, just do a load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:43,load,load,43,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['load']
Performance,"// If Stride isn't constant, then we can't know how much it will load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:65,load,load,65,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,"// If TLI says that this fpimm is illegal, then we'll expand to a; // constant pool load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:84,load,load,84,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['load'],['load']
Performance,"// If V is not a store, we can traverse the expression tree to find loads; // that feed it. The type of the loaded value may indicate a more suitable; // width than V's type. We want to base the vector element size on the width; // of memory operations where possible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:68,load,loads,68,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// If V is used as the pointer operand of a compatible memory operation,; // sets the pointer operand to NewV. This replacement does not change; // the element type, so the resultant load/store is still valid.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/InferAddressSpaces.cpp:183,load,load,183,interpreter/llvm-project/llvm/lib/Transforms/Scalar/InferAddressSpaces.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/InferAddressSpaces.cpp,1,['load'],['load']
Performance,"// If VL=1, then we don't need to do a strided load and can just do a; // regular load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,2,['load'],['load']
Performance,"// If VPU is enabled, always expand non-mask vector loads to VVP",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp:52,load,loads,52,interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/VE/VEISelLowering.cpp,1,['load'],['loads']
Performance,"// If a BNE on the cmpxchg comparison result immediately follows the cmpxchg; // operation, it can be folded into the cmpxchg expansion by; // modifying the branch within 'LoopHead' (which performs the same; // comparison). This is a valid transformation because after altering the; // LoopHead's BNE destination, the BNE following the cmpxchg becomes; // redundant and and be deleted. In the case of a masked cmpxchg, an; // appropriate AND and BNE must be matched.; //; // On success, returns true and deletes the matching BNE or AND+BNE, sets the; // LoopHeadBNETarget argument to the target that should be used within the; // loop head, and removes that block as a successor to MBB.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVExpandAtomicPseudoInsts.cpp:189,perform,performs,189,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVExpandAtomicPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVExpandAtomicPseudoInsts.cpp,1,['perform'],['performs']
Performance,"// If a FP immediate is precise when represented as a float and if the; // target can do an extending load from float to double, we put it into; // the constant pool as a float, even if it's is statically typed as a; // double. This shrinks FP constants and canonicalizes them for targets where; // an FP extending load is the same cost as a normal load (such as on the x87; // fp stack or PPC FP unit).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:102,load,load,102,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,3,['load'],['load']
Performance,"// If a LoadInst has more than one use, changing the type of loaded; // value may create another bitcast.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:8,Load,LoadInst,8,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,2,"['Load', 'load']","['LoadInst', 'loaded']"
Performance,"// If a MBB does not dominate loop exiting blocks then it may not safe; // to hoist loads from this block.; // Tri-state: 0 - false, 1 - true, 2 - unknown",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:84,load,loads,84,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,1,['load'],['loads']
Performance,// If a PCH is loaded and validation is disabled for PCH then disable; // validation for the PCH and the modules it loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:15,load,loaded,15,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// If a TerminatedPath doesn't dominate Target, then it wasn't a legal; // optimization for the prior phi.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:75,optimiz,optimization,75,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['optimiz'],['optimization']
Performance,"// If a VCTP is part of a chain, it's already profitable and shouldn't be; // optimized, else LSR may block tail-predication.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp:78,optimiz,optimized,78,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetTransformInfo.cpp,1,['optimiz'],['optimized']
Performance,"// If a buffer is attached, i.e. we are ready to load into a RooAbsReal outside of our dataset,; // we can directly map our spans to this real.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooVectorDataStore.cxx:49,load,load,49,roofit/roofitcore/src/RooVectorDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooVectorDataStore.cxx,1,['load'],['load']
Performance,"// If a byte boundary is included in any load or store, a slice starting or; // ending at the boundary is not splittable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,"// If a condition can fold to true or false, the corresponding branch; // will be removed. Create a region with both counters hard-coded to; // zero. This allows us to visualize them in a special way.; // Alternatively, we can prevent any optimization done via; // constant-folding by ensuring that ConstantFoldsToSimpleInteger() in; // CodeGenFunction.c always returns false, but that is very heavy-handed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CoverageMappingGen.cpp:239,optimiz,optimization,239,interpreter/llvm-project/clang/lib/CodeGen/CoverageMappingGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CoverageMappingGen.cpp,1,['optimiz'],['optimization']
Performance,"// If a conditional trap instruction got optimized to an; // unconditional trap, eliminate all the instructions after; // the trap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:41,optimiz,optimized,41,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,1,['optimiz'],['optimized']
Performance,"// If a constant can be materialized without loads, this does not make sense.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,"// If a fixed length vector operation has no side effects when applied to; // undefined elements, we can safely use scalable vectors to perform the same; // operation without needing to worry about predication.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:116,scalab,scalable,116,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,"['perform', 'scalab']","['perform', 'scalable']"
Performance,"// If a function has been called more than twice, we do not have to emit a; // load instruction to get the function address from the GOT, but can; // instead reuse the address that has been loaded before.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsOptimizePICCall.cpp:79,load,load,79,interpreter/llvm-project/llvm/lib/Target/Mips/MipsOptimizePICCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsOptimizePICCall.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If a function is marked as arm_locally_streaming, then the runtime value of; // vscale in the prologue/epilogue is different the runtime value of vscale; // in the function's body. To avoid having to consider multiple vscales,; // we can use `addsvl` to allocate any scalable stack-slots, which under; // most circumstances will be only locals, not callee-save slots.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:270,scalab,scalable,270,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,1,['scalab'],['scalable']
Performance,"// If a generic pointer is loaded from the constant address space, it; // could only be a GLOBAL or CONSTANT one as that address space is solely; // prepared on the host side, where only GLOBAL or CONSTANT variables are; // visible. Note that this even holds for regular functions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp:27,load,loaded,27,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAliasAnalysis.cpp,1,['load'],['loaded']
Performance,"// If a library is loaded before gROOT is initialized we can assume; // it is hard linked along side libCore (or is libCore) thus can't; // really be unloaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/cont/src/TClassTable.cxx:19,load,loaded,19,core/cont/src/TClassTable.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/cont/src/TClassTable.cxx,1,['load'],['loaded']
Performance,"// If a load arrives and ldp-aligned-only feature is opted, check that the; // alignment of the source pointer is at least double the alignment of the; // type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If a load occurs in two partitions PartI and PartJ, merge all; // partitions (PartI, PartJ] into PartI.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDistribute.cpp:8,load,load,8,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDistribute.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDistribute.cpp,1,['load'],['load']
Performance,"// If a loop can throw, we have to insert a store along each unwind edge.; // That said, we can't actually make the unwind edge explicit. Therefore,; // we have to prove that the store is dead along the unwind edge. We do; // this by proving that the caller can't have a reference to the object; // after return and thus can't possibly load from the object.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:336,load,load,336,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If a magic bitvector captures the entire comparison state; // of this load, replace it with computation that does:; // ((magic_cst >> i) & 1) != 0",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp:73,load,load,73,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,1,['load'],['load']
Performance,"// If a module summary index is supplied, load it so linkInModule can treat; // local functions/variables as exported and promote if necessary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp:42,load,load,42,interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-link/llvm-link.cpp,1,['load'],['load']
Performance,"// If a new pointer is loaded in the loop, the pointer references a different; // object in every iteration. E.g.:; // for (i); // int *p = a[i];; // ...",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp:23,load,loaded,23,interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,1,['load'],['loaded']
Performance,"// If a positive value is specified, we are going to use the LBR in; // latency-mode.; //; // Note:; // - A small value is preferred, but too low a value could result in; // throttling.; // - A prime number is preferred to avoid always skipping certain blocks.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/Target.cpp:72,latency,latency-mode,72,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/Target.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/X86/Target.cpp,1,['latency'],['latency-mode']
Performance,"// If a potentially clobbering instruction comes before the load,; // we can still safely sink the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp:60,load,load,60,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,2,['load'],['load']
Performance,"// If a reference to this global requires an extra load, we can't fold it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:51,load,load,51,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If a selector value needs to be passed, emit the load before the call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp:52,load,load,52,interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,1,['load'],['load']
Performance,"// If a server is redirected, the cached normalization set might not point; // to the right observables anymore. We need to reset it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddPdf.cxx:34,cache,cached,34,roofit/roofitcore/src/RooAddPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddPdf.cxx,1,['cache'],['cached']
Performance,"// If a valid insertion position is found, then we can promote; // the load/store pair to a memcpy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:71,load,load,71,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['load'],['load']
Performance,"// If after narrowing, the required slide is still greater than LMUL2,; // fallback to generic expansion and go through the stack. This is done; // for a subtle reason: extracting *all* elements out of a vector is; // widely expected to be linear in vector size, but because vslidedown; // is linear in LMUL, performing N extracts using vslidedown becomes; // O(n^2) / (VLEN/ETYPE) work. On the surface, going through the stack; // seems to have the same problem (the store is linear in LMUL), but the; // generic expansion *memoizes* the store, and thus for many extracts of; // the same vector we end up with one store and a bunch of loads.; // TODO: We don't have the same code for insert_vector_elt because we; // have BUILD_VECTOR and handle the degenerate case there. Should we; // consider adding an inverse BUILD_VECTOR node?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:309,perform,performing,309,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,"['load', 'perform']","['loads', 'performing']"
Performance,"// If alias analysis claims that it really won't modify the load,; // ignore it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:60,load,load,60,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,1,['load'],['load']
Performance,"// If alignment information is not available, fall back to the; // default function param optimized type alignment",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:90,optimiz,optimized,90,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['optimiz'],['optimized']
Performance,"// If all cases cover a contiguous range, it is not necessary to jump to; // the default block after the last bit test fails. This is because the; // range check during bit test header creation has guaranteed that every; // case here doesn't go outside the range. In this case, there is no need; // to perform the last bit test, as it will always be true. Instead, make; // the second-to-last bit-test fall through to the target of the last bit; // test, and delete the last bit test.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:302,perform,perform,302,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,1,['perform'],['perform']
Performance,"// If all elements are constants and the case above didn't get hit, fall back; // to the default expansion, which will generate a load from the constant; // pool.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:130,load,load,130,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['load'],['load']
Performance,"// If all elements are constants, create a load from the constant pool.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,"// If all elements are loads, use VLREP/VLEs (below).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['loads']
Performance,"// If all else fails, just use a sequence of INSERT_VECTOR_ELT when we; // know the default expansion would otherwise fall back on something even; // worse. For a vector with one or two non-undef values, that's; // scalar_to_vector for the elements followed by a shuffle (provided the; // shuffle is valid for the target) and materialization element by element; // on the stack followed by a load for everything else.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:392,load,load,392,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['load']
Performance,"// If all of the base pointers of the PHI'd GEPs are from allocas, don't; // bother doing this transformation. At best, this will just save a bit of; // offset calculation, but all the predecessors will have to materialize the; // stack address into a register anyway. We'd actually rather *clone* the; // load up into the predecessors so that we have a load of a gep of an alloca,; // which can usually all be folded into the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp:306,load,load,306,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,3,['load'],['load']
Performance,"// If all of the loads and stores that feed the value have the same AA tags,; // then we can propagate them onto any newly inserted loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:17,load,loads,17,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,2,['load'],['loads']
Performance,"// If all operands are constant, constant fold the shuffle. This; // transformation depends on the value of the mask which is not known at; // compile time for scalable vectors",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp:160,scalab,scalable,160,interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,1,['scalab'],['scalable']
Performance,"// If all the uses are load / store addresses, then don't do the; // transformation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If allowed, upgrade public vcall visibility metadata to linkage unit; // visibility before whole program devirtualization in the optimizer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp:132,optimiz,optimizer,132,interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,1,['optimiz'],['optimizer']
Performance,"// If already cached - retun cached result",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp:14,cache,cached,14,interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp,6,['cache'],['cached']
Performance,"// If already computed, return the cached result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp:35,cache,cached,35,interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp,1,['cache'],['cached']
Performance,"// If already loaded don't do it again",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TPackMgr.cxx:14,load,loaded,14,proof/proof/src/TPackMgr.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TPackMgr.cxx,1,['load'],['loaded']
Performance,"// If an entry list or friend trees are present, we need to generate clusters with global entry numbers,; // so we do it here for all files.; // Otherwise we can do it later, concurrently for each file, and clusters will contain local entry numbers.; // TODO: in practice we could also find clusters per-file in the case of no friends and a TEntryList with; // sub-entrylists.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeProcessorMT.cxx:175,concurren,concurrently,175,tree/treeplayer/src/TTreeProcessorMT.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeProcessorMT.cxx,1,['concurren'],['concurrently']
Performance,"// If an expression is covered by the loop guard, compare again and; // proceed with optimization if equal.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:85,optimiz,optimization,85,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,1,['optimiz'],['optimization']
Performance,"// If an identical load doesn't depends on any local instructions, it can; // be safely moved to PredBB.; // Also check for the implicit control flow instructions. See the comments; // in PerformLoadPRE for details.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,"['Perform', 'load']","['PerformLoadPRE', 'load']"
Performance,"// If an inbounds GEP would have to start from an out of bounds address; // for the two to alias, then we can assume noalias.; // TODO: Remove !isScalable() once BasicAA fully support scalable location; // size",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp:184,scalab,scalable,184,interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,1,['scalab'],['scalable']
Performance,"// If an overflow happens for every value pair in these two constant ranges,; // we must return Empty set. In signed case, we get that for free, because we; // get lucky that intersection of sub() with ssub_sat() results in an; // empty set. But for unsigned we must perform the overflow check manually.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantRange.cpp:267,perform,perform,267,interpreter/llvm-project/llvm/lib/IR/ConstantRange.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantRange.cpp,1,['perform'],['perform']
Performance,"// If any identifiers with corresponding top-level declarations have; // been loaded, load those declarations now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:78,load,loaded,78,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,2,['load'],"['load', 'loaded']"
Performance,// If any of M's operands demand more bits than MinBW then M cannot be; // performed safely in MinBW.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:75,perform,performed,75,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,1,['perform'],['performed']
Performance,"// If any of the S2 bits are poisoned, the whole thing is poisoned.; // Otherwise perform the same shift on S0 and S1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp:82,perform,perform,82,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp,1,['perform'],['perform']
Performance,"// If any of the S2 bits are poisoned, the whole thing is poisoned.; // Otherwise perform the same shift on S1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp:82,perform,perform,82,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/MemorySanitizer.cpp,2,['perform'],['perform']
Performance,"// If any of the instruction's operands are uniform after vectorization,; // the instruction cannot be scalarized. This prevents, for example, a; // masked load from being scalarized.; //; // We assume we will only emit a value for lane zero of an instruction; // marked uniform after vectorization, rather than VF identical values.; // Thus, if we scalarize an instruction that uses a uniform, we would; // create uses of values corresponding to the lanes we aren't emitting code; // for. This behavior can be changed by allowing getScalarValue to clone; // the lane zero values for uniforms rather than asserting.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:156,load,load,156,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['load'],['load']
Performance,"// If any of the other arguments to the intrinsic are divergent, we can't; // optimize the operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:78,optimiz,optimize,78,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,1,['optimiz'],['optimize']
Performance,// If any of the records has external storage and we do a minimal check (or; // AST import) we assume they are equivalent. (If we didn't have this; // assumption then `RecordDecl::LoadFieldsFromExternalStorage` could trigger; // another AST import which in turn would call the structural equivalency; // check again and finally we'd have an improper result.),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ASTStructuralEquivalence.cpp:180,Load,LoadFieldsFromExternalStorage,180,interpreter/llvm-project/clang/lib/AST/ASTStructuralEquivalence.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ASTStructuralEquivalence.cpp,1,['Load'],['LoadFieldsFromExternalStorage']
Performance,"// If any of the terminated paths don't dominate the phi we'll try to; // optimize, we need to figure out what they are and quit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:74,optimiz,optimize,74,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['optimiz'],['optimize']
Performance,"// If any of these blocks has more than one successor (i.e. if the edge we; // just traversed was critical), then there are other paths through this; // block along which the load may not be anticipated. Hoisting the load; // above this block would be adding the load to execution paths along; // which it was not previously executed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:175,load,load,175,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,3,['load'],['load']
Performance,"// If any predecessor block is an EH pad that does not allow non-PHI; // instructions before the terminator, we can't PRE the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:126,load,load,126,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['load'],['load']
Performance,"// If any register operand is dependent, this load is dependent and we; // needn't check it.; // FIXME: Is this true in the case where we are hardening loads after; // they complete? Unclear, need to investigate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:46,load,load,46,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,2,['load'],"['load', 'loads']"
Performance,"// If at least one of Op's registers is in the score brackets, the; // value is likely loaded outside of the loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:87,load,loaded,87,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,1,['load'],['loaded']
Performance,"// If authentication is required, we need to find out which library; // has to be loaded (preparation for near future, 9/7/05)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TSocket.cxx:82,load,loaded,82,net/net/src/TSocket.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TSocket.cxx,1,['load'],['loaded']
Performance,"// If binned likelihood flag is set, pdf is a RooRealSumPdf representing a yield vector; // for a binned likelihood calculation; // Retrieve and cache bin widths needed to convert un-normalized binnedPdf values back to yields",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooNLLVar.cxx:145,cache,cache,145,roofit/roofitcore/src/RooNLLVar.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooNLLVar.cxx,1,['cache'],['cache']
Performance,"// If bitcast (%13) has one use, combine bitcast and store to amx store.; // %src = call x86_amx @llvm.x86.tileloadd64.internal(%row, %col, %addr,; // %stride);; // %13 = bitcast x86_amx %src to <256 x i32>; // store <256 x i32> %13, <256 x i32>* %addr, align 64; // -->; // call void @llvm.x86.tilestored64.internal(%row, %col, %addr,; // %stride64, %13); //; // If bitcast (%13) has multi-use, transform as below.; // %13 = bitcast x86_amx %src to <256 x i32>; // store <256 x i32> %13, <256 x i32>* %addr, align 64; // %add = <256 x i32> %13, <256 x i32> %src2; // -->; // %13 = bitcast x86_amx %src to <256 x i32>; // call void @llvm.x86.tilestored64.internal(%row, %col, %addr,; // %stride64, %13); // %14 = load <256 x i32>, %addr; // %add = <256 x i32> %14, <256 x i32> %src2; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86LowerAMXType.cpp:713,load,load,713,interpreter/llvm-project/llvm/lib/Target/X86/X86LowerAMXType.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86LowerAMXType.cpp,1,['load'],['load']
Performance,"// If block end has been reached, add the fall-through edge to the queue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/BitTracker.cpp:67,queue,queue,67,interpreter/llvm-project/llvm/lib/Target/Hexagon/BitTracker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/BitTracker.cpp,1,['queue'],['queue']
Performance,"// If both SrcIdx and DstIdx are set, correct rematerialization would widen; // the register substantially (beyond both source and dest size). This is bad; // for performance since it can cascade through a function, introducing many; // extra spills and fills (e.g. ARM can easily end up copying QQQQPR registers; // around after a few subreg copies).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegisterCoalescer.cpp:163,perform,performance,163,interpreter/llvm-project/llvm/lib/CodeGen/RegisterCoalescer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegisterCoalescer.cpp,1,['perform'],['performance']
Performance,// If both are loaded from different AST files.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:15,load,loaded,15,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['load'],['loaded']
Performance,"// If both constants have multiple uses, then we won't need to do an extra; // load. The values are likely around in registers for other users.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:79,load,load,79,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If both instructions are loads or stores, they are equal only if both; // are dereferenceable invariant loads with the same number of bits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,2,['load'],['loads']
Performance,// If both load and store instruction reference the same variable; // we won't be able to optimize it. Add all such reference edges; // to RefEdges set.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ModuleSummaryAnalysis.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Analysis/ModuleSummaryAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ModuleSummaryAnalysis.cpp,2,"['load', 'optimiz']","['load', 'optimize']"
Performance,"// If both of the Target and displacement is out of range, then; // there isn't optimization chance.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/x86_64.cpp:80,optimiz,optimization,80,interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/x86_64.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/x86_64.cpp,1,['optimiz'],['optimization']
Performance,"// If both operand are shifted by imm and shift amount is not greater than 4; // for one operand, swap LHS and RHS to put operand with smaller shift amount; // on RHS.; //; // On many AArch64 processors (Cortex A78, Neoverse N1/N2/V1, etc), ADD with; // LSL shift (shift <= 4) has smaller latency and larger throughput than ADD; // with LSL (shift > 4). For the rest of processors, this is no-op for; // performance or correctness.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:289,latency,latency,289,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,"['latency', 'perform', 'throughput']","['latency', 'performance', 'throughput']"
Performance,"// If both operands are a SET_CC, then we don't want to perform this; // folding and create another csel as this results in more instructions; // (and higher register usage).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:56,perform,perform,56,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If both operands are constants, just perform the operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/SimpleSValBuilder.cpp:40,perform,perform,40,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/SimpleSValBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/SimpleSValBuilder.cpp,1,['perform'],['perform']
Performance,// If both operands are integer constants there's a possibility that we; // can do some interesting optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:100,optimiz,optimizations,100,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['optimiz'],['optimizations']
Performance,"// If both operands are the same, then try to optimize or fold the cmp.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp:46,optimiz,optimize,46,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/FastISel.cpp,1,['optimiz'],['optimize']
Performance,"// If both operands are undef, the result is undef. If 1 operand is undef,; // the result is NaN. This should match the behavior of the IR optimizer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:139,optimiz,optimizer,139,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['optimiz'],['optimizer']
Performance,"// If both operands of the binop are vector concatenations, then perform the; // narrow binop on each pair of the source operands followed by concatenation; // of the results.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:65,perform,perform,65,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['perform'],['perform']
Performance,"// If both vXi64 are representable as (unsigned) i32, then we can perform; // the multiple with a single PMULUDQ instruction.; // TODO: Add (SSE41+) PMULDQ handling for signed extensions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:66,perform,perform,66,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['perform'],['perform']
Performance,"// If bundling is enabled and this fragment has instructions in it, it has to; // obey the bundling restrictions. With padding, we'll have:; //; //; // BundlePadding; // |||; // -------------------------------------; // Prev |##########| F |; // -------------------------------------; // ^; // |; // F->Offset; //; // The fragment's offset will point to after the padding, and its computed; // size won't include the padding.; //; // When the -mc-relax-all flag is used, we optimize bundling by writting the; // padding directly into fragments when the instructions are emitted inside; // the streamer. When the fragment is larger than the bundle size, we need to; // ensure that it's bundle aligned. This means that if we end up with; // multiple fragments, we must emit bundle padding between fragments.; //; // "".align N"" is an example of a directive that introduces multiple; // fragments. We could add a special case to handle "".align N"" by emitting; // within-fragment padding (which would produce less padding when N is less; // than the bundle size), but for now we don't.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCAssembler.cpp:474,optimiz,optimize,474,interpreter/llvm-project/llvm/lib/MC/MCAssembler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCAssembler.cpp,1,['optimiz'],['optimize']
Performance,"// If cache doesn't have our configuration, recalculate here",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooProdPdf.cxx:6,cache,cache,6,roofit/roofitcore/src/RooProdPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooProdPdf.cxx,1,['cache'],['cache']
Performance,"// If cache has been sterilized, revive this slot",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddModel.cxx:6,cache,cache,6,roofit/roofitcore/src/RooAddModel.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddModel.cxx,2,['cache'],['cache']
Performance,"// If cache is invalidated and we should return immediately.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCacheUnzip.cxx:6,cache,cache,6,tree/tree/src/TTreeCacheUnzip.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCacheUnzip.cxx,1,['cache'],['cache']
Performance,"// If cacheAlpha is true employ slice iterator here to fill all slices",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofit/src/RooIntegralMorph.cxx:6,cache,cacheAlpha,6,roofit/roofit/src/RooIntegralMorph.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofit/src/RooIntegralMorph.cxx,1,['cache'],['cacheAlpha']
Performance,"// If cacheObs was filled, cache only observables in there",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFFTConvPdf.cxx:6,cache,cacheObs,6,roofit/roofitcore/src/RooFFTConvPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooFFTConvPdf.cxx,2,['cache'],"['cache', 'cacheObs']"
Performance,"// If cached is a promise, wait for it to resolve",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/modules/three.mjs:6,cache,cached,6,js/modules/three.mjs,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/modules/three.mjs,1,['cache'],['cached']
Performance,"// If cached is not a promise (i.e., it's already an imageBitmap)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/modules/three.mjs:6,cache,cached,6,js/modules/three.mjs,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/modules/three.mjs,1,['cache'],['cached']
Performance,"// If check == false, try to load the library",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx:29,load,load,29,core/base/src/TROOT.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/base/src/TROOT.cxx,1,['load'],['load']
Performance,"// If class is already known, don't load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooWorkspace.cxx:36,load,load,36,roofit/roofitcore/src/RooWorkspace.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooWorkspace.cxx,1,['load'],['load']
Performance,"// If client is not cached recurse",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsData.cxx:20,cache,cached,20,roofit/roofitcore/src/RooAbsData.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsData.cxx,1,['cache'],['cached']
Performance,"// If cling cannot find a name it should ask ROOT before it issues an error.; // If ROOT knows the name then it has to create a new variable with that name; // and type in dedicated for that namespace (eg. __ROOT_SpecialObjects).; // For example if the interpreter is looking for h in h-Draw(), this routine; // will create; // namespace __ROOT_SpecialObjects {; // THist* h = (THist*) the_address;; // }; //; // Later if h is called again it again won't be found by the standart lookup; // because it is in our hidden namespace (nobody should do using namespace; // __ROOT_SpecialObjects). It caches the variable declarations and their; // last address. If the newly found decl with the same name (h) has different; // address than the cached one it goes directly at the address and updates it.; //; // returns true when declaration is found and no error should be emitted.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx:594,cache,caches,594,core/metacling/src/TClingCallbacks.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx,2,['cache'],"['cached', 'caches']"
Performance,"// If code completion is enabled, don't perform any end-of-translation-unit; // work.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp:40,perform,perform,40,interpreter/llvm-project/clang/lib/Sema/Sema.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp,1,['perform'],['perform']
Performance,"// If coercing a fixed vector from a scalable vector for ABI; // compatibility, and the types match, use the llvm.vector.extract; // intrinsic to perform the conversion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:37,scalab,scalable,37,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,"['perform', 'scalab']","['perform', 'scalable']"
Performance,"// If coercing a fixed vector to a scalable vector for ABI compatibility, and; // the types match, use the llvm.vector.insert intrinsic to perform the; // conversion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:35,scalab,scalable,35,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,"['perform', 'scalab']","['perform', 'scalable']"
Performance,"// If compiling implementation of a module, load its module map file now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp:44,load,load,44,interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp,1,['load'],['load']
Performance,"// If converted from fixed-length to scalable, convert back",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:37,scalab,scalable,37,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// If converting to offset representation fails (for scalable vectors),; // fall back to type-based implementation:",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:53,scalab,scalable,53,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['scalab'],['scalable']
Performance,// If current instruction is load instructions; // make sure it's a simple load (non atomic & non volatile),MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopVersioningLICM.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopVersioningLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopVersioningLICM.cpp,2,['load'],['load']
Performance,"// If dmask is 0, this is a no-op load. This can be eliminated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,"// If doing VFE, load from the vtable with a type.checked.load intrinsic; // call. Note that we use the GEP to calculate the address to load from; // and pass 0 as the offset to the intrinsic. This is because every; // vtable slot of the correct type is marked with matching metadata, and; // we know that the load must be from one of these slots.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp:17,load,load,17,interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,4,['load'],['load']
Performance,"// If either load is 0, then we should generate XXLXOR to set to 0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If either node is scheduling for latency, sort them by height/depth; // and latency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:36,latency,latency,36,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,2,['latency'],['latency']
Performance,"// If either node is scheduling for latency, sort them by height/depth; // and latency.; // If neither instruction stalls (!LStall && !RStall) and HazardRecognizer; // is enabled, grouping instructions by cycle, then its height is already; // covered so only its depth matters. We also reach this point if both stall; // but have the same height.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp:36,latency,latency,36,interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/GCNILPSched.cpp,2,['latency'],['latency']
Performance,"// If either the LHS or RHS of the xor is a constant, don't do this; // optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:72,optimiz,optimization,72,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['optimiz'],['optimization']
Performance,"// If either/both ops are a shuffle that can scale to v2x64,; // then see if we can perform this as a v4x32 post shuffle.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:84,perform,perform,84,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If evaluation error printing is disabled, we don't need to collect the; // errors and only need to count them. This significantly reduces the; // performance overhead when having evaluation errors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMinimizer.cxx:149,perform,performance,149,roofit/roofitcore/src/RooMinimizer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMinimizer.cxx,1,['perform'],['performance']
Performance,"// If false, all ThinLTO backend compilations through code gen are performed; // using multiple threads in the gold-plugin, before handing control back to; // gold. If true, write individual backend index files which reflect; // the import decisions, and exit afterwards. The assumption is; // that the build system will launch the backend processes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/gold-plugin.cpp:67,perform,performed,67,interpreter/llvm-project/llvm/tools/gold/gold-plugin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/gold/gold-plugin.cpp,1,['perform'],['performed']
Performance,"// If fast isel succeeded, skip over all the folded instructions, and; // then see if there is a load right before the selected instructions.; // Try to fold the load if so.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp:97,load,load,97,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,2,['load'],['load']
Performance,"// If fatbin is available from early finalization, create a string; // literal containing the fat binary loaded from the given file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp:105,load,loaded,105,interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCUDANV.cpp,1,['load'],['loaded']
Performance,"// If for some reason the load is a sextload, the and is needed to zero; // out the high 8 bits",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['load']
Performance,"// If from cache, just move the cache file",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProof.cxx:11,cache,cache,11,proof/proof/src/TProof.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TProof.cxx,2,['cache'],['cache']
Performance,"// If gc_relocate does not match the actual type, cast it to the right type.; // In theory, there must be a bitcast after gc_relocate if the type does not; // match, and we should reuse it to get the derived pointer. But it could be; // cases like this:; // bb1:; // ...; // %g1 = call coldcc i8 addrspace(1)*; // @llvm.experimental.gc.relocate.p1i8(...) br label %merge; //; // bb2:; // ...; // %g2 = call coldcc i8 addrspace(1)*; // @llvm.experimental.gc.relocate.p1i8(...) br label %merge; //; // merge:; // %p1 = phi i8 addrspace(1)* [ %g1, %bb1 ], [ %g2, %bb2 ]; // %cast = bitcast i8 addrspace(1)* %p1 in to i32 addrspace(1)*; //; // In this case, we can not find the bitcast any more. So we insert a new; // bitcast no matter there is already one or not. In this way, we can handle; // all cases, and the extra bitcast should be optimized away in later; // passes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:836,optimiz,optimized,836,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimized']
Performance,"// If hoisting an invariant group, we only need to check that there; // is no store to the loaded pointer between the start of the loop,; // and the load (since all values must be the same).; // This can be checked in two conditions:; // 1) if the memoryaccess is outside the loop; // 2) the earliest access is at the loop header,; // if the memory loaded is the phi node",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:91,load,loaded,91,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,3,['load'],"['load', 'loaded']"
Performance,"// If in read mode, uses the cached file status, if available, to avoid; // costly dc_stat() call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/dcache/src/TDCacheFile.cxx:29,cache,cached,29,io/dcache/src/TDCacheFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/dcache/src/TDCacheFile.cxx,1,['cache'],['cached']
Performance,"// If in the path to join the two instructions,; // there is another high latency instruction,; // or instructions colored for another block; // abort the merge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp:74,latency,latency,74,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMachineScheduler.cpp,1,['latency'],['latency']
Performance,"// If instruction I has debug info, then we should not update it.; // Also, if I has a null DebugLoc, then it is still potentially incorrect; // to propagate Load's DebugLoc because Load may not post-dominate I.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:158,Load,Load,158,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['Load'],['Load']
Performance,"// If integer and float booleans have different contents then we can't; // reliably optimize in all cases. There is a full explanation for this in; // DAGCombiner::visitSELECT() where the same issue affects folding; // (select C, 0, 1) to (xor C, 1).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:84,optimiz,optimize,84,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['optimiz'],['optimize']
Performance,"// If intrinsic is pointing at a live SSA value, there may be an; // earlier optimization bug: if we know the location of the variable,; // why isn't the scope of the location alive?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ADCE.cpp:77,optimiz,optimization,77,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ADCE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ADCE.cpp,1,['optimiz'],['optimization']
Performance,"// If invoke wrapper has already been generated for this call in; // previous EH phase, search for the load instruction; // %__THREW__.val = __THREW__;; // in postamble after the invoke wrapper call",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyLowerEmscriptenEHSjLj.cpp:103,load,load,103,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyLowerEmscriptenEHSjLj.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyLowerEmscriptenEHSjLj.cpp,1,['load'],['load']
Performance,"// If isWeakAccess to true, there will be an implicit; // load which requires a cleanup.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:58,load,load,58,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['load'],['load']
Performance,"// If it could be in range, we need to load from the given builtin.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SPIRV/SPIRVBuiltins.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/SPIRV/SPIRVBuiltins.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SPIRV/SPIRVBuiltins.cpp,1,['load'],['load']
Performance,"// If it generates more than 8 stores it is likely to be expanded as an; // inline memcpy so we take that as an upper bound. Otherwise we assume; // one load and one store per word copied.; // FIXME: The maxStoresPerMemcpy setting from the target should be used; // here instead of a magic number of 8, but it's not available via; // DataLayout.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp:153,load,load,153,interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,1,['load'],['load']
Performance,"// If it is a load instruction with range metadata, use it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,1,['load'],['load']
Performance,"// If it is a load instruction with range metadata, use the metadata.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,1,['load'],['load']
Performance,"// If it is a load then check if it is guaranteed to execute by making sure; // that it dominates all exiting blocks. If it doesn't, then there is a path; // out of the loop which does not execute this load, so we can't hoist it.; // Loads from constant memory are safe to speculate, for example indexed load; // from a jump table.; // Stores and side effects are already checked by isSafeToMove.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,4,"['Load', 'load']","['Loads', 'load']"
Performance,"// If it is a load, then just use one of the destination registers; // as the new base. Will no longer be writeback in Thumb1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If it is a loop, then resolve and perform the loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp:37,perform,perform,37,interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/TableGen/TGParser.cpp,1,['perform'],['perform']
Performance,"// If it is accessed as got-indirect, we need an extra LWZ/LD to load; // the address.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:65,load,load,65,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,1,['load'],['load']
Performance,// If it is an atomic and alignment is less than the size then we will; // introduce the unaligned memory access which will be later transformed; // into libcall in CodeGen. This is not evident performance gain so disable; // it now.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp:194,perform,performance,194,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,1,['perform'],['performance']
Performance,"// If it is cached, check whether it's the target method, and if so,; // remove it from the cache. Note, the call to 'get' might invalidate; // the iterator and the LazyDeclPtr object within the map.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp:12,cache,cached,12,interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,2,['cache'],"['cache', 'cached']"
Performance,"// If it is double-word aligned, just load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Sparc/SparcISelLowering.cpp,1,['load'],['load']
Performance,"// If it is not LOAD, can not do such combine.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:16,LOAD,LOAD,16,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['LOAD'],['LOAD']
Performance,"// If it is small or large code model, module locals are accessed; // indirectly by loading their address from .toc/.got.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:84,load,loading,84,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loading']
Performance,"// If it would take more than three instructions to adjust the stack pointer; // using tADDspi/tSUBspi, load an immediate instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb1FrameLowering.cpp:104,load,load,104,interpreter/llvm-project/llvm/lib/Target/ARM/Thumb1FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb1FrameLowering.cpp,1,['load'],['load']
Performance,"// If it's a G_OR, save it and continue to walk. If it's not, then it's; // something that may be a load + arithmetic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:100,load,load,100,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// If it's a REG_SEQUENCE/COPY, use its destination instruction to determine; // the correct latency.; // If there are multiple uses of the def of COPY/REG_SEQUENCE, set the latency; // only if the latencies on all the uses are equal, otherwise set it to; // default.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp:93,latency,latency,93,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonSubtarget.cpp,2,['latency'],['latency']
Performance,"// If it's a VLA, we have to load the stored size. Note that; // this is the size of the VLA in bytes, not its size in elements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp:29,load,load,29,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,1,['load'],['load']
Performance,"// If it's an l-value, load through the appropriate subobject l-value.; // Note that we have to ask E because Op might be an l-value that; // this won't work for, e.g. an Obj-C property.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:23,load,load,23,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,2,['load'],['load']
Performance,"// If it's both a load and a store, then we won't handle it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonFrameLowering.cpp,1,['load'],['load']
Performance,"// If it's known that both DScnt and either LOADcnt or STOREcnt (but not; // both) need to be waited for, ensure that there are no existing; // individual wait count instructions for these.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:44,LOAD,LOADcnt,44,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,1,['LOAD'],['LOADcnt']
Performance,"// If it's not a store or a variant load, we're done.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAGInstrs.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAGInstrs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAGInstrs.cpp,1,['load'],['load']
Performance,"// If it's not cached, there's nothing to do.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp:15,cache,cached,15,interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,1,['cache'],['cached']
Performance,"// If latency is unknown, then conservatively assume a MaxLatency of 100cy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/InstrBuilder.cpp:6,latency,latency,6,interpreter/llvm-project/llvm/lib/MCA/InstrBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/InstrBuilder.cpp,1,['latency'],['latency']
Performance,"// If lazy-loading is enabled, we try recursively to load the operand; // instead of creating a temporary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp:11,load,loading,11,interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Bitcode/Reader/MetadataLoader.cpp,4,['load'],"['load', 'loading']"
Performance,"// If load has mutli-user, duplicate a vector load.; // %src = load <256 x i32>, <256 x i32>* %addr, align 64; // %2 = bitcast <256 x i32> %src to x86_amx; // %add = add <256 x i32> %src, <256 x i32> %src2; // -->; // %src = load <256 x i32>, <256 x i32>* %addr, align 64; // %2 = call x86_amx @llvm.x86.tileloadd64.internal(i16 %row, i16 %col,; // i8* %addr, i64 %stride64); // %add = add <256 x i32> %src, <256 x i32> %src2; // If load has one user, the load will be eliminated in DAG ISel.; // %src = load <256 x i32>, <256 x i32>* %addr, align 64; // %2 = bitcast <256 x i32> %src to x86_amx; // -->; // %2 = call x86_amx @llvm.x86.tileloadd64.internal(i16 %row, i16 %col,; // i8* %addr, i64 %stride64)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86LowerAMXType.cpp:6,load,load,6,interpreter/llvm-project/llvm/lib/Target/X86/X86LowerAMXType.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86LowerAMXType.cpp,7,['load'],['load']
Performance,"// If load is legal, just bitcast the src pointer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:6,load,load,6,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['load']
Performance,"// If load is not volatile and there are no uses of the loaded value (and; // the updated indexed value in case of indexed loads), change uses of the; // chain value into uses of the chain input (i.e. delete the dead load).; // TODO: Allow this for unordered atomics (see D66309)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:6,load,load,6,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],"['load', 'loaded', 'loads']"
Performance,"// If load or execute request we must make sure that we have the files.; // If not we ask the client to send them, blocking until we have everything.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationServer.cxx:6,load,load,6,net/net/src/TApplicationServer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/net/net/src/TApplicationServer.cxx,1,['load'],['load']
Performance,"// If loading from a FrameIndex, fold directly from the FrameIndex.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:6,load,loading,6,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['loading']
Performance,// If loads occur at a distance that is not a multiple of a feasible vector; // factor store-load forwarding does not take place.; // Positive dependences might cause troubles because vectorizing them might; // prevent store-load forwarding making vectorized code run a lot slower.; // a[i] = a[i-3] ^ a[i-8];; // The stores to a[i:i+1] don't align with the stores to a[i-3:i-2] and; // hence on your typical architecture store-load forwarding does not take; // place. Vectorizing in such cases does not make sense.; // Store-load forwarding distance.; // After this many iterations store-to-load forwarding conflicts should not; // cause any slowdowns.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:6,load,loads,6,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,6,['load'],"['load', 'loads']"
Performance,"// If marked as volatile, perform a copy even when marked as constant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:26,perform,perform,26,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['perform'],['perform']
Performance,"// If modules are not available, there is no reason to perform this merge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:55,perform,perform,55,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,4,['perform'],['perform']
Performance,"// If modules were created with lazy metadata loading, materialize it; // now, before linking it (otherwise this will be a noop).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionImport.cpp:46,load,loading,46,interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionImport.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionImport.cpp,1,['load'],['loading']
Performance,"// If necessary, perform additional analysis.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:17,perform,perform,17,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['perform'],['perform']
Performance,"// If necessary, reserve queue entries in the load-store unit (LSU).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/HardwareUnits/Scheduler.cpp:25,queue,queue,25,interpreter/llvm-project/llvm/lib/MCA/HardwareUnits/Scheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/HardwareUnits/Scheduler.cpp,2,"['load', 'queue']","['load-store', 'queue']"
Performance,"// If neither file nor tree has a cache, use the current default.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx:34,cache,cache,34,tree/tree/src/TTree.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx,1,['cache'],['cache']
Performance,"// If neither the SrcReg nor the TrgReg are $0, we need AT to perform the; // expansions. If it is not available, we return.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:62,perform,perform,62,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,1,['perform'],['perform']
Performance,"// If no -O was passed, pass -O0 to ptxas -- no opt flag should correspond; // to no optimizations, but ptxas's default is -O3.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Cuda.cpp:85,optimiz,optimizations,85,interpreter/llvm-project/clang/lib/Driver/ToolChains/Cuda.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Cuda.cpp,1,['optimiz'],['optimizations']
Performance,"// If no cache element found, return 0 ;",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooExpensiveObjectCache.cxx:9,cache,cache,9,roofit/roofitcore/src/RooExpensiveObjectCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooExpensiveObjectCache.cxx,1,['cache'],['cache']
Performance,"// If no cache hits, then back() is the end of the inline chain, that is,; // the DILocation whose scope ends in the Subprogram to be replaced.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/DebugLoc.cpp:9,cache,cache,9,interpreter/llvm-project/llvm/lib/IR/DebugLoc.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/DebugLoc.cpp,1,['cache'],['cache']
Performance,"// If no explicit cloneData command is specified, cloneData is set to true if optimization is activated",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/FitHelpers.cxx:78,optimiz,optimization,78,roofit/roofitcore/src/FitHelpers.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/FitHelpers.cxx,1,['optimiz'],['optimization']
Performance,"// If no loads or stores are left, there is no pre-splitting to be done for; // this alloca.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:9,load,loads,9,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// If no output file was provided, figure out where this module would go; // in the module cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendActions.cpp:91,cache,cache,91,interpreter/llvm-project/clang/lib/Frontend/FrontendActions.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendActions.cpp,1,['cache'],['cache']
Performance,"// If no throughput value was calculated, assume that we can execute at the; // maximum issue width scaled by number of micro-ops for the schedule class.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCSchedule.cpp:9,throughput,throughput,9,interpreter/llvm-project/llvm/lib/MC/MCSchedule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCSchedule.cpp,1,['throughput'],['throughput']
Performance,"// If node can be optimized and hasn't been identified yet, add it to the list",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx:18,optimiz,optimized,18,roofit/roofitcore/src/RooAbsArg.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx,1,['optimiz'],['optimized']
Performance,"// If non-null triggers saving of the performance info into fPerfTree",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProof.h:38,perform,performance,38,proof/proof/inc/TProof.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/inc/TProof.h,1,['perform'],['performance']
Performance,// If none of the select types are supported then skip this pass.; // This is an optimization pass. Legality issues will be handled by; // instruction selection.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp:81,optimiz,optimization,81,interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,2,['optimiz'],['optimization']
Performance,"// If not 1/2/4/8 bytes, exit.; // If it is an atomic and alignment is less than the size then we will; // introduce the unaligned memory access which will be later transformed; // into libcall in CodeGen. This is not evident performance gain so disable; // it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp:226,perform,performance,226,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,1,['perform'],['performance']
Performance,"// If not in C++, we perform name lookup for the translation unit via the; // IdentifierInfo chains, don't bother to build a visible-declarations table.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp:21,perform,perform,21,interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,1,['perform'],['perform']
Performance,"// If not jitting lazily, load the whole bitcode file eagerly too.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp:26,load,load,26,interpreter/llvm-project/llvm/tools/lli/lli.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp,1,['load'],['load']
Performance,"// If not loading a power-of-2 number of bits, expand as two loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:10,load,loading,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,2,['load'],"['loading', 'loads']"
Performance,"// If not, see if next level down can be cached",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx:41,cache,cached,41,roofit/roofitcore/src/RooAbsArg.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsArg.cxx,1,['cache'],['cached']
Performance,"// If not, try unfolding a hoistable load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,1,['load'],['load']
Performance,"// If not, we may be able to unfold a load and hoist that.; // First test whether the instruction is loading from an amenable; // memory location.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,2,['load'],"['load', 'loading']"
Performance,"// If one is and the other isn't, it isn't strictly safe but we can fake; // this result if necessary for performance. This does not appear to be; // a common problem in practice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp:106,perform,performance,106,interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,1,['perform'],['performance']
Performance,"// If one is based on an indirect global and the other isn't, it isn't; // strictly safe but we can fake this result if necessary for performance.; // This does not appear to be a common problem in practice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp:134,perform,performance,134,interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,1,['perform'],['performance']
Performance,// If one is local while the other is loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:38,load,loaded,38,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['load'],['loaded']
Performance,"// If one of the invalidated SCCs had a cached proxy to a function; // analysis manager, we need to create a proxy in the new current SCC as; // the invalidated SCCs had their functions moved.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp:40,cache,cached,40,interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp,1,['cache'],['cached']
Performance,"// If one operation reads from invariant memory, and the other may store, they; // cannot alias. These should really be checking the equivalent of mayWrite,; // but it only matters for memory nodes other than load /store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:209,load,load,209,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If one pointer is the result of a call/invoke or load and the other is a; // non-escaping local object within the same function, then we know the; // object couldn't escape to a point where the call could return it.; //; // Note that if the pointers are in different functions, there are a; // variety of complications. A call with a nocapture argument may still; // temporary store the nocapture argument's value in a temporary memory; // location if that memory location doesn't escape. Or it may pass a; // nocapture value to other functions as long as they don't capture it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,1,['load'],['load']
Performance,"// If one the accesses may be before the accessed pointer, canonicalize this; // by using unknown after-pointer sizes for both accesses. This is; // equivalent, because regardless of which pointer is lower, one of them; // will always came after the other, as long as the underlying objects aren't; // disjoint. We do this so that the rest of BasicAA does not have to deal; // with accesses before the base pointer, and to improve cache utilization by; // merging equivalent states.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp:431,cache,cache,431,interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/BasicAliasAnalysis.cpp,1,['cache'],['cache']
Performance,"// If only some modules were split, flag this in the index so that; // we can skip or error on optimizations that need consistently split; // modules (whole program devirt and lower type tests).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp:95,optimiz,optimizations,95,interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,1,['optimiz'],['optimizations']
Performance,"// If only some of the modules were split, we cannot correctly handle; // code that contains type tests or type checked loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp:120,load,loads,120,interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/LTO.cpp,1,['load'],['loads']
Performance,"// If only some of the modules were split, we cannot correctly perform; // this transformation. We already checked for the presense of type tests; // with partially split modules during the thin link, and would have emitted; // an error if any were found, so here we can simply return.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp:63,perform,perform,63,interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,2,['perform'],['perform']
Performance,"// If only the offset is divergent, emit a MUBUF buffer load instead. We can; // assume that the buffer is unswizzled.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp:56,load,load,56,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPURegisterBankInfo.cpp,1,['load'],['load']
Performance,"// If optimization is enabled, and the value was held in a; // __strong variable, we need to tell the optimizer that this; // value has to stay alive until we're doing the store back.; // This is because the temporary is effectively unretained,; // and so otherwise we can violate the high-level semantics.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:6,optimiz,optimization,6,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"// If original load is a SEXTLOAD then we can't simply replace it by a; // ZEXTLOAD (we could potentially replace it by a more narrow SEXTLOAD; // followed by a ZEXT, but that is not handled at the moment). Similarly if; // the original load is a ZEXTLOAD and we want to use a SEXTLOAD.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// If our one-entry cache covers this offset, just return it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/SourceManager.h:20,cache,cache,20,interpreter/llvm-project/clang/include/clang/Basic/SourceManager.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Basic/SourceManager.h,1,['cache'],['cache']
Performance,"// If performing final tracking of transfers, report this variable definition; // to the TransferTracker too.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp:6,perform,performing,6,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp,1,['perform'],['performing']
Performance,"// If post-load hardening is enabled, this load is compatible with; // post-load hardening, and we aren't already going to harden one of the; // address registers, queue it up to be hardened post-load. Notably,; // even once hardened this won't introduce a useful dependency that; // could prune out subsequent loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,6,"['load', 'queue']","['load', 'loads', 'queue']"
Performance,// If probability list is empty it means we don't use it (disabled; // optimization).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBasicBlock.cpp:71,optimiz,optimization,71,interpreter/llvm-project/llvm/lib/CodeGen/MachineBasicBlock.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineBasicBlock.cpp,2,['optimiz'],['optimization']
Performance,"// If profile is CS, the function offset section is expected to consist of; // sequences of contexts in pre-order layout; // (e.g. [A, A:1 @ B, A:1 @ B:2.3 @ C] [D, D:1 @ E]), so that when a matched; // context in the module is found, the profiles of all its callees are; // recursively loaded. A list is needed since the order of profiles matters.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp:287,load,loaded,287,interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ProfileData/SampleProfReader.cpp,1,['load'],['loaded']
Performance,"// If reached MaxLevel,; // or if V1 and V2 are not instructions,; // or if they are SPLAT,; // or if they are not consecutive,; // or if profitable to vectorize loads or extractelements, early return; // the current cost.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:162,load,loads,162,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['loads']
Performance,// If reference to barrier id is not an inlinable constant then it must be; // referenced with M0[4:0]. Perform an OR with the member count to include; // it in M0 for S_BARRIER_INIT.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:104,Perform,Perform,104,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,1,['Perform'],['Perform']
Performance,// If reference to barrier id is not an inline constant then it must be; // referenced with M0[4:0]. Perform an OR with the member count to; // include it in M0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:101,Perform,Perform,101,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,1,['Perform'],['Perform']
Performance,"// If requested set the no cache mode",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TEventIter.cxx:27,cache,cache,27,proof/proofplayer/src/TEventIter.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proofplayer/src/TEventIter.cxx,1,['cache'],['cache']
Performance,"// If result not in cache - call system function and cache result",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp:20,cache,cache,20,interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/DynamicLibraryManagerSymbol.cpp,6,['cache'],['cache']
Performance,"// If resulting type is vec3, there is no point in trimming the; // load with updated offset, as the vec3 would most likely be widened to; // vec4 anyway during lowering.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:68,load,load,68,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,1,['load'],['load']
Performance,"// If return value of memcmp is not used in a zero equality, we need to; // calculate which source was larger. The calculation requires the; // two loaded source values of each load compare block.; // These will be saved in the phi nodes created by setupResultBlockPHINodes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:148,load,loaded,148,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If s is a constant pointer pointing to a string literal, we can fold; // strlen(s + x) to strlen(s) - x, when x is known to be in the range; // [0, strlen(s)] or the string has a single null terminator '\0' at the end.; // We only try to simplify strlen when the pointer s points to an array; // of CharSize elements. Otherwise, we would need to scale the offset x before; // doing the subtraction. This will make the optimization more complex, and; // it's not very useful because calling strlen for a pointer of other types is; // very uncommon.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp:421,optimiz,optimization,421,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,1,['optimiz'],['optimization']
Performance,"// If scalar writes are used, the cache must be flushed or else the next; // wave to reuse the same scratch memory can be clobbered.; //; // Insert s_dcache_wb at wave termination points if there were any scalar; // stores, and only if the cache hasn't already been flushed. This could; // be improved by looking across blocks for flushes in postdominating; // blocks from the stores but an explicitly requested flush is probably; // very rare.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:34,cache,cache,34,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,2,['cache'],['cache']
Performance,"// If servers are redirected, the cached normalization integrals and; // normalization sets are most likely invalid.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsPdf.cxx:34,cache,cached,34,roofit/roofitcore/src/RooAbsPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsPdf.cxx,1,['cache'],['cached']
Performance,"// If set, skip indexing inside some declarations for performance.; // This prevents traversal, so skipping a struct means its declaration an; // members won't be indexed, but references elsewhere to that struct will be.; // Currently this is only checked for top-level declarations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Index/IndexingOptions.h:54,perform,performance,54,interpreter/llvm-project/clang/include/clang/Index/IndexingOptions.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Index/IndexingOptions.h,1,['perform'],['performance']
Performance,"// If several filters occur in a row then reorder them so that the shortest; // filters come first (those with the smallest number of elements). This is; // advantageous because shorter filters are more likely to match, speeding up; // unwinding, but mostly because it increases the effectiveness of the other; // filter optimizations below.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:321,optimiz,optimizations,321,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['optimiz'],['optimizations']
Performance,"// If shape is not cached, or a capture to cache is already in; // progress perform a direct draw DL can be nested, but not created; // in nested fashion. As we only build DL on draw demands have to; // protected against this here.; // MT: I can't see how this could happen right now ... with; // rendering from a flat drawable-list.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLLogicalShape.cxx:19,cache,cached,19,graf3d/gl/src/TGLLogicalShape.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLLogicalShape.cxx,3,"['cache', 'perform']","['cache', 'cached', 'perform']"
Performance,"// If si = 1111111111110000 and the msb of the d/ds field of the load equals; // 1, then fusion does not occur.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMacroFusion.cpp:65,load,load,65,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMacroFusion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMacroFusion.cpp,1,['load'],['load']
Performance,"// If size of the set is the same as total number of coro.begin, that means we; // found a coro.free or coro.destroy referencing each coro.begin, so we can; // perform heap elision.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroElide.cpp:160,perform,perform,160,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroElide.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroElide.cpp,1,['perform'],['perform']
Performance,"// If so, this load is partially redundant. Remember this info so that we; // can create a PHI node.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// If so, we can queue them all as live in loads. We don't have an; // efficient way to tell which on is first in the block and don't want to; // scan large blocks, so just add all loads as live ins.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp:17,queue,queue,17,interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp,3,"['load', 'queue']","['loads', 'queue']"
Performance,"// If some of the uses of IntA.reg is already coalesced away, return false.; // It's not possible to determine whether it's safe to perform the coalescing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegisterCoalescer.cpp:132,perform,perform,132,interpreter/llvm-project/llvm/lib/CodeGen/RegisterCoalescer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegisterCoalescer.cpp,1,['perform'],['perform']
Performance,"// If somehow the TClass has already been loaded (maybe it was registered several time),; // we skip it. Otherwise, the existing TClass is in mode kInterpreted, kEmulated or; // maybe even kForwardDeclared and needs to replaced.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:42,load,loaded,42,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,1,['load'],['loaded']
Performance,"// If starting offset isn't zero, insert a MI to materialize a new base.; // But only do so if it is cost effective, i.e. merging more than two; // loads / stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:148,load,loads,148,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['loads']
Performance,"// If store instructions are allowed, the path from the entry of the function; // to each load may be not free of instructions that potentially invalidate; // the load, and this is an admissible situation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp:90,load,load,90,interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,2,['load'],['load']
Performance,"// If successful, these function will implicitly finalize all loaded objects.; // To get a function address within MCJIT without causing a finalize, use; // getSymbolAddress.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.h:62,load,loaded,62,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.h,1,['load'],['loaded']
Performance,"// If that base is not DWORD aligned, it's not safe to perform the following; // transforms.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp:55,perform,perform,55,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULateCodeGenPrepare.cpp,1,['perform'],['perform']
Performance,"// If that fails, we'll need to perform an FCMP + CSEL sequence. Go ahead; // and do the comparison.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:32,perform,perform,32,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If the ""cost"" of materializing the integer immediate is less than the cost; // of a load, then it is cost effective to turn the load into the immediate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:87,load,load,87,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,2,['load'],['load']
Performance,"// If the 'class' is not loaded, we do not have a TClass bootstrap and thus; // the 'RealData' might not have enough information because of the lack; // of proper ShowMember implementation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx:25,load,loaded,25,io/io/src/TStreamerInfo.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx,1,['load'],['loaded']
Performance,"// If the (non-volatile) load only has one use, we can rewrite this to a; // load from a GEP. This reduces the size of the load. If a load is used; // only by extractvalue instructions then this either must have been; // optimized before, or it is a struct with padding, in which case we; // don't want to do the transformation as it loses padding knowledge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,5,"['load', 'optimiz']","['load', 'optimized']"
Performance,"// If the -locally-hot-callsite-threshold is explicitly specified, use it to; // populate LocallyHotCallSiteThreshold. Later, we populate; // Params.LocallyHotCallSiteThreshold from -locally-hot-callsite-threshold if; // we know that optimization level is O3 (in the getInlineParams variant that; // takes the opt and size levels).; // FIXME: Remove this check (and make the assignment unconditional) after; // addressing size regression issues at O2.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp:234,optimiz,optimization,234,interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,1,['optimiz'],['optimization']
Performance,"// If the BB is updated, it may still has chance to be optimized.; // This usually happen at sink optimization.; // For example:; //; // bb0; // %and = and i32 %a, 4; // %cmp = icmp eq i32 %and, 0; //; // If the %cmp sink to other BB, the %and will has chance to sink.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:55,optimiz,optimized,55,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,2,['optimiz'],"['optimization', 'optimized']"
Performance,"// If the BaseReg has been modified, then we cannot do the optimization.; // For example, in the following pattern; // ldr x1 [x2]; // ldr x2 [x3]; // ldr x4 [x2, #8],; // the first and third ldr cannot be converted to ldp x1, x4, [x2]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:59,optimiz,optimization,59,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['optimiz'],['optimization']
Performance,"// If the BitWidth is 0, do not try to optimize the type",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:39,optimiz,optimize,39,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimize']
Performance,"// If the CoCreateInstance call above failed, msdia*.dll is not registered.; // Try loading the DLL corresponding to the #included DIA SDK.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/DIA/DIASession.cpp:84,load,loading,84,interpreter/llvm-project/llvm/lib/DebugInfo/PDB/DIA/DIASession.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/PDB/DIA/DIASession.cpp,1,['load'],['loading']
Performance,"// If the EHPad isn't a terminator, then we can insert a load in this block; // that will dominate all uses.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp,1,['load'],['load']
Performance,"// If the GEP and the gather/scatter aren't in the same BB, don't optimize.; // FIXME: We should support this by sinking the GEP.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:66,optimiz,optimize,66,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimize']
Performance,"// If the ISD::CopyToReg has a glue operand, we conservatively assume it; // isn't safe to perform a tail call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp:91,perform,perform,91,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp,2,['perform'],['perform']
Performance,"// If the LHS is '(and load, const)', the RHS is 0, the test is for; // equality or unsigned, and all 1 bits of the const are in the same; // partial word, see if we can shorten the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,2,['load'],['load']
Performance,"// If the LHS is a ZERO_EXTEND, perform the comparison on the input.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:32,perform,perform,32,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['perform'],['perform']
Performance,// If the LHS is zero-extended then we can perform the USUBSAT as DstVT by; // clamping RHS.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:43,perform,perform,43,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['perform'],['perform']
Performance,"// If the Lo in (ADD_LO hi, lo) is a global variable's address; // (its low part, really), then we can rely on the alignment of that; // variable to provide a margin of safety before low part can overflow; // the 12 bits of the load/store offset. Check if CVal falls within; // that margin; if so (low part + CVal) can't overflow.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp:228,load,load,228,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If the Load isn't completely contained within the stored bits, we don't; // have all the bits to feed it. We could do something crazy in the future; // (issue a smaller load then merge the bits in) but this seems unlikely to be; // valuable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp:10,Load,Load,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,2,"['Load', 'load']","['Load', 'load']"
Performance,"// If the MMO suggests this isn't a load of a full vector, leave; // things alone. For a built-in, we have to make the change for; // correctness, so if there is a size problem that will be a bug.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If the Maximum element used from V1 and V2 are not larger than the new; // vectors, the vectors are already packes and performing the optimization; // again will likely not help any further. This also prevents us from getting; // stuck in a cycle in case the costs do not also rule it out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp:122,perform,performing,122,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,2,"['optimiz', 'perform']","['optimization', 'performing']"
Performance,"// If the OPFL_MemRefs glue is set on this node, slap all of the; // accumulated memrefs onto it.; //; // FIXME: This is vastly incorrect for patterns with multiple outputs; // instructions that access memory and for ComplexPatterns that match; // loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp:248,load,loads,248,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,1,['load'],['loads']
Performance,"// If the Offsets aren't needed, don't query the struct layout. This allows; // us to support structs with scalable vectors for operations that don't; // need offsets.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/Analysis.cpp:107,scalab,scalable,107,interpreter/llvm-project/llvm/lib/CodeGen/Analysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/Analysis.cpp,2,['scalab'],['scalable']
Performance,"// If the PHI is of volatile loads and the load block has multiple; // successors, sinking it would remove a load of the volatile value from; // the path through the other successor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp:29,load,loads,29,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,6,['load'],"['load', 'loads']"
Performance,"// If the ParentPath has not yet been resolved, resolve and cache it for; // future look-ups.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DWARFLinker/Classic/DWARFLinkerDeclContext.h:60,cache,cache,60,interpreter/llvm-project/llvm/include/llvm/DWARFLinker/Classic/DWARFLinkerDeclContext.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DWARFLinker/Classic/DWARFLinkerDeclContext.h,2,['cache'],['cache']
Performance,"// If the RHS (False) is 0, we swap the order of the operands; // of ISD::SELECT (obviously also inverting the condition) so that we can; // take advantage of conditional moves using the $0 register.; // Example:; // return (a != 0) ? x : 0;; // load $reg, x; // movz $reg, $0, a",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:246,load,load,246,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['load']
Performance,"// If the Rt of the second instruction (destination register of the; // load) was not modified or used between the two instructions and none; // of the instructions between the second and first alias with the; // second, we can combine the second into the first.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:72,load,load,72,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the SSAUpdater didn't use the load in the preheader, just zap it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If the SelectOptimize pass is enabled, cmovs have already been optimized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp:66,optimiz,optimized,66,interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86CmovConversion.cpp,1,['optimiz'],['optimized']
Performance,"// If the SelectOptimize pass is enabled, selects have already been optimized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:68,optimiz,optimized,68,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimized']
Performance,"// If the TClass is being generated from a ROOT dictionary,; // even though we do not seem to have a CINT dictionary for; // the class, we will will try to load it anyway UNLESS; // the class is an STL container (or string).; // This is because we do not expect the CINT dictionary; // to be present for all STL classes (and we can handle; // the lack of CINT dictionary in that cases).; // However, the cling the dictionary no longer carries; // an instantiation with it, unless we request the loading; // here *or* the user explicitly instantiate the template; // we would not have a ClassInfo for the template; // instantiation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:156,load,load,156,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,2,['load'],"['load', 'loading']"
Performance,"// If the TStreamerElement we found is storing the information in the; // cache and is a repeater, we need to use the real one (the next one).; // (At least until the cache/repeat mechanism is properly handle by; // ReadLeaves).; // fID = i+1;",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBranchElement.cxx:74,cache,cache,74,tree/tree/src/TBranchElement.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBranchElement.cxx,2,['cache'],['cache']
Performance,"// If the TypoExpr hasn't been seen before, record it. Otherwise, return the; // cached transformation result if there is one and the TypoExpr isn't the; // first one that was encountered.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:81,cache,cached,81,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['cache'],['cached']
Performance,"// If the VersionMap has already been loaded or if there is no dynamic symtab; // or version table, there is nothing to do.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/ELFDumper.cpp:38,load,loaded,38,interpreter/llvm-project/llvm/tools/llvm-readobj/ELFDumper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-readobj/ELFDumper.cpp,1,['load'],['loaded']
Performance,"// If the Virtual Function Elim module flag is present and set to zero, then; // the vcall_visibility metadata was inserted for another optimization (WPD); // and we may not have type checked loads on all accesses to the vtable.; // Don't attempt VFE in that case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalDCE.cpp:136,optimiz,optimization,136,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalDCE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalDCE.cpp,2,"['load', 'optimiz']","['loads', 'optimization']"
Performance,"// If the XCOFF object file does not have a loader section, it is not; // loadable, so align at the minimum value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp:44,load,loader,44,interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp,2,['load'],"['loadable', 'loader']"
Performance,"// If the `load` is not simple, we can't speculatively execute it,; // but we could handle this via a CFG modification. But can we?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,"// If the actual instruction is a branch, the only thing that remains to be; // checked is whether the CCUser chain is a predecessor of the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp:140,load,load,140,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,1,['load'],['load']
Performance,// If the address is not even local to this DSO we will have to load it from; // a got and then add the offset.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:64,load,load,64,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['load'],['load']
Performance,"// If the address mode is DS-Form or DQ-Form, check if the FI is aligned.; // Select an X-Form load if it is not.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:95,load,load,95,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If the alignment for the load is 16 or bigger, we don't need the; // permutated mask to get the required value. The value must be the 0; // element in big endian target or 7/15 in little endian target in the; // result vsx register of lvx instruction.; // Select the instruction in the .td file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If the alignment requirements of the scaled wide load/store; // instruction can't express the offset of the scaled narrow input,; // bail and keep looking. For promotable zero stores, allow only when; // the stored value is the same (i.e., WZR).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the alloca describes the variable itself, i.e. the expression in the; // dbg.declare doesn't start with a dereference, we can perform the; // conversion if the value covers the entire fragment of DII.; // If the alloca describes the *address* of DIVar, i.e. DIExpr is; // *just* a DW_OP_deref, we use DV as is for the dbg.value.; // We conservatively ignore other dereferences, because the following two are; // not equivalent:; // dbg.declare(alloca, ..., !Expr(deref, plus_uconstant, 2)); // dbg.value(DV, ..., !Expr(deref, plus_uconstant, 2)); // The former is adding 2 to the address of the variable, whereas the latter; // is adding 2 to the value of the variable. As such, we insist on just a; // deref expression.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp:132,perform,perform,132,interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,2,['perform'],['perform']
Performance,"// If the alloca is only read and written in one basic block, just perform a; // linear sweep over the block to eliminate it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:67,perform,perform,67,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,1,['perform'],['perform']
Performance,"// If the argument doesn't match, perform a bitcast to coerce it. This; // can happen due to trivial type mismatches.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:34,perform,perform,34,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,['perform'],['perform']
Performance,"// If the argument type is a class template specialization, we; // perform template argument deduction using its template; // arguments.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp:67,perform,perform,67,interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,1,['perform'],['perform']
Performance,"// If the assumptions about the DECL_PARM_VAR abbrev are true, use it. Here; // we dynamically check for the properties that we optimize for, but don't; // know are true of all PARM_VAR_DECLs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriterDecl.cpp:128,optimiz,optimize,128,interpreter/llvm-project/clang/lib/Serialization/ASTWriterDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriterDecl.cpp,1,['optimiz'],['optimize']
Performance,"// If the atomicrmw's result is used by a single bit AND, we may use; // bts/btr/btc instruction for these operations.; // Note: InstCombinePass can cause a de-optimization here. It replaces the; // SETCC(And(AtomicRMW(P, power_of_2), power_of_2)) with LShr and Xor; // (depending on CC). This pattern can only use bts/btr/btc but we don't; // detect it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:160,optimiz,optimization,160,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// If the attribute is named `directive`, we can consume its argument list; // and push the tokens from it into the cached token stream for a new OpenMP; // pragma directive.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseDeclCXX.cpp:116,cache,cached,116,interpreter/llvm-project/clang/lib/Parse/ParseDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseDeclCXX.cpp,1,['cache'],['cached']
Performance,"// If the auxiliary header does not have both MaxAlignOfData and; // MaxAlignOfText field, it is not a loadable shared object file, so align at; // the minimum value. The 'ModuleType' member is located right after; // 'MaxAlignOfData' in the AuxiliaryHeader.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp:103,load,loadable,103,interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp,1,['load'],['loadable']
Performance,"// If the available queue is empty, it is safe to reset MinAvailableCycle.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:20,queue,queue,20,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,1,['queue'],['queue']
Performance,"// If the available queue is empty, it is safe to reset MinReadyCycle.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:20,queue,queue,20,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,2,['queue'],['queue']
Performance,"// If the base class doesn't have a simple move assignment, we'll eagerly; // declare it and perform overload resolution to determine which function; // it actually calls. If it does have a simple move assignment, this; // check is correct.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp:93,perform,perform,93,interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp,1,['perform'],['perform']
Performance,"// If the base class doesn't have a simple move constructor, we'll eagerly; // declare it and perform overload resolution to determine which function; // it actually calls. If it does have a simple move constructor, this; // check is correct.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp:94,perform,perform,94,interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclCXX.cpp,1,['perform'],['perform']
Performance,"// If the base register overlaps a source/destination register, we can't; // merge the update. This does not apply to tag store instructions which; // ignore the address part of the source register.; // This does not apply to STGPi as well, which does not have unpredictable; // behavior in this case unlike normal stores, and always performs writeback; // after reading the source register value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:334,perform,performs,334,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['perform'],['performs']
Performance,"// If the begin/end of the range lie in the same FileID, do the optimization; // where we skip preprocessed entities that do not come from the same; // FileID.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp:64,optimiz,optimization,64,interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndex.cpp,1,['optimiz'],['optimization']
Performance,"// If the bit set is sufficiently small, we can avoid a load by bit testing; // a constant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp:56,load,load,56,interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,1,['load'],['load']
Performance,"// If the bitcode files contain ARC code and were compiled with optimization,; // the ObjCARCContractPass must be run, so do it unconditionally here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp:64,optimiz,optimization,64,interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/LTO/ThinLTOCodeGenerator.cpp,1,['optimiz'],['optimization']
Performance,"// If the block *is* completely transparent to the load, we need to check; // the predecessors of this block. Add them to our worklist.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:51,load,load,51,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['load'],['load']
Performance,"// If the block didn't have a branch, add all successor edges to the; // work queue. (There should really be only one successor in such case.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstPropagation.cpp:78,queue,queue,78,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstPropagation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonConstPropagation.cpp,1,['queue'],['queue']
Performance,"// If the block got deleted, there is no need for the symbol. If the symbol; // was already emitted, we can just forget about it, otherwise we need to; // queue it up for later emission when the function is output.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp:155,queue,queue,155,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,1,['queue'],['queue']
Performance,"// If the block has a dependency (i.e. it isn't completely transparent to; // the value), remember the reverse association because we just added it; // to Cache!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:155,Cache,Cache,155,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['Cache'],['Cache']
Performance,"// If the block has been evicted out of the queue or it has already been; // marked dead (due to it being predicated), then skip it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp:44,queue,queue,44,interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/IfConversion.cpp,1,['queue'],['queue']
Performance,"// If the body of the case is just a 'break', try to not emit an empty block.; // If we're profiling or we're not optimizing, leave the block in for better; // debug and coverage analysis.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp:114,optimiz,optimizing,114,interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,1,['optimiz'],['optimizing']
Performance,"// If the branch has a condition wrapped by __builtin_unpredictable,; // create metadata that specifies that the branch is unpredictable.; // Don't bother if not optimizing because that metadata would not be used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp:162,optimiz,optimizing,162,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,1,['optimiz'],['optimizing']
Performance,"// If the cache did not contain a suitable object, compile the object",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp:10,cache,cache,10,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,1,['cache'],['cache']
Performance,"// If the cache entry was unresolved, populate it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:10,cache,cache,10,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['cache'],['cache']
Performance,"// If the cache is valid, we're okay.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp:10,cache,cache,10,interpreter/llvm-project/clang/lib/AST/Type.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp,1,['cache'],['cache']
Performance,// If the cache of LibrariesShortNames is not built up do that first for; // all the Libraries.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp:10,cache,cache,10,interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,1,['cache'],['cache']
Performance,"// If the cache size isn't too large, do a lookup and if necessary default; // construct an entry. We can then return it to the caller for direct; // use. When they update the value, the cache will get automatically; // updated as well.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:10,cache,cache,10,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,2,['cache'],['cache']
Performance,"// If the cache was allocated, free it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp:10,cache,cache,10,interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp,1,['cache'],['cache']
Performance,"// If the cached entry is non-dirty, just return it. Note that this depends; // on MemDepResult's default constructing to 'dirty'.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:10,cache,cached,10,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['cache'],['cached']
Performance,"// If the cached range ends before the index at which the current; // aggregate starts, recurse for the parent.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ConstantInitBuilder.cpp:10,cache,cached,10,interpreter/llvm-project/clang/lib/CodeGen/ConstantInitBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ConstantInitBuilder.cpp,1,['cache'],['cached']
Performance,"// If the call is known to never store to the pointer, and if this is a; // load query, we can safely ignore it (scan past it).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:76,load,load,76,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['load'],['load']
Performance,"// If the call result is in ST0 / ST1, it needs to be popped off the x87; // stack. Therefore, if it's not used by the call it is not safe to optimize; // this into a sibcall.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:142,optimiz,optimize,142,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['optimiz'],['optimize']
Performance,"// If the cast operand is a constant vector, perform the cast by; // operating on each element. In the cast of bitcasts, the element; // count may be mismatched; don't attempt to handle that here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp:45,perform,perform,45,interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/ConstantFold.cpp,1,['perform'],['perform']
Performance,"// If the class owning the TStreamerElement and the base class are not; // loaded, on the file their streamer info might be in the following; // order (derived class,base class) and hence the base class is not; // yet emulated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TStreamerElement.cxx:75,load,loaded,75,core/meta/src/TStreamerElement.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TStreamerElement.cxx,1,['load'],['loaded']
Performance,"// If the client is only demanding bits we know to be zero, return; // `llvm.ptrmask(p, 0)`. We can't return `null` here due to pointer; // provenance, but making the mask zero will be easily optimizable in; // the backend.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSimplifyDemanded.cpp:192,optimiz,optimizable,192,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSimplifyDemanded.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSimplifyDemanded.cpp,1,['optimiz'],['optimizable']
Performance,"// If the code isn't optimized, there may be outstanding folding; // opportunities. Attempt to fold the expression using DataLayout as a; // last resort before giving up.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp:21,optimiz,optimized,21,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,2,['optimiz'],['optimized']
Performance,"// If the common expression is an opaque expression, we visit it; // here once so we have its value cached.; // FIXME: This might be necessary (or useful) for all expressions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp:100,cache,cached,100,interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Interp/ByteCodeExprGen.cpp,1,['cache'],['cached']
Performance,"// If the condition is a chain of ORs/AND and the successor only has the; // current block as predecessor, queue conditions for the successor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp:107,queue,queue,107,interpreter/llvm-project/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/ConstraintElimination.cpp,1,['queue'],['queue']
Performance,"// If the condition is in a loop, consider it predictable if the condition; // itself or all its operands are loop-invariant. E.g. this considers a load; // from a loop-invariant address predictable; we were unable to prove that it; // doesn't alias any of the memory-writes in the loop, but it is likely to; // read to same value multiple times.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/EarlyIfConversion.cpp:148,load,load,148,interpreter/llvm-project/llvm/lib/CodeGen/EarlyIfConversion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/EarlyIfConversion.cpp,1,['load'],['load']
Performance,"// If the constant is in range, use a load-immediate.; // Since LI will sign extend the constant we need to make sure that for; // our zeroext constants that the sign extended constant fits into 16-bits -; // a range of 0..0x7fff.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:38,load,load-immediate,38,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,1,['load'],['load-immediate']
Performance,"// If the constant is negative, trying inverting and using our trailing zero; // optimizations. Use an xori to invert the final value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp:81,optimiz,optimizations,81,interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp,1,['optimiz'],['optimizations']
Performance,"// If the constants line up, perform the transform!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:29,perform,perform,29,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If the copy has a glue operand, we conservatively assume it isn't safe to; // perform a tail call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:81,perform,perform,81,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,4,['perform'],['perform']
Performance,"// If the copy isn't mandatory, add !clang.arc.copy_on_escape to; // tell the optimizer that it doesn't need to do this copy if the; // block doesn't escape, where being passed as an argument doesn't; // count as escaping.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:78,optimiz,optimizer,78,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['optimiz'],['optimizer']
Performance,"// If the coroutine frame is an Argument, store it in an alloca to improve; // its availability (e.g. registers may be clobbered).; // Avoid this if optimizations are enabled (they would remove the alloca) or; // if the value is guaranteed to be available through other means (e.g. swift; // ABI guarantees).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp:149,optimiz,optimizations,149,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,1,['optimiz'],['optimizations']
Performance,"// If the corresponding template argument is NULL or non-existent, it's; // because we are performing instantiation from explicitly-specified; // template arguments in a function template, but there were some; // arguments left unspecified.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp:91,perform,performing,91,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,3,['perform'],['performing']
Performance,"// If the current CPSR has high latency, try to avoid the false dependency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp:32,latency,latency,32,interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/Thumb2SizeReduction.cpp,1,['latency'],['latency']
Performance,"// If the current instruction is a load, update MaxWidth to reflect the; // width of the loaded value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If the current value is not known, insert a dummy load and lower it on; // the second pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUPromoteAlloca.cpp,1,['load'],['load']
Performance,"// If the data is already loaded from this address and hasn't been clobbered; // by any stores or calls, this load is likely to be redundant and can be; // eliminated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp:26,load,loaded,26,interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InlineCost.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If the data variable is not referenced by code (if we don't emit; // @llvm.instrprof.value.profile, NS will be 0), and the counter keeps the; // data variable live under linker GC, the data variable can be private. This; // optimization applies to ELF.; //; // On COFF, a comdat leader cannot be local so we require DataReferencedByCode; // to be false.; //; // If profd is in a deduplicate comdat, NS==0 with a hash suffix guarantees; // that other copies must have the same CFG and cannot have value profiling.; // If no hash suffix, other profd copies may be referenced by code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/InstrProfiling.cpp:227,optimiz,optimization,227,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/InstrProfiling.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/InstrProfiling.cpp,1,['optimiz'],['optimization']
Performance,"// If the declaration already has attributes, we assume that some other; // AST file already loaded them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:93,load,loaded,93,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,1,['load'],['loaded']
Performance,"// If the defining load-immediate has no other uses, we can just replace; // the immediate with the new immediate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:19,load,load-immediate,19,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load-immediate']
Performance,"// If the defs from this range reach SI via all predecessors, it is live.; // It can happen that SI is reached by the defs through some paths, but; // not all. In the IR coming into this optimization, SI would not be; // considered live, since the defs would then not jointly dominate SI.; // That means that SI is an overwriting def, and no implicit use is; // needed at this point. Do not add SI to the extension points, since; // extendToIndices will abort if there is no joint dominance.; // If the abort was avoided by adding extra undefs added to Undefs,; // extendToIndices could actually indicate that SI is live, contrary; // to the original IR.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonExpandCondsets.cpp:187,optimiz,optimization,187,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonExpandCondsets.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonExpandCondsets.cpp,1,['optimiz'],['optimization']
Performance,"// If the dependence is to a store that writes to a superset of the bits; // read by the load, we can extract the bits we need for the load from the; // stored value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:89,load,load,89,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],['load']
Performance,"// If the element is just cached and not repeat, we need to inject an element; // to insure the writing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx:26,cache,cached,26,io/io/src/TStreamerInfo.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx,2,['cache'],['cached']
Performance,"// If the element type is i1 and we're not promoting the result, then we may; // end up loading the wrong data since the bits are packed tightly into; // bytes. For example, if we extract a v4i1 (legal) from a nxv4i1 (legal); // type at index 4, then we will load a byte starting at index 0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:88,load,loading,88,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,2,['load'],"['load', 'loading']"
Performance,"// If the element.atomic memcpy is not lowered into explicit; // loads/stores later, then it will be lowered into an element-size; // specific lib call. If the lib call doesn't exist for our store size, then; // we shouldn't generate the memcpy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:65,load,loads,65,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,1,['load'],['loads']
Performance,"// If the end of the cached extent precedes the beginning of the written; // extent, ignore this alloc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp:21,cache,cached,21,interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,1,['cache'],['cached']
Performance,"// If the end of the written extent precedes the beginning of the cached; // extent, ignore this map entry.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp:66,cache,cached,66,interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,1,['cache'],['cached']
Performance,"// If the expression is already an AVRMCExpr (i.e. a lo8(symbol),; // we shouldn't perform any more fixups. Without this check, we would; // instead create a fixup to the symbol named 'lo8(symbol)' which; // is not correct.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/MCTargetDesc/AVRMCCodeEmitter.cpp:83,perform,perform,83,interpreter/llvm-project/llvm/lib/Target/AVR/MCTargetDesc/AVRMCCodeEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/MCTargetDesc/AVRMCCodeEmitter.cpp,1,['perform'],['perform']
Performance,"// If the field is not a pointer, we need to save the actual value; // and load it as a void pointer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp:75,load,load,75,interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmtOpenMP.cpp,1,['load'],['load']
Performance,// If the file of the SLocEntry changed we could still have loaded it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:60,load,loaded,60,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['load'],['loaded']
Performance,"// If the final type is not the same as the loaded type, this means that; // we have to pad with zero. Create a zero extend for that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:44,load,loaded,44,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loaded']
Performance,"// If the first elt is at least as large as what we're looking for, or if the; // first element is the same size as the whole struct, we can enter it. The; // comparison must be made on the store size and not the alloca size. Using; // the alloca size may overstate the size of the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:282,load,load,282,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['load']
Performance,"// If the first operand is a register and it exists in the RegisterMap, we; // know this is a DBG_VALUE that uses the result of a load that was moved,; // and is therefore a candidate to also be moved, add it to the; // RegisterMap and InstrMap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:130,load,load,130,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the first operand of a load matches with a DBG_VALUE in RegisterMap,; // then move that DBG_VALUE to below the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,2,['load'],['load']
Performance,"// If the flag is set to force any use of scalable vectors, override the loop; // hints.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp:42,scalab,scalable,42,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,1,['scalab'],['scalable']
Performance,"// If the font has previously been allocated, but the ""fontValid"" bitmap; // shows it is no longer valid, then mark it for freeing later. We use; // a policy of allocate-before-free because xclass' font cache operates; // much more efficiently that way.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/guihtml/src/TGHtml.cxx:203,cache,cache,203,gui/guihtml/src/TGHtml.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/guihtml/src/TGHtml.cxx,1,['cache'],['cache']
Performance,"// If the full hash value matches, check deeply for a match. The common; // case here is that we are only looking at the buckets (for item info; // being non-null and for the full hash value) not at the items. This; // is important for cache locality.; // Do the comparison like this because Name isn't necessarily; // null-terminated!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/StringMap.cpp:236,cache,cache,236,interpreter/llvm-project/llvm/lib/Support/StringMap.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/StringMap.cpp,1,['cache'],['cache']
Performance,"// If the full hash value matches, check deeply for a match. The common; // case here is that we are only looking at the buckets (for item info; // being non-null and for the full hash value) not at the items. This; // is important for cache locality.; // Do the comparison like this because NameStart isn't necessarily; // null-terminated!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/StringMap.cpp:236,cache,cache,236,interpreter/llvm-project/llvm/lib/Support/StringMap.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/StringMap.cpp,1,['cache'],['cache']
Performance,"// If the function is empty and the object file uses .subsections_via_symbols,; // then we need to emit *something* to the function body to prevent the; // labels from collapsing together. Just emit a noop.; // Similarly, don't emit empty functions on Windows either. It can lead to; // duplicate entries (two functions with the same RVA) in the Guard CF Table; // after linking, causing the kernel not to load the binary:; // https://developercommunity.visualstudio.com/content/problem/45366/vc-linker-creates-invalid-dll-with-clang-cl.html; // FIXME: Hide this behind some API in e.g. MCAsmInfo or MCTargetStreamer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp:406,load,load,406,interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AsmPrinter/AsmPrinter.cpp,1,['load'],['load']
Performance,"// If the function is marked as non-lazy, generate an indirect call; // which loads from the GOT directly. This avoids run-time overhead; // at the cost of eager binding.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kSubtarget.cpp:78,load,loads,78,interpreter/llvm-project/llvm/lib/Target/M68k/M68kSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kSubtarget.cpp,1,['load'],['loads']
Performance,"// If the function is marked as non-lazy, generate an indirect call; // which loads from the GOT directly. This avoids runtime overhead; // at the cost of eager binding (and one extra byte of encoding).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86Subtarget.cpp:78,load,loads,78,interpreter/llvm-project/llvm/lib/Target/X86/X86Subtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86Subtarget.cpp,1,['load'],['loads']
Performance,"// If the function is not ending in aligned barriers, we need the stores to; // be in aligned barriers. The load being in one is not sufficient since the; // store might be executed by a thread that disappears after, causing the; // aligned barrier guarding the load to unblock and the load to read a value; // that has no CFG path to the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:108,load,load,108,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,4,['load'],['load']
Performance,"// If the function return type is x86_fp80 and the callee return type is not,; // then the FP_EXTEND of the call result is not a nop. It's not safe to; // perform a tailcall optimization here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:155,perform,perform,155,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"// If the function template has a non-dependent explicit specification,; // exclude it now if appropriate; we are not permitted to perform deduction; // and substitution in this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:131,perform,perform,131,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,2,['perform'],['perform']
Performance,"// If the function we are wrapping was ExternWeak, it may be null.; // The original code before calling this wrapper may have checked for null,; // but replacing with a known-to-not-be-null wrapper can break this check.; // When replacing uses of the extern weak function with the wrapper we try; // to avoid replacing uses in conditionals, but this is not perfect.; // In the case where we fail, and accidentally optimize out a null check; // for a extern weak function, add a check here to help identify the issue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp:414,optimiz,optimize,414,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/DataFlowSanitizer.cpp,1,['optimiz'],['optimize']
Performance,"// If the given branch is recognized as a foldable branch (i.e. conditional; // branch with constant condition), it will perform following analyses and; // transformation.; // 1) If the dead out-coming edge is a critical-edge, split it. Let; // R be the target of the dead out-coming edge.; // 1) Identify the set of dead blocks implied by the branch's dead outcoming; // edge. The result of this step will be {X| X is dominated by R}; // 2) Identify those blocks which haves at least one dead predecessor. The; // result of this step will be dominance-frontier(R).; // 3) Update the PHIs in DF(R) by replacing the operands corresponding to; // dead blocks with ""UndefVal"" in an hope these PHIs will optimized away.; //; // Return true iff *NEW* dead code are found.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:121,perform,perform,121,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,"['optimiz', 'perform']","['optimized', 'perform']"
Performance,"// If the global is never loaded (but may be stored to), it is dead.; // Delete it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:26,load,loaded,26,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['loaded']
Performance,"// If the hash isn't in the cache, call a runtime handler to perform the; // hard work of checking whether the vptr is for an object of the right; // type. This will either fill in the cache and return, or produce a; // diagnostic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:28,cache,cache,28,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,3,"['cache', 'perform']","['cache', 'perform']"
Performance,"// If the hash of top-level entities differs from the hash of the top-level; // entities the last time we rebuilt the preamble, clear out the completion; // cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:157,cache,cache,157,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['cache'],['cache']
Performance,"// If the home of the module is the current working directory, then we; // want to pick up the cwd of the build process loading the module, not; // our cwd, when we load this module.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp:120,load,loading,120,interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,2,['load'],"['load', 'loading']"
Performance,"// If the immediate value is too big for the immoffset field, put only bits; // that would normally fit in the immoffset field. The remaining value that; // is copied/added for the voffset field is a large power of 2, and it; // stands more chance of being CSEd with the copy/add for another similar; // load/store.; // However, do not do that rounding down if that is a negative; // number, as it appears to be illegal to have a negative offset in the; // vgpr, even if adding the immediate offset makes it positive.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:304,load,load,304,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,3,['load'],['load']
Performance,"// If the incoming type is a vector that is being promoted, then; // we know that the elements are arranged differently and that we; // must perform the conversion using a stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:141,perform,perform,141,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['perform'],['perform']
Performance,"// If the index is dependent on the store we will introduce a cycle when; // creating the load (the load uses the index, and by replacing the chain; // we will make the index dependent on the load). Also, the store might be; // dependent on the extractelement and introduce a cycle when creating; // the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:90,load,load,90,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,4,['load'],['load']
Performance,"// If the initial extent of the cached item is beyond the ending extent; // of the request, there is no overlap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp:32,cache,cached,32,interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,1,['cache'],['cached']
Performance,"// If the initial extent of the request is beyond the ending extent of; // the cached item, there is no overlap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp:79,cache,cached,79,interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,1,['cache'],['cached']
Performance,"// If the initial strategy fails for any of the operand indexes, then we; // perform reordering again in a second pass. This helps avoid assigning; // high priority to the failed strategy, and should improve reordering for; // the non-failed operand indexes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:77,perform,perform,77,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['perform'],['perform']
Performance,"// If the initializer is an ExtVecEltExpr (a swizzle), and the swizzle's; // input is the same width as the vector being constructed, generate an; // optimized shuffle of the swizzle input into the result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:150,optimiz,optimized,150,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['optimiz'],['optimized']
Performance,"// If the initializer of the extending declaration is a constant; // initializer, we should have a cached constant initializer for this; // temporary. Note that this might have a different value from the value; // computed by evaluating the initializer if the surrounding constant; // expression modifies the temporary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp:99,cache,cached,99,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,1,['cache'],['cached']
Performance,"// If the input is a concat_vectors, just make a larger concat by padding; // with smaller undefs.; //; // Legalizing in AArch64TargetLowering::LowerCONCAT_VECTORS() and combining; // here could cause an infinite loop. That legalizing happens when LegalDAG; // is true and input of AArch64TargetLowering::LowerCONCAT_VECTORS() is; // scalable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:334,scalab,scalable,334,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['scalab'],['scalable']
Performance,"// If the input load is an extending load, it will be an i32 -> i64; // extending load and isValidSplatLoad() will update NewOpcode.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,3,['load'],['load']
Performance,"// If the input to XVCVDPSP is a vector that was built (even; // partially) out of FRSP's, the FRSP(s) can safely be removed; // since this instruction performs the same operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:152,perform,performs,152,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,1,['perform'],['performs']
Performance,"// If the input vector is a concatenation, and the insert replaces; // one of the pieces, we can optimize into a single concat_vectors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:97,optimiz,optimize,97,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['optimiz'],['optimize']
Performance,"// If the inserted element was extracted from some other fixed-length vector; // and both indexes are valid constants, try to turn this into a shuffle.; // Can not handle scalable vector type, the number of elements needed to; // create shuffle mask is not a compile-time constant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp:171,scalab,scalable,171,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp,1,['scalab'],['scalable']
Performance,"// If the instruction doesn't load at all, it isn't an invariant load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,2,['load'],['load']
Performance,"// If the instruction has a latency of 0, we need to handle; // the execution and retirement now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/Stages/InOrderIssueStage.cpp:28,latency,latency,28,interpreter/llvm-project/llvm/lib/MCA/Stages/InOrderIssueStage.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/Stages/InOrderIssueStage.cpp,1,['latency'],['latency']
Performance,"// If the instruction has a non-zero latency dependence with an instruction in; // the current packet, then it should not be scheduled yet. The case occurs; // when the dependent instruction is scheduled in a new packet, so the; // scheduler updates the current cycle and pending instructions become; // available.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/VLIWMachineScheduler.cpp:37,latency,latency,37,interpreter/llvm-project/llvm/lib/CodeGen/VLIWMachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/VLIWMachineScheduler.cpp,1,['latency'],['latency']
Performance,"// If the instruction has live CPSR def, then it's not safe to fold it; // into load / store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:80,load,load,80,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the instruction has lost its memoperands, conservatively assume that; // it may not be an invariant load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp:106,load,load,106,interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,1,['load'],['load']
Performance,"// If the instruction instruction neither has float exception nor is; // a load/store instruction, or the instruction is x87 control; // instruction, do not insert wait.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InsertWait.cpp:75,load,load,75,interpreter/llvm-project/llvm/lib/Target/X86/X86InsertWait.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InsertWait.cpp,1,['load'],['load']
Performance,"// If the instruction is a part of the GOT to PC-Rel link time optimization; // instruction pair, return a value, otherwise return std::nullopt. A true; // returned value means the instruction is the PLDpc and a false value means; // it is the user instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCELFStreamer.cpp:63,optimiz,optimization,63,interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCELFStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/MCTargetDesc/PPCELFStreamer.cpp,1,['optimiz'],['optimization']
Performance,"// If the instruction sets the flag, do not attempt this optimization; // since it may change the semantics of the code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:57,optimiz,optimization,57,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['optimiz'],['optimization']
Performance,"// If the instruction uses a register pair but we got a single, lower; // register we perform a ""class cast"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AsmParser/AVRAsmParser.cpp:86,perform,perform,86,interpreter/llvm-project/llvm/lib/Target/AVR/AsmParser/AVRAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AsmParser/AVRAsmParser.cpp,1,['perform'],['perform']
Performance,// If the instruction wasn't a matching load or store. Stop searching if we; // encounter a call instruction that might modify memory.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the latency is 0 and there is a data dependence between this; // instruction and any instruction in the current packet, we disregard any; // potential stalls due to the instructions in the previous packet. Most of; // the instruction pairs that can go together in the same packet have 0; // latency between them. The exceptions are; // 1. NewValueJumps as they're generated much later and the latencies can't; // be changed at that point.; // 2. .cur instructions, if its consumer has a 0 latency successor (such as; // .new). In this case, the latency between .cur and the consumer stays; // non-zero even though we can have both .cur and .new in the same packet.; // Changing the latency to 0 is not an option as it causes software pipeliner; // to not pipeline in some cases.; // For Example:; // {; // I1: v6.cur = vmem(r0++#1); // I2: v7 = valign(v6,v4,r2); // I3: vmem(r5++#1) = v7.new; // }; // Here I2 and I3 has 0 cycle latency, but I1 and I2 has 2.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp:10,latency,latency,10,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVLIWPacketizer.cpp,6,['latency'],['latency']
Performance,"// If the load address doesn't alias the given address, it doesn't read; // or write the specified memory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/AliasAnalysis.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Analysis/AliasAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/AliasAnalysis.cpp,1,['load'],['load']
Performance,"// If the load aligning is disabled or the load can be broken up into two; // smaller legal loads, do the default (target-independent) expansion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,3,['load'],"['load', 'loads']"
Performance,"// If the load and store are consecutive, use the loadInst location to; // reduce register pressure.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp,2,['load'],"['load', 'loadInst']"
Performance,"// If the load and store are in different blocks, use BB dominance to; // check their relationships. If the store doesn't dom the use, bail; // out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,1,['load'],['load']
Performance,"// If the load command is present but the data offset has been zeroed out,; // as is the case for dylib stubs, return std::nullopt (no error).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,1,['load'],['load']
Performance,"// If the load contains vectors, build the vector using concat vector.; // All of the vectors used to load are power-of-2, and the scalar loads can be; // combined to make a power-of-2 vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,3,['load'],"['load', 'loads']"
Performance,"// If the load does a pre/post increment, then insert a MOV after as; // well to update the real base register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64FalkorHWPFFix.cpp,1,['load'],['load']
Performance,"// If the load has a builtin retain, insert a plain retain for it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,2,['load'],['load']
Performance,"// If the load has other users and the truncate is not free, the ext; // probably isn't free.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h:10,load,load,10,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,1,['load'],['load']
Performance,"// If the load instruction reads directly from the address to which the; // store instruction writes and the stored value is not modified, we can; // promote the load. Since we do not handle stores with pre-/post-index,; // it's unnecessary to check if BaseReg is modified by the store itself.; // Also we can't handle stores without an immediate offset operand,; // while the operand might be the address for a global variable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,2,['load'],['load']
Performance,// If the load is conditional we can't hoist its 0-iteration instance to; // the preheader because that would make it unconditional. Thus we would; // access a memory location that the original loop did not access.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopLoadElimination.cpp,1,['load'],['load']
Performance,"// If the load is defined in a block with exactly one predecessor, it can't be; // partially redundant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// If the load is defined in an EH pad, it can't be partially redundant,; // because the edges between the invoke and the EH pad cannot have other; // instructions between them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// If the load is shifted left (and the result isn't shifted back right), we; // can fold a truncate through the shift. The typical scenario is that N; // points at a TRUNCATE here so the attempted fold is:; // (truncate (shl (load x), c))) -> (shl (narrow load x), c); // ShLeftAmt will indicate how much a narrowed load should be shifted left.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],['load']
Performance,"// If the load is the first instruction in the block, there's obviously; // not any matching store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the load is volatile, we only want to change the load type if the; // resulting load is legal. Otherwise we might increase the number of; // memory accesses. We don't care if the original type was legal or not; // as we assume software couldn't rely on the number of accesses of an; // illegal type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['load'],['load']
Performance,"// If the load of the hash table is more than 3/4, or if fewer than 1/8 of; // the buckets are empty (meaning that many are filled with tombstones),; // grow the table.; //; // The later case is tricky. For example, if we had one empty bucket with; // tons of tombstones, failing lookups (e.g. for insertion) would have to; // probe almost the entire table until it found the empty bucket. If the; // table completely filled with tombstones, no lookup would ever succeed,; // causing infinite loops in lookup.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/DenseMap.h:10,load,load,10,interpreter/llvm-project/llvm/include/llvm/ADT/DenseMap.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/DenseMap.h,1,['load'],['load']
Performance,"// If the load ptrs can be decomposed into a common (Base + Index) with a; // common constant stride, then return the constant stride.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,"// If the load return value 0 has more than one user except the; // shufflevector instruction, it is not profitable to replace the; // shufflevector with a reverse load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,// If the load that we're shrinking is an extload and we're not just; // discarding the extension we can't simply shrink the load. Bail.; // TODO: It would be possible to merge the extensions in some cases.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// If the load type was an EXTLOAD, convert to ZEXTLOAD in order to; // preserve semantics once we get rid of the AND.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If the load value is used only by N, replace it via CombineTo N.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['load'],['load']
Performance,"// If the load was marked as nonnull we don't want to lose that information; // when we erase this Load. So we preserve it with an assume. As !nonnull; // returns poison while assume violations are immediate undefined behavior,; // we can only do this if the value is known non-poison.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,2,"['Load', 'load']","['Load', 'load']"
Performance,"// If the load/store is the first instruction in the block, there's obviously; // not any matching update. Ditto if the memory offset isn't zero.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:10,load,load,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the loaded operand is defined in the LoadBB and its not a phi,; // it can't be available in predecessors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:10,load,loaded,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,2,"['Load', 'load']","['LoadBB', 'loaded']"
Performance,"// If the loaded value is smaller than the available value, then we can; // extract out a piece from it. If the available value is too small, then we; // can't do anything.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp:10,load,loaded,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,1,['load'],['loaded']
Performance,"// If the loaded value isn't available in any predecessor, it isn't partially; // redundant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:10,load,loaded,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['loaded']
Performance,"// If the loaded/stored value is a first class array/struct, or scalable type,; // don't try to transform them. We need to be able to bitcast to integer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp:10,load,loaded,10,interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/VNCoercion.cpp,4,"['load', 'scalab']","['loaded', 'scalable']"
Performance,"// If the loads are reversed, see if we can rotate the halves into place.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:10,load,loads,10,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,"// If the logical-not of the result is required, perform that now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:49,perform,perform,49,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['perform'],['perform']
Performance,"// If the loop's critical path involves loop-carried dependences, the gradient; // of the gain needs to be at least GainGradientThreshold% (defaults to 25%).; // This check ensures that the latency reduction for the loop's critical path; // keeps decreasing with sufficient rate beyond the two analyzed loop; // iterations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp:190,latency,latency,190,interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,1,['latency'],['latency']
Performance,"// If the lower bound was in something that no longer dominates us, we; // have to reset it.; // We can't simply track stack size, because the stack may have had; // pushes/pops in the meantime.; // XXX: This is non-optimal, but only is slower cases with heavily; // branching dominator trees. To get the optimal number of queries would; // be to make lowerbound and lastkill a per-loc stack, and pop it until; // the top of that stack dominates us. This does not seem worth it ATM.; // A much cheaper optimization would be to always explore the deepest; // branch of the dominator tree first. This will guarantee this resets on; // the smallest set of blocks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:502,optimiz,optimization,502,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['optimiz'],['optimization']
Performance,"// If the macro expansion of (d)div(u) or (d)rem(u) would always trap or; // break, insert the trap/break and exit. This gives a different result to; // GAS. GAS has an inconsistency/missed optimization in that not all cases; // are handled equivalently. As the observed behaviour is the same, we're ok.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp:190,optimiz,optimization,190,interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/AsmParser/MipsAsmParser.cpp,1,['optimiz'],['optimization']
Performance,"// If the main source file was not remapped, load it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:45,load,load,45,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['load'],['load']
Performance,"// If the mask is all ones or undefs, this is a plain vector load of the 1st; // argument.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp:61,load,load,61,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,1,['load'],['load']
Performance,"// If the mask is known to be all ones, optimize to an unmasked intrinsic;; // the selection of the masked intrinsics doesn't do this for us.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:40,optimiz,optimize,40,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,4,['optimiz'],['optimize']
Performance,"// If the member doesn't have an auxiliary header, it isn't a loadable object; // and so it just needs aligning at the minimum value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp:62,load,loadable,62,interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/ArchiveWriter.cpp,1,['load'],['loadable']
Performance,"// If the member wasn't found in the cache, lazily construct and add it to the; // type (used when a limited form of the type is emitted).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp:37,cache,cache,37,interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,1,['cache'],['cache']
Performance,"// If the memory can't be changed, then loads of the memory can't be; // clobbered.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:40,load,loads,40,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['load'],['loads']
Performance,"// If the memory operand has ordering side effects, we can't move the; // instruction. Such an instruction is technically an invariant load,; // but the caller code would need updated to expect that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp:135,load,load,135,interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,1,['load'],['load']
Performance,"// If the metadata doesn't explicitly specify whether to enable scalable; // vectorization, then decide based on the following criteria (increasing; // level of priority):; // - Target default; // - Metadata width; // - Force option (always overrides)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp:64,scalab,scalable,64,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,1,['scalab'],['scalable']
Performance,"// If the method is -retain, and the receiver's being loaded from; // a __weak variable, peephole the entire operation to objc_loadWeakRetained.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:54,load,loaded,54,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['load'],['loaded']
Performance,"// If the method is declared on a class that has a non-invariant; // type parameter, don't warn about parameter mismatches after performing; // substitution. This prevents warning when the programmer has purposely; // casted the receiver to a super type or unspecialized type but the analyzer; // has a more precise tracked type than the programmer intends at the call; // site.; //; // For example, consider NSArray (which has a covariant type parameter); // and NSMutableArray (a subclass of NSArray where the type parameter is; // invariant):; // NSMutableArray *a = [[NSMutableArray<NSString *> alloc] init;; //; // [a containsObject:number]; // Safe: -containsObject is defined on NSArray.; // NSArray<NSObject *> *other = [a arrayByAddingObject:number] // Safe; //; // [a addObject:number] // Unsafe: -addObject: is defined on NSMutableArray; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/DynamicTypePropagation.cpp:129,perform,performing,129,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/DynamicTypePropagation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/DynamicTypePropagation.cpp,1,['perform'],['performing']
Performance,"// If the module cache directory doesn't exist at all and the; // object file is inside a static library, we assume that the; // static library was built on a different machine. We don't want; // to discourage module debugging for convenience libraries within; // a project though.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/DwarfLinkerForBinary.cpp:17,cache,cache,17,interpreter/llvm-project/llvm/tools/dsymutil/DwarfLinkerForBinary.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/DwarfLinkerForBinary.cpp,1,['cache'],['cache']
Performance,"// If the module is in either the ""loaded"" or ""finalized"" sections it; // has been loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.h:35,load,loaded,35,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.h,2,['load'],['loaded']
Performance,"// If the module's parent directory exists, we assume that the; // module cache has expired and was pruned by clang. A more; // adventurous dsymutil would invoke clang to rebuild the module; // now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/DwarfLinkerForBinary.cpp:74,cache,cache,74,interpreter/llvm-project/llvm/tools/dsymutil/DwarfLinkerForBinary.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/DwarfLinkerForBinary.cpp,1,['cache'],['cache']
Performance,"// If the nested loop is an innermost loop, prefer to a 32-byte alignment,; // so that we can decrease cache misses and branch-prediction misses.; // Actual alignment of the loop will depend on the hotness check and other; // logic in alignBlocks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:103,cache,cache,103,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['cache'],['cache']
Performance,"// If the new LLT cannot cover all of the remaining bits, then consider; // issuing a (or a pair of) unaligned and overlapping load / store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:127,load,load,127,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['load'],['load']
Performance,"// If the new VT cannot cover all of the remaining bits, then consider; // issuing a (or a pair of) unaligned and overlapping load / store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:126,load,load,126,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['load'],['load']
Performance,"// If the new constant immediate is all-zeros or all-ones, let the target; // independent DAG combine optimize this node.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:102,optimiz,optimize,102,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['optimiz'],['optimize']
Performance,"// If the next instruction is different, or if there are no other pairs,; // emit a remark for the collated subset. e.g.; // [(load, vf1), (load, vf2))]; // to emit:; // remark: invalid costs for 'load' at VF=(vf, vf2)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:127,load,load,127,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,3,['load'],['load']
Performance,"// If the node is the paired load/store intrinsics, compute flags for; // address computation and return early.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If the notional 'delete this' expression requires a non-trivial; // conversion from 'this' to the type of a destroying operator delete's; // first parameter, perform that conversion now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:161,perform,perform,161,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['perform']
Performance,// If the number of vector iteration between the store and the load are; // small we could incur conflicts.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:63,load,load,63,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,1,['load'],['load']
Performance,// If the object cache is enabled then set a custom compile function; // creator to use the cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp:17,cache,cache,17,interpreter/llvm-project/llvm/tools/lli/lli.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/lli/lli.cpp,2,['cache'],['cache']
Performance,"// If the object was constructed with a constructor, its value is a; // LazyCompoundVal. If it's a raw CompoundVal, it means that we're; // performing aggregate initialization. The only exception from this; // rule is sending an Objective-C++ message that returns a C++ object; // to a nil receiver; in this case the semantics is to return a; // zero-initialized object even if it's a C++ object that doesn't have; // this sort of constructor; the CompoundVal is empty in this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:140,perform,performing,140,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,1,['perform'],['performing']
Performance,"// If the offset constant didn't happen to dominate the load/store, we can; // just clone it as needed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:56,load,load,56,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// If the offset is more than 24-bits, it won't fit in a scattered; // relocation offset field, so we fall back to using a non-scattered; // relocation. This is a bit risky, as if the offset reaches out of; // the block and the linker is doing scattered loading on this; // symbol, things can go badly.; //; // Required for 'as' compatibility.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp:254,load,loading,254,interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MachObjectWriter.cpp,1,['load'],['loading']
Performance,// If the offset is too big we have to adjust and restore the frame pointer; // to materialize a valid load/store with displacement.; //: TODO: consider using only one adiw/sbiw chain for more than one frame; //: index,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRRegisterInfo.cpp:103,load,load,103,interpreter/llvm-project/llvm/lib/Target/AVR/AVRRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AVR/AVRRegisterInfo.cpp,1,['load'],['load']
Performance,"// If the old exception specification hasn't been parsed yet, or the new; // exception specification can't be computed yet, remember that we need to; // perform this check when we get to the end of the outermost; // lexically-surrounding class.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExceptionSpec.cpp:153,perform,perform,153,interpreter/llvm-project/clang/lib/Sema/SemaExceptionSpec.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExceptionSpec.cpp,1,['perform'],['perform']
Performance,"// If the only user of the value is a scalable vector splat, it is; // preferable to do a replicating load (ld1r*).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,scalab,scalable,38,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,"['load', 'scalab']","['load', 'scalable']"
Performance,"// If the operand is a fixed-length vector, convert to a scalable one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:57,scalab,scalable,57,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// If the operands are different or the input is not a load and has more; // uses than just this BV node, then it isn't a splat.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:55,load,load,55,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If the oprofile daemon is not running, don't load the opagent library",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/OProfileJIT/OProfileWrapper.cpp:48,load,load,48,interpreter/llvm-project/llvm/lib/ExecutionEngine/OProfileJIT/OProfileWrapper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/OProfileJIT/OProfileWrapper.cpp,1,['load'],['load']
Performance,"// If the optimization level is not supported, fall back on the default; // optimization",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInvocation.cpp:10,optimiz,optimization,10,interpreter/llvm-project/clang/lib/Frontend/CompilerInvocation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInvocation.cpp,2,['optimiz'],['optimization']
Performance,"// If the optimization was successful, we can't optimize any other; // branches because doing so would clobber the NZCV flags.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp:10,optimiz,optimization,10,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64CondBrTuning.cpp,2,['optimiz'],"['optimization', 'optimize']"
Performance,"// If the original argument was split (e.g. i128), we need; // to load all parts of it here (using the same address).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:66,load,load,66,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['load']
Performance,"// If the original argument was split and passed by reference (e.g. i128; // on RV32), we need to load all parts of it here (using the same; // address). Vectors may be partly split to registers and partly to the; // stack, in which case the base address is partly offset and subsequent; // stores are relative to that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:98,load,load,98,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,"// If the original argument was split and passed by reference, we need to; // load all parts of it here (using the same address).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp:78,load,load,78,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchISelLowering.cpp,1,['load'],['load']
Performance,"// If the original import came from a file explicitly generated by the user,; // don't check the diagnostic mappings.; // FIXME: currently this is approximated by checking whether this is not a; // module import of an implicitly-loaded module file.; // Note: ModuleMgr.rbegin() may not be the current module, but it must be in; // the transitive closure of its imports, since unrelated modules cannot be; // imported until after this module finishes validation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:229,load,loaded,229,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loaded']
Performance,"// If the original load and store to xmm/ymm were consecutive; // then the partial copies were also created in; // a consecutive order to reduce register pressure,; // and the location of the last load is before the last store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86AvoidStoreForwardingBlocks.cpp,2,['load'],['load']
Performance,"// If the original load is NON_EXTLOAD, the hi part load must be ZEXTLOAD.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,2,['load'],['load']
Performance,"// If the original lookup table does not have local linkage and is; // not dso_local, do not generate a relative lookup table.; // This optimization creates a relative lookup table that consists of; // offsets between the start of the lookup table and its elements.; // To be able to generate these offsets, relative lookup table and; // its elements should have internal linkage and be dso_local, which means; // that they should resolve to symbols within the same linkage unit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp:136,optimiz,optimization,136,interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp,1,['optimiz'],['optimization']
Performance,"// If the original shl may be shifting out bits, do not perform this; // transformation.; // TODO: Add MaskedValueIsZero check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:56,perform,perform,56,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['perform'],['perform']
Performance,"// If the other operand is a TLS address, we should fold it instead.; // This produces; // movl %gs:0, %eax; // leal i@NTPOFF(%eax), %eax; // instead of; // movl $i@NTPOFF, %eax; // addl %gs:0, %eax; // if the block also has an access to a second TLS address this will save; // a load.; // FIXME: This is probably also true for non-TLS addresses.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:280,load,load,280,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If the parameter is a c++ class type and it has to be destructed in the; // callee function, declare the destructor so that it can be called by the; // callee function. Do not perform any direct access check on the dtor here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp:179,perform,perform,179,interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,1,['perform'],['perform']
Performance,"// If the parse faile, cacheSize stays at -1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx:23,cache,cacheSize,23,tree/tree/src/TTree.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTree.cxx,1,['cache'],['cacheSize']
Performance,"// If the path from the entry of the function to each load is free of; // instructions that potentially invalidate the load, we can make the; // transformation!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp:54,load,load,54,interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,2,['load'],['load']
Performance,"// If the personality functions match, then we can perform the; // inlining. Otherwise, we can't inline.; // TODO: This isn't 100% true. Some personality functions are proper; // supersets of others and can be used in place of the other.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:51,perform,perform,51,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['perform'],['perform']
Performance,"// If the pointer is store in VGPRs, then we need to move them to; // SGPRs using v_readfirstlane. This is safe because we only select; // loads with uniform pointers to SMRD instruction so we know the; // pointer value is uniform.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:139,load,loads,139,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,1,['load'],['loads']
Performance,"// If the pointer operand is divergent, then each lane is doing an atomic; // operation on a different address, and we cannot optimize that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:126,optimiz,optimize,126,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,1,['optimiz'],['optimize']
Performance,// If the predicate for the sign- or zero-extended offset is the; // same as the predicate used for this load and the sign-/zero-extension; // was from a 32-bits...,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:105,load,load,105,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['load']
Performance,"// If the preferred size is smaller, then keep the extend but extend; // from the result of the extending load. For example:; // %1:_(s8) = G_LOAD ...; // %2:_(s32) = G_SEXT %1(s8); // %3:_(s64) = G_ANYEXT %1(s8); // ... = ... %3(s64); /// rewrites to:; // %2:_(s32) = G_SEXTLOAD ...; // %3:_(s64) = G_ANYEXT %2:_(s32); // ... = ... %3(s64)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:106,load,load,106,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// If the previous cached token is being merged, delete it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp:19,cache,cached,19,interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp,1,['cache'],['cached']
Performance,"// If the previous instruction will already produce the correct carry, do not; // emit a carry generating instruction. E.g. for G_UADDE/G_USUBE sequences; // generated during legalization of wide add/sub. This optimization depends on; // these sequences not being interrupted by other instructions.; // We have to select the previous instruction before the carry-using; // instruction is deleted by the calling function, otherwise the previous; // instruction might become dead and would get deleted.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:210,optimiz,optimization,210,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['optimiz'],['optimization']
Performance,"// If the previous loads defined a super-reg, then we have to mark earlier; // operands undef; Replicate the super-reg def on the merged instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['loads']
Performance,"// If the previous token could have been a simple key, insert the key token; // into the token queue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/YAMLParser.cpp:95,queue,queue,95,interpreter/llvm-project/llvm/lib/Support/YAMLParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/YAMLParser.cpp,1,['queue'],['queue']
Performance,"// If the qualifiers lost were because we were applying the; // (deprecated) C++ conversion from a string literal to a char*; // (or wchar_t*), then there was no error (C++ 4.2p2). FIXME:; // Ideally, this check would be performed in; // checkPointerTypesForAssignment. However, that would require a; // bit of refactoring (so that the second argument is an; // expression, rather than a type), which should be done as part; // of a larger effort to fix checkPointerTypesForAssignment for; // C++ semantics.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp:221,perform,performed,221,interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,1,['perform'],['performed']
Performance,"// If the query's AATags are inconsistent with the cached one,; // conservatively throw out the cached data and restart the query with; // no tag if needed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:51,cache,cached,51,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,['cache'],['cached']
Performance,"// If the range check checks an i64 value, we cannot optimize it out because; // the i64 index is truncated to an i32, making values over 2^32; // indistinguishable from small numbers. There are also other strange edge; // cases that can arise in practice that we don't want to reason about, so; // conservatively only perform the optimization if the range check is the; // normal case of an i32.gt_u.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyFixBrTableDefaults.cpp:53,optimiz,optimize,53,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyFixBrTableDefaults.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyFixBrTableDefaults.cpp,3,"['optimiz', 'perform']","['optimization', 'optimize', 'perform']"
Performance,"// If the record alignment is less than the type width, we can't enforce a; // aligned load, bail out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp:87,load,load,87,interpreter/llvm-project/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGRecordLayoutBuilder.cpp,1,['load'],['load']
Performance,"// If the reduction can be performed in a smaller type, we need to extend; // the reduction to the wider type before we branch to the original loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanRecipes.cpp:27,perform,performed,27,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanRecipes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanRecipes.cpp,1,['perform'],['performed']
Performance,"// If the register is defined within loop, then we can't perform TP.; // TODO: Check whether this is just a mov of a register that would be; // available.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp:57,perform,perform,57,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,1,['perform'],['perform']
Performance,"// If the register of operand 1 is equal to the Lo register, then swap the; // order of loading the Lo and Hi statements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVExpandPseudoInsts.cpp:88,load,loading,88,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVExpandPseudoInsts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVExpandPseudoInsts.cpp,1,['load'],['loading']
Performance,"// If the relative offsets of each split in the load and; // store match exactly, then we can split them and we; // don't need to remove them here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:48,load,load,48,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,"// If the replacement value is the load, this must occur in unreachable; // code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,2,['load'],['load']
Performance,"// If the result of an integer load is only used by an integer-to-float; // conversion, use a fp load instead and a AdvSIMD scalar {S|U}CVTF instead.; // This eliminates an ""integer-to-vector-move"" UOP and improves throughput.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,"['load', 'throughput']","['load', 'throughput']"
Performance,"// If the result of the load is an illegal type, then we can't build a; // valid chain for reuse since the legalised loads and token factor node that; // ties the legalised loads together uses a different output chain then the; // illegal load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,4,['load'],"['load', 'loads']"
Performance,"// If the result type of vextract is wider than the load, then issue an; // extending load instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// If the resulting expr has constant-addend, this constant-addend is; // desirable to reside at the top of the resulting expression tree. Placing; // constant close to super-expr(s) will potentially reveal some; // optimization opportunities in super-expr(s). Here we do not implement; // this logic intentionally and rely on SimplifyAssociativeOrCommutative; // call later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp:216,optimiz,optimization,216,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp,1,['optimiz'],['optimization']
Performance,"// If the returned value is the load itself, replace with poison. This can; // only happen in dead loops.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// If the root changed (e.g. it was a dead load, update the root).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// If the scope containing the declaration is the translation unit,; // then we'll need to perform our checks based on the matching; // DeclContexts rather than matching scopes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:91,perform,perform,91,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['perform'],['perform']
Performance,"// If the second instruction isn't even a mergable/pairable load/store, bail; // out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:60,load,load,60,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If the second token is a hashhash token, then we need to translate it to; // unknown so the token lexer doesn't try to perform token pasting.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp:122,perform,perform,122,interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,1,['perform'],['perform']
Performance,"// If the select type is not supported, no point optimizing it.; // Instruction selection will take care of it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp:49,optimiz,optimizing,49,interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,1,['optimiz'],['optimizing']
Performance,"// If the set is used internally by RooFit to cache parameters or; // constraints, it is invalidated by object removal. We will keep track; // of its name to remove the cache set later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooWorkspace.cxx:46,cache,cache,46,roofit/roofitcore/src/RooWorkspace.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooWorkspace.cxx,2,['cache'],['cache']
Performance,// If the shift amount is larger than the input type then we're not; // accessing any of the loaded bytes. If the load was a zextload/extload; // then the result of the shift+trunc is zero/undef (handled elsewhere).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:93,load,loaded,93,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],"['load', 'loaded']"
Performance,// If the shift amount is larger than the memory type then we're not; // accessing any of the loaded bytes.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:94,load,loaded,94,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loaded']
Performance,"// If the shift is not a no-op (in which case this should be just a sign; // extend already), the truncated to type is legal, sign_extend is legal; // on that type, and the truncate to that type is both legal and free,; // perform the transform.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:223,perform,perform,223,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['perform'],['perform']
Performance,"// If the sign extend extends from a narrower width than the load's width,; // then we can narrow the load width when we combine to a G_SEXTLOAD.; // Avoid widening the load at all.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:61,load,load,61,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,3,['load'],['load']
Performance,"// If the size of memory access is unknown, do not use it to analysis.; // One example of unknown size memory access is to load/store scalable; // vector objects on the stack.; // BasePtr1 is PtrDiff away from BasePtr0. They alias if none of the; // following situations arise:",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGAddressAnalysis.cpp:123,load,load,123,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGAddressAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGAddressAnalysis.cpp,2,"['load', 'scalab']","['load', 'scalable']"
Performance,"// If the size of memory access is unknown, do not use it to do analysis.; // One example of unknown size memory access is to load/store scalable; // vector objects on the stack.; // BasePtr1 is PtrDiff away from BasePtr0. They alias if none of the; // following situations arise:",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LoadStoreOpt.cpp:126,load,load,126,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LoadStoreOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LoadStoreOpt.cpp,2,"['load', 'scalab']","['load', 'scalable']"
Performance,"// If the size of the reduction item is non-constant, load it from global; // threadprivate variable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp:54,load,load,54,interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp,3,['load'],['load']
Performance,"// If the sole user is a token factor, we should make sure we have a; // chance to merge them together. This prevents TF chains from inhibiting; // optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:148,optimiz,optimizations,148,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['optimiz'],['optimizations']
Performance,// If the source register is bigger than the destination we need to; // perform a subregister copy.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:72,perform,perform,72,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['perform'],['perform']
Performance,"// If the source register is known to be 0 or non-0, the comparison can; // be folded to a load of a constant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp:91,load,load,91,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonBitSimplify.cpp,1,['load'],['load']
Performance,"// If the source type is smaller than the destination type of the; // coerce-to logic, copy the source value into a temp alloca the size; // of the destination type to allow loading all of it. The bits past; // the source value are left undef.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:174,load,loading,174,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['loading']
Performance,"// If the stack slot is dead, then this was optimized away.; // FIXME: stack slot colouring should account for slots that get merged.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp:44,optimiz,optimized,44,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugValues/InstrRefBasedImpl.cpp,1,['optimiz'],['optimized']
Performance,"// If the starting value is not the same as the phi node, we speculatively; // looked through an 'and' instruction when evaluating a potential; // arithmetic reduction to determine if it may have been type-promoted.; //; // We now compute the minimal bit width that is required to represent the; // reduction. If this is the same width that was indicated by the 'and', we; // can represent the reduction in the smaller type. The 'and' instruction; // will be eliminated since it will essentially be a cast instruction that; // can be ignore in the cost model. If we compute a different type than we; // did when evaluating the 'and', the 'and' will not be eliminated, and we; // will end up with different kinds of operations in the recurrence; // expression (e.g., IntegerAND, IntegerADD). We give up if this is; // the case.; //; // The vectorizer relies on InstCombine to perform the actual; // type-shrinking. It does this by inserting instructions to truncate the; // exit value of the reduction to the width indicated by RecurrenceType and; // then extend this value back to the original width. If IsSigned is false,; // a 'zext' instruction will be generated; otherwise, a 'sext' will be; // used.; //; // TODO: We should not rely on InstCombine to rewrite the reduction in the; // smaller type. We should just generate a correctly typed expression; // to begin with.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/IVDescriptors.cpp:875,perform,perform,875,interpreter/llvm-project/llvm/lib/Analysis/IVDescriptors.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/IVDescriptors.cpp,1,['perform'],['perform']
Performance,// If the store and the load are using the same stack slot then the; // store isn't valid for tail predication,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,1,['load'],['load']
Performance,"// If the store is a memcpy instruction, we must check if it will write to; // the load memory locations. So remove it from the ignored stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:83,load,load,83,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,1,['load'],['load']
Performance,"// If the stride is equal to the element size in bytes, we can use; // a masked.load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:80,load,load,80,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,"// If the struct contains a scalable vector type, don't consider it sized.; // This prevents it from being used in loads/stores/allocas/GEPs. The ONLY; // special case right now is a structure of homogenous scalable vector; // types and is handled by the if-statement before this for-loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Type.cpp:28,scalab,scalable,28,interpreter/llvm-project/llvm/lib/IR/Type.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Type.cpp,3,"['load', 'scalab']","['loads', 'scalable']"
Performance,"// If the superclass doesn't have type parameters, then there is no; // substitution to perform.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp:88,perform,perform,88,interpreter/llvm-project/clang/lib/AST/Type.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp,1,['perform'],['perform']
Performance,"// If the switch has a condition wrapped by __builtin_unpredictable,; // create metadata that specifies that the switch is unpredictable.; // Don't bother if not optimizing because that metadata would not be used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp:162,optimiz,optimizing,162,interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,1,['optimiz'],['optimizing']
Performance,"// If the symbol is external the linker will handle it.; // FIXME: Should we handle it as an optimization?; // If the symbol is out of range, produce a relocation and hope the; // linker can handle it. GNU AS produces an error in this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/MCTargetDesc/ARMAsmBackend.cpp:93,optimiz,optimization,93,interpreter/llvm-project/llvm/lib/Target/ARM/MCTargetDesc/ARMAsmBackend.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/MCTargetDesc/ARMAsmBackend.cpp,1,['optimiz'],['optimization']
Performance,"// If the tail is to be folded by masking, round the number of iterations N; // up to a multiple of Step instead of rounding down. This is done by first; // adding Step-1 and then rounding down. Note that it's ok if this addition; // overflows: the vector induction variable will eventually wrap to zero given; // that it starts at zero and its Step is a power of two; the loop will then; // exit, with the last early-exit vector comparison also producing all-true.; // For scalable vectors the VF is not guaranteed to be a power of 2, but this; // is accounted for in emitIterationCountCheck that adds an overflow check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:474,scalab,scalable,474,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['scalab'],['scalable']
Performance,"// If the target doesn't support this, we have to spill the input vector; // to a temporary stack slot, update the element, then reload it. This is; // badness. We could also load the value into a vector register (either; // with a ""move to register"" or ""extload into register"" instruction, then; // permute it into place, if the idx is a constant and if the idx is; // supported by the target.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:175,load,load,175,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['load']
Performance,"// If the target has a fast compare for the given size, it will return a; // preferred load type for that size. Require that the load VT is legal and; // that the target supports unaligned loads of that type. Otherwise, return; // INVALID.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:87,load,load,87,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,3,['load'],"['load', 'loads']"
Performance,"// If the target has hardware branch prediction that can handle indirect; // branches, duplicating them can often make them predictable when there; // are common paths through the code. The limit needs to be high enough; // to allow undoing the effects of tail merging and other optimizations; // that rearrange the predecessors of the indirect branch.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TailDuplicator.cpp:279,optimiz,optimizations,279,interpreter/llvm-project/llvm/lib/CodeGen/TailDuplicator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TailDuplicator.cpp,1,['optimiz'],['optimizations']
Performance,"// If the target supports paired load, adjust the cost accordingly.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:33,load,load,33,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If the template argument is a pack expansion, perform template argument; // deduction against the pattern of that expansion. This only occurs during; // partial ordering.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp:49,perform,perform,49,interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,1,['perform'],['perform']
Performance,"// If the template function is marked as late template parsed at this; // point, it has not been instantiated and therefore we have not; // performed semantic analysis on it yet, so we cannot know if the type; // can be considered complete.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp:140,perform,performed,140,interpreter/llvm-project/clang/lib/Sema/Sema.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/Sema.cpp,1,['perform'],['performed']
Performance,"// If the time between now and when the instruction will be ready can cover; // the spill code, then avoid adding it to the ready queue. This gives long; // stalls highest priority and allows hoisting across calls. It should also; // speed up processing the available queue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp:130,queue,queue,130,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGRRList.cpp,2,['queue'],['queue']
Performance,"// If the transfer is from a stack slot to a stack slot, then we may be able; // to perform the stack-move optimization. See the comments in; // performStackMoveOptzn() for more details.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:84,perform,perform,84,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,3,"['optimiz', 'perform']","['optimization', 'perform', 'performStackMoveOptzn']"
Performance,"// If the transfer is using the alloca as a source of the transfer, then; // ignore it since it is a load (unless the transfer is volatile).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:101,load,load,101,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['load']
Performance,// If the translation unit has external storage force external decls to load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/HLSLExternalSemaSource.cpp:72,load,load,72,interpreter/llvm-project/clang/lib/Sema/HLSLExternalSemaSource.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/HLSLExternalSemaSource.cpp,1,['load'],['load']
Performance,"// If the tree has not been deserialised automatically, it is time to load; // it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx:70,load,load,70,roofit/roofitcore/src/RooTreeDataStore.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooTreeDataStore.cxx,1,['load'],['load']
Performance,"// If the trip count is computed in terms of a max (due to ScalarEvolution; // being unable to find a sufficient guard, for example), change the loop; // comparison to use SLT or ULT instead of NE.; // One consequence of doing this now is that it disrupts the count-down; // optimization. That's not always a bad thing though, because in such; // cases it may still be worthwhile to avoid a max.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp:275,optimiz,optimization,275,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,1,['optimiz'],['optimization']
Performance,"// If the type is scalable and the constant is not zero (vscale * n * 0 =; // 0) bailout.; // TODO: If the runtime value is accessible at any point before DWARF; // emission, then we could potentially keep a forward reference to it; // in the debug value to be filled in later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Operator.cpp:18,scalab,scalable,18,interpreter/llvm-project/llvm/lib/IR/Operator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Operator.cpp,1,['scalab'],['scalable']
Performance,"// If the type itself could have nullability but does not, infer pointer; // nullability and perform consistency checking.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaType.cpp:93,perform,perform,93,interpreter/llvm-project/clang/lib/Sema/SemaType.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaType.cpp,1,['perform'],['perform']
Performance,"// If the type of the l-value is atomic, then do an atomic load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:59,load,load,59,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['load'],['load']
Performance,"// If the types mismatch and we can't handle it, reject reuse of the load.; // If the stored value is larger or equal to the loaded value, we can reuse; // it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:69,load,load,69,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If the upper bits are zero, then attempt to perform as a truncated op.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp:47,perform,perform,47,interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,1,['perform'],['perform']
Performance,"// If the upper half is all sign bits, then we can perform the ABS on the; // lower half and zero-extend.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp:51,perform,perform,51,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,1,['perform'],['perform']
Performance,// If the upper half of a ymm/zmm load is undef then just load the lower half.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If the upper halves are all sign bits, then we can perform the MINMAX on; // the lower half and sign-extend the result to the upper half.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp:54,perform,perform,54,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,1,['perform'],['perform']
Performance,"// If the user is a later load/store that can be post-indexed, then don't; // combine this one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// If the user wants the entire vector, just load the entire vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp:45,load,load,45,interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGBuiltin.cpp,1,['load'],['load']
Performance,"// If the user wants to set the DSCR using command-line options,; // load in the specified value at the start of main.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCPreEmitPeephole.cpp:69,load,load,69,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCPreEmitPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCPreEmitPeephole.cpp,1,['load'],['load']
Performance,"// If the v_cmp target is in use between v_cmp and s_and_saveexec or after the; // s_and_saveexec, skip the optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:108,optimiz,optimization,108,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,1,['optimiz'],['optimization']
Performance,"// If the value being copied is not unknown, load from its location to get; // an aggregate rvalue.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCXX.cpp:45,load,load,45,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/ExprEngineCXX.cpp,1,['load'],['load']
Performance,"// If the value is not extended, a simple load will suffice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,1,['load'],['load']
Performance,"// If the value is produced by the terminator of the predecessor (an; // invoke) or it has side-effects, there is no valid place to put a load; // in the predecessor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:138,load,load,138,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['load']
Performance,"// If the value is the load that we will be eliminating, and the block it's; // available in is the block that the load is in, then don't add it as; // SSAUpdater will resolve the value to the relevant phi which may let it; // avoid phi construction entirely if there's actually only one value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['load'],['load']
Performance,"// If the value is unavailable in one of predecessors, we will end up; // inserting a new instruction into them. It is only valid if all the; // instructions before LoadI are guaranteed to pass execution to its; // successor, or if LoadI is safe to speculate.; // TODO: If this logic becomes more complex, and we will perform PRE insertion; // farther than to a predecessor, we need to reuse the code from GVN's PRE.; // It requires domination tree analysis, so for this simple case it is an; // overkill.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:165,Load,LoadI,165,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,3,"['Load', 'perform']","['LoadI', 'perform']"
Performance,"// If the value is unknown or undefined, we can't perform this check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NonNullParamChecker.cpp:50,perform,perform,50,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NonNullParamChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NonNullParamChecker.cpp,1,['perform'],['perform']
Performance,"// If the value is used as a load/store, then the pointer must be non null.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ValueTracking.cpp,1,['load'],['load']
Performance,"// If the value is zero- or sign-extended, its size becomes 4 bytes, so; // that's what we should load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp:98,load,load,98,interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMCallLowering.cpp,1,['load'],['load']
Performance,"// If the value isn't available in all predecessors, then there will be; // exactly one where it isn't available. Insert a load on that edge and add; // it to the AvailablePreds list.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:123,load,load,123,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// If the value of the load is locally available within the block, just use; // it. This frequently occurs for reg2mem'd allocas.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['load']
Performance,"// If the value operand is divergent, each lane is contributing a different; // value to the atomic calculation. We can only optimize divergent values if; // we have DPP available on our subtarget, and the atomic operation is 32; // bits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:125,optimiz,optimize,125,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,2,['optimiz'],['optimize']
Performance,"// If the values are not derived from 16-bit values, we cannot optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp:63,optimiz,optimize,63,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstCombineIntrinsic.cpp,1,['optimiz'],['optimize']
Performance,// If the vector element type is not a multiple of a byte then we are unable; // to correctly compute an address to load only the extracted element as a; // scalar.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:116,load,load,116,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,2,['load'],['load']
Performance,"// If the vector element width is a whole number of bytes, test if its legal; // to BSWAP shuffle the bytes and then perform the BITREVERSE on the byte; // vector. This greatly reduces the number of bit shifts necessary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorOps.cpp:117,perform,perform,117,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorOps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorOps.cpp,1,['perform'],['perform']
Performance,"// If the vector is scalable, SVE is enabled, implying support for complex; // numbers. Otherwise, we need to ensure complex number support is available",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:20,scalab,scalable,20,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// If the vector reduction can be performed in a smaller type, we truncate; // then extend the loop exit value to enable InstCombine to evaluate the; // entire expression in the smaller type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:34,perform,performed,34,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['perform'],['performed']
Performance,"// If the vector reduction can be performed in a smaller type, we truncate; // then extend the loop exit value to enable InstCombine to evaluate the; // entire expression in the smaller type.; // TODO: Handle this in truncateToMinBW.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanRecipes.cpp:34,perform,performed,34,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanRecipes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlanRecipes.cpp,1,['perform'],['performed']
Performance,"// If the vector type is of an MMA type (v256i1, v512i1), an invalid; // instruction cost is returned. This is to signify to other cost computing; // functions to return the maximum instruction cost in order to prevent any; // opportunities for the optimizer to produce MMA types within the IR.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp:249,optimiz,optimizer,249,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,1,['optimiz'],['optimizer']
Performance,"// If the verify is done following an optimization, it's possible that; // ClobberAt was a conservative clobbering, that we can now infer is not a; // true clobbering access. Don't fail the verify if that's the case.; // We do have accesses that claim they're optimized, but could be optimized; // further. Updating all these can be expensive, so allow it for now (FIXME).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:38,optimiz,optimization,38,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,3,['optimiz'],"['optimization', 'optimized']"
Performance,"// If the we have a class or struct or union, the order; // of data members is the list is essential since it determines their; // order on file. So we must always load. Also, the list is fixed; // since the language does not allow to add members.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:164,load,load,164,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,1,['load'],['load']
Performance,"// If the width is set, but the metadata says nothing about the scalable; // property, then assume it concerns only a fixed-width UserVF.; // If width is not set, the flag takes precedence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp:64,scalab,scalable,64,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorizationLegality.cpp,1,['scalab'],['scalable']
Performance,"// If the width of the load does not reach byte we are trying to provide for; // and it is not a ZEXTLOAD, then the load does not provide for the byte in; // question",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],['load']
Performance,"// If the width of the load is the same as the width of the splat,; // loading with an offset would load the wrong memory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,3,['load'],"['load', 'loading']"
Performance,"// If there are any AST files to merge, create a frontend action; // adaptor to perform the merge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp:80,perform,perform,80,interpreter/llvm-project/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/FrontendTool/ExecuteCompilerInvocation.cpp,1,['perform'],['perform']
Performance,// If there are any load-<modified> options then turn on flag overrides; // to avoid flag mismatch errors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink.cpp:20,load,load,20,interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink.cpp,1,['load'],['load']
Performance,"// If there are any members of the array that get value-initialized, check; // that is possible. That happens if we know the bound and don't have; // enough elements, or if we're performing an array new with an unknown; // bound.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:179,perform,performing,179,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['perform'],['performing']
Performance,"// If there are any other uses other than scalar to vector, then we should; // keep it as a scalar load -> direct move pattern to prevent multiple; // loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:99,load,load,99,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,"// If there are automatically generated ROOT events in the queue, they; // are let to be handled first",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/recorder/src/TRecorder.cxx:59,queue,queue,59,gui/recorder/src/TRecorder.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/recorder/src/TRecorder.cxx,1,['queue'],['queue']
Performance,"// If there are calls to setjmp or sigsetjmp, don't perform coloring. Virtual; // registers could be modified before the longjmp is executed, resulting in; // the wrong value being used afterwards.; // TODO: Does WebAssembly need to care about setjmp for register coloring?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyRegColoring.cpp:52,perform,perform,52,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyRegColoring.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyRegColoring.cpp,1,['perform'],['perform']
Performance,"// If there are calls to setjmp or sigsetjmp, don't perform stack slot; // coloring. The stack could be modified before the longjmp is executed,; // resulting in the wrong value being used afterwards.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/StackSlotColoring.cpp:52,perform,perform,52,interpreter/llvm-project/llvm/lib/CodeGen/StackSlotColoring.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/StackSlotColoring.cpp,1,['perform'],['perform']
Performance,"// If there are escaping uses of invariant.start instruction, the load maybe; // non-invariant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:66,load,load,66,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If there are interfering Uses (i.e. their defining access is in the; // loop), or ordered loads (stored as Defs!), don't move this store.; // Could do better here, but this is conservatively correct.; // TODO: Cache set of Uses on the first walk in runOnLoop, update when; // moving accesses. Can also extend to dominating uses.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:93,load,loads,93,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,2,"['Cache', 'load']","['Cache', 'loads']"
Performance,"// If there are modifications in the queue, process them now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/textinput/src/textinput/TextInput.cpp:37,queue,queue,37,core/textinput/src/textinput/TextInput.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/textinput/src/textinput/TextInput.cpp,1,['queue'],['queue']
Performance,"// If there are no loads or stores, the access is dead. We mark that as; // a size zero access.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// If there are no local declarations in our lookup result, we; // don't need to write an entry for the name at all. If we can't; // write out a lookup set without performing more deserialization,; // just skip this entry.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp:164,perform,performing,164,interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,1,['perform'],['performing']
Performance,"// If there are no stores, the load takes the undef value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,1,['load'],['load']
Performance,"// If there are only 2 instructions in the block, at this point,; // this is the call to free and unconditional.; // If there are more than 2 instructions, check that they are noops; // i.e., they won't hurt the performance of the generated code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:212,perform,performance,212,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['perform'],['performance']
Performance,// If there aren't any synthesized ivars then reuse the interface; // entry. Note we can't cache this because we simply free all; // entries later; however we shouldn't look up implementations; // frequently.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp:91,cache,cache,91,interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,1,['cache'],['cache']
Performance,"// If there is a @ symbol (followed by a version number) then this is a synthetic class name created; // from an already normalized name for the purpose of supporting schema evolution.; // There is no dictionary or interpreter information about this kind of class, the only; // (undesirable) side-effect of doing the search would be a waste of CPU time and potential; // auto-loading or auto-parsing based on the scope of the name.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:376,load,loading,376,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,1,['load'],['loading']
Performance,"// If there is a Likelihood knowledge for the cond, lower it.; // Note that if not optimizing this won't emit anything.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp:83,optimiz,optimizing,83,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.cpp,1,['optimiz'],['optimizing']
Performance,"// If there is a bug connected to invalid cache, turn on ExpensiveAsserts to; // catch this situation as early as possible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionPrecedenceTracking.cpp:42,cache,cache,42,interpreter/llvm-project/llvm/lib/Analysis/InstructionPrecedenceTracking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionPrecedenceTracking.cpp,1,['cache'],['cache']
Performance,"// If there is a current fragment, mark the symbol as pointing into it.; // Otherwise queue the label and set its fragment pointer when we emit the; // next fragment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCObjectStreamer.cpp:86,queue,queue,86,interpreter/llvm-project/llvm/lib/MC/MCObjectStreamer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCObjectStreamer.cpp,1,['queue'],['queue']
Performance,"// If there is a function with a live 'returned' argument but a dead return; // value, then there are two possible actions:; // 1) Eliminate the return value and take off the 'returned' attribute on the; // argument.; // 2) Retain the 'returned' attribute and treat the return value (but not the; // entire function) as live so that it is not eliminated.; //; // It's not clear in the general case which option is more profitable because,; // even in the absence of explicit uses of the return value, code generation; // is free to use the 'returned' attribute to do things like eliding; // save/restores of registers across calls. Whether this happens is target and; // ABI-specific as well as depending on the amount of register pressure, so; // there's no good way for an IR-level pass to figure this out.; //; // Fortunately, the only places where 'returned' is currently generated by; // the FE are places where 'returned' is basically free and almost always a; // performance win, so the second option can just be used always for now.; //; // This should be revisited if 'returned' is ever applied more liberally.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/DeadArgumentElimination.cpp:970,perform,performance,970,interpreter/llvm-project/llvm/lib/Transforms/IPO/DeadArgumentElimination.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/DeadArgumentElimination.cpp,1,['perform'],['performance']
Performance,"// If there is a modulemap module or prebuilt module, load it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:54,load,load,54,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,1,['load'],['load']
Performance,"// If there is a non-register sized type, the cost estimation may expand; // it to be several instructions to load into multiple registers on the; // target. But, if the only use of the load is a trunc instruction to a; // register sized type, the instruction selector can combine these; // instructions to be a single load. So, in this case, we use the; // destination type of the trunc instruction rather than the load to; // accurately estimate the cost of this load instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h:110,load,load,110,interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h,5,['load'],['load']
Performance,"// If there is a sequence of one or more load instructions, each loaded; // value is used as address of later load instruction, bitcast is; // necessary to change the value type, don't optimize it. For; // simplicity we give up if the load address comes from another load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,6,"['load', 'optimiz']","['load', 'loaded', 'optimize']"
Performance,"// If there is an API notes file here, try to load it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/APINotes/APINotesManager.cpp:46,load,load,46,interpreter/llvm-project/clang/lib/APINotes/APINotesManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/APINotes/APINotesManager.cpp,1,['load'],['load']
Performance,"// If there is an MO_JALR operand, insert:; //; // .reloc tmplabel, R_{MICRO}MIPS_JALR, symbol; // tmplabel:; //; // This is an optimization hint for the linker which may then replace; // an indirect call with a direct branch.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsAsmPrinter.cpp:128,optimiz,optimization,128,interpreter/llvm-project/llvm/lib/Target/Mips/MipsAsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsAsmPrinter.cpp,1,['optimiz'],['optimization']
Performance,"// If there is an external AST source, load any declarations it knows about; // with this declaration's name.; // If the lookup table contains an entry about this name it means that we; // have already checked the external source.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp:39,load,load,39,interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,1,['load'],['load']
Performance,"// If there is an non-load/store instruction in the loop, we can't promote; // it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If there is enough room in the type to upscale the LHS or downscale the; // RHS before the division, we can perform it in this type without having to; // resize. For signed operations, the LHS headroom is the number of; // redundant sign bits, and for unsigned ones it is the number of zeroes.; // The headroom for the RHS is the number of trailing zeroes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:111,perform,perform,111,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['perform'],['perform']
Performance,// If there is no DataInCodeLoadCmd return a load command with zero'ed fields.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp:45,load,load,45,interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,1,['load'],['load']
Performance,// If there is no DysymtabLoadCmd return a load command with zero'ed fields.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,1,['load'],['load']
Performance,// If there is no LinkOptHintsLoadCmd return a load command with zero'ed; // fields.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,1,['load'],['load']
Performance,// If there is no SymtabLoadCmd return a load command with zero'ed fields.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Object/MachOObjectFile.cpp,1,['load'],['load']
Performance,"// If there is no TuneInfo for this CPU, we fail back to generic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVSubtarget.cpp:18,Tune,TuneInfo,18,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVSubtarget.cpp,1,['Tune'],['TuneInfo']
Performance,"// If there is no linkage to be performed or we are linking from the source,; // bring SF over.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp:32,perform,performed,32,interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,1,['perform'],['performed']
Performance,"// If there is no linkage to be performed or we're linking from the source,; // bring over SGA.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp:32,perform,performed,32,interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,1,['perform'],['performed']
Performance,"// If there is no store at all, cache the result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp:32,cache,cache,32,interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineSink.cpp,1,['cache'],['cache']
Performance,"// If there is only a single store to this value, replace any loads of; // it that are directly dominated by the definition with the value stored.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:62,load,loads,62,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,1,['load'],['loads']
Performance,"// If there is only one use of save exec register and that use is SI_END_CF,; // we can optimize SI_IF by returning the full saved exec mask instead of; // just cleared bits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:88,optimiz,optimize,88,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,1,['optimiz'],['optimize']
Performance,"// If there is overlap between the found cluster and the authorized range; // update the cache data members with the information about the current cluster.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx:89,cache,cache,89,tree/tree/src/TTreeCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx,1,['cache'],['cache']
Performance,"// If there isn't a class that fits, we can't perform the transform.; // This is needed for correctness with a mixture of VSX and Altivec; // instructions to make sure that a low VSX register is not assigned to; // the Altivec instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXFMAMutate.cpp:46,perform,perform,46,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXFMAMutate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXFMAMutate.cpp,1,['perform'],['perform']
Performance,"// If there was a local def before us, we must have the same effect it; // did. Because every may-def is the same, any phis/etc we would create, it; // would also have created. If there was no local def before us, we; // performed a global update, and have to search all successors and make; // sure we update the first def in each of them (following all paths until; // we hit the first def along each path). This may also insert phi nodes.; // TODO: There are other cases we can skip this work, such as when we have a; // single successor, and only used a straight line of single pred blocks; // backwards to find the def. To make that work, we'd have to track whether; // getDefRecursive only ever used the single predecessor case. These types; // of paths also only exist in between CFG simplifications.; // If this is the first def in the block and this insert is in an arbitrary; // place, compute IDF and place phis.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp:221,perform,performed,221,interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,1,['perform'],['performed']
Performance,// If there was an error loading the file then skip it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink.cpp:25,load,loading,25,interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-jitlink/llvm-jitlink.cpp,1,['load'],['loading']
Performance,"// If there was no agent registered when the original object was loaded then; // we won't have created a debug object for it, so bail out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/OProfileJIT/OProfileJITEventListener.cpp:65,load,loaded,65,interpreter/llvm-project/llvm/lib/ExecutionEngine/OProfileJIT/OProfileJITEventListener.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/OProfileJIT/OProfileJITEventListener.cpp,1,['load'],['loaded']
Performance,"// If there was no mapping for a value ID, it's optimized out. Create no; // DBG_PHI, and any variables using this value will become optimized out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp:48,optimiz,optimized,48,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp,2,['optimiz'],['optimized']
Performance,"// If there was only one constant value used and for more than one lane,; // start by splatting that value, then replace the non-constant lanes. This; // is better than the default, which will perform a separate initialization; // for each lane.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:193,perform,perform,193,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If there's an active flag, load it and skip the cleanup if it's; // false.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCleanup.cpp:30,load,load,30,interpreter/llvm-project/clang/lib/CodeGen/CGCleanup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCleanup.cpp,1,['load'],['load']
Performance,"// If there's no definition yet, then DC's definition is added by an update; // record, but we've not yet loaded that update record. In this case, we; // commit to DC being the canonical definition now, and will fix this when; // we load the update record.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:106,load,loaded,106,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If there's no external storage, just perform a normal lookup and copy; // the results.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp:40,perform,perform,40,interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,1,['perform'],['perform']
Performance,"// If there's no index, just search through the CUs in the DWO - there's; // probably only one unless this is something like LTO - though an in-process; // built/cached lookup table could be used in that case to improve repeated; // lookups of different CUs in the DWO.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp:162,cache,cached,162,interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp,1,['cache'],['cached']
Performance,"// If there's nothing left to search, then all paths led to valid clobbers; // that we got from our cache; pick the nearest to the start, and allow; // the rest to be cached back.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:100,cache,cache,100,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,2,['cache'],"['cache', 'cached']"
Performance,"// If there's only one load per block, we just compare the loaded values.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,2,['load'],"['load', 'loaded']"
Performance,// If this RHS is a constant splat vector we can widen this and let; // division/remainder by constant optimize it.; // TODO: Can we do something for non-splat?,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:103,optimiz,optimize,103,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimize']
Performance,"// If this a 64 bit atomic load on a 32-bit target and SSE2 is enabled, we; // can use movq to do the load. If we have X87 we can load into an 80-bit; // X87 register and store it to a stack temporary.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,['load'],['load']
Performance,"// If this a vzmovl of a full vector load, replace it with a vzload, unless; // the load is volatile.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If this address is being looked up in ""load"" mode, return the content; // pointer, otherwise return the target address.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldChecker.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldChecker.cpp,1,['load'],['load']
Performance,// If this arm64 and is an load register (PC-relative) instruction the; // ReferenceValue is the PC plus the immediate value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/MachODump.cpp:27,load,load,27,interpreter/llvm-project/llvm/tools/llvm-objdump/MachODump.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/MachODump.cpp,1,['load'],['load']
Performance,// If this block has a store to the stack slot before any loads then we; // can ignore the block,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp:58,load,loads,58,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLowOverheadLoops.cpp,1,['load'],['loads']
Performance,"// If this block is empty, make everyone use its fall-through, not the block; // explicitly. Landing pads should not do this since the landing-pad table; // points to this block. Blocks with their addresses taken shouldn't be; // optimized away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp:230,optimiz,optimized,230,interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,1,['optimiz'],['optimized']
Performance,"// If this call site doesn't modify the memory, then we know it is just; // a load (but one that potentially returns the value itself), so we can; // ignore it if we know that the value isn't captured.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:78,load,load,78,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['load']
Performance,"// If this can be matched by a zero extend, don't optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:50,optimiz,optimize,50,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimize']
Performance,"// If this condition is equivalent to #ifndef X, and if this is the first; // directive seen, handle it for the multiple-include optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp:129,optimiz,optimization,129,interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,1,['optimiz'],['optimization']
Performance,"// If this condition is false, typo correction must be performed eagerly; // rather than delayed in many places, as it makes use of dependent types.; // the condition is false for clang's C-only codepath, as it doesn't support; // dependent types yet.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/ASTContext.h:55,perform,performed,55,interpreter/llvm-project/clang/include/clang/AST/ASTContext.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/ASTContext.h,1,['perform'],['performed']
Performance,"// If this conversion sequence involved a scalar -> atomic conversion, perform; // that conversion now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:71,perform,perform,71,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['perform'],['perform']
Performance,"// If this declaration is from a merged context, make a note that we need to; // check that the canonical definition of that context contains the decl.; //; // Note that we don't perform ODR checks for decls from the global module; // fragment.; //; // FIXME: We should do something similar if we merge two definitions of the; // same template specialization into the same CXXRecordDecl.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:179,perform,perform,179,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,1,['perform'],['perform']
Performance,"// If this expression is a call to a builtin function in HIP device; // compilation, allow a pointer-type argument to default address space to be; // passed as a pointer-type parameter to a non-default address space.; // If Arg is declared in the default address space and Param is declared; // in a non-default address space, perform an implicit address space cast to; // the parameter type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp:327,perform,perform,327,interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExpr.cpp,1,['perform'],['perform']
Performance,"// If this extract is from a loaded vector value and will be used as an; // integer, that requires a potentially expensive XMM -> GPR transfer.; // Additionally, if we can convert to a scalar integer load, that will likely; // be folded into a subsequent integer op.; // Note: Unlike the related fold for this in DAGCombiner, this is not limited; // to a single-use of the loaded vector. For the reasons above, we; // expect this to be profitable even if it creates an extra load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:29,load,loaded,29,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,4,['load'],"['load', 'loaded']"
Performance,"// If this extraction is not the 'mixed' case where a fixed vector is; // extracted from a scalable vector, ensure that the extraction does not; // overrun the parent vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Verifier.cpp:91,scalab,scalable,91,interpreter/llvm-project/llvm/lib/IR/Verifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Verifier.cpp,1,['scalab'],['scalable']
Performance,"// If this function is vararg, store any remaining integer argument regs; // to their spots on the stack so that they may be loaded by dereferencing; // the result of va_next.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:125,load,loaded,125,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['loaded']
Performance,"// If this function takes a flag argument, the implementation may use it to; // perform extra checks. Don't fold into the non-checking variant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp:80,perform,perform,80,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyLibCalls.cpp,1,['perform'],['perform']
Performance,"// If this has a literal constant source that is the same as the; // reversed bits of an inline immediate, replace with a bitreverse of; // that constant. This saves 4 bytes in the common case of materializing; // sign bits.; // Test if we are after regalloc. We only want to do this after any; // optimizations happen because this will confuse them.; // XXX - not exactly a check for post-regalloc run.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp:298,optimiz,optimizations,298,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIShrinkInstructions.cpp,1,['optimiz'],['optimizations']
Performance,"// If this include corresponds to a module but that module is; // unavailable, diagnose the situation and bail out.; // FIXME: Remove this; loadModule does the same check (but produces; // slightly worse diagnostics).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp:140,load,loadModule,140,interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,1,['load'],['loadModule']
Performance,"// If this insertion is not the 'mixed' case where a fixed vector is; // inserted into a scalable vector, ensure that the insertion of the; // subvector does not overrun the parent vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Verifier.cpp:89,scalab,scalable,89,interpreter/llvm-project/llvm/lib/IR/Verifier.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/Verifier.cpp,1,['scalab'],['scalable']
Performance,"// If this instruction cannot load, nothing to do.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['load']
Performance,"// If this instruction has an immediate form and one of its operands is a; // result of a load-immediate or an add-immediate, convert it to; // the immediate form if the constant is in range.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:90,load,load-immediate,90,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load-immediate']
Performance,"// If this instruction is a comparison against zero and isn't comparing a; // physical register, we can try to optimize it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp:111,optimiz,optimize,111,interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,1,['optimiz'],['optimize']
Performance,"// If this instruction is not guaranteed to execute, and we haven't seen a; // load or store at this offset before (or it had lower alignment), then we; // need to remember that requirement.; // Note that skipping instructions of previously seen offsets is only; // correct because we only allow a single type for a given offset, which; // also means that the number of accessed bytes will be the same.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp:79,load,load,79,interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/ArgumentPromotion.cpp,1,['load'],['load']
Performance,"// If this instruction may clobber the loads and is in middle of the BCE cmp; // block instructions, then bail for now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp:39,load,loads,39,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,1,['load'],['loads']
Performance,"// If this instruction may read from memory or throw (and potentially read; // from memory in the exception handler), forget LastStore. Load/store; // intrinsics will indicate both a read and a write to memory. The target; // may override this (e.g. so that a store intrinsic does not read from; // memory, and thus will be treated the same as a regular store for; // commoning purposes).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:136,Load,Load,136,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,1,['Load'],['Load']
Performance,"// If this is ""store (or X, Y), P"" and X is ""(and (load P), cst)"", where cst; // is a byte mask indicating a consecutive number of bytes, check to see if; // Y is known to provide just those bytes. If so, we try to replace the; // load + replace + store sequence with a single (narrower) store, which makes; // the load dead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:51,load,load,51,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['load'],['load']
Performance,"// If this is SEW=64 on RV32, use a strided load with a stride of x0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,"// If this is SSE/AVX CMPP, bitcast the result back to integer to match; // the result type of SETCC. The bitcast is expected to be optimized; // away during combining/isel.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:132,optimiz,optimized,132,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimized']
Performance,"// If this is a 64-bit AND with an immediate that fits in 32-bits,; // prefer using the smaller and over folding the load. This is needed to; // make sure immediates created by shrinkAndImmediate are always folded.; // Ideally we would narrow the load during DAG combine and get the; // best of both worlds.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:117,load,load,117,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,2,['load'],['load']
Performance,"// If this is a 64-bit load, but the spill slot is 32, then we can do; // a 32-bit load which is implicitly zero-extended. This likely is; // due to live interval analysis remat'ing a load from stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,3,['load'],['load']
Performance,"// If this is a 64-bit operation in which both 32-bit halves are nonzero,; // split the operation into two. If both operands here happen to be; // constant, leave this to common code to optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp:186,optimiz,optimize,186,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,1,['optimiz'],['optimize']
Performance,"// If this is a PHI node, we can't insert a load of the value before the; // use. Instead insert the load in the predecessor block corresponding; // to the incoming value.; //; // Note that if there are multiple edges from a basic block to this PHI; // node that we cannot have multiple loads. The problem is that the; // resulting PHI node will have multiple values (from each load) coming in; // from the same block, which is illegal SSA form. For this reason, we; // keep track of and reuse loads we insert.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp,5,['load'],"['load', 'loads']"
Performance,"// If this is a PHI node, we can't insert a load of the value before; // the use. Instead insert the load in the predecessor block; // corresponding to the incoming value.; //; // Note that if there are multiple edges from a basic block to this; // PHI node that we cannot have multiple loads. The problem is that; // the resulting PHI node will have multiple values (from each load); // coming in from the same block, which is illegal SSA form.; // For this reason, we keep track of and reuse loads we insert.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp,5,['load'],"['load', 'loads']"
Performance,"// If this is a XMM/YMM load of the same lower bits as another YMM/ZMM; // load, then just extract the lower subvector and avoid the second load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,3,['load'],['load']
Performance,// If this is a ZEXTLoad and we are looking at the loaded value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:51,load,loaded,51,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,2,['load'],['loaded']
Performance,"// If this is a broadcast load inserted into an upper undef, use a larger; // broadcast load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If this is a case we can't handle, return null and let the default; // expansion code take care of it. If we CAN select this case, and if it; // selects to a single instruction, return Op. Otherwise, if we can codegen; // this case more efficiently than a constant pool load, lower it to the; // sequence of ops that should be used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:273,load,load,273,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If this is a class, load only if the user allowed interpreter lookup; // If this is a namespace and the user did not allow for interpreter lookup, load but before disable; // autoparsing if enabled.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TEnum.cxx:23,load,load,23,core/meta/src/TEnum.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TEnum.cxx,2,['load'],['load']
Performance,"// If this is a cold call, we can sink the addressing calculation into; // the cold path. See optimizeCallInst",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:94,optimiz,optimizeCallInst,94,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['optimiz'],['optimizeCallInst']
Performance,"// If this is a constructor call, try to optimize it out, and failing that; // emit a single loop to initialize all remaining elements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprCXX.cpp:41,optimiz,optimize,41,interpreter/llvm-project/clang/lib/CodeGen/CGExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprCXX.cpp,1,['optimiz'],['optimize']
Performance,"// If this is a ctrl dep, latency is 1.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp:26,latency,latency,26,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/ScheduleDAGSDNodes.cpp,1,['latency'],['latency']
Performance,"// If this is a data-invariant load and there is no EFLAGS; // interference, we want to try and sink any hardening as far as; // possible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['load']
Performance,"// If this is a delegating constructor call, just load the VTT.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp:50,load,load,50,interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,1,['load'],['load']
Performance,"// If this is a direct call, avoid the wrapper if we don't need to do any; // loads or adds. This allows SDAG ISel to match direct calls.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:78,load,loads,78,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loads']
Performance,"// If this is a directory or a file descriptor is not needed and we have; // no cache, just go to the file system.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp:80,cache,cache,80,interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp,1,['cache'],['cache']
Performance,"// If this is a dynamic alloca, the value may depend on the loaded kernargs,; // so loads will need to be inserted before it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:60,load,loaded,60,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,4,['load'],"['loaded', 'loads']"
Performance,"// If this is a file context, we need to perform unqualified name; // lookup considering using directives.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:41,perform,perform,41,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['perform'],['perform']
Performance,"// If this is a fixed vector, we need to convert it to a scalable vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:57,scalab,scalable,57,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,3,['scalab'],['scalable']
Performance,"// If this is a legal vector load, try to combine it into a VLD1_UPD.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['load'],['load']
Performance,"// If this is a less-than-standard-aligned load/store, change the type to; // match the standard alignment.; // The alignment is overlooked when selecting _UPD variants; and it's; // easier to introduce bitcasts here than fix that.; // There are 3 ways to get to this base-update combine:; // - intrinsics: they are assumed to be properly aligned (to the standard; // alignment of the memory type), so we don't need to do anything.; // - ARMISD::VLDx nodes: they are only generated from the aforementioned; // intrinsics, so, likewise, there's nothing to do.; // - generic load/store instructions: the alignment is specified as an; // explicit operand, rather than implicitly as the standard alignment; // of the memory type (like the intrisics). We need to change the; // memory type to match the explicit alignment. That way, we don't; // generate non-standard-aligned ARMISD::VLDx nodes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['load'],['load']
Performance,"// If this is a load and the token chain is identical, replace the select; // of two loads with a load through a select of the address to load from.; // This triggers in things like ""select bool X, 10.0, 123.0"" after the FP; // constants have been dropped into the constant pool.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,4,['load'],"['load', 'loads']"
Performance,"// If this is a load followed by a store to the same location, then the store; // is dead/noop. Peek through any truncates if canCombineTruncStore failed.; // TODO: Add big-endian truncate support with test coverage.; // TODO: Can relax for unordered atomics (see D66309)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If this is a load following a store, make sure it's not to the same or; // overlapping address.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCHazardRecognizers.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCHazardRecognizers.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCHazardRecognizers.cpp,1,['load'],['load']
Performance,"// If this is a load from an unrelated pointer, ignore it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp,1,['load'],['load']
Performance,"// If this is a load instruction where the result has been coalesced with an operand, then we cannot clause it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIFormMemoryClauses.cpp,1,['load'],['load']
Performance,"// If this is a load of Ptr, the loaded value is available.; // (This is true even if the load is volatile or atomic, although; // those cases are unlikely.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,3,['load'],"['load', 'loaded']"
Performance,"// If this is a load of a single register via a 'pop', then we should use; // a post-indexed LDR instruction instead, per the ARM ARM.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/AsmParser/ARMAsmParser.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/ARM/AsmParser/ARMAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/AsmParser/ARMAsmParser.cpp,1,['load'],['load']
Performance,"// If this is a load of a single register, then we should use; // a post-indexed LDR instruction instead, per the ARM ARM.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/AsmParser/ARMAsmParser.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/ARM/AsmParser/ARMAsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/AsmParser/ARMAsmParser.cpp,1,['load'],['load']
Performance,"// If this is a load on a phi pointer, phi-translate it and search; // for available load/store to the pointer in predecessors.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,2,['load'],['load']
Performance,"// If this is a load or store with a zero offset, or within the alignment,; // we may be able to fold an add-immediate into the memory operation.; // The check against alignment is below, as it can't occur until we check; // the arguments to N",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If this is a load that still has uses, then the load must have been added; // as a live value in the SSAUpdate data structure for a block (e.g. because; // the loaded value was stored later). In this case, we need to recursively; // propagate the updates until we get to the real value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SSAUpdater.cpp,3,['load'],"['load', 'loaded']"
Performance,// If this is a load we have to track that it can't participate in any; // pre-splitting. If this is a store of a load we have to track that; // that load also can't participate in any pre-splitting.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,3,['load'],['load']
Performance,"// If this is a load, make sure the first load does not clobber the base; // register before the second load reads it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,3,['load'],['load']
Performance,"// If this is a load, save it. If this instruction can read from memory; // but is not a load, then we quit. Notice that we don't handle function; // calls that read or write.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,2,['load'],['load']
Performance,"// If this is a load, we have to stop. However, if the loaded value is from; // the pointer we're loading and is producing the pointer we're storing,; // then *this* store is dead (X = load P; store X -> P).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,4,['load'],"['load', 'loaded', 'loading']"
Performance,"// If this is a load-and-splat, we can do that with a single instruction; // in some cases. However if the load has multiple uses, we don't want to; // combine it because that will just produce multiple loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:16,load,load-and-splat,16,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,3,['load'],"['load', 'load-and-splat', 'loads']"
Performance,"// If this is a load-store pair from a stack slot to a stack slot, we; // might be able to perform the stack-move optimization just as we do for; // memcpys from an alloca to an alloca.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:16,load,load-store,16,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,3,"['load', 'optimiz', 'perform']","['load-store', 'optimization', 'perform']"
Performance,"// If this is a load/store of an alloca, we might have upgraded the alloca's; // alignment earlier. Get the new alignment.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,1,['load'],['load']
Performance,"// If this is a local unpromoted type, which doesn't have a metadata string,; // treat as Unknown and delay lowering, so that we can still utilize it for; // later optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp:164,optimiz,optimizations,164,interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,1,['optimiz'],['optimizations']
Performance,"// If this is a macro expansion in the ""#if !defined(x)"" line for the file,; // then the macro could expand to different things in other contexts, we need; // to disable the optimization in this case.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPMacroExpansion.cpp:174,optimiz,optimization,174,interpreter/llvm-project/clang/lib/Lex/PPMacroExpansion.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPMacroExpansion.cpp,1,['optimiz'],['optimization']
Performance,"// If this is a masked load followed by an UUNPKLO, fold this into a masked; // extending load. We can do this even if this is already a masked; // {z,}extload.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['load']
Performance,"// If this is a masked load with an all ones mask, we can use a unmasked load.; // FIXME: Can we do this for indexed, compressing, or truncating stores?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// If this is a masked load with an all ones mask, we can use a unmasked load.; // FIXME: Can we do this for indexed, expanding, or extending loads?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,3,['load'],"['load', 'loads']"
Performance,"// If this is a module entry function, we can also sanity check against; // the static frame. Strictly it would be better to check against the; // attribute, i.e. that the variable is within the always-allocated; // section, and not within some other non-absolute-address object; // allocated here, but the extra error detection is minimal and we would; // have to pass the Function around or cache the attribute value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp:393,cache,cache,393,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUMachineFunction.cpp,1,['cache'],['cache']
Performance,"// If this is a named pipe, immediately load the buffer to ensure subsequent; // calls to ContentCache::getSize() are accurate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:40,load,load,40,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['load'],['load']
Performance,"// If this is a node in an expression tree, climb to the expression root; // and add that since that's where optimization actually happens.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp:109,optimiz,optimization,109,interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Reassociate.cpp,1,['optimiz'],['optimization']
Performance,"// If this is a non-integer load, we can try folding it as an int load and; // then bitcast the result. This can be useful for union cases. Note; // that address spaces don't matter here since we're not going to result in; // an actual new load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,3,['load'],['load']
Performance,"// If this is a non-volatile load, process it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,1,['load'],['load']
Performance,"// If this is a normal instruction, just insert a load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp:50,load,load,50,interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp,1,['load'],['load']
Performance,"// If this is a one-part value or the first part of a multi-part value,; // create a stack object for the entire argument value type and return a; // load from our portion of it. This assumes that if the first part of an; // argument is in memory, the rest will also be in memory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:150,load,load,150,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['load'],['load']
Performance,"// If this is a pc-relative load off _GLOBAL_OFFSET_TABLE_:; // leaq _GLOBAL_OFFSET_TABLE_(%rip), %r15; // this needs to be a GOTPC32 relocation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/MCTargetDesc/X86MCCodeEmitter.cpp,1,['load'],['load']
Performance,"// If this is a potential VSX load with an offset of 0, a VSX indexed load can; // be used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFastISel.cpp,2,['load'],['load']
Performance,"// If this is a reference field, load the reference right now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:33,load,load,33,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,"// If this is a select between two integer constants, try to do some; // optimizations. Note that the operands are ordered the opposite of SELECT; // operands.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:73,optimiz,optimizations,73,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimizations']
Performance,"// If this is a select where the false operand is zero and the compare is a; // check of the sign bit, see if we can perform the ""gzip trick"":; // select_cc setlt X, 0, A, 0 -> and (sra X, size(X)-1), A; // select_cc setgt X, 0, A, 0 -> and (not (sra X, size(X)-1)), A",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:117,perform,perform,117,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['perform'],['perform']
Performance,"// If this is a series of conditions that are or'd or and'd together, emit; // this as a sequence of branches instead of setcc's with and/or operations.; // As long as jumps are not expensive (exceptions for multi-use logic ops,; // unpredictable branches, and vector extracts because those jumps are likely; // expensive for any target), this should improve performance.; // For example, instead of something like:; // cmp A, B; // C = seteq; // cmp D, E; // F = setle; // or C, F; // jnz foo; // Emit:; // cmp A, B; // je foo; // cmp D, E; // jle foo",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:359,perform,performance,359,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,2,['perform'],['performance']
Performance,"// If this is a short vector argument loaded from the stack,; // extend from i64 to full vector size and then bitcast.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:38,load,loaded,38,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,1,['load'],['loaded']
Performance,"// If this is a single bit test that can't be handled by ANDI, shift the; // bit to be tested to the MSB and perform a signed compare with 0.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:109,perform,perform,109,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['perform'],['perform']
Performance,"// If this is a splat fed by a splatting load, the splat is; // redundant. Replace with a copy. This doesn't happen directly due; // to code in PPCDAGToDAGISel.cpp, but it can happen when converting; // a load of a double to a vector of 64-bit integers.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,2,['load'],['load']
Performance,"// If this is a tag declaration with a typedef name for linkage, it's safe; // to load that typedef now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:82,load,load,82,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,1,['load'],['load']
Performance,// If this is a truncate of a non extending load we can just narrow it to; // use a broadcast_load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If this is a truncate of load that has been shifted right, we can; // offset the pointer and use a narrower load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If this is a truncation of a denormal number, and the target semantics; // has larger exponent range than the source semantics (this can happen; // when truncating from PowerPC double-double to double format), the; // right shift could lose result mantissa bits. Adjust exponent instead; // of performing excessive shift.; // Also do a similar trick in case shifting denormal would produce zero; // significand as this case isn't handled correctly by normalize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp:297,perform,performing,297,interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,1,['perform'],['performing']
Performance,"// If this is a truncation, perform the shift before we narrow the storage.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp:28,perform,perform,28,interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,1,['perform'],['perform']
Performance,"// If this is a volatile load, don't mess with it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If this is a volatile load/store, don't mess with it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64InstrInfo.cpp,2,['load'],['load']
Performance,"// If this is a write - check other reads and writes for conflicts. If; // this is a read only check other writes for conflicts (but only if; // there is no other write to the ptr - this is an optimization to; // catch ""a[i] = a[i] + "" without having to do a dependence check).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:193,optimiz,optimization,193,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,1,['optimiz'],['optimization']
Performance,"// If this is a zext/sext of a load, return 0 if the corresponding; // extending load exists on target and the result type is legal.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h:31,load,load,31,interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,2,['load'],['load']
Performance,"// If this is an (1) AVX vector load with (2) multiple uses and (3) all of; // those uses are extracted directly into a store, then the extract + store; // can be store-folded. Therefore, it's probably not worth splitting the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If this is an alloca for a scalar variable, insert a dbg.value; // at each load and store to the alloca and erase the dbg.declare.; // The dbg.values allow tracking a variable even if it is not; // stored on the stack, while the dbg.declare can only describe; // the stack slot (and at a lexical-scope granularity). Later; // passes will attempt to elide the stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp:78,load,load,78,interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/Local.cpp,1,['load'],['load']
Performance,"// If this is an allocation, and if we know that the accessed pointer is to; // the allocation, return Def. This means that there is no dependence and; // the access can be optimized based on that. For example, a load could; // turn into undef. Note that we can bypass the allocation itself when; // looking for a clobber in many cases; that's an alias property and is; // handled by BasicAA.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:173,optimiz,optimized,173,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,"['load', 'optimiz']","['load', 'optimized']"
Performance,"// If this is an already optimized use or def, return the optimized result.; // Note: Currently, we store the optimized def result in a separate field,; // since we can't use the defining access.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:25,optimiz,optimized,25,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,3,['optimiz'],['optimized']
Performance,"// If this is an any-extending load from the FPR bank, split it into a regular; // load + extend.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,2,['load'],['load']
Performance,"// If this is an archive, we might have either the object or the archive; // cached. In this case we can load it without accessing the file system.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp:77,cache,cached,77,interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,2,"['cache', 'load']","['cached', 'load']"
Performance,"// If this is an atomic load or store, print out the atomic marker.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/AsmWriter.cpp:24,load,load,24,interpreter/llvm-project/llvm/lib/IR/AsmWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/IR/AsmWriter.cpp,1,['load'],['load']
Performance,"// If this is an enum being completed, then we flush all non-struct types from; // the cache. This allows function types and other things that may be derived; // from the enum to be recomputed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenTypes.cpp:87,cache,cache,87,interpreter/llvm-project/clang/lib/CodeGen/CodeGenTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenTypes.cpp,1,['cache'],['cache']
Performance,// If this is an extending load and the mask type is not the same as; // load's type then we have to extend the mask type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['load']
Performance,"// If this is an extension, perform the shift now that the storage is; // available.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp:28,perform,perform,28,interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,1,['perform'],['perform']
Performance,"// If this is an i16 load, insert the truncate.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:21,load,load,21,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,"// If this is an insertion of 32-bits into the low 32-bits of; // a vector, we prefer to generate a blend with immediate rather; // than an insertps. Blends are simpler operations in hardware and so; // will always have equal or better performance than insertps.; // But if optimizing for size and there's a load folding opportunity,; // generate insertps because blendps does not have a 32-bit memory; // operand form.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:236,perform,performance,236,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,"['load', 'optimiz', 'perform']","['load', 'optimizing', 'performance']"
Performance,"// If this is an instruction with a load folded into it, try unfolding; // the load, e.g. avoid this:; // movq %rdx, %rcx; // addq (%rax), %rcx; // in favor of this:; // movq (%rax), %rcx; // addq %rdx, %rcx; // because it's preferable to schedule a load than a register copy.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TwoAddressInstructionPass.cpp,3,['load'],['load']
Performance,// If this is an integer load past the end of the slice (which means the; // bytes outside the slice are undef or this load is dead) just forcibly; // fix the integer size with correct handling of endianness.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,2,['load'],['load']
Performance,"// If this is an non-standard-aligned LOAD, the first result is the loaded; // value. Bitcast it to the expected result type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:38,LOAD,LOAD,38,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,"['LOAD', 'load']","['LOAD', 'loaded']"
Performance,"// If this is an object, we might have it cached. If not we'll have to load; // it from the file system and cache it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp:42,cache,cached,42,interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/dsymutil/BinaryHolder.cpp,3,"['cache', 'load']","['cache', 'cached', 'load']"
Performance,"// If this is an unaligned load and the target doesn't support it,; // expand it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,2,['load'],['load']
Performance,// If this is arm64 and the reference is a load register instruction and we; // have seen an adrp instruction just before it and the adrp's Xd register; // matches this add's Xn register reconstruct the value being referenced and; // look to see if it is a literal pointer. Note the load register; // instruction is passed in ReferenceValue.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/MachODump.cpp:43,load,load,43,interpreter/llvm-project/llvm/tools/llvm-objdump/MachODump.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-objdump/MachODump.cpp,2,['load'],['load']
Performance,"// If this is being added as part of loading an external declaration,; // this may not be the only external declaration with this name.; // In this case, we never try to replace an existing declaration; we'll; // handle that when we finalize the list of declarations for this name.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp:37,load,loading,37,interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,1,['load'],['loading']
Performance,"// If this is byval, the loads are already explicit in the function. We just; // need to rewrite the pointer values.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp:25,load,loads,25,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULowerKernelArguments.cpp,1,['load'],['loads']
Performance,"// If this is loading from the same pointer, replace this load's value; // with that one.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:14,load,loading,14,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,2,['load'],"['load', 'loading']"
Performance,"// If this is not a fall-through branch or optimizations are switched off,; // emit the branch.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:43,optimiz,optimizations,43,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,2,['optimiz'],['optimizations']
Performance,"// If this is not a system module or -Wsystem-headers was passed, don't; // optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp:76,optimiz,optimize,76,interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Tooling/DependencyScanning/ModuleDepCollector.cpp,1,['optimiz'],['optimize']
Performance,"// If this is optimized to re-use TStreamerElement(s) in case of variable renaming,; // then we must revisit the code in TBranchElement::InitInfo that recalculate the; // fID (i.e. the index of the TStreamerElement to be used for streaming).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx:14,optimiz,optimized,14,io/io/src/TStreamerInfo.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/io/io/src/TStreamerInfo.cxx,1,['optimiz'],['optimized']
Performance,// If this is storing to the same pointer and has the same size etc.; // replace this load's value with the stored value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:86,load,load,86,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['load'],['load']
Performance,"// If this is the address operand of a load or store, make it relative to SP; // and fold the frame offset directly in.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyRegisterInfo.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyRegisterInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/WebAssembly/WebAssemblyRegisterInfo.cpp,1,['load'],['load']
Performance,"// If this is the assignment operator, we only perform overload resolution; // if the left-hand side is a class or enumeration type. This is actually; // a hack. The standard requires that we do overload resolution between the; // various built-in candidates, but as DR507 points out, this can lead to; // problems. So we do it this way, which pretty much follows what GCC does.; // Note that we go the traditional code path for compound assignment forms.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:47,perform,perform,47,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,1,['perform'],['perform']
Performance,"// If this is the complete variant, just invoke the base variant;; // the epilogue will destruct the virtual bases. But we can't do; // this optimization if the body is a function-try-block, because; // we'd introduce *two* handler blocks. In the Microsoft ABI, we; // always delegate because we might not have a definition in this TU.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp:141,optimiz,optimization,141,interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGClass.cpp,1,['optimiz'],['optimization']
Performance,// If this is the function being called then we treat it like a load and; // ignore it.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:64,load,load,64,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['load']
Performance,"// If this is the last Def in the block, we may need additional Phis.; // Compute IDF in all cases, as renaming needs to be done even when MD is; // not the last access, because it can introduce a new access past which a; // previous access was optimized; that access needs to be reoptimized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp:245,optimiz,optimized,245,interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,1,['optimiz'],['optimized']
Performance,// If this is the special case where we use a MOV32rm to load a 32-bit; // value and zero-extend the top bits. Change the destination register; // to a 32-bit one.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,"// If this isn't a gather, we may have excess loaded elements in the; // IR type. Check the dmask for the real number of elements loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp:46,load,loaded,46,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIISelLowering.cpp,2,['load'],['loaded']
Performance,// If this isn't an integer load we can't fold it directly.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,1,['load'],['load']
Performance,"// If this load comes from anywhere in a uniform constant global, the value; // is always the same, regardless of the loaded offset.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If this load follows a GEP, see if we can PRE the indices before analyzing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['load'],['load']
Performance,"// If this load has an opaque TargetConstant offset, then we cannot split; // the indexing into an add/sub directly (that TargetConstant may not be; // valid for a different type of node, and we cannot convert an opaque; // target constant into a regular constant).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If this load is a load from a GEP with a constant offset from an alloca,; // then we don't want to sink it. In its present form, it will be; // load [constant stack offset]. Sinking it will cause us to have to; // materialize the stack addresses in each predecessor in a register only to; // do a shared load from register in the successor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,4,['load'],['load']
Performance,"// If this load is directly stored, replace the load value with the stored; // value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['load'],['load']
Performance,"// If this load is of a struct or the load is volatile, just mark the result; // as overdefined.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SCCPSolver.cpp,2,['load'],['load']
Performance,"// If this load occurs either right after a lifetime begin,; // then the loaded value is undefined.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If this load really doesn't depend on anything, then we must be loading an; // undef value. This can happen when loading for a fresh allocation with no; // intervening stores, for example. Note that this is only true in the case; // that the result of the allocation is pointer equal to the load ptr.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp:11,load,load,11,interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,4,['load'],"['load', 'loading']"
Performance,"// If this loop appears elsewhere within the queue, we also need to remove it; // there. However, we have to be careful to not remove the back of the queue; // as that is assumed to match the current loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp:45,queue,queue,45,interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,2,['queue'],['queue']
Performance,"// If this loop executes exactly one time, then it should be peeled, not; // optimized by this pass.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp:77,optimiz,optimized,77,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopIdiomRecognize.cpp,1,['optimiz'],['optimized']
Performance,// If this loop has metadata indicating that LICM is not to be performed then; // just exit.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:63,perform,performed,63,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['perform'],['performed']
Performance,"// If this member fn was defaulted on its first declaration, we will have; // already performed the checking in CheckCompletedCXXClass. Such a; // declaration doesn't trigger an implicit definition.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:86,perform,performed,86,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['performed']
Performance,"// If this memory access can be shown to *statically* extend outside the; // bounds of the allocation, it's behavior is undefined, so simply; // ignore it. Note that this is more strict than the generic clamping; // behavior of insertUse. We also try to handle cases which might run the; // risk of overflow.; // FIXME: We should instead consider the pointer to have escaped if this; // function is being instrumented for addressing bugs or race conditions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:441,race condition,race conditions,441,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['race condition'],['race conditions']
Performance,"// If this module has already been finalized in the ModuleCache, we're stuck; // with it; we can only load a single version of each module.; //; // This can happen when a module is imported in two contexts: in one, as a; // user module; in another, as a system module (due to an import from; // another module marked with the [system] flag). It usually indicates a; // bug in the module map: this module should also be marked with [system].; //; // If -Wno-system-headers (the default), and the first import is as a; // system module, then validation will fail during the as-user import,; // since -Werror flags won't have been validated. However, it's reasonable; // to treat this consistently as a system module.; //; // If -Wsystem-headers, the PCM on disk was built with; // -Wno-system-headers, and the first import is as a user module, then; // validation will fail during the as-system import since the PCM on disk; // doesn't guarantee that -Werror was respected. However, the -Werror; // flags were checked during the initial as-user import.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:102,load,load,102,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['load']
Performance,// If this node generates masked gather load then it is not a terminal node.; // Hence address operand cost is estimated separately.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['load']
Performance,"// If this node has memory references (i.e. is a load or store), tell the; // interpreter to capture them in the memref array.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/DAGISelMatcherGen.cpp:49,load,load,49,interpreter/llvm-project/llvm/utils/TableGen/DAGISelMatcherGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/DAGISelMatcherGen.cpp,1,['load'],['load']
Performance,"// If this node is not the root and the subtree underneath it produces a; // chain, then the result of matching the node is also produce a chain.; // Beyond that, this means that we're also folding (at least) the root node; // into the node that produce the chain (for example, matching; // ""(add reg, (load ptr))"" as a add_with_memory on X86). This is; // problematic, if the 'reg' node also uses the load (say, its chain).; // Graphically:; //; // [LD]; // ^ ^; // | \ DAG's like cheese.; // / |; // / [YY]; // | ^; // [XX]--/; //; // It would be invalid to fold XX and LD. In this case, folding the two; // nodes together would induce a cycle in the DAG, making it a 'cyclic DAG'; // To prevent this, we emit a dynamic check for legality before allowing; // this to be folded.; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/DAGISelMatcherGen.cpp:303,load,load,303,interpreter/llvm-project/llvm/utils/TableGen/DAGISelMatcherGen.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/DAGISelMatcherGen.cpp,2,['load'],['load']
Performance,"// If this operand is a scalable type, bail out early.; // TODO: Make isLegalAddressingMode TypeSize aware.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h:24,scalab,scalable,24,interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h,1,['scalab'],['scalable']
Performance,"// If this phi reaches some ""real"" uses, add it to the queue for upward; // propagation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RDFLiveness.cpp:55,queue,queue,55,interpreter/llvm-project/llvm/lib/CodeGen/RDFLiveness.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RDFLiveness.cpp,1,['queue'],['queue']
Performance,"// If this pointer is always safe to load, or if we can prove that there; // is already a load in the block, then we can move the load to the pred; // block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,3,['load'],['load']
Performance,"// If this protocol is marked 'objc_protocol_requires_explicit_implementation'; // then we should check if any class in the super class hierarchy also; // conforms to this protocol, either directly or via protocol inheritance.; // If so, we can skip checking this protocol completely because we; // know that a parent class already satisfies this protocol.; //; // Note: we could generalize this logic for all protocols, and merely; // add the limit on looking at the super class chain for just; // specially marked protocols. This may be a good optimization. This; // change is restricted to 'objc_protocol_requires_explicit_implementation'; // protocols for now for controlled evaluation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclObjC.cpp:546,optimiz,optimization,546,interpreter/llvm-project/clang/lib/Sema/SemaDeclObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclObjC.cpp,1,['optimiz'],['optimization']
Performance,"// If this really a zext_inreg that can be represented with a movzx; // instruction, prefer that.; // TODO: We could shrink the load and fold if it is non-volatile.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:128,load,load,128,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If this select has a condition (setcc) with narrower operands than the; // select, try to widen the compare to match the select width.; // TODO: This should be extended to handle any constant.; // TODO: This could be extended to handle non-loading patterns, but that; // requires thorough testing to avoid regressions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:243,load,loading,243,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loading']
Performance,"// If this sign extend is only used by a truncate, let the truncate be; // eliminated before we try to optimize this sext.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:103,optimiz,optimize,103,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,1,['optimiz'],['optimize']
Performance,// If this tool chain is used for an OpenMP offloading device we have to make; // sure we always generate a shared library regardless of the commands the; // user passed to the host. This is required because the runtime library; // is required to load the device image dynamically at run time.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Gnu.cpp:247,load,load,247,interpreter/llvm-project/clang/lib/Driver/ToolChains/Gnu.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/ToolChains/Gnu.cpp,1,['load'],['load']
Performance,"// If this type test is only used by llvm.assume instructions, it; // was used for whole program devirtualization, and is being kept; // for use by other optimization passes. We do not need or want to; // lower it here. We also don't want to rewrite any associated globals; // unnecessarily. These will be removed by a subsequent LTT invocation; // with the DropTypeTests flag set.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp:154,optimiz,optimization,154,interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,1,['optimiz'],['optimization']
Performance,"// If this value is a load from a constant offset w.r.t. a base address, and; // there are no other users of the load or address, returns the base address and; // the offset.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MergeICmps.cpp,2,['load'],['load']
Performance,"// If this value is an array or struct with a statically determinable; // constant initializer, there are optimizations we can do.; //; // TODO: We should constant-evaluate the initializer of any variable,; // as long as it is initialized by a constant expression. Currently,; // isConstantInitializer produces wrong answers for structs with; // reference or bitfield members, and a few other cases, and checking; // for POD-ness protects us from some of these.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp:106,optimiz,optimizations,106,interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDecl.cpp,1,['optimiz'],['optimizations']
Performance,"// If this was a task in a group, notify also threads waiting for tasks; // in this function on QueueCondition, to make a recursive wait() return; // after the group it's been waiting for has finished.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/ThreadPool.cpp:96,Queue,QueueCondition,96,interpreter/llvm-project/llvm/lib/Support/ThreadPool.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/ThreadPool.cpp,1,['Queue'],['QueueCondition']
Performance,"// If this was a volatile load that we are merging, make sure to loop through; // and mark all the input loads as non-volatile. If we don't do this, we will; // insert a new volatile load and the old ones will not be deletable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombinePHI.cpp,3,['load'],"['load', 'loads']"
Performance,"// If this was an error, refuse to perform any rewriting.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/Rewrite/FixItRewriter.cpp:35,perform,perform,35,interpreter/llvm-project/clang/lib/Frontend/Rewrite/FixItRewriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/Rewrite/FixItRewriter.cpp,1,['perform'],['perform']
Performance,"// If this wasn't a cache hit, we hit a clobber when walking. That's a; // failure.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:20,cache,cache,20,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['cache'],['cache']
Performance,"// If this wasn't already an extending load, we need to widen the result; // register to avoid creating a load with a narrower result than the source.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,2,['load'],['load']
Performance,"// If this write overlapped a read which previously came from the pool,; // someone may still be holding a pointer to that alloc which is now invalid.; // Compute the overlapping range and update the cache entry, so any; // outstanding buffers are automatically updated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp:200,cache,cache,200,interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/MSF/MappedBlockStream.cpp,1,['cache'],['cache']
Performance,"// If this zero extend is only used by a truncate, let the truncate be; // eliminated before we try to optimize this zext.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:103,optimiz,optimize,103,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,1,['optimiz'],['optimize']
Performance,"// If trap checking is enabled, add support to compare the virtual; // function pointer to the devirtualized target. In case of a mismatch,; // perform a debug trap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp:144,perform,perform,144,interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp,1,['perform'],['perform']
Performance,// If true indicates that this global variable might be accessed; // purely by non-volatile load instructions. This in turn means; // it can be internalized in source and destination modules during; // thin LTO import because it neither modified nor its address; // is taken.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h:92,load,load,92,interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h,1,['load'],['load']
Performance,"// If true indicates that variable is possibly only written to, so; // its value isn't loaded and its address isn't taken anywhere.; // False, when 'Constant' attribute is set.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h:87,load,loaded,87,interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h,1,['load'],['loaded']
Performance,"// If true, both (x,alpha) are cached",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofit/inc/RooIntegralMorph.h:31,cache,cached,31,roofit/roofit/inc/RooIntegralMorph.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofit/inc/RooIntegralMorph.h,1,['cache'],['cached']
Performance,"// If true, split the backedge of a loop when placing the safepoint, otherwise; // split the latch block itself. Both are useful to support for; // experimentation, but in practice, it looks like splitting the backedge; // optimizes better.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/PlaceSafepoints.cpp:223,optimiz,optimizes,223,interpreter/llvm-project/llvm/lib/Transforms/Scalar/PlaceSafepoints.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/PlaceSafepoints.cpp,1,['optimiz'],['optimizes']
Performance,"// If true, value of this integral is cached if it is (partially numeric)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooRealIntegral.h:38,cache,cached,38,roofit/roofitcore/inc/RooRealIntegral.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooRealIntegral.h,2,['cache'],['cached']
Performance,"// If two modules, share the same LLVMContext, different threads must; // not access them concurrently without locking the associated LLVMContext; // this implementation follows this contract.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Speculation.cpp:90,concurren,concurrently,90,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Speculation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Speculation.cpp,1,['concurren'],['concurrently']
Performance,"// If typeinfos matched if and only if equal, then the elements of a filter L; // that occurs later than a filter F could be replaced by the intersection of; // the elements of F and L. In reality two typeinfos can match without being; // equal (for example if one represents a C++ class, and the other some class; // derived from it) so it would be wrong to perform this transform in general.; // However the transform is correct and useful if F is a subset of L. In that; // case L can be replaced by F, and thus removed altogether since repeating a; // filter is pointless. So here we look at all pairs of filters F and L where; // L follows F in the list of clauses, and remove L if every element of F is; // an element of L. This can occur when inlining C++ functions with exception; // specifications.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:359,perform,perform,359,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['perform'],['perform']
Performance,"// If unfolding produced a load that wasn't loop-invariant or profitable to; // hoist, discard the new instructions and bail.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineLICM.cpp,1,['load'],['load']
Performance,"// If uniforms are marked as clean, they don't need to be loaded to the GPU.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js:58,load,loaded,58,js/build/jsroot.js,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/js/build/jsroot.js,2,['load'],['loaded']
Performance,"// If unsafe fp math optimization is enabled and there are no other uses of; // the CMP operands, and the condition code is EQ or NE, we can optimize it; // to an integer comparison.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:21,optimiz,optimization,21,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,2,['optimiz'],"['optimization', 'optimize']"
Performance,"// If upper bound loop trip count (TC) is known at compile time there is no; // point in choosing VF greater than TC (as done in the loop below). Select; // maximum power of two which doesn't exceed TC. If MaxVectorElementCount is; // scalable, we only fall back on a fixed VF when the TC is less than or; // equal to the known number of lanes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:235,scalab,scalable,235,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['scalab'],['scalable']
Performance,// If upper demanded elements are not demanded then simplify to a; // scalar_to_vector(load()).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:87,load,load,87,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If useLoadStackGuardNode returns true, generate LOAD_STACK_GUARD.; // Otherwise, emit a volatile load to retrieve the stack guard value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp:100,load,load,100,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/IRTranslator.cpp,2,['load'],['load']
Performance,"// If user just wants to list available options, skip module loading",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llc/llc.cpp:61,load,loading,61,interpreter/llvm-project/llvm/tools/llc/llc.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llc/llc.cpp,1,['load'],['loading']
Performance,"// If value caching mode optimization has happened, process it now on object being inserted",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooObjCacheManager.cxx:25,optimiz,optimization,25,roofit/roofitcore/src/RooObjCacheManager.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooObjCacheManager.cxx,1,['optimiz'],['optimization']
Performance,// If value is passed via pointer - do a load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['load'],['load']
Performance,// If value is passed via pointer - do a load.; // TODO Make sure this handling on indirect arguments is correct,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,1,['load'],['load']
Performance,"// If value loaded in entry block, cache it and use it everywhere in; // function.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp:12,load,loaded,12,interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGOpenMPRuntime.cpp,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"// If value numbering later sees that an instruction in the scope is equal; // to 'LHS' then ensure it will be turned into 'RHS'. In order to preserve; // the invariant that instructions only occur in the leader table for their; // own value number (this is used by removeFromLeaderTable), do not do this; // if RHS is an instruction (if an instruction in the scope is morphed into; // LHS then it will be turned into RHS by the next GVN iteration anyway, so; // using the leader table is about compiling faster, not optimizing better).; // The leader table only tracks basic blocks, not edges. Only add to if we; // have the simple case where the edge dominates the end.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:517,optimiz,optimizing,517,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['optimiz'],['optimizing']
Performance,"// If vector multiply is legal, assume that's faster than shl + add/sub.; // Multiply is a complex op with higher latency and lower throughput in; // most implementations, sub-vXi32 vector multiplies are always fast,; // vXi32 mustn't have a SlowMULLD implementation, and anything larger (vXi64); // is always going to be slow.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:114,latency,latency,114,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,"['latency', 'throughput']","['latency', 'throughput']"
Performance,"// If vector shifting is potentially needed, accumulate metadata; // from source sections of twice the load width.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:103,load,load,103,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,1,['load'],['load']
Performance,"// If vector uses the builtin load, lower to a LoadInst",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:30,load,load,30,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,2,"['Load', 'load']","['LoadInst', 'load']"
Performance,"// If we allow overlapping loads and the load sequence is not already optimal,; // use overlapping loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:27,load,loads,27,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,3,['load'],"['load', 'loads']"
Performance,"// If we already have a cache entry for this CacheKey, we may need to do some; // work to reconcile the cache entry and the current query.; // Invariant loads don't participate in caching. Thus no need to reconcile.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:24,cache,cache,24,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,4,"['Cache', 'cache', 'load']","['CacheKey', 'cache', 'loads']"
Performance,"// If we already have a lookup data structure, perform the insertion into; // it. If we might have externally-stored decls with this name, look them; // up and perform the insertion. If this decl was declared outside its; // semantic context, buildLookup won't add it, so add it now.; //; // FIXME: As a performance hack, don't add such decls into the translation; // unit unless we're in C++, since qualified lookup into the TU is never; // performed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp:47,perform,perform,47,interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,4,['perform'],"['perform', 'performance', 'performed']"
Performance,"// If we already have a pre/post index load/store then set BaseAccess,; // IncrementOffset and NewBaseReg to the values it already produces,; // allowing us to update and subsequent uses of BaseOp reg with the; // incremented value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If we already performed cloning of this function, confirm that the; // requested number of clones matches (the thin link should ensure the; // number of clones for each constituent callsite is consistent within; // each function), before returning.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp:17,perform,performed,17,interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/MemProfContextDisambiguation.cpp,1,['perform'],['performed']
Performance,"// If we also load/broadcast this to a wider type, then just extract the; // lowest subvector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we are about to process a token that is either an argument to; // __VA_OPT__ or its closing rparen, then:; // 1) If the token is the closing rparen that exits us out of __VA_OPT__,; // perform any necessary stringification or placemarker processing,; // and/or skip to the next token.; // 2) else if macro was invoked without variadic arguments skip this; // token.; // 3) else (macro was invoked with variadic arguments) process the token; // normally.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/TokenLexer.cpp:191,perform,perform,191,interpreter/llvm-project/clang/lib/Lex/TokenLexer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/TokenLexer.cpp,1,['perform'],['perform']
Performance,"// If we are called indirectly from within another call to; // TCling::UpdateClassInfo, we delay the update until the dictionary loading; // is finished (i.e. when we return to the top level TCling::UpdateClassInfo).; // This allows for the dictionary to be fully populated when we actually; // update the TClass object. The updating of the TClass sometimes; // (STL containers and when there is an emulated class) forces the building; // of the TClass object's real data (which needs the dictionary info).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:129,load,loading,129,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,1,['load'],['loading']
Performance,"// If we are casting a fixed i8 vector to a scalable 16 x i1 predicate; // vector, use a vector insert and bitcast the result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:44,scalab,scalable,44,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,['scalab'],['scalable']
Performance,"// If we are casting a scalable 16 x i1 predicate vector to a fixed i8; // vector, bitcast the source and use a vector extract.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:23,scalab,scalable,23,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,['scalab'],['scalable']
Performance,// If we are collecting assumes to be deleted we are in the manifest stage.; // It's problematic to collect the potential copies again now so we use the; // cached ones.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp:157,cache,cached,157,interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/AttributorAttributes.cpp,1,['cache'],['cached']
Performance,"// If we are converting to 32-bit integers, we need to add an FP_ROUND.; // This is not valid if the input was originally double precision. It is; // also not profitable to do unless this is an extending load in which; // case doing this combine will allow us to combine consecutive loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:204,load,load,204,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,"// If we are dealing with a pointer global that is initialized to null and; // only has one (non-null) value stored into it, then we can optimize any; // users of the loaded value (often calls and loads) that would trap if the; // value was null.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:137,optimiz,optimize,137,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,3,"['load', 'optimiz']","['loaded', 'loads', 'optimize']"
Performance,"// If we are dealing with scalable vectors on a big endian platform the; // calculation of offsets below becomes trickier, since we do not know at; // compile time the absolute size of the vector. Until we've done more; // analysis on big-endian platforms it seems better to bail out for now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:26,scalab,scalable,26,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['scalab'],['scalable']
Performance,"// If we are done linking global value bodies (i.e. we are performing; // metadata linking), don't link in the global value due to this; // reference, simply map it to null.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp:59,perform,performing,59,interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,1,['perform'],['performing']
Performance,"// If we are explicitly unroll and jamming, we are done. Otherwise there are a; // number of extra performance heuristics to check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopUnrollAndJamPass.cpp:99,perform,performance,99,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopUnrollAndJamPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopUnrollAndJamPass.cpp,1,['perform'],['performance']
Performance,"// If we are extracting from 2 different indexes, then one operand must be; // shuffled before performing the vector operation. The shuffle mask is; // poison except for 1 lane that is being translated to the remaining; // extraction lane. Therefore, it is a splat shuffle. Ex:; // ShufMask = { poison, poison, 0, poison }; // TODO: The cost model has an option for a ""broadcast"" shuffle; // (splat-from-element-0), but no option for a more general splat.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp:95,perform,performing,95,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,1,['perform'],['performing']
Performance,"// If we are given load addresses for the sections, we need to adjust:; // SymAddr = (Address of Symbol Or Section in File) -; // (Address of Section in File) +; // (Load Address of Section); // RSec is now either the section being targeted or the section; // containing the symbol being targeted. In either case,; // we need to perform the same computation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp,3,"['Load', 'load', 'perform']","['Load', 'load', 'perform']"
Performance,"// If we are in a dependent context, template instantiation will; // perform this type-checking again. Just save the arguments that we; // received in a ParenListExpr.; // FIXME: This isn't quite ideal, since our ASTs don't capture all; // of the information that we have about the base; // initializer. However, deconstructing the ASTs is a dicey process,; // and this approach is far more likely to get the corner cases right.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:69,perform,perform,69,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,2,['perform'],['perform']
Performance,"// If we are in neither a class nor a category, there's no; // substitution to perform.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp:79,perform,perform,79,interpreter/llvm-project/clang/lib/AST/Type.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Type.cpp,1,['perform'],['perform']
Performance,"// If we are in target mode, load the metadata from the host IR. This code has; // to match the metadata creation in createOffloadEntriesAndInfoMetadata().",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenMP/OMPIRBuilder.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Frontend/OpenMP/OMPIRBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenMP/OMPIRBuilder.cpp,1,['load'],['load']
Performance,"// If we are inserting one variable into a vector of non-zero constants, try; // to avoid loading each constant element as a scalar. Load the constants as a; // vector and then insert the variable scalar element. If insertion is not; // supported, fall back to a shuffle to get the scalar blended with the; // constants. Insertion into a zero vector is handled as a special-case; // somewhere below here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:90,load,loading,90,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,"['Load', 'load']","['Load', 'loading']"
Performance,"// If we are late in the legalization process and nothing has optimised; // the trunc to anything better, lower it to a stack store and reload,; // performing the truncation whilst keeping the lanes in the correct order:; // VSTRH.32 a, stack; VSTRH.32 b, stack+8; VLDRW.32 stack;",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:148,perform,performing,148,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['perform'],['performing']
Performance,"// If we are loading a legal type, this is a non-extload followed by a; // full extend.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp:13,load,loading,13,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeDAG.cpp,1,['load'],['loading']
Performance,"// If we are loading from NSError**/CFErrorRef* parameter, mark the resulting; // SVal so that we can later check it when handling the; // ImplicitNullDerefEvent event.; // FIXME: Cumbersome! Maybe add hook at construction of SVals at start of; // function ?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NSErrorChecker.cpp:13,load,loading,13,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NSErrorChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/NSErrorChecker.cpp,1,['load'],['loading']
Performance,"// If we are loading from the memory location, we are creating an; // alias of the original pointer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp:13,load,loading,13,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,1,['load'],['loading']
Performance,"// If we are loading the first and last elements of a vector, it is safe and; // always faster to load the whole vector. Replace the masked load with a; // vector load and select.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:13,load,loading,13,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,4,['load'],"['load', 'loading']"
Performance,"// If we are looking at replacing an FPR register we don't expect to; // have any offset. The only compressible FP instructions with an offset; // are loads and stores, for which the offset applies to the GPR operand; // not the FPR operand.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp:151,load,loads,151,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVMakeCompressible.cpp,1,['load'],['loads']
Performance,// If we are looking at the loaded value of the SDNode.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:28,load,loaded,28,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,1,['load'],['loaded']
Performance,// If we are looking at the loaded value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:28,load,loaded,28,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,2,['load'],['loaded']
Performance,"// If we are moving a pair of loads / stores, see if it makes sense; // to try to allocate a pair of registers that can form register pairs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp:30,load,loads,30,interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMLoadStoreOptimizer.cpp,1,['load'],['loads']
Performance,"// If we are moving a scalar into a vector (Ld must be set and all elements; // but 1 are undef) and that operation is not obviously supported by; // vmovd/vmovq/vmovss/vmovsd, then keep trying to form a broadcast.; // That's better than general shuffling and may eliminate a load to GPR and; // move from scalar to vector register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:276,load,load,276,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we are optimizing for code size, 2 instructions in common is enough if; // we don't have to split a block. At worst we will be introducing 1 new; // branch instruction, which is likely to be smaller than the 2; // instructions that would be deleted in the merge.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp:13,optimiz,optimizing,13,interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/BranchFolding.cpp,1,['optimiz'],['optimizing']
Performance,"// If we are optimizing for size, see if the branch in the predecessor can be; // lowered to cbn?z by the constant island lowering pass, and return false if; // so. This results in a shorter instruction sequence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:13,optimiz,optimizing,13,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['optimiz'],['optimizing']
Performance,"// If we are performing a ThinLTO importing compile, load the function index; // into memory and pass it into runThinLTOBackend, which will run the; // function importer and invoke LTO passes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp:13,perform,performing,13,interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,2,"['load', 'perform']","['load', 'performing']"
Performance,"// If we are performing frame pointer elimination and if the callee pops; // something off the stack pointer, add it back.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430FrameLowering.cpp:13,perform,performing,13,interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/MSP430/MSP430FrameLowering.cpp,1,['perform'],['performing']
Performance,"// If we are performing frame pointer elimination and if the callee pops; // something off the stack pointer, add it back. We do this until we have; // more advanced stack pointer tracking ability.; // We are not tracking the stack pointer adjustment by the callee, so make; // sure we restore the stack pointer immediately after the call, there may; // be spill code inserted between the CALL and ADJCALLSTACKUP instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.cpp:13,perform,performing,13,interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.cpp,1,['perform'],['performing']
Performance,"// If we are performing nextDown, swap sign so we have -nextUp(-x)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp:13,perform,performing,13,interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,1,['perform'],['performing']
Performance,"// If we are performing nextDown, swap sign so we have -x.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp:13,perform,performing,13,interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/APFloat.cpp,1,['perform'],['performing']
Performance,"// If we are performing substituting explicitly-specified template arguments; // or deduced template arguments into a function template and we reach this; // point, we are now past the point where SFINAE applies and have committed; // to keeping the new function template specialization. We therefore; // convert the active template instantiation for the function template; // into a template instantiation for this specific function template; // specialization, which is not a SFINAE context, so that we diagnose any; // further errors in the declaration itself.; //; // FIXME: This is a hack.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:13,perform,performing,13,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,1,['perform'],['performing']
Performance,"// If we are planning to perform ThinLTO later, we don't bloat the code with; // unrolling/vectorization/... now. Just simplify the module as much as we; // can.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:25,perform,perform,25,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['perform'],['perform']
Performance,"// If we are reading an object of class type, there may still be more; // things we need to check: if there are any mutable subobjects, we; // cannot perform this read. (This only happens when performing a trivial; // copy or assignment.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:150,perform,perform,150,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,2,['perform'],"['perform', 'performing']"
Performance,"// If we are reducing to a 32-bit load or a smaller multi-dword load,; // this is always better.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelLowering.cpp,2,['load'],['load']
Performance,"// If we are saving a 32-byte vector and 32-byte stores are slow, such as on; // Sandy Bridge, perform two 16-byte stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:95,perform,perform,95,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,// If we are shuffling values from one half - check how many different DWORD; // pairs we need to create. If only 1 or 2 then we can perform this as a; // PSHUFLW/PSHUFHW + PSHUFD instead of the PSHUFD+PSHUFLW+PSHUFHW chain below.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:133,perform,perform,133,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,// If we are tail calling and generating PIC/GOT style code load the; // address of the callee into ECX. The value in ecx is used as target of; // the tail jump. This is done to circumvent the ebx/callee-saved problem; // for tail calls on PIC/GOT architectures. Normally we would just put the; // address of GOT into ebx and then call target@PLT. But for tail calls; // ebx would be restored (since ebx is callee saved) before jumping to the; // target@PLT.; // Note: The actual moving to ECX is done further down.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp:60,load,load,60,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLoweringCall.cpp,1,['load'],['load']
Performance,"// If we are truncating the result of this SHL, and if it's a shift of an; // inrange amount, we can always perform a SHL in a smaller type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp:108,perform,perform,108,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCasts.cpp,1,['perform'],['perform']
Performance,"// If we are unable to perform the extend for free, get the default cost.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp:23,perform,perform,23,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetTransformInfo.cpp,1,['perform'],['perform']
Performance,"// If we are using XMM registers in the ABI and the condition of the select is; // a floating-point compare and we have blendv or conditional move, then it is; // cheaper to select instead of doing a cross-register move and creating a; // load that depends on the compare result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:239,load,load,239,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we are using a TTreeCache, disable reading from the default cache; // temporarily, to force reading directly from file",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBasket.cxx:66,cache,cache,66,tree/tree/src/TBasket.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TBasket.cxx,1,['cache'],['cache']
Performance,"// If we are using a wider index than needed for this platform, shrink; // it to what we need. If narrower, sign-extend it to what we need.; // This explicit cast can make subsequent optimizations more obvious.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:183,optimiz,optimizations,183,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['optimiz'],['optimizations']
Performance,"// If we aren't changing the mask, just return true to keep it and prevent; // the caller from optimizing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:95,optimiz,optimizing,95,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimizing']
Performance,"// If we aren't loading a module (which has its own exports), make; // all of the imported modules visible.; // FIXME: Deal with macros-only imports.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:16,load,loading,16,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loading']
Performance,"// If we can compute the condition, there's no need for a select.; // Like the above fold, we are attempting to reduce compile-time cost by; // putting this fold here with limitations rather than in InstSimplify.; // The motivation for this call into value tracking is to take advantage of; // the assumption cache, so make sure that is populated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp:309,cache,cache,309,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp,1,['cache'],['cache']
Performance,"// If we can determine that the high bit of the shift is zero or one, even if; // the low bits are variable, emit this shift in an optimized form.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp:131,optimiz,optimized,131,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeIntegerTypes.cpp,1,['optimiz'],['optimized']
Performance,"// If we can find a pointer to load from, use it half the time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/FuzzMutate/RandomIRBuilder.cpp:31,load,load,31,interpreter/llvm-project/llvm/lib/FuzzMutate/RandomIRBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/FuzzMutate/RandomIRBuilder.cpp,1,['load'],['load']
Performance,"// If we can find the address of the symbol, we have loaded it. Skip.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:53,load,loaded,53,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,1,['load'],['loaded']
Performance,"// If we can go arbitrarily backwards we will eventually reach an entry point; // that can reach ToI. Only if a set of blocks through which we cannot go is; // provided, or once we track internal functions not accessible from the; // outside, it makes sense to perform backwards analysis in the absence of a; // GoBackwardsCB.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Attributor.cpp:261,perform,perform,261,interpreter/llvm-project/llvm/lib/Transforms/IPO/Attributor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/Attributor.cpp,1,['perform'],['perform']
Performance,"// If we can perform the initialization, and we've not already done so,; // do it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:13,perform,perform,13,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['perform'],['perform']
Performance,"// If we can prove either side non-zero, then equality must imply; // equivalence.; // FIXME: We should do this optimization if 'no signed zeros' is; // applicable via an instruction-level fast-math-flag or some other; // indicator that relaxed FP semantics are being used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:112,optimiz,optimization,112,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,['optimiz'],['optimization']
Performance,"// If we can prove that LHS >= RHS then use LHS as the result. Likewise for; // RHS. Ideally our caller would already have spotted these cases and; // optimized away the umax operation, but we handle them here for; // completeness.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/KnownBits.cpp:151,optimiz,optimized,151,interpreter/llvm-project/llvm/lib/Support/KnownBits.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/KnownBits.cpp,1,['optimiz'],['optimized']
Performance,"// If we can replace 4 or more scalar stores, there will be a reduction; // in instructions even after we add a vector constant load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.h:128,load,load,128,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.h,1,['load'],['load']
Performance,"// If we can replace more than 2 scalar stores, there will be a reduction; // in instructions even after we add a vector constant load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h:130,load,load,130,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.h,1,['load'],['load']
Performance,"// If we can unconditionally load from this address, replace with a; // load/select idiom. TODO: use DT for context sensitive query",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCalls.cpp,2,['load'],['load']
Performance,"// If we can use LEA for SP but we shouldn't, check that none; // of the terminators uses the eflags. Otherwise we will insert; // a ADD that will redefine the eflags and break the condition.; // Alternatively, we could move the ADD, but this may not be possible; // and is an optimization anyway.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FrameLowering.cpp:277,optimiz,optimization,277,interpreter/llvm-project/llvm/lib/Target/X86/X86FrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86FrameLowering.cpp,1,['optimiz'],['optimization']
Performance,"// If we can use the permutation-based load sequence, then this is also; // relatively cheap (not counting loop-invariant instructions): one load plus; // one permute (the last load in a series has extra cost, but we're; // neglecting that here). Note that on the P7, we could do unaligned loads; // for Altivec types using the VSX instructions, but that's more expensive; // than using the permutation-based load sequence. On the P8, that's no; // longer true.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp:39,load,load,39,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCTargetTransformInfo.cpp,5,['load'],"['load', 'loads']"
Performance,"// If we can't broadcast from a register, check that the input is a load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:68,load,load,68,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we can't load the module, exit early since we likely; // will rebuild the module anyway. The stream may be in the; // middle of a block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:15,load,load,15,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['load']
Performance,"// If we can't make use of BEXTR then we can't fuse shift+mask stages.; // Let's perform the mask first, and apply shift later. Note that we need to; // widen the mask to account for the fact that we'll apply shift afterwards!",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:81,perform,perform,81,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['perform'],['perform']
Performance,"// If we can't recognize overflow as undefined behavior, assume that; // overflow saturates. This protects against normal optimizations if we are; // compiling with non-standard FP semantics.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:122,optimiz,optimizations,122,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['optimiz'],['optimizations']
Performance,"// If we can, perform BSWAP first and then the mask+swap the i4, then i2; // and finally the i1 pairs.; // TODO: We can easily support i4/i2 legal types if any target ever does.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:14,perform,perform,14,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,2,['perform'],['perform']
Performance,"// If we cannot hoist the load either, give up.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If we cannot issue immediately, the HWS will add IR to its ready queue for; // execution later, so we must return early here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/Stages/ExecuteStage.cpp:68,queue,queue,68,interpreter/llvm-project/llvm/lib/MCA/Stages/ExecuteStage.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MCA/Stages/ExecuteStage.cpp,1,['queue'],['queue']
Performance,"// If we changed the class of the store, we want to ensure nothing finds the; // old store expression. In particular, loads do not compare against stored; // value, so they will find old store expressions (and associated class; // mappings) if we leave them in the table.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp:118,load,loads,118,interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,1,['load'],['loads']
Performance,"// If we changed the computation somewhere in the DAG, this change will; // affect all users of Cond. Update all the nodes so that we do not use; // the generic VSELECT anymore. Otherwise, we may perform wrong; // optimizations as we messed with the actual expectation for the vector; // boolean values.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:196,perform,perform,196,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,"['optimiz', 'perform']","['optimizations', 'perform']"
Performance,"// If we consider two stores and one smaller in size is a scalable; // vector type and another one a bigger size store with a fixed type,; // then we could not allow the scalable store removal because we don't; // know its final size in the end.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:58,scalab,scalable,58,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,2,['scalab'],['scalable']
Performance,"// If we couldn't prove we can hoist the load, bail.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If we detect that an instruction becomes unprofitable to sink,; // all earlier instructions won't be sunk either,; // so preemptively keep InstructionsProfitableToSink in sync.; // FIXME: is this the most performant approach?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:208,perform,performant,208,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,1,['perform'],['performant']
Performance,"// If we did not add any cached completion results, just forward the; // results we were given to the next consumer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:25,cache,cached,25,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['cache'],['cached']
Performance,"// If we did not find a more suitable source, there is nothing to optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp:66,optimiz,optimize,66,interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,1,['optimiz'],['optimize']
Performance,"// If we did not wait for the initial sync, then we should perform the; // scan when we enter the thread.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp:59,perform,perform,59,interpreter/llvm-project/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/DirectoryWatcher/windows/DirectoryWatcher-windows.cpp,1,['perform'],['perform']
Performance,"// If we didn't fold a load, try to match broadcast. No widening limitation; // for this. But only 32 and 64 bit types are supported.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If we didn't pre-compute the expected return address into a register, then; // red zones are enabled and the return address is still available on the; // stack immediately after the call. As the very first instruction, we load it; // into a register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:225,load,load,225,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['load']
Performance,"// If we do not find a new register covering this PHI, then register; // allocation has dropped its location, for example because it's not live.; // The old VReg will not be mapped to a physreg, and the instruction; // number will have been optimized out.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp:241,optimiz,optimized,241,interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/LiveDebugVariables.cpp,1,['optimiz'],['optimized']
Performance,// If we do not perform SPMDzation we do not need the virtual uses below.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp:16,perform,perform,16,interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/OpenMPOpt.cpp,1,['perform'],['perform']
Performance,"// If we do not support the input floating point vector type, use the base; // one which will calculate as:; // ScalarizeCost + Num * Cost for fixed vector,; // InvalidCost for scalable vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp:177,scalab,scalable,177,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVTargetTransformInfo.cpp,1,['scalab'],['scalable']
Performance,"// If we don't already have information on this module, load the module now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:56,load,load,56,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,1,['load'],['load']
Performance,"// If we don't have 96-bit result scalar loads, widening to 128-bit should; // always be legal. We may need to restore this to a 96-bit result if it turns; // out this needs to be converted to a vector load during RegBankSelect.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:41,load,loads,41,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,2,['load'],"['load', 'loads']"
Performance,// If we don't have PSHUFB then its worth avoiding an AND constant mask; // by performing 3 byte shifts. Shuffle combining can kick in above that.; // TODO: There may be some cases where VSH{LR}DQ+PAND is still better.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:79,perform,performing,79,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performing']
Performance,"// If we don't have VSX on the subtarget, don't do anything.; // Also, on Power 9 the load and store ops preserve element order and so; // the swaps are not required.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp:86,load,load,86,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCVSXSwapRemoval.cpp,1,['load'],['load']
Performance,"// If we don't have a TTree yet only record the cache size wanted",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TChain.cxx:48,cache,cache,48,tree/tree/src/TChain.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TChain.cxx,1,['cache'],['cache']
Performance,"// If we don't have a cached result for this function, look up the pass and; // run it to produce a result, which we then add to the cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/PassManagerImpl.h:22,cache,cached,22,interpreter/llvm-project/llvm/include/llvm/IR/PassManagerImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/PassManagerImpl.h,2,['cache'],"['cache', 'cached']"
Performance,"// If we don't have a definition of the variable template, we won't perform; // any instantiation. Rather, we rely on the user to instantiate this; // definition (or provide a specialization for it) in another translation; // unit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:68,perform,perform,68,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,1,['perform'],['perform']
Performance,"// If we don't have a list of values cached, start constructing it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp:37,cache,cached,37,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/RegionStore.cpp,1,['cache'],['cached']
Performance,"// If we don't have a module cache path or aren't supposed to use one, we; // can't do anything.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:29,cache,cache,29,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['cache'],['cache']
Performance,"// If we don't have it in the cache, convert it now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenTypes.cpp:30,cache,cache,30,interpreter/llvm-project/clang/lib/CodeGen/CodeGenTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenTypes.cpp,1,['cache'],['cache']
Performance,"// If we don't know which one comes first, we can't perform this test.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CStringChecker.cpp:52,perform,perform,52,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CStringChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/CStringChecker.cpp,1,['perform'],['perform']
Performance,"// If we don't need the upper xmm, then perform as a xmm hop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:40,perform,perform,40,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If we during isel used a load-and-test as a compare with 0, the; // def operand is dead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp:28,load,load-and-test,28,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZElimCompare.cpp,1,['load'],['load-and-test']
Performance,"// If we emitted an assembly marker for this call (and the; // ARCEntrypoints field should have been set if so), go looking; // for that call. If we can't find it, we can't do this; // optimization. But it should always be the immediately previous; // instruction, unless we needed bitcasts around the call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:185,optimiz,optimization,185,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['optimiz'],['optimization']
Performance,"// If we encounter a store aliased with the load, return early.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoadStoreOptimizer.cpp,1,['load'],['load']
Performance,"// If we ever want to perform this synthesis more generally, we will need to; // apply the temporary materialization conversion to the operands.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp:22,perform,perform,22,interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaOverload.cpp,1,['perform'],['perform']
Performance,"// If we find a dead NZCV implicit-def, we; // - try to convert the operation to a non-flag-setting equivalent; // - or mark the def as dead to aid later peephole optimizations.; // Use cases:; // 1); // Consider the following code:; // FCMPSrr %0, %1, implicit-def $nzcv; // %sel1:gpr32 = CSELWr %_, %_, 12, implicit $nzcv; // %sub:gpr32 = SUBSWrr %_, %_, implicit-def $nzcv; // FCMPSrr %0, %1, implicit-def $nzcv; // %sel2:gpr32 = CSELWr %_, %_, 12, implicit $nzcv; // This kind of code where we have 2 FCMPs each feeding a CSEL can happen; // when we have a single IR fcmp being used by two selects. During selection,; // to ensure that there can be no clobbering of nzcv between the fcmp and the; // csel, we have to generate an fcmp immediately before each csel is; // selected.; // However, often we can essentially CSE these together later in MachineCSE.; // This doesn't work though if there are unrelated flag-setting instructions; // in between the two FCMPs. In this case, the SUBS defines NZCV; // but it doesn't have any users, being overwritten by the second FCMP.; //; // 2); // The instruction selector always emits the flag-setting variant of ADC/SBC; // while selecting G_UADDE/G_SADDE/G_USUBE/G_SSUBE. If the carry-out of these; // instructions is never used, we can switch to the non-flag-setting variant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp:163,optimiz,optimizations,163,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostSelectOptimize.cpp,1,['optimiz'],['optimizations']
Performance,"// If we find a register that is loaded inside the loop, 1. and 2.; // are invalidated and we can exit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp:33,load,loaded,33,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInsertWaitcnts.cpp,2,['load'],['loaded']
Performance,"// If we find an equality fact, canonicalize all dominated uses in this block; // to one of the two values. We heuristically choice the ""oldest"" of the; // two where age is determined by value number. (Note that propagateEquality; // above handles the cross block case.); //; // Key case to cover are:; // 1); // %cmp = fcmp oeq float 3.000000e+00, %0 ; const on lhs could happen; // call void @llvm.assume(i1 %cmp); // ret float %0 ; will change it to ret float 3.000000e+00; // 2); // %load = load float, float* %addr; // %cmp = fcmp oeq float %load, %0; // call void @llvm.assume(i1 %cmp); // ret float %load ; will change it to ret float %0",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:488,load,load,488,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,4,['load'],['load']
Performance,"// If we found a constant offset in the left operand, stop and return that.; // This shortcut might cause us to miss opportunities of combining the; // constant offsets in both operands, e.g., (a + 4) + (b + 5) => (a + b) + 9.; // However, such cases are probably already handled by -instcombine,; // given this pass runs after the standard optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp:341,optimiz,optimizations,341,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,1,['optimiz'],['optimizations']
Performance,"// If we found a non-zero constant offset, add it to the path for; // rebuildWithoutConstOffset. Zero is a valid constant offset, but doesn't; // help this optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp:156,optimiz,optimization,156,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SeparateConstOffsetFromGEP.cpp,1,['optimiz'],['optimization']
Performance,"// If we found no optimization opportunities on the first iteration, we; // won't find them on later ones too.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp:18,optimiz,optimization,18,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopUnrollPass.cpp,1,['optimiz'],['optimization']
Performance,"// If we found non-loop-inc, non-root users of Base, assume they are; // for the zeroth root index. This is because ""add %a, 0"" gets optimized; // away.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopRerollPass.cpp:133,optimiz,optimized,133,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopRerollPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopRerollPass.cpp,1,['optimiz'],['optimized']
Performance,"// If we found that truncation is beneficial, perform the truncation and; // update 'Op'.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp:46,perform,perform,46,interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kISelLowering.cpp,1,['perform'],['perform']
Performance,"// If we found that we need the queue pointer, nothing else to do.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp:32,queue,queue,32,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAttributor.cpp,1,['queue'],['queue']
Performance,"// If we found the case and skipped declarations, we can't do the; // optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp:70,optimiz,optimization,70,interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,2,['optimiz'],['optimization']
Performance,"// If we generate a single load, we can use that for the chain. Otherwise,; // build a factor node to remember the multiple loads are independent and; // chain to that.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,2,['load'],"['load', 'loads']"
Performance,// If we get here we could have other crazy uses that are transitively; // loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:75,load,loaded,75,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['loaded']
Performance,"// If we get here, the AND is unnecessary. Just replace it with the load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp:68,load,load,68,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelLowering.cpp,1,['load'],['load']
Performance,"// If we get here, the symbols have not been found in the current process,; // so no need to check that again. Instead search for the library that; // provides the symbol and create one MaterializationUnit per library to; // actually load it if needed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx:234,load,load,234,core/metacling/src/TClingCallbacks.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TClingCallbacks.cxx,1,['load'],['load']
Performance,"// If we get here, we can optimize the atomic using a single wavefront-wide; // atomic operation to do the calculation for the entire wavefront, so; // remember the instruction so we can come back to it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:26,optimiz,optimize,26,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,2,['optimiz'],['optimize']
Performance,"// If we got a cache path, then we are supposed to place any modules; // we have to build in this directory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp:15,cache,cache,15,interpreter/cling/lib/Interpreter/CIFactory.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/CIFactory.cpp,1,['cache'],['cache']
Performance,"// If we got here, the loaded value is transparent through to the start of the; // block. Check to see if it is available in any of the predecessor blocks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:23,load,loaded,23,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['load'],['loaded']
Performance,"// If we had a cached FAM proxy originally, we will want to create more of; // them for each SCC that was split off.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp:15,cache,cached,15,interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/CGSCCPassManager.cpp,1,['cache'],['cached']
Performance,// If we had an unaligned load of a float we've converted it to an regular; // load. Now we must move from the GRP to the FP register.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFastISel.cpp,2,['load'],['load']
Performance,"// If we had at least one store that could be merged in, add the starting; // store as well. We try to avoid this unless there is at least something; // interesting as a small compile-time optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:189,optimiz,optimization,189,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['optimiz'],['optimization']
Performance,"// If we had to process more than one hundred blocks to find the; // dependencies, this load isn't worth worrying about. Optimizing; // it will be too expensive.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:88,load,load,88,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,2,"['Optimiz', 'load']","['Optimizing', 'load']"
Performance,"// If we had to widen to perform the insert, then we have to demote back to; // the original size to get the result we want.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:25,perform,perform,25,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['perform'],['perform']
Performance,"// If we happen to be doing an i64 load or store into a stack slot that has; // less than a 4-byte alignment, then the frame-index elimination may need to; // use an indexed load or store instruction (because the offset may not be a; // multiple of 4). The extra register needed to hold the offset comes from the; // register scavenger, and it is possible that the scavenger will need to use; // an emergency spill slot. As a result, we need to make sure that a spill slot; // is allocated when doing an i64 load/store into a less-than-4-byte-aligned; // stack slot.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:35,load,load,35,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,3,['load'],['load']
Performance,"// If we have (sext (setcc A B)) and A and B are cheap to extend,; // we can move the sext into the arguments and have the same result. For; // example, if A and B are both loads, we can make those extending loads and; // avoid an extra instruction. This pattern appears often in VLS code; // generation where the inputs to the setcc have a different size to the; // instruction that wants to use the result of the setcc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:173,load,loads,173,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['loads']
Performance,"// If we have 96-bit memory operations, we shouldn't touch them. Note we may; // end up widening these for a scalar load during RegBankSelect, if we don't; // have 96-bit scalar loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:116,load,load,116,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,2,['load'],"['load', 'loads']"
Performance,"// If we have AVX, we can use VPERMILPS which will allow folding a load; // into the shuffle.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:67,load,load,67,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If we have AVX, we can use a variable vector select (VBLENDV) instead; // of 3 logic instructions for size savings and potentially speed.; // Unfortunately, there is no scalar form of VBLENDV.; // If either operand is a +0.0 constant, don't try this. We can expect to; // optimize away at least one of the logic instructions later in that; // case, so that sequence would be faster than a variable blend.; // BLENDV was introduced with SSE 4.1, but the 2 register form implicitly; // uses XMM0 as the selection register. That may need just as many; // instructions as the AND/ANDN/OR sequence due to register moves, so; // don't bother.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:275,optimiz,optimize,275,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimize']
Performance,"// If we have GEP, we can perform the following folds:; // (ptrtoint (gep null, x)) -> x; // (ptrtoint (gep (gep null, x), y) -> x + y, etc.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp:26,perform,perform,26,interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,1,['perform'],['perform']
Performance,"// If we have Native ARC, set nonlazybind attribute for these APIs for; // performance.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PreISelIntrinsicLowering.cpp:75,perform,performance,75,interpreter/llvm-project/llvm/lib/CodeGen/PreISelIntrinsicLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PreISelIntrinsicLowering.cpp,1,['perform'],['performance']
Performance,"// If we have S_Use or S_CanRelease, perform our check for cfg hazard; // checks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:37,perform,perform,37,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['perform'],['perform']
Performance,"// If we have TBM we can use an immediate for the control. If we have BMI; // we should only do this if the BEXTR instruction is implemented well.; // Otherwise moving the control into a register makes this more costly.; // TODO: Maybe load folding, greater than 32-bit masks, or a guarantee of LICM; // hoisting the move immediate would make it worthwhile with a less optimal; // BEXTR?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:236,load,load,236,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If we have a 64 bit Splat value, we perform a similar sequence to the; // above:; //; // MIPS32: MIPS64:; // lui $res, %highest(val) lui $res, %highest(val); // ori $res, $res, %higher(val) ori $res, $res, %higher(val); // lui $res2, %hi(val) lui $res2, %hi(val); // ori $res2, %res2, %lo(val) ori $res2, %res2, %lo(val); // $res3 = fill $res2 dinsu $res, $res2, 0, 32; // $res4 = insert.w $res3[1], $res fill.d $res; // splat.d $res4, 0; //; // The ability to use dinsu is guaranteed as MSA requires MIPSR5.; // This saves having to materialize the value by shifts and ors.; //; // FIXME: Implement the preferred sequence for MIPS64R6:; //; // MIPS64R6:; // ori $res, $zero, %lo(val); // daui $res, $res, %hi(val); // dahi $res, $res, %higher(val); // dati $res, $res, %highest(cal); // fill.d $res; //",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelDAGToDAG.cpp:39,perform,perform,39,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelDAGToDAG.cpp,1,['perform'],['perform']
Performance,// If we have a Lexer Error we are on an Error Token. Load in Lexer Error; // for printing ErrMsg via Lex() only if no (presumably better) parser error; // exists.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCParser/AsmParser.cpp:54,Load,Load,54,interpreter/llvm-project/llvm/lib/MC/MCParser/AsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/MC/MCParser/AsmParser.cpp,3,['Load'],['Load']
Performance,"// If we have a TChain, we will need to use the branch name; // and we better disambiguate them (see atlasFlushed.root for example); // in order to cache all the requested branches.; // We do not do this all the time as GetMother is slow (it contains; // a linear search from list of top level branch).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx:148,cache,cache,148,tree/tree/src/TTreeCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx,1,['cache'],['cache']
Performance,// If we have a ZEXTLOAD then change the load's type to be a narrower reg; // and zero_extend with SUBREG_TO_REG.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:41,load,load,41,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['load'],['load']
Performance,"// If we have a binary expr, dispatch to the subcode of the binop. A smart; // optimizer (e.g. LLVM) will fold this comparison into the switch stmt; // below.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/StmtVisitor.h:79,optimiz,optimizer,79,interpreter/llvm-project/clang/include/clang/AST/StmtVisitor.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/AST/StmtVisitor.h,1,['optimiz'],['optimizer']
Performance,"// If we have a cache, use it to resolve the stat query.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp:16,cache,cache,16,interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/FileSystemStatCache.cpp,1,['cache'],['cache']
Performance,"// If we have a cached SCEV for the original instruction, make sure the; // new LCSSA phi node is also cached. This makes sures that BECounts; // based on it will be invalidated when the LCSSA phi node is invalidated,; // which some passes rely on.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LCSSA.cpp:16,cache,cached,16,interpreter/llvm-project/llvm/lib/Transforms/Utils/LCSSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LCSSA.cpp,2,['cache'],['cached']
Performance,"// If we have a cached entry, and it is non-dirty, use it as the value for; // this dependency.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:16,cache,cached,16,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['cache'],['cached']
Performance,"// If we have a cached local dependence query for this instruction, remove it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:16,cache,cached,16,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['cache'],['cached']
Performance,"// If we have a cold call site, try to sink addressing computation into the; // cold block. This interacts with our handling for loads and stores to; // ensure that we can fold all uses of a potential addressing computation; // into their uses. TODO: generalize this to work over profiling data",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:129,load,loads,129,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,1,['load'],['loads']
Performance,"// If we have a complex type and the base type is smaller than 8 bytes,; // the ABI calls for the real and imaginary parts to be right-adjusted; // in separate doublewords. However, Clang expects us to produce a; // pointer to a structure with the two parts packed tightly. So generate; // loads of the real and imaginary parts relative to the va_list pointer,; // and store them to a temporary structure.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/PPC.cpp:290,load,loads,290,interpreter/llvm-project/clang/lib/CodeGen/Targets/PPC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/PPC.cpp,1,['load'],['loads']
Performance,"// If we have a complex type and the base type is smaller than the register; // size, the ABI calls for the real and imaginary parts to be right-adjusted; // in separate words in 32bit mode or doublewords in 64bit mode. However,; // Clang expects us to produce a pointer to a structure with the two parts; // packed tightly. So generate loads of the real and imaginary parts relative; // to the va_list pointer, and store them to a temporary structure. We do the; // same as the PPC64ABI here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/PPC.cpp:337,load,loads,337,interpreter/llvm-project/clang/lib/CodeGen/Targets/PPC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/Targets/PPC.cpp,1,['load'],['loads']
Performance,"// If we have a concat of v4i8 loads, convert them to a buildvector of f32; // loads to prevent having to go through the v4i8 load legalization that; // needs to extend each element into a larger type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:31,load,loads,31,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,3,['load'],"['load', 'loads']"
Performance,"// If we have a constant address, prefer to put the constant into the; // offset. This can save moves to load the constant address since multiple; // operations can share the zero base address register, and enables merging; // into read2 / write2 instructions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp:105,load,load,105,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUISelDAGToDAG.cpp,1,['load'],['load']
Performance,// If we have a constant temporary array or record try to promote it into a; // constant global under the same rules a normal constant would've been; // promoted. This is easier on the optimizer and generally emits fewer; // instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:185,optimiz,optimizer,185,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['optimiz'],['optimizer']
Performance,"// If we have a guard variable, check whether we've already performed; // these initializations. This happens for TLS initialization functions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDeclCXX.cpp:60,perform,performed,60,interpreter/llvm-project/clang/lib/CodeGen/CGDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDeclCXX.cpp,1,['perform'],['performed']
Performance,"// If we have a mask, skip instrumentation if we've already; // instrumented the full object. But don't add to TempsToInstrument; // because we might get another load/store with a different mask.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp:162,load,load,162,interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Instrumentation/AddressSanitizer.cpp,1,['load'],['load']
Performance,"// If we have a module map that might map this header, load it and; // check whether we'll have a suggestion for a module.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:55,load,load,55,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['load'],['load']
Performance,"// If we have a non-empty module path, load the named module.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Pragma.cpp:39,load,load,39,interpreter/llvm-project/clang/lib/Lex/Pragma.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/Pragma.cpp,1,['load'],['load']
Performance,"// If we have a pointer to a private array passed into a function; // it will not be optimized out, leaving scratch usage.; // This function calculates the total size in bytes of the memory that would; // end in scratch if the call was not inlined.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:85,optimiz,optimized,85,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,1,['optimiz'],['optimized']
Performance,"// If we have a predicated load/store, it will need extra i1 extracts and; // conditional branches, but may not be executed for each vector lane. Scale; // the cost by the probability of executing the predicated block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['load'],['load']
Performance,"// If we have a receiver expression, perform appropriate promotions; // and determine receiver type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprObjC.cpp:37,perform,perform,37,interpreter/llvm-project/clang/lib/Sema/SemaExprObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprObjC.cpp,1,['perform'],['perform']
Performance,"// If we have a template template parameter with translation unit context,; // then we're performing substitution into a default template argument of; // this template template parameter before we've constructed the template; // that will own this template template parameter. In this case, we; // use empty template parameter lists for all of the outer templates; // to avoid performing any substitutions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp:90,perform,performing,90,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,2,['perform'],['performing']
Performance,"// If we have a truncating store or an extending load with a data size larger; // than 32-bits, we need to reduce to a 32-bit type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:49,load,load,49,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,"// If we have a use that is in the same block as the store, compare the; // indices of the two instructions to see which one came first. If the; // load came before the store, we can't handle it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp:148,load,load,148,interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/PromoteMemoryToRegister.cpp,1,['load'],['load']
Performance,"// If we have a valid (non-zero) ID, see if the logical is already cached.; // If it is not, try to create a direct renderer object.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScenePad.cxx:67,cache,cached,67,graf3d/gl/src/TGLScenePad.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScenePad.cxx,1,['cache'],['cached']
Performance,"// If we have alias analysis and it says the store won't modify the; // loaded value, ignore the store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:72,load,loaded,72,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,1,['load'],['loaded']
Performance,"// If we have already computed it, return the cached result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonGenInsert.cpp:46,cache,cached,46,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonGenInsert.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonGenInsert.cpp,1,['cache'],['cached']
Performance,"// If we have an Objective-C class name followed by an identifier; // and either ':' or ']', this is an Objective-C class message; // send that's missing the opening '['. Recovery; // appropriately. Also take this path if we're performing code; // completion after an Objective-C class name.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp:228,perform,performing,228,interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp,1,['perform'],['performing']
Performance,"// If we have an aggregate, we try to promote it to memset regardless; // of opportunity for merging as it can expose optimization opportunities; // in subsequent passes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:118,optimiz,optimization,118,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['optimiz'],['optimization']
Performance,"// If we have an available version of this load, and if it is the right; // generation or the load is known to be from an invariant location,; // replace this instruction.; //; // If either the dominating load or the current load are invariant, then; // we can assume the current load loads the same value as the dominating; // load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:43,load,load,43,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,7,['load'],"['load', 'loads']"
Performance,"// If we have an external source, ensure that any later redeclarations of this; // context have been loaded, since they may add names to the result of this; // lookup (or add external visible storage).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp:101,load,loaded,101,interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/DeclBase.cpp,1,['load'],['loaded']
Performance,"// If we have an external source, load the entire class method; // pool from the AST file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp:34,load,load,34,interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,2,['load'],['load']
Performance,"// If we have an inalloca parameter that we can safely remove the; // inalloca attribute from, do so. This unlocks optimizations that; // wouldn't be safe in the presence of inalloca.; // FIXME: We should also hoist alloca affected by this to the entry; // block if possible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:115,optimiz,optimizations,115,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['optimiz'],['optimizations']
Performance,"// If we have an instruction which loads or stores, we can't guarantee that; // it is identical.; //; // For example, we may have; //; // %x1 = G_LOAD %addr (load N from @somewhere); // ...; // call @foo; // ...; // %x2 = G_LOAD %addr (load N from @somewhere); // ...; // %or = G_OR %x1, %x2; //; // It's possible that @foo will modify whatever lives at the address we're; // loading from. To be safe, let's just assume that all loads and stores; // are different (unless we have something which is guaranteed to not; // change.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:35,load,loads,35,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,5,['load'],"['load', 'loading', 'loads']"
Performance,"// If we have an object cache, tell it about the new object.; // Note that we're using the compiled image, not the loaded image (as below).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp:24,cache,cache,24,interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/MCJIT/MCJIT.cpp,2,"['cache', 'load']","['cache', 'loaded']"
Performance,"// If we have an unbundling job, we need to create results for all the; // outputs. We also update the results cache so that other actions using; // this unbundling action can get the right results.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp:111,cache,cache,111,interpreter/llvm-project/clang/lib/Driver/Driver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Driver.cpp,1,['cache'],['cache']
Performance,"// If we have any cached dependencies on this instruction, remove; // them.; // If the instruction is a pointer, remove it from both the load info and the; // store info.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:18,cache,cached,18,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,2,"['cache', 'load']","['cached', 'load']"
Performance,"// If we have any vulnerable SVE stack objects then the stack protector; // needs to be placed at the top of the SVE stack area, as the SVE locals; // are placed above the other locals, so we allocate it as if it were a; // scalable vector.; // FIXME: It may be worthwhile having a specific interface for this rather; // than doing it here in finalizeLowering.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:224,scalab,scalable,224,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// If we have at least one (non-frame-index, non-RIP) register operand,; // and neither operand is load-dependent, we need to check the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:99,load,load-dependent,99,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,2,['load'],"['load', 'load-dependent']"
Performance,"// If we have at least one direct use in a FP instruction,; // assume this was a floating point load in the IR. If it was; // not, we would have had a bitcast before reaching that; // instruction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/GISel/RISCVRegisterBankInfo.cpp:96,load,load,96,interpreter/llvm-project/llvm/lib/Target/RISCV/GISel/RISCVRegisterBankInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/GISel/RISCVRegisterBankInfo.cpp,1,['load'],['load']
Performance,"// If we have at least one direct use in a FP instruction,; // assume this was a floating point load in the IR. If it was; // not, we would have had a bitcast before reaching that; // instruction.; //; // Int->FP conversion operations are also captured in; // onlyDefinesFP().",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp:96,load,load,96,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64RegisterBankInfo.cpp,2,['load'],['load']
Performance,"// If we have conditional immediate loads, we always prefer; // using those over an IPM sequence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp:36,load,loads,36,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelDAGToDAG.cpp,1,['load'],['loads']
Performance,"// If we have deserialized a declaration that has a definition the; // AST consumer might need to know about, queue it.; // We don't pass it to the consumer immediately because we may be in recursive; // loading, and some declarations may still be initializing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:110,queue,queue,110,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,2,"['load', 'queue']","['loading', 'queue']"
Performance,"// If we have direct moves, we can do all the conversion, skip the store/load; // however, without FPCVT we can't do most conversions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:73,load,load,73,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If we have either PHIs or Selects to speculate, add them to those; // worklists and re-queue the new alloca so that we promote in on the; // next iteration.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:90,queue,queue,90,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['queue'],['queue']
Performance,"// If we have fewer files than slots and we run multiple event loops, we can reuse fCurrentRanges and don't need; // to worry about loading the fNextRanges. I.e., in this case we don't enter the if block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/src/RNTupleDS.cxx:132,load,loading,132,tree/dataframe/src/RNTupleDS.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/dataframe/src/RNTupleDS.cxx,1,['load'],['loading']
Performance,"// If we have fewer than 8 stores, it can still be worthwhile to do this.; // For example, merging 4 i8 stores into an i32 store is useful almost always.; // However, merging 2 32-bit stores isn't useful on a 32-bit architecture (the; // memset will be split into 2 32-bit stores anyway) and doing so can; // pessimize the llvm optimizer.; //; // Since we don't have perfect knowledge here, make some assumptions: assume; // the maximum GPR width is the same size as the largest legal integer; // size. If so, check to see whether we will end up actually reducing the; // number of stores used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:328,optimiz,optimizer,328,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['optimiz'],['optimizer']
Performance,"// If we have less than 16 loads in a row, and the offsets are within 64; // bytes, then schedule together.; // A cacheline is 64 bytes (for global memory).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:27,load,loads,27,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,2,"['cache', 'load']","['cacheline', 'loads']"
Performance,"// If we have load/store pair instructions and we only have two values,; // don't bother merging.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If we have loads being hardened and we've asked for call and ret edges to; // get a full fence-based mitigation, inject that fence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:14,load,loads,14,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['loads']
Performance,"// If we have multiple loads per block, we need to generate a composite; // comparison using xor+or.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,1,['load'],['loads']
Performance,"// If we have multiple loads per block, we need to generate a composite; // comparison using xor+or. The type for the combinations is the largest load; // type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,2,['load'],"['load', 'loads']"
Performance,"// If we have no interesting conditions or loads, nothing to do here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:43,load,loads,43,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,1,['load'],['loads']
Performance,"// If we have no predecessors that produce a known value for this load, exit; // early.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:66,load,load,66,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['load'],['load']
Performance,"// If we have not seen the use block, create a load instruction to reload; // the spilled value from the coroutine frame. Populates the Value pointer; // reference provided with the frame GEP.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp:47,load,load,47,interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Coroutines/CoroFrame.cpp,1,['load'],['load']
Performance,"// If we have null operands and no critical edges, optimize.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:51,optimiz,optimize,51,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['optimiz'],['optimize']
Performance,"// If we have one element to load/store, return it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp:29,load,load,29,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/LegalizeVectorTypes.cpp,1,['load'],['load']
Performance,"// If we have optimization hints which agree with each other along different; // paths, preserve them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp:14,optimiz,optimization,14,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,1,['optimiz'],['optimization']
Performance,"// If we have physical we should have logical cached, too.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScenePad.cxx:46,cache,cached,46,graf3d/gl/src/TGLScenePad.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/graf3d/gl/src/TGLScenePad.cxx,1,['cache'],['cached']
Performance,"// If we have reached to a load, we need this extra profitability check; // as it could potentially be merged into an ext(load).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/CodeGenPrepare.cpp,2,['load'],['load']
Performance,"// If we have register broadcast instructions, use the scalar size as the; // element type for the shuffle. Then cast to the wider element type. The; // widened bits won't be used, and this might allow the use of a broadcast; // load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:229,load,load,229,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we have some pattern that leaves only some low bits set, and then performs; // left-shift of those bits, if none of the bits that are left after the final; // shift are modified by the mask, we can omit the mask.; //; // There are many variants to this pattern:; // a) (x & ((1 << MaskShAmt) - 1)) << ShiftShAmt; // b) (x & (~(-1 << MaskShAmt))) << ShiftShAmt; // c) (x & (-1 l>> MaskShAmt)) << ShiftShAmt; // d) (x & ((-1 << MaskShAmt) l>> MaskShAmt)) << ShiftShAmt; // e) ((x << MaskShAmt) l>> MaskShAmt) << ShiftShAmt; // f) ((x << MaskShAmt) a>> MaskShAmt) << ShiftShAmt; // All these patterns can be simplified to just:; // x << ShiftShAmt; // iff:; // a,b) (MaskShAmt+ShiftShAmt) u>= bitwidth(x); // c,d,e,f) (ShiftShAmt-MaskShAmt) s>= 0 (i.e. ShiftShAmt u>= MaskShAmt)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp:72,perform,performs,72,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineShifts.cpp,1,['perform'],['performs']
Performance,"// If we have the following code:; // %mask = G_CONSTANT 255; // %ld = G_LOAD %ptr, (load s16); // %and = G_AND %ld, %mask; //; // Try to fold it into; // %ld = G_ZEXTLOAD %ptr, (load s8)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:85,load,load,85,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,2,['load'],['load']
Performance,"// If we have the following:; // %ld = G_LOAD %ptr, (load 2); // %ext = G_SEXT_INREG %ld, 8; // ==>; // %ld = G_SEXTLOAD %ptr (load 1)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,2,['load'],['load']
Performance,// If we have the same ModuleCachePath and PrebuiltModulePath pointing; // to the same folder we should not cache the file lookup failure as it; // may be currently building an implicit module.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:108,cache,cache,108,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['cache'],['cache']
Performance,"// If we have to perform the shuffle with wider elt type than our data type,; // then we will first need to anyext (we don't care about the new bits); // the source elements, and then truncate Dst elements.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:17,perform,perform,17,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['perform'],['perform']
Performance,// If we have to use more than one INSERT_VECTOR_ELT then this; // optimization is likely to increase code size; avoid peforming it in; // such a case. We can use a load from a constant pool in this case.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:67,optimiz,optimization,67,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,2,"['load', 'optimiz']","['load', 'optimization']"
Performance,"// If we have type source information for the underlying type, it means it; // has been explicitly set by the user. Perform substitution on it before; // moving on.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:116,Perform,Perform,116,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,1,['Perform'],['Perform']
Performance,"// If we have valid cached information for exactly the block we are; // investigating, just return it with no recomputation.; // Don't use cached information for invariant loads since it is valid for; // non-invariant loads only.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:20,cache,cached,20,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,4,"['cache', 'load']","['cached', 'loads']"
Performance,"// If we have widened and can perform the transformation, do that here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFlatten.cpp:30,perform,perform,30,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFlatten.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopFlatten.cpp,1,['perform'],['perform']
Performance,"// If we haven't been provided a hint, use the target type for now.; //; // TODO: Take a look at potentially removing this: This is *slightly* wrong; // as it's possible to have a GEP with a foldable target type but a memory; // access that isn't foldable. For example, this load isn't foldable on; // RISC-V:; //; // %p = getelementptr i32, ptr %base, i32 42; // %x = load <2 x i32>, ptr %p",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h:275,load,load,275,interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/TargetTransformInfoImpl.h,2,['load'],['load']
Performance,"// If we haven't found a load, we can't narrow it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['load']
Performance,"// If we haven't found an unwind dest for CurrentPad, we may have queued its; // children, so move on to the next in the worklist.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:66,queue,queued,66,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['queue'],['queued']
Performance,"// If we haven't looked up the dependencies for DependencyJD yet, do it; // now and cache the result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Core.cpp:84,cache,cache,84,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Core.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Core.cpp,1,['cache'],['cache']
Performance,"// If we haven't scheduled anything yet, then we aren't latency limited.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:56,latency,latency,56,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,1,['latency'],['latency']
Performance,"// If we haven't seen any uses in this scheduling region, create a; // dependence edge to ExitSU to model the live-out latency. This is required; // for vreg defs with no in-region use, and prefetches with no vreg def.; //; // FIXME: NumDataSuccs would be more precise than NumSuccs here. This; // check currently relies on being called before adding chain deps.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAGInstrs.cpp:119,latency,latency,119,interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAGInstrs.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ScheduleDAGInstrs.cpp,1,['latency'],['latency']
Performance,"// If we hit a load/store with an invariant.group metadata and the same; // pointer operand, we can assume that value pointed to by the pointer; // operand didn't change.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,1,['load'],['load']
Performance,// If we hit load/store with the same invariant.group metadata (and the; // same pointer operand) we can assume that value pointed by pointer; // operand didn't change.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp:13,load,load,13,interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryDependenceAnalysis.cpp,1,['load'],['load']
Performance,"// If we inferred constant or undef return values for a function, we replaced; // all call uses with the inferred value. This means we don't need to bother; // actually returning anything from the function. Replace all return; // instructions with return undef.; //; // Do this in two stages: first identify the functions we should process, then; // actually zap their returns. This is important because we can only do this; // if the address of the function isn't taken. In cases where a return is the; // last use of a function, the order of processing functions would affect; // whether other functions are optimizable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SCCP.cpp:610,optimiz,optimizable,610,interpreter/llvm-project/llvm/lib/Transforms/IPO/SCCP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SCCP.cpp,1,['optimiz'],['optimizable']
Performance,"// If we inserted a phi node, check to see if it has a single value (e.g. all; // the entries are the same or undef). If so, remove the PHI so it doesn't; // block other optimizations.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp:170,optimiz,optimizations,170,interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/InlineFunction.cpp,1,['optimiz'],['optimizations']
Performance,"// If we just cloned a new assumption, add it the assumption cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:61,cache,cache,61,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['cache'],['cache']
Performance,"// If we just emptied the queue, destroy its bin.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp:26,queue,queue,26,interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,1,['queue'],['queue']
Performance,"// If we just loaded a module map file, look for the module again.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:14,load,loaded,14,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['load'],['loaded']
Performance,"// If we know LHS/RHS share the same sign bit at each element we can; // make this signed.; // NOTE: `computeKnownBits` on a vector type aggregates common bits; // across all lanes. So a pattern where the sign varies from lane to; // lane, but at each lane Sign(LHS) is known to equal Sign(RHS), will be; // missed. We could get around this by demanding each lane; // independently, but this isn't the most important optimization and; // that may eat into compile time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:417,optimiz,optimization,417,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimization']
Performance,"// If we know `AR` == {`PreStart`+`Step`,+,`Step`} is `WrapType` (FlagNSW; // or FlagNUW) and that `PreStart` + `Step` is `WrapType` too, then; // `PreAR` == {`PreStart`,+,`Step`} is also `WrapType`. Cache this fact.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp:200,Cache,Cache,200,interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ScalarEvolution.cpp,1,['Cache'],['Cache']
Performance,"// If we know the exact VLEN and our fixed length vector completely fills; // the container, use a whole register load instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:114,load,load,114,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['load'],['load']
Performance,"// If we know the index we're going to insert at, we can shrink Vec so that; // we're performing the scalar inserts and slideup on a smaller LMUL.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:86,perform,performing,86,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['perform'],['performing']
Performance,"// If we known this is a NaN, and it's scalable vector, we must have a splat; // on our hands. Grab that before splatting a QNaN constant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp:39,scalab,scalable,39,interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/InstructionSimplify.cpp,1,['scalab'],['scalable']
Performance,"// If we load MinVecNumElts, will our target element still be loaded?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VectorCombine.cpp,2,['load'],"['load', 'loaded']"
Performance,"// If we load from memory that may alias the memory we store to,; // memmove must be used to preserve semantic. If not, memcpy can; // be used. Also, if we load from constant memory, memcpy can be used; // as the constant memory won't be modified.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:9,load,load,9,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,2,['load'],['load']
Performance,"// If we loaded from an AST file, balance out the BeginSourceFile call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:9,load,loaded,9,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['load'],['loaded']
Performance,"// If we make it this far we have a load with an 32-bit immediate offset.; // It is OK to select this using a sgpr offset, because we have already; // failed trying to select this load into one of the _IMM variants since; // the _IMM Patterns are considered before the _SGPR patterns.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp:36,load,load,36,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUInstructionSelector.cpp,2,['load'],['load']
Performance,"// If we might be in a template, perform a tentative parse to check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseCXXInlineMethods.cpp:33,perform,perform,33,interpreter/llvm-project/clang/lib/Parse/ParseCXXInlineMethods.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseCXXInlineMethods.cpp,1,['perform'],['perform']
Performance,"// If we need this expected index to be a zero element, then update the; // relevant zero mask and perform the known bits at the end to minimize; // repeated computes.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:99,perform,perform,99,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If we need to check for the named return value optimization, save the; // return statement in our scope for later processing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp:50,optimiz,optimization,50,interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,1,['optimiz'],['optimization']
Performance,"// If we need to check for the named return value optimization,; // or if we need to infer the return type,; // save the return statement in our scope for later processing.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp:50,optimiz,optimization,50,interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,1,['optimiz'],['optimization']
Performance,"// If we need to check two pointers to the same underlying object; // with a non-constant difference, we shouldn't perform any pointer; // grouping with those pointers. This is because we can easily get; // into cases where the resulting check would return false, even when; // the accesses are safe.; //; // The following example shows this:; // for (i = 0; i < 1000; ++i); // a[5000 + i * m] = a[i] + a[i + 9000]; //; // Here grouping gives a check of (5000, 5000 + 1000 * m) against; // (0, 10000) which is always false. However, if m is 1, there is no; // dependence. Not grouping the checks for a[i] and a[i + 9000] allows; // us to perform an accurate check in this case.; //; // The above case requires that we have an UnknownDependence between; // accesses to the same underlying object. This cannot happen unless; // FoundNonConstantDistanceDependence is set, and therefore UseDependencies; // is also false. In this case we will use the fallback path and create; // separate checking groups for all pointers.; // If we don't have the dependency partitions, construct a new; // checking pointer group for each pointer. This is also required; // for correctness, because in this case we can have checking between; // pointers to the same underlying object.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp:115,perform,perform,115,interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopAccessAnalysis.cpp,2,['perform'],['perform']
Performance,"// If we need to copy, then the load has to be conditional, which; // means we need control flow.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:32,load,load,32,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['load']
Performance,"// If we need to insert new load in multiple predecessors, reject it.; // FIXME: If we could restructure the CFG, we could make a common pred with; // all the preds that don't have an available Load and insert a new load into; // that one block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:28,load,load,28,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,3,"['Load', 'load']","['Load', 'load']"
Performance,"// If we need to keep track of the depth, we can't perform data recursion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/ASTMatchers/ASTMatchFinder.cpp:51,perform,perform,51,interpreter/llvm-project/clang/lib/ASTMatchers/ASTMatchFinder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/ASTMatchers/ASTMatchFinder.cpp,1,['perform'],['perform']
Performance,"// If we need to widen, we can't fold the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:42,load,load,42,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['load'],['load']
Performance,"// If we nuked all of the loads, then none of the stores are needed either,; // nor is the global.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:26,load,loads,26,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['loads']
Performance,"// If we optimistically assume that the MIG routine never re-uses the storage; // that was passed to it as arguments when it invalidates it (but at most when; // it assigns to parameter variables directly), this procedure correctly; // determines if the value was loaded from the transitive closure of MIG; // routine arguments in the heap.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/MIGChecker.cpp:264,load,loaded,264,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/MIGChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/MIGChecker.cpp,1,['load'],['loaded']
Performance,"// If we optimize for code size, try to move the call to free before the null; // test so that simplify cfg can remove the empty block and dead code; // elimination the branch. I.e., helps to turn something like:; // if (foo) free(foo);; // into; // free(foo);; //; // Note that we can only do this for 'free' and not for any flavor of; // 'operator delete'; there is no 'operator delete' symbol for which we are; // permitted to invent a call, even if we're passing in a null pointer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp:9,optimiz,optimize,9,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstructionCombining.cpp,1,['optimiz'],['optimize']
Performance,"// If we pass an invariant load, we know that memory location is; // indefinitely constant from the moment of first dereferenceability.; // We conservatively treat the invariant_load as that moment. If we; // pass a invariant load after already establishing a scope, don't; // restart it since we want to preserve the earliest point seen.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp:27,load,load,27,interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/EarlyCSE.cpp,2,['load'],['load']
Performance,// If we performed any kills then recompute EXEC,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp:9,perform,performed,9,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIWholeQuadMode.cpp,1,['perform'],['performed']
Performance,"// If we performed any translation on the file name at all, we need to; // save this string, since the generator will refer to it later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp:9,perform,performed,9,interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTWriter.cpp,1,['perform'],['performed']
Performance,"// If we performed lookup into a dependent context and did not find anything,; // that's fine: just build a dependent nested-name-specifier.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp:9,perform,performed,9,interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp,1,['perform'],['performed']
Performance,"// If we performed typo correction, we might have added a scope specifier; // and changed the decl context.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:9,perform,performed,9,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['performed']
Performance,"// If we reached here with a non-empty index file name, then the index; // file was empty and we are not performing ThinLTO backend compilation; // (used in testing in a distributed build environment).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp:105,perform,performing,105,interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/BackendUtil.cpp,1,['perform'],['performing']
Performance,"// If we really need inline code and the target declined to provide it,; // use a (potentially long) sequence of loads and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp:113,load,loads,113,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp,2,['load'],['loads']
Performance,"// If we require an extra load to get this address, as in PIC mode, we; // can't accept it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we run into an instruction we can't fold across, discard; // the load candidates. Note: We might be able to fold *into* this; // instruction, so this needs to be after the folding logic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp:71,load,load,71,interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PeepholeOptimizer.cpp,1,['load'],['load']
Performance,"// If we see a load, then we keep track of any values defined by it.; // In the following example, STP formation will fail anyway because; // the latter store is using a load result that appears after the; // the prior store. In this situation if we factor out the offset then; // we increase code size for no benefit.; // G_STORE %v1:_(s64), %base:_(p0) :: (store (s64)); // %v2:_(s64) = G_LOAD %ldptr:_(p0) :: (load (s64)); // G_STORE %v2:_(s64), %base:_(p0) :: (store (s64))",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp:15,load,load,15,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64PostLegalizerCombiner.cpp,3,['load'],['load']
Performance,// If we should not perform sign-extension then we must add/or/subtract zero.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp:20,perform,perform,20,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineAddSub.cpp,1,['perform'],['perform']
Performance,"// If we should perform a cleanup, force them now. Note that; // this ends the cleanup scope before rescoping any labels.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.h:16,perform,perform,16,interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenFunction.h,1,['perform'],['perform']
Performance,"// If we skip the left (true) side of the if statement we may,; // skip some of the branch loading (since we remove duplicate branch; // request (in TTreeFormula constructor) and so we need to force the; // loading here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx:91,load,loading,91,tree/treeplayer/src/TTreeFormula.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx,2,['load'],['loading']
Performance,"// If we started from the first queue, we're done.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp:32,queue,queue,32,interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/OptimizedStructLayout.cpp,1,['queue'],['queue']
Performance,"// If we succeed in our optimization, fall through.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp:24,optimiz,optimization,24,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCContract.cpp,1,['optimiz'],['optimization']
Performance,"// If we succeeded, don't re-select the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGISel.cpp,1,['load'],['load']
Performance,"// If we suppressed any diagnostics while performing template argument; // deduction, and if we haven't already instantiated this declaration,; // keep track of these diagnostics. They'll be emitted if this specialization; // is actually used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp:42,perform,performing,42,interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateDeduction.cpp,1,['perform'],['performing']
Performance,"// If we want to emit all the vtables, we need to mark it as used. This; // is especially required for cases like vtable assumption loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:132,load,loads,132,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['load'],['loads']
Performance,// If we were able to delete all uses of the loads,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['loads']
Performance,"// If we were able to eliminate all unsafe uses for a type checked load,; // eliminate the associated type tests by replacing them with true.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp:67,load,load,67,interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/WholeProgramDevirt.cpp,1,['load'],['load']
Performance,"// If we were asked to load any module files, do so now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp:23,load,load,23,interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp,1,['load'],['load']
Performance,"// If we were asked to load any module map files, do so now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp:23,load,load,23,interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/FrontendAction.cpp,1,['load'],['load']
Performance,"// If we were unable to make anything above, all we can is to canonicalize; // the comparison hoping that it will open the doors for other; // optimizations. If we find out that we compare two non-negative values,; // we turn the instruction's predicate to its unsigned version. Note that; // we cannot rely on Pred here unless we check if we have swapped it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp:143,optimiz,optimizations,143,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyIndVar.cpp,1,['optimiz'],['optimizations']
Performance,"// If we were unable to open the file, then we are in an inconsistent; // situation where the content cache referenced a file which no longer; // exists. Most likely, we were using a stat cache with an invalid entry but; // the file could also have been removed during processing. Since we can't; // really deal with this situation, just create an empty buffer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:102,cache,cache,102,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,2,['cache'],['cache']
Performance,"// If we won't be able to fold this to the memory form of UNPCKL, use; // MOVHPD instead. Done as custom because we can't have this in the load; // table twice.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:139,load,load,139,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['load'],['load']
Performance,"// If we would have cleared the state foregoing the fact that we are known; // safe, stop code motion. This is because whether or not it is safe to; // remove RR pairs via KnownSafe is an orthogonal concept to whether we; // are allowed to perform code motion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:240,perform,perform,240,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['perform'],['perform']
Performance,"// If we're about to fall out of a scope without hitting a 'break;', we; // can't perform the optimization if there were any decls in that scope; // (we'd lose their end-of-lifetime).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp:82,perform,perform,82,interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGStmt.cpp,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"// If we're actually at the argument expression (rather than prior to the; // selector), we're actually performing code completion for an expression.; // Determine whether we have a single, best method. If so, we can; // code-complete the expression using the corresponding parameter type as; // our preferred type, improving completion results.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp:104,perform,performing,104,interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,2,['perform'],['performing']
Performance,"// If we're anywhere in a function, method, or closure context, don't perform; // completeness checks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaType.cpp:70,perform,perform,70,interpreter/llvm-project/clang/lib/Sema/SemaType.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaType.cpp,1,['perform'],['perform']
Performance,"// If we're at -O1 and above, we don't want to litter the code; // with this marker yet, so leave a breadcrumb for the ARC; // optimizer to pick up.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:127,optimiz,optimizer,127,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['optimiz'],['optimizer']
Performance,"// If we're at the end of generating a preamble, we should record the; // unterminated \#pragma clang assume_nonnull so we can restore it later; // when the preamble is loaded into the main file.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPLexerChange.cpp:169,load,loaded,169,interpreter/llvm-project/clang/lib/Lex/PPLexerChange.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPLexerChange.cpp,1,['load'],['loaded']
Performance,"// If we're backed by a real file system, perform; // the detection only once and save the result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Distro.cpp:42,perform,perform,42,interpreter/llvm-project/clang/lib/Driver/Distro.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Driver/Distro.cpp,1,['perform'],['perform']
Performance,"// If we're building a module and are supposed to load API notes,; // notify the API notes manager.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:50,load,load,50,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,1,['load'],['load']
Performance,"// If we're building a vector out of consecutive loads, just load that; // vector type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:49,load,loads,49,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],"['load', 'loads']"
Performance,"// If we're caching global code-completion results, and the top-level; // declarations have changed, clear out the code-completion cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:131,cache,cache,131,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['cache'],['cache']
Performance,"// If we're compiling for a specific vector-length, we can check if the; // pattern's VL equals that of the scalable vector at runtime.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:108,scalab,scalable,108,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['scalab'],['scalable']
Performance,"// If we're compiling for an exact VLEN value and we have a known; // constant index, we can always perform the extract in m1 (or; // smaller) as we can determine the register corresponding to; // the index in the register group.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:100,perform,perform,100,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['perform'],['perform']
Performance,"// If we're compiling for an exact VLEN value, we can always perform; // the insert in m1 as we can determine the register corresponding to; // the index in the register group.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:61,perform,perform,61,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,1,['perform'],['perform']
Performance,"// If we're concatenating a series of vector loads like; // concat_vectors (load v4i8, p+0), (load v4i8, p+n), (load v4i8, p+n*2) ...; // Then we can turn this into a strided load by widening the vector elements; // vlse32 p, stride=n",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp:45,load,loads,45,interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/RISCVISelLowering.cpp,5,['load'],"['load', 'loads']"
Performance,"// If we're converting from a float, to an int, and back to a float again,; // then we don't need the store/load pair at all.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:108,load,load,108,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['load'],['load']
Performance,"// If we're dealing with two known constants, just perform the operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/SimpleSValBuilder.cpp:51,perform,perform,51,interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/SimpleSValBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Core/SimpleSValBuilder.cpp,1,['perform'],['perform']
Performance,"// If we're emitting a method, and self is const (meaning just ARC, for now),; // and the receiver is a load of self, then self is a valid object.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCRuntime.cpp:104,load,load,104,interpreter/llvm-project/clang/lib/CodeGen/CGObjCRuntime.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCRuntime.cpp,1,['load'],['load']
Performance,"// If we're enabling GP optimizations, use hardware square root",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:24,optimiz,optimizations,24,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,1,['optimiz'],['optimizations']
Performance,"// If we're extracting a single element from a broadcast load and there are; // no other users, just create a single load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:57,load,load,57,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,2,['load'],['load']
Performance,"// If we're extracting non-least-significant bits, shift so we can truncate.; // Hopefully, we can fold away the trunc/srl/load into the broadcast.; // Even if we can't (and !isShuffleFoldableLoad(Scalar)), prefer; // vpbroadcast+vmovd+shr to vpshufb(m)+vmovd.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:123,load,load,123,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we're extracting the lowest subvector and we're the only user,; // we may be able to perform this with a smaller vector width.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:91,perform,perform,91,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['perform']
Performance,"// If we're generating memory instructions, don't load in the value for; // the register with the stack pointer as it will be used later to finish; // the setup.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Assembler.cpp:50,load,load,50,interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Assembler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-exegesis/lib/Assembler.cpp,1,['load'],['load']
Performance,"// If we're generating more than one load, compute the base address of; // subsequent loads as an offset from the previous.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:37,load,load,37,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,4,['load'],"['load', 'loads']"
Performance,"// If we're going to generate more than one load, reset the sub-vector type; // to something legal.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['load']
Performance,"// If we're implicitly building modules but not currently recursively; // building a module, check whether we need to prune the module cache.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:135,cache,cache,135,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,1,['cache'],['cache']
Performance,"// If we're implicitly loading a module, the base directory can't; // change between the build and use.; // Don't emit module relocation error if we have -fno-validate-pch",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:23,load,loading,23,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loading']
Performance,"// If we're indexing into an object of a known size, and the outer index is; // not a constant, but having any value but zero would lead to undefined; // behavior, replace it with zero.; //; // For example, if we have:; // @f.a = private unnamed_addr constant [1 x i32] [i32 12], align 4; // ...; // %arrayidx = getelementptr inbounds [1 x i32]* @f.a, i64 0, i64 %x; // ... = load i32* %arrayidx, align 4; // Then we know that we can replace %x in the GEP with i64 0.; //; // FIXME: We could fold any GEP index to zero that would cause UB if it were; // not zero. Currently, we only handle the first such index. Also, we could; // also search through non-zero constant indices if we kept track of the; // offsets those indices implied.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:376,load,load,376,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['load']
Performance,"// If we're initializing to null, just write null to memory; no need; // to get the runtime involved. But don't do this if optimization; // is enabled, because accounting for this would make the optimizer; // much more complicated.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:123,optimiz,optimization,123,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,2,['optimiz'],"['optimization', 'optimizer']"
Performance,"// If we're inserting an atomic load in the preheader, we must be able to; // lower it. We're only guaranteed to be able to lower naturally aligned; // atomics.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:32,load,load,32,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If we're inserting an element from a vbroadcast load, fold the; // load into the X86insertps instruction. We need to convert the scalar; // load to a vector and clear the source lane of the INSERTPS control.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:51,load,load,51,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,['load'],['load']
Performance,"// If we're inside a class definition, cache the tokens; // corresponding to the default argument. We'll actually parse; // them when we see the end of the class definition.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseDecl.cpp:39,cache,cache,39,interpreter/llvm-project/clang/lib/Parse/ParseDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseDecl.cpp,1,['cache'],['cache']
Performance,"// If we're just performing this lookup for error-recovery purposes,; // don't extend the nested-name-specifier. Just return now.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp:17,perform,performing,17,interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCXXScopeSpec.cpp,1,['perform'],['performing']
Performance,"// If we're loading from a volatile type, force the destination; // into existence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp:12,load,loading,12,interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprAgg.cpp,1,['load'],['loading']
Performance,"// If we're loading off the beginning of the global, some bytes may be valid.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp:12,load,loading,12,interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ConstantFolding.cpp,1,['load'],['loading']
Performance,"// If we're loading retained from a __strong xvalue, we can avoid; // an extra retain/release pair by zeroing out the source of this; // ""move"" operation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp:12,load,loading,12,interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjC.cpp,1,['load'],['loading']
Performance,"// If we're loading/storing from an alloca, align it if possible.; //; // FIXME: We eagerly upgrade the alignment, regardless of whether TTI; // tells us this is beneficial. This feels a bit odd, but it matches; // existing tests. This isn't *so* bad, because at most we align to 4; // bytes (current value of StackAdjustedAlignment).; //; // FIXME: We will upgrade the alignment of the alloca even if it turns out; // we can't vectorize for some other reason.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp:12,load,loading,12,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoadStoreVectorizer.cpp,1,['load'],['loading']
Performance,"// If we're negating a FMUL node on a target with FMA, then we can avoid the; // use of a constant by performing (-0 - A*B) instead.; // FIXME: Check rounding control flags as well once it becomes available.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:102,perform,performing,102,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performing']
Performance,"// If we're not allowed to implicitly load API notes files, we're done.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/APINotes/APINotesManager.cpp:38,load,load,38,interpreter/llvm-project/clang/lib/APINotes/APINotesManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/APINotes/APINotesManager.cpp,1,['load'],['load']
Performance,"// If we're not expecting to pull this file out of the module cache, it; // might have a different mtime due to being moved across filesystems in; // a distributed build. The size must still match, though. (As must the; // contents, but we can't check that.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ModuleManager.cpp:62,cache,cache,62,interpreter/llvm-project/clang/lib/Serialization/ModuleManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ModuleManager.cpp,1,['cache'],['cache']
Performance,"// If we're not folding a load into a subreg, the size of the load is the; // size of the spill slot. But if we are, we need to figure out what the; // actual load size is.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp:26,load,load,26,interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/TargetInstrInfo.cpp,3,['load'],['load']
Performance,"// If we're not in aggressive mode, we only optimize if we have some; // confidence that by optimizing we'll allow P and/or Q to be if-converted.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp:44,optimiz,optimize,44,interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/SimplifyCFG.cpp,2,['optimiz'],"['optimize', 'optimizing']"
Performance,"// If we're not loading an AST context, we don't care about most records.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:16,load,loading,16,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loading']
Performance,"// If we're not performing a select/blend shuffle, see if we can convert the; // shuffle into a AND node, with all the out-of-lane elements are known zero.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:16,perform,performing,16,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['perform'],['performing']
Performance,"// If we're not performing redeclaration lookup, do not look for local; // extern declarations outside of a function scope.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:16,perform,performing,16,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['perform'],['performing']
Performance,"// If we're not running as a child, the first thing that we must do is; // determine what the problem is. Does the optimization series crash the; // compiler, or does it produce illegal code? We make the top-level; // decision by trying to run all of the passes on the input program,; // which should generate a bitcode file. If it does generate a bitcode; // file, then we know the compiler didn't crash, so try to diagnose a; // miscompilation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/BugDriver.cpp:115,optimiz,optimization,115,interpreter/llvm-project/llvm/tools/bugpoint/BugDriver.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/bugpoint/BugDriver.cpp,1,['optimiz'],['optimization']
Performance,"// If we're on ELFv1, then we need to load the actual function pointer; // from the function descriptor.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp:38,load,load,38,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCAsmPrinter.cpp,1,['load'],['load']
Performance,"// If we're only optimizing for size, ignore non-minsize functions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalMerge.cpp:17,optimiz,optimizing,17,interpreter/llvm-project/llvm/lib/CodeGen/GlobalMerge.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalMerge.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're optimizing an atomic within a pixel shader, we need to wrap the; // entire atomic operation in a helper-lane check. We do not want any helper; // lanes that are around only for the purposes of derivatives to take part; // in any cross-lane communication, and we use a branch on whether the lane is; // live to do this.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp:12,optimiz,optimizing,12,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUAtomicOptimizer.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're optimizing for minimum size and the function is called three or; // more times in this block, we can improve codesize by calling indirectly; // as BLXr has a 16-bit encoding.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:12,optimiz,optimizing,12,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're optimizing for size, only fold if at least one of the constants is; // only used once or the combined shuffle has included a variable mask; // shuffle, this is to avoid constant pool bloat.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:12,optimiz,optimizing,12,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're optimizing for size, try to use MOVSD/MOVSS.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:12,optimiz,optimizing,12,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're optimizing the function for size, we shouldn't specialize it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionSpecialization.cpp:12,optimiz,optimizing,12,interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionSpecialization.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/FunctionSpecialization.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're optimizing, collapse all calls to trap down to just one per; // check-type per function to save on code size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:12,optimiz,optimizing,12,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['optimiz'],['optimizing']
Performance,"// If we're performing a partial argument substitution, allow any trailing; // pack expansions; they might be empty. This can happen even if; // PartialTemplateArgs is false (the list of arguments is complete but; // still dependent).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplate.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplate.cpp,1,['perform'],['performing']
Performance,"// If we're performing a partial expansion but we also have a full expansion,; // expand to the number of common arguments. For example, given:; //; // template<typename ...T> struct A {; // template<typename ...U> void f(pair<T, U>...);; // };; //; // ... a call to 'A<int, int>().f<int>' should expand the pack once and; // retain an expansion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateVariadic.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateVariadic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateVariadic.cpp,1,['perform'],['performing']
Performance,"// If we're performing a partial substitution during template argument; // deduction, we may not have values for template parameters yet.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiate.cpp,1,['perform'],['performing']
Performance,"// If we're performing a partial substitution during template argument; // deduction, we may not have values for template parameters yet. They; // just map to themselves.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,1,['perform'],['performing']
Performance,"// If we're performing an unary shuffle on a SETCC result, try to shuffle the; // ops instead.; // TODO: What other unary shuffles would benefit from this?",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:12,perform,performing,12,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['perform'],['performing']
Performance,"// If we're performing qualified name lookup into a dependent class,; // then we are actually looking into a current instantiation. If we have any; // dependent base classes, then we either have to delay lookup until; // template instantiation time (at which point all bases will be available); // or we have to fail.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['perform'],['performing']
Performance,"// If we're performing recursive template instantiation, create our own; // queue of pending implicit instantiations that we will instantiate later,; // while we're still within our own instantiation context.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,2,"['perform', 'queue']","['performing', 'queue']"
Performance,"// If we're performing recursive template instantiation, create our own; // queue of pending implicit instantiations that we will instantiate later,; // while we're still within our own instantiation context.; // This has to happen before LateTemplateParser below is called, so that; // it marks vtables used in late parsed templates as used.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,2,"['perform', 'queue']","['performing', 'queue']"
Performance,"// If we're performing recursive template instantiation, create our own; // queue of pending implicit instantiations that we will instantiate; // later, while we're still within our own instantiation context.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,2,"['perform', 'queue']","['performing', 'queue']"
Performance,"// If we're performing substitution while we're inside the template; // definition, we'll find our own context. We're done.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp:12,perform,performing,12,interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaTemplateInstantiateDecl.cpp,1,['perform'],['performing']
Performance,// If we're reducing the load width in order to avoid having to use an extra; // instruction to do extension then it's probably a good idea.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['load']
Performance,"// If we're setting the CR, the original load-immediate must be kept (as an; // operand to ANDI_rec/ANDI8_rec).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:41,load,load-immediate,41,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load-immediate']
Performance,"// If we're spilling the source of an LDGR or LGDR, load the; // destination register instead.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZInstrInfo.cpp,1,['load'],['load']
Performance,"// If we're splatting the lower half subvector of a full vector load into the; // upper half, attempt to create a subvector broadcast.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:64,load,load,64,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['load']
Performance,"// If we've added to the set of alive bits (or the operand has not; // been previously visited), then re-queue the operand to be visited; // again.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/DemandedBits.cpp:105,queue,queue,105,interpreter/llvm-project/llvm/lib/Analysis/DemandedBits.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/DemandedBits.cpp,1,['queue'],['queue']
Performance,"// If we've already decided to harden a non-load, we must have sunk; // some other post-load hardened instruction to it and it must itself; // be data-invariant.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp:44,load,load,44,interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86SpeculativeLoadHardening.cpp,2,['load'],['load']
Performance,"// If we've already handled this import, just return the cached result.; // This one-element cache is important to eliminate redundant diagnostics; // when both the preprocessor and parser see the same import declaration.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp:57,cache,cached,57,interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/CompilerInstance.cpp,2,['cache'],"['cache', 'cached']"
Performance,"// If we've already loaded a module map file covering this module, we may; // have a better path for it (relative to the current build).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:20,load,loaded,20,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['load'],['loaded']
Performance,"// If we've already loaded the decl, perform the updates when we finish; // loading this block.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:20,load,loaded,20,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,6,"['load', 'perform']","['loaded', 'loading', 'perform']"
Performance,"// If we've already loaded this input file, return it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:20,load,loaded,20,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,2,['load'],['loaded']
Performance,"// If we've already loaded this library, tell the caller.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/DynamicLibrary.cpp:20,load,loaded,20,interpreter/llvm-project/llvm/lib/Support/DynamicLibrary.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/DynamicLibrary.cpp,1,['load'],['loaded']
Performance,"// If we've already performed the exhaustive search for module maps in this; // search directory, don't do it again.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp:20,perform,performed,20,interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/HeaderSearch.cpp,1,['perform'],['performed']
Performance,"// If we've already performed zero-initialization, we're already done.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp:20,perform,performed,20,interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/ExprConstant.cpp,1,['perform'],['performed']
Performance,"// If we've already replaced the input, StoredVal will be a cast or; // select instruction. If not, it will be a load of the original; // global.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:113,load,load,113,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['load']
Performance,"// If we've already visited this path with this MemoryLocation, we don't; // need to do so again.; //; // NOTE: That we just drop these paths on the ground makes caching; // behavior sporadic. e.g. given a diamond:; // A; // B C; // D; //; // ...If we walk D, B, A, C, we'll only cache the result of phi; // optimization for A, B, and D; C will be skipped because it dies here.; // This arguably isn't the worst thing ever, since:; // - We generally query things in a top-down order, so if we got below D; // without needing cache entries for {C, MemLoc}, then chances are; // that those cache entries would end up ultimately unused.; // - We still cache things for A, so C only needs to walk up a bit.; // If this behavior becomes problematic, we can fix without a ton of extra; // work.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp:280,cache,cache,280,interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSA.cpp,5,"['cache', 'optimiz']","['cache', 'optimization']"
Performance,"// If we've flagged this as an IR file, cache it",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/toy.cpp:40,cache,cache,40,interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/toy.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/examples/Kaleidoscope/MCJIT/cached/toy.cpp,4,['cache'],['cache']
Performance,// If we've just reached a new JITDylib then perform some setup.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Core.cpp:45,perform,perform,45,interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Core.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/Orc/Core.cpp,1,['perform'],['perform']
Performance,"// If we've loaded all of the category information we care about from; // this module file, we're done.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp:12,load,loaded,12,interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReaderDecl.cpp,1,['load'],['loaded']
Performance,"// If we've past an instruction from a future iteration that may have; // side effects, and this instruction might also, then we can't reorder; // them, and this matching fails. As an exception, we allow the alias; // set tracker to handle regular (unordered) load/store dependencies.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopRerollPass.cpp:260,load,load,260,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopRerollPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopRerollPass.cpp,1,['load'],['load']
Performance,"// If we've performed some kind of recovery, (re-)build the type source; // information.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:12,perform,performed,12,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['perform'],['performed']
Performance,"// If we've still failed to prove we can sink the store, hoist the load; // only, if possible.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp:67,load,load,67,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LICM.cpp,1,['load'],['load']
Performance,"// If we've used a zero-extending load that we will sign-extend,; // just do a sign-extending load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCMIPeephole.cpp,4,['load'],['load']
Performance,"// If would not be profitable if the common base has only one load/store, ISEL; // should already be able to choose best load/store form based on offset for; // single load/store. Set minimal profitable value default to 2 and make it as; // an option.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp:62,load,load,62,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCLoopInstrFormPrep.cpp,3,['load'],['load']
Performance,"// If/when we have access to a copy constructor (or better to a move; // constructor), this function should also perform the data move.; // For now we just information the repository.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:113,perform,perform,113,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,1,['perform'],['perform']
Performance,"// Ignore A if it's already in a group or isn't the same kind of memory; // operation as B.; // Note that mayReadFromMemory() isn't mutually exclusive to; // mayWriteToMemory in the case of atomic loads. We shouldn't see those; // here, canVectorizeMemory() should have returned false - except for the; // case we asked for optimization remarks.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp:197,load,loads,197,interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/VectorUtils.cpp,2,"['load', 'optimiz']","['loads', 'optimization']"
Performance,// Ignore address space of reference type at this point and perform address; // space conversion after the reference binding step.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:60,perform,perform,60,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['perform'],['perform']
Performance,"// Ignore an explicit cast to void, except in C++98 if the operand is a; // volatile glvalue for which we would trigger an implicit read in any; // other language mode. (Such an implicit read always happens as part of; // the lvalue conversion in C, and happens in C++ for expressions of all; // forms where it seems likely the user intended to trigger a volatile; // load.)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Expr.cpp:368,load,load,368,interpreter/llvm-project/clang/lib/AST/Expr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/Expr.cpp,1,['load'],['load']
Performance,// Ignore annotation calls. This is important to stop the; // optimizer from treating annotations as uses which would; // make the state of the pointers they are attempting to; // elucidate to be incorrect.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ObjCARCInstKind.cpp:62,optimiz,optimizer,62,interpreter/llvm-project/llvm/lib/Analysis/ObjCARCInstKind.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/ObjCARCInstKind.cpp,1,['optimiz'],['optimizer']
Performance,"// Ignore atomics not aliasing with the original load, any atomic is a; // universal MemoryDef from MSSA's point of view too, just like a fence.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp:49,load,load,49,interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/Utils/AMDGPUMemoryUtils.cpp,1,['load'],['load']
Performance,"// Ignore cyclic paths. Since we are doing a recursive DFS walk, if the source; // frame being visited is already in the stack, it means we are seeing a; // cycle. This is done before querying the cached result because the cached; // result may be computed based on the same path. Consider the following case:; // A -> B, B -> A, A -> D; // When computing unique reachablity from A to D, the cached result for (B,D); // should not be counted since the unique path B->A->D is basically the same; // path as A->D. Counting that with invalidate the uniqueness from A to D.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp:197,cache,cached,197,interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/MissingFrameInferrer.cpp,3,['cache'],['cached']
Performance,"// Ignore filenames not beginning with ""llvmcache-"" or ""Thin-"". This; // includes the timestamp file as well as any files created by the user.; // This acts as a safeguard against data loss if the user specifies the; // wrong directory as their cache directory.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CachePruning.cpp:245,cache,cache,245,interpreter/llvm-project/llvm/lib/Support/CachePruning.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/CachePruning.cpp,1,['cache'],['cache']
Performance,// Ignore non-selectable and non-constructible passes! Ignore; // non-optimizations.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/LegacyPassNameParser.h:70,optimiz,optimizations,70,interpreter/llvm-project/llvm/include/llvm/IR/LegacyPassNameParser.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/LegacyPassNameParser.h,1,['optimiz'],['optimizations']
Performance,"// Ignore non-volatile loads, they are always ok.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp:23,load,loads,23,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineLoadStoreAlloca.cpp,1,['load'],['loads']
Performance,// Ignore relocations for sections that were not loaded,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp:49,load,loaded,49,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyld.cpp,1,['load'],['loaded']
Performance,"// Ignore stack guard loads, otherwise the register that holds CSEed value may; // be spilled and get loaded back with corrupted data.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp:22,load,loads,22,interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineCSE.cpp,2,['load'],"['loaded', 'loads']"
Performance,"// Ignore stuff that we obviously can't move.; //; // Treat volatile loads as stores. This is not strictly necessary for; // volatiles, but it is required for atomic loads. It is not allowed to move; // a load across an atomic load with Ordering > Monotonic.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp:69,load,loads,69,interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineInstr.cpp,4,['load'],"['load', 'loads']"
Performance,"// Ignore the bundle if the return type is void. Global optimization passes; // can turn the called function's return type to void. That should happen only; // if the call doesn't return and the call to @llvm.objc.clang.arc.noop.use; // no longer consumes the function return or is deleted. In that case, it's; // not necessary to emit the marker instruction or calls to the ARC runtime; // functions.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/ObjCARCUtil.h:56,optimiz,optimization,56,interpreter/llvm-project/llvm/include/llvm/Analysis/ObjCARCUtil.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Analysis/ObjCARCUtil.h,1,['optimiz'],['optimization']
Performance,// Ignore the direct read of src in the load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp:40,load,load,40,interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/MemCpyOptimizer.cpp,1,['load'],['load']
Performance,"// Ignore the null directive with regards to the multiple-include; // optimization, i.e. allow the null directive to appear outside of the; // include guard and still enable the multiple-include optimization.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp:70,optimiz,optimization,70,interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Lex/PPDirectives.cpp,2,['optimiz'],['optimization']
Performance,"// Ignore volatile loads. The execution of a volatile load cannot; // be used to prove an address is backed by regular memory; it can,; // for example, point to an MMIO register.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,2,['load'],"['load', 'loads']"
Performance,// Ignore volatile stores (see comment for loads).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp:43,load,loads,43,interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/Loads.cpp,1,['load'],['loads']
Performance,// Ignores COPYs during conformance checks.; // FIXME scalable vectors.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:54,scalab,scalable,54,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['scalab'],['scalable']
Performance,// Ignores COPYs during lookups.; // FIXME scalable vectors,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:43,scalab,scalable,43,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['scalab'],['scalable']
Performance,// Illegal bitcasts are done by storing and loading from a stack slot.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h:44,load,loading,44,interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,1,['load'],['loading']
Performance,"// Image atomic instructions are using DMask to specify how many bits; // input/output data will have. 32-bits (s32, v2s16) or 64-bits (s64, v4s16).; // DMaskLanes for image atomic has default value '0'.; // We must be sure that atomic variants (especially packed) will not be; // truncated from v2s16 or v4s16 to s16 type.; //; // ChangeElementCount will be needed for image load where Ty is always scalar.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:376,load,load,376,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['load']
Performance,// Immediate has larger code size than register. So avoid folding the; // immediate if it has more than 1 use and we are optimizing for size.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp:121,optimiz,optimizing,121,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrInfo.cpp,1,['optimiz'],['optimizing']
Performance,"// Implement EmitInstrWithCustomInserter for subword pseudo ATOMIC_LOADW_* or; // ATOMIC_SWAPW instruction MI. BinOpcode is the instruction that performs; // the binary operation elided by ""*"", or 0 for ATOMIC_SWAPW. Invert says; // whether the field should be inverted after performing BinOpcode (e.g. for; // NAND).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp:145,perform,performs,145,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZISelLowering.cpp,2,['perform'],"['performing', 'performs']"
Performance,// Implement dot-formatted dump by performing plain-text dump into the; // temporary storage followed by some post-processing.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp:35,perform,performing,35,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/VPlan.cpp,1,['perform'],['performing']
Performance,"// Implementation notes:; // 1) LOCK prefix creates a full read/write reordering barrier for memory; // operations issued by the current processor. As such, the location; // referenced is not relevant for the ordering properties of the instruction.; // See: Intel 64 and IA-32 ArchitecturesSoftware Developers Manual,; // 8.2.3.9 Loads and Stores Are Not Reordered with Locked Instructions; // 2) Using an immediate operand appears to be the best encoding choice; // here since it doesn't require an extra register.; // 3) OR appears to be very slightly faster than ADD. (Though, the difference; // is small enough it might just be measurement noise.); // 4) When choosing offsets, there are several contributing factors:; // a) If there's no redzone, we default to TOS. (We could allocate a cache; // line aligned stack object to improve this case.); // b) To minimize our chances of introducing a false dependence, we prefer; // to offset the stack usage from TOS slightly.; // c) To minimize concerns about cross thread stack usage - in particular,; // the idiomatic MyThreadPool.run([&StackVars]() {...}) pattern which; // captures state in the TOS frame and accesses it from many threads -; // we want to use an offset such that the offset is in a distinct cache; // line from the TOS frame.; //; // For a general discussion of the tradeoffs and benchmark results, see:; // https://shipilev.net/blog/2014/on-the-fence-with-dependencies/",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:332,Load,Loads,332,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,3,"['Load', 'cache']","['Loads', 'cache']"
Performance,"// Implementation of TBuffer to load and write to a SQL database",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/inc/TBufferSQL.h:32,load,load,32,tree/tree/inc/TBufferSQL.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/inc/TBufferSQL.h,1,['load'],['load']
Performance,// Implementation of the generic part of the loadObject algorithm.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldImpl.h:45,load,loadObject,45,interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/RuntimeDyld/RuntimeDyldImpl.h,1,['load'],['loadObject']
Performance,// Implementation of unsigned i64 to f64 following the algorithm in; // __floatundidf in compiler_rt. This implementation performs rounding; // correctly in all rounding modes with the exception of converting 0; // when rounding toward negative infinity. In that case the fsub will produce; // -0.0. This will be added to +0.0 and produce -0.0 which is incorrect.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp:122,perform,performs,122,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/TargetLowering.cpp,1,['perform'],['performs']
Performance,"// Implements the hardware dispatch logic.; //; // This class is responsible for the dispatch stage, in which instructions are; // dispatched in groups to the Scheduler. An instruction can be dispatched if; // the following conditions are met:; // 1) There are enough entries in the reorder buffer (see class; // RetireControlUnit) to write the opcodes associated with the instruction.; // 2) There are enough physical registers to rename output register operands.; // 3) There are enough entries available in the used buffered resource(s).; //; // The number of micro opcodes that can be dispatched in one cycle is limited by; // the value of field 'DispatchWidth'. A ""dynamic dispatch stall"" occurs when; // processor resources are not available. Dispatch stall events are counted; // during the entire execution of the code, and displayed by the performance; // report when flag '-dispatch-stats' is specified.; //; // If the number of micro opcodes exceedes DispatchWidth, then the instruction; // is dispatched in multiple cycles.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/MCA/Stages/DispatchStage.h:849,perform,performance,849,interpreter/llvm-project/llvm/include/llvm/MCA/Stages/DispatchStage.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/MCA/Stages/DispatchStage.h,1,['perform'],['performance']
Performance,"// Import the results of the last fit performed, interpreting; // the fit parameters as the given varList of parameters.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMinimizer.cxx:38,perform,performed,38,roofit/roofitcore/src/RooMinimizer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooMinimizer.cxx,1,['perform'],['performed']
Performance,"// Import, load and save parameter value snapshots",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooWorkspace.h:11,load,load,11,roofit/roofitcore/inc/RooWorkspace.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/inc/RooWorkspace.h,1,['load'],['load']
Performance,"// Important - defines name of openui5 widget; // ""localapp"" prefix will be point on current directory, where script executed; // ""localapp.view.TestPanel"" means file ./view/TestPanel.view.xml will be loaded",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/panel/webpanel.cxx:201,load,loaded,201,tutorials/webgui/panel/webpanel.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tutorials/webgui/panel/webpanel.cxx,1,['load'],['loaded']
Performance,"// Important note on validity of BPI/BFI. JumpThreading tries to preserve; // BPI/BFI as it goes. Thus if cached instance exists it will be updated.; // Otherwise, new instance of BPI/BFI is created (up to date by definition).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp:106,cache,cached,106,interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/JumpThreading.cpp,1,['cache'],['cached']
Performance,"// Imported SelectionDAG rules can handle every bitcast except those that; // bitcast from a type to the same type. Ideally, these shouldn't occur; // but we might not run an optimizer that deletes them. The other exception; // is bitcasts involving pointer types, as SelectionDAG has no knowledge; // of them.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp:175,optimiz,optimizer,175,interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/GISel/AArch64InstructionSelector.cpp,1,['optimiz'],['optimizer']
Performance,// Improve estimate for the vector width if it is scalable.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp:50,scalab,scalable,50,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp,1,['scalab'],['scalable']
Performance,"// Improve gather cost for gather of loads, if we can group some of the; // loads into vector loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:37,load,loads,37,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,3,['load'],['loads']
Performance,// Improve performance for some FP/SIMD code for A57.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp:11,perform,performance,11,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64TargetMachine.cpp,1,['perform'],['performance']
Performance,// In 64-bit mode we always perform an alignment step after laying out vbases.; // In 32-bit mode we do not. The check to see if we need to perform alignment; // checks the RequiredAlignment field and performs alignment if it isn't 0.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp:28,perform,perform,28,interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/AST/RecordLayoutBuilder.cpp,3,['perform'],"['perform', 'performs']"
Performance,"// In ARC mode, we load retained and then consume the value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:19,load,load,19,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,"// In ARC, move out of consumed arguments so that the release cleanup; // entered by StartFunction doesn't cause an over-release. This isn't; // optimal -O0 code generation, but it should get cleaned up when; // optimization is enabled. This also assumes that delegate calls are; // performed exactly once for a set of arguments, but that should be safe.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:212,optimiz,optimization,212,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,2,"['optimiz', 'perform']","['optimization', 'performed']"
Performance,"// In ASpan, each block will be either a single aligned load, or a; // valign of a pair of loads. In the latter case, an aligned load j; // will belong to the current valign, and the one in the previous; // block (for j > 0).; // Place the load at a location which will dominate the valign, assuming; // the valign will be placed right before the earliest user.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp:56,load,load,56,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonVectorCombine.cpp,4,['load'],"['load', 'loads']"
Performance,"// In Armv6-M, this sequence will generate a branch without corrupting; // any registers. We use two stack words; in the second, we construct the; // address we'll pop into pc, and the first is used to save and restore; // r0 which we use as a temporary register.; //; // To support position-independent use cases, the offset of the target; // function is stored as a relative offset (which will expand into an; // R_ARM_REL32 relocation in ELF, and presumably the equivalent in other; // object file types), and added to pc after we load it. (The alternative; // B.W is automatically pc-relative.); //; // There are five 16-bit Thumb instructions here, so the .balign 4 adds a; // sixth halfword of padding, and then the offset consumes a further 4; // bytes, for a total of 16, which is very convenient since entries in; // this jump table need to have power-of-two size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp:534,load,load,534,interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/LowerTypeTests.cpp,1,['load'],['load']
Performance,"// In C++, if the base pointer evaluates to a null pointer value,; // the only valid pointer this inbounds GEP can produce is also; // a null pointer, so the offset must also evaluate to zero.; // Likewise, if we have non-zero base pointer, we can not get null pointer; // as a result, so the offset can not be -intptr_t(BasePtr).; // In other words, both pointers are either null, or both are non-null,; // or the behaviour is undefined.; //; // C, however, is more strict in this regard, and gives more; // optimization opportunities: in C, additionally, nullptr+0 is undefined.; // So both the input to the 'gep inbounds' AND the output must not be null.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:509,optimiz,optimization,509,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['optimiz'],['optimization']
Performance,"// In C++11 onwards, narrowing checks are performed on the contents of; // braced-init-lists, even when they occur within unevaluated operands.; // Therefore we still need to instantiate constexpr functions used in such; // a context.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/EnterExpressionEvaluationContext.h:42,perform,performed,42,interpreter/llvm-project/clang/include/clang/Sema/EnterExpressionEvaluationContext.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/include/clang/Sema/EnterExpressionEvaluationContext.h,1,['perform'],['performed']
Performance,"// In C++1y, we need to perform overload resolution.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:24,perform,perform,24,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['perform']
Performance,"// In C++20, if the underlying destination type is a RecordType, Clang; // attempts to perform parentesized aggregate initialization if constructor; // overload fails:; //; // C++20 [expr.static.cast]p4:; // An expression E can be explicitly converted to a type T...if overload; // resolution for a direct-initialization...would find at least one viable; // function ([over.match.viable]), or if T is an aggregate type having a; // first element X and there is an implicit conversion sequence from E to; // the type of X.; //; // If that fails, then we'll generate the diagnostics from the failed; // previous constructor overload attempt. Array initialization, however, is; // not done after attempting constructor overloading, so we exit as there; // won't be a failed overload result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCast.cpp:87,perform,perform,87,interpreter/llvm-project/clang/lib/Sema/SemaCast.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCast.cpp,1,['perform'],['perform']
Performance,"// In C++98, we are not supposed to perform overload resolution here, but we; // treat that as a language defect, as suggested on cxx-abi-dev, to treat; // cases like B as having a non-trivial copy constructor:; // struct A { template<typename T> A(T&); };; // struct B { mutable A a; };",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:36,perform,perform,36,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['perform']
Performance,"// In C++98, we are not supposed to perform overload resolution here, but we; // treat that as a language defect.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:36,perform,perform,36,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['perform']
Performance,"// In CUDA/HIP device compilation with -fgpu-rdc, the mangled name of a; // static device variable depends on whether the variable is referenced by; // a host or device host function. Therefore the mangled name cannot be; // cached.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp:225,cache,cached,225,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,1,['cache'],['cached']
Performance,"// In CUDA/HIP device compilation, a lambda may capture a reference variable; // referencing a global host variable by copy. In this case the lambda should; // make a copy of the value of the global host variable. The DRE of the; // captured reference variable cannot be emitted as load from the host; // global variable as compile time constant, since the host variable is not; // accessible on device. The DRE of the captured reference variable has to be; // loaded from captures.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:282,load,load,282,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,2,['load'],"['load', 'loaded']"
Performance,"// In ILP32 (x32) mode, pointers are 32 bits and need to be zero-extended to; // 64 bits. Instructions with 32-bit register addresses perform this zero; // extension for us and we can safely ignore the high bits of Offset.; // Instructions with only a 32-bit immediate address do not, though: they; // sign extend instead. This means only address the low 2GB of address space; // is directly addressable, we need indirect addressing for the high 2GB of; // address space.; // TODO: Some of the earlier checks may be relaxed for ILP32 mode as the; // implicit zero extension of instructions would cover up any problem.; // However, we have asserts elsewhere that get triggered if we do, so keep; // the checks for now.; // TODO: We would actually be able to accept these, as well as the same; // addresses in LP64 mode, by adding the EIZ pseudo-register as an operand; // to get an address size override to be emitted. However, this; // pseudo-register is not part of any register class and therefore causes; // MIR verification to fail.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:134,perform,perform,134,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,1,['perform'],['perform']
Performance,"// In MRC mode, we do a load+autorelease.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:24,load,load,24,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,1,['load'],['load']
Performance,"// In MS mode, don't perform any extra checking of call return types within a; // decltype expression.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp:21,perform,perform,21,interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaExprCXX.cpp,1,['perform'],['perform']
Performance,"// In Mach-o files, the relocations do not need to be applied if; // there is no load offset to apply. The value read at the; // relocation point already factors in the section address; // (actually applying the relocations will produce wrong results; // as the section address will be added twice).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp:81,load,load,81,interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/DWARF/DWARFContext.cpp,1,['load'],['load']
Performance,"// In Microsoft mode, don't perform typo correction in a template member; // function dependent context because it interferes with the ""lookup into; // dependent bases of class templates"" feature.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp:28,perform,perform,28,interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaLookup.cpp,1,['perform'],['perform']
Performance,"// In NaCl, instructions that must be masked are forbidden in delay slots.; // We only check for loads, stores and SP changes. Calls, returns and; // branches are not checked because non-NaCl targets never put them in; // delay slots.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsDelaySlotFiller.cpp:97,load,loads,97,interpreter/llvm-project/llvm/lib/Target/Mips/MipsDelaySlotFiller.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsDelaySlotFiller.cpp,1,['load'],['loads']
Performance,"// In Obj-C and Microsoft mode, require the enumeration value to be; // representable in the underlying type of the enumeration. In C++11,; // we perform a non-narrowing conversion as part of converted constant; // expression checking.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp:146,perform,perform,146,interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDecl.cpp,1,['perform'],['perform']
Performance,"// In ObjC ARC mode with no ObjC ARC exception safety, tell the ARC; // optimizer it can aggressively ignore unwind edges.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:72,optimiz,optimizer,72,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,3,['optimiz'],['optimizer']
Performance,// In OpenMP loop region loop control variable must be captured and be; // private. Perform analysis of first part (if any).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseStmt.cpp:84,Perform,Perform,84,interpreter/llvm-project/clang/lib/Parse/ParseStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseStmt.cpp,2,['Perform'],['Perform']
Performance,// In OpenMP loop region loop control variable must be private. Perform; // analysis of first part (if any).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp:64,Perform,Perform,64,interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaStmt.cpp,1,['Perform'],['Perform']
Performance,"// In OptimizeIndividualCalls, we have strength reduced all optimizable; // objc_retainBlocks to objc_retains. Thus at this point any; // objc_retainBlocks that we see are not optimizable.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:6,Optimiz,OptimizeIndividualCalls,6,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,3,"['Optimiz', 'optimiz']","['OptimizeIndividualCalls', 'optimizable']"
Performance,"// In OptimizeIndividualCalls, we have strength reduced all optimizable; // objc_retainBlocks to objc_retains. Thus at this point any; // objc_retainBlocks that we see are not optimizable. We need to break since; // a retain can be a potential use.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:6,Optimiz,OptimizeIndividualCalls,6,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,3,"['Optimiz', 'optimiz']","['OptimizeIndividualCalls', 'optimizable']"
Performance,"// In RVV scalable vector types, we encode 64 bits in the fixed part.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp:10,scalab,scalable,10,interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGDebugInfo.cpp,1,['scalab'],['scalable']
Performance,"// In ThinLTO mode, when flattened profile is used, all the available; // profile information will be annotated in PreLink phase so there is; // no need to load the profile again in PostLink.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp:156,load,load,156,interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Passes/PassBuilderPipelines.cpp,1,['load'],['load']
Performance,"// In VerifyOnly mode, there's no point performing empty initialization; // more than once.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp:40,perform,performing,40,interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaInit.cpp,1,['perform'],['performing']
Performance,"// In Windows, initialisers are sorted by the suffix. XCL is for library; // initialisers, which run before user initialisers. We are running; // Objective-C loads at the end of library load. This means +load methods; // will run before any other static constructors, but that static; // constructors can see a fully initialised Objective-C state.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp:158,load,loads,158,interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCGNU.cpp,3,['load'],"['load', 'loads']"
Performance,"// In addition to the cases above, we also disable Tail Call Optimization if; // the calling convention code that at least one outgoing argument needs to; // go on the stack. We cannot check that here because at this point that; // information is not available.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp:61,Optimiz,Optimization,61,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonISelLowering.cpp,1,['Optimiz'],['Optimization']
Performance,"// In addition to the cases handle just above, we need to prevent; // allocas and loads/stores from moving below a stacksave or a; // stackrestore. Avoiding moving allocas below stackrestore is currently; // thought to be conservatism. Moving loads/stores below a stackrestore; // can lead to incorrect code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:82,load,loads,82,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,2,['load'],['loads']
Performance,"// In asan mode emit a function call instead of a regular load and let the; // run-time deal with it: if the shadow is properly poisoned return the; // cookie, otherwise return 0 to avoid an infinite loop calling DTORs.; // We can't simply ignore this load using nosanitize metadata because; // the metadata may be lost.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp:58,load,load,58,interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/ItaniumCXXABI.cpp,2,['load'],['load']
Performance,"// In big-endian mode we must adjust the pointer when the load size is smaller; // than the argument slot size. We must also reduce the known alignment to; // match. For example in the N64 ABI, we must add 4 bytes to the offset to get; // the correct half of the slot, and reduce the alignment from 8 (slot; // alignment) down to 4 (type alignment).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp:58,load,load,58,interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsISelLowering.cpp,1,['load'],['load']
Performance,"// In case any misguided DAG-level optimizations form an ADD with a; // TargetConstant operand, crash here instead of miscompiling (by selecting; // an r+r add instead of some kind of r+i add).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:35,optimiz,optimizations,35,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,1,['optimiz'],['optimizations']
Performance,"// In case of a TClonesArray we need to load the data and read the; // clonesArray object before being able to look into the class inside.; // We need to do that because we are never interested in the TClonesArray; // itself but only in the object inside.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx:40,load,load,40,tree/treeplayer/src/TTreeFormula.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx,1,['load'],['load']
Performance,"// In case of a memmove, the call to memmove will be executed instead; // of the loop, so we need to make sure that there is nothing else in; // the loop than the load, store and instructions that these two depend; // on.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonLoopIdiomRecognition.cpp:163,load,load,163,interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonLoopIdiomRecognition.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Hexagon/HexagonLoopIdiomRecognition.cpp,1,['load'],['load']
Performance,"// In case of libraries we get .L lib.so, which might automatically pull in; // decls (from header files). Thus we want to take the restore point before; // loading of the file and revert exclusively if needed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/MetaProcessor/MetaSema.cpp:157,load,loading,157,interpreter/cling/lib/MetaProcessor/MetaSema.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/MetaProcessor/MetaSema.cpp,1,['load'],['loading']
Performance,"// In case of load -> bswap -> store, return normal cost for the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/SystemZ/SystemZTargetTransformInfo.cpp,2,['load'],['load']
Performance,"// In case the modification time changes but not the content,; // accept the cached file as legit.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp:77,cache,cached,77,interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Serialization/ASTReader.cpp,1,['cache'],['cached']
Performance,"// In case the tokens were cached, have Preprocessor replace them with the; // annotation token.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp:27,cache,cached,27,interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseTemplate.cpp,1,['cache'],['cached']
Performance,"// In case the tokens were cached, have Preprocessor replace them; // with the annotation token. We don't need to do this if we've; // just reverted back to a prior state.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/Parser.cpp:27,cache,cached,27,interpreter/llvm-project/clang/lib/Parse/Parser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/Parser.cpp,1,['cache'],['cached']
Performance,"// In case the tokens were cached, have Preprocessor replace; // them with the annotation token.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/Parser.cpp:27,cache,cached,27,interpreter/llvm-project/clang/lib/Parse/Parser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/Parser.cpp,1,['cache'],['cached']
Performance,"// In case we are directly loading the library via gSystem->Load() without; // specifying the relevant include paths we should try loading the; // modulemap next to the library location.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx:27,load,loading,27,core/metacling/src/TCling.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/metacling/src/TCling.cxx,3,"['Load', 'load']","['Load', 'loading']"
Performance,"// In cases without unreachable blocks, because uses do not create new; // may-defs, there are only two cases:; // 1. There was a def already below us, and therefore, we should not have; // created a phi node because it was already needed for the def.; //; // 2. There is no def below us, and therefore, there is no extra renaming work; // to do.; // In cases with unreachable blocks, where the unnecessary Phis were; // optimized out, adding the Use may re-insert those Phis. Hence, when; // inserting Uses outside of the MSSA creation process, and new Phis were; // added, rename all uses if we are asked.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp:421,optimiz,optimized,421,interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemorySSAUpdater.cpp,1,['optimiz'],['optimized']
Performance,"// In functions that realign the stack, it can be an advantage to spill the; // callee-saved vector registers after realigning the stack. The vst1 and vld1; // instructions take alignment hints that can improve performance.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp:211,perform,performance,211,interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMFrameLowering.cpp,1,['perform'],['performance']
Performance,"// In general there's no single instruction that can perform an S <-> S; // move in NEON space, but a pair of VEXT instructions *can* do the; // job. It turns out that the VEXTs needed will only use DSrc once, with; // the position based purely on the combination of lane-0 and lane-1; // involved. For example; // vmov s0, s2 -> vext.32 d0, d0, d1, #1 vext.32 d0, d0, d0, #1; // vmov s1, s3 -> vext.32 d0, d1, d0, #1 vext.32 d0, d0, d0, #1; // vmov s0, s3 -> vext.32 d0, d0, d0, #1 vext.32 d0, d1, d0, #1; // vmov s1, s2 -> vext.32 d0, d0, d0, #1 vext.32 d0, d0, d1, #1; //; // Pattern of the MachineInstrs is:; // %DDst = VEXTd32 %DSrc1, %DSrc2, Lane, 14, %noreg (;implicits)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp:53,perform,perform,53,interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMBaseInstrInfo.cpp,1,['perform'],['perform']
Performance,"// In general, an object file might contain multiple sections of a given type,; // but in a loadable module, there must be exactly one .text, .data, .bss, and; // .loader section. A loadable object might also have one .tdata section and; // one .tbss section.; // Set these section-related values if not set explicitly. We assume that the; // input YAML matches the format of the loadable object, but if multiple input; // sections still have the same type, the first section with that type; // prevails.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ObjectYAML/XCOFFEmitter.cpp:92,load,loadable,92,interpreter/llvm-project/llvm/lib/ObjectYAML/XCOFFEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ObjectYAML/XCOFFEmitter.cpp,4,['load'],"['loadable', 'loader']"
Performance,"// In general, we allow ints, floats and pointers to be loaded and stored.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp:56,load,loaded,56,interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaChecking.cpp,1,['load'],['loaded']
Performance,"// In general, we're looking for any cut of the graph which ensures; // there's a call safepoint along every edge between Header and Pred.; // For the moment, we look only for the 'cuts' that consist of a single call; // instruction in a block which is dominated by the Header and dominates the; // loop latch (Pred) block. Somewhat surprisingly, walking the entire chain; // of such dominating blocks gets substantially more occurrences than just; // checking the Pred and Header blocks themselves. This may be due to the; // density of loop exit conditions caused by range and null checks.; // TODO: structure this as an analysis pass, cache the result for subloops,; // avoid dom tree recalculations",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/PlaceSafepoints.cpp:638,cache,cache,638,interpreter/llvm-project/llvm/lib/Transforms/Scalar/PlaceSafepoints.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/PlaceSafepoints.cpp,1,['cache'],['cache']
Performance,"// In implementations which use a barrier to achieve release semantics, we can; // delay emitting this barrier until we know a store is actually going to be; // attempted. The cost of this delay is that we need 2 copies of the block; // emitting the load-linked, affecting code size.; //; // Ideally, this logic would be unconditional except for the minsize check; // since in other cases the extra blocks naturally collapse down to the; // minimal loop. Unfortunately, this puts too much stress on later; // optimisations so we avoid emitting the extra logic in those cases too.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp:250,load,load-linked,250,interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,1,['load'],['load-linked']
Performance,"// In most cases TID / wavefrontsize is uniform.; //; // However, if a kernel has uneven dimesions we can have a value of; // workitem-id-x divided by the wavefrontsize non-uniform. For example; // dimensions (65, 2) will have workitems with address (64, 0) and (0, 1); // packed into a same wave which gives 1 and 0 after the division by 64; // respectively.; //; // FIXME: limit it to 1D kernels only, although that shall be possible; // to perform this optimization is the size of the X dimension is a power; // of 2, we just do not currently have infrastructure to query it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp:443,perform,perform,443,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPUTargetTransformInfo.cpp,2,"['optimiz', 'perform']","['optimization', 'perform']"
Performance,"// In non-aggressive mode, only warn on use-after-move of local variables; // (or local rvalue references) and of STL objects. The former is possible; // because local variables (or local rvalue references) are not tempting; // their user to re-use the storage. The latter is possible because STL; // objects are known to end up in a valid but unspecified state after the; // move and their state-reset methods are also known, which allows us to; // predict precisely when use-after-move is invalid.; // Some STL objects are known to conform to additional contracts after move,; // so they are not tracked. However, smart pointers specifically are tracked; // because we can perform extra checking over them.; // In aggressive mode, warn on any use-after-move because the user has; // intentionally asked us to completely eliminate use-after-move; // in his code.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/MoveChecker.cpp:675,perform,perform,675,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/MoveChecker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/MoveChecker.cpp,1,['perform'],['perform']
Performance,"// In normal use, ASTUnit's diagnostics should not change unless we reparse.; // Currently they can only change by using the internal testing flag; // '-error-on-deserialized-decl' which will error during deserialization of; // a declaration. What will happen is:; //; // -c-index-test gets a CXTranslationUnit; // -checks the diagnostics, the diagnostics set is lazily created,; // no errors are reported; // -later does an operation, like annotation of tokens, that triggers; // -error-on-deserialized-decl, that will emit a diagnostic error,; // that ASTUnit will catch and add to its stored diagnostics vector.; // -c-index-test wants to check whether an error occurred after performing; // the operation but can only query the lazily created set.; //; // We check here if a new diagnostic was appended since the last time the; // diagnostic set was created, in which case we reset it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndexDiagnostic.cpp:680,perform,performing,680,interpreter/llvm-project/clang/tools/libclang/CIndexDiagnostic.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/tools/libclang/CIndexDiagnostic.cpp,1,['perform'],['performing']
Performance,"// In order to avoid register pressure, on an average, the number of DWORDS; // loaded together by all clustered mem ops should not exceed 8. This is an; // empirical value based on certain observations and performance related; // experiments.; // The good thing about this heuristic is - it avoids clustering of too many; // sub-word loads, and also avoids clustering of wide loads. Below is the; // brief summary of how the heuristic behaves for various `LoadSize`.; // (1) 1 <= LoadSize <= 4: cluster at max 8 mem ops; // (2) 5 <= LoadSize <= 8: cluster at max 4 mem ops; // (3) 9 <= LoadSize <= 12: cluster at max 2 mem ops; // (4) 13 <= LoadSize <= 16: cluster at max 2 mem ops; // (5) LoadSize >= 17: do not cluster",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp:80,load,loaded,80,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIInstrInfo.cpp,10,"['Load', 'load', 'perform']","['LoadSize', 'loaded', 'loads', 'performance']"
Performance,"// In order to coalesce close-by pages, we collect the sizes of the gaps between pages on disk. We then order; // the gaps by size, sum them up and find a cutoff for the largest gap that we tolerate when coalescing pages.; // The size of the cutoff is given by the fraction of extra bytes we are willing to read in order to reduce; // the number of read requests. We thus schedule the lowest number of requests given a tolerable fraction; // of extra bytes.; // TODO(jblomer): Eventually we may want to select the parameter at runtime according to link latency and speed,; // memory consumption, device block size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RPageStorageFile.cxx:553,latency,latency,553,tree/ntuple/v7/src/RPageStorageFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/ntuple/v7/src/RPageStorageFile.cxx,1,['latency'],['latency']
Performance,"// In order to know that the underlying object cannot alias the; // non-addr-taken global, we must know that it would have to be an escape.; // Thus if the underlying object is a function argument, a load from; // a global, or the return of a function, it cannot alias. We can also; // recurse through PHI nodes and select nodes provided all of their inputs; // resolve to one of these known-escaping roots.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp:200,load,load,200,interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/GlobalsModRef.cpp,1,['load'],['load']
Performance,"// In order to know when we should re-process instructions that have; // phi-of-ops, we track the set of expressions that they needed as; // leaders. When we discover new leaders for those expressions, we process the; // associated phi-of-op instructions again in case they have changed. The; // other way they may change is if they had leaders, and those leaders; // disappear. However, at the point they have leaders, there are uses of the; // relevant operands in the created phi node, and so they will get reprocessed; // through the normal user marking we perform.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp:561,perform,perform,561,interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/NewGVN.cpp,1,['perform'],['perform']
Performance,// In order to lower atomic loads with stronger guarantees we would need to; // use load.acquire or insert fences. However these features were only added; // with PTX ISA 6.0 / sm_70.; // TODO: Check if we can actually use the new instructions and implement them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,2,['load'],"['load', 'loads']"
Performance,// In order to lower atomic loads with stronger guarantees we would need to; // use store.release or insert fences. However these features were only added; // with PTX ISA 6.0 / sm_70.; // TODO: Check if we can actually use the new instructions and implement them.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp:28,load,loads,28,interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/NVPTX/NVPTXISelDAGToDAG.cpp,1,['load'],['loads']
Performance,"// In order to perform a partial update, we need the existing bitwise; // value, which we can only extract for a constant int.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprConstant.cpp:15,perform,perform,15,interpreter/llvm-project/clang/lib/CodeGen/CGExprConstant.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprConstant.cpp,1,['perform'],['perform']
Performance,"// In order to perform the transform, we need to drop the poison generating; // flags on this instruction (if any).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp:15,perform,perform,15,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,1,['perform'],['perform']
Performance,"// In order to perform this optimization inside tryTLSXForm[Load|Store],; // Base is expected to be an ADD_TLS node.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp:15,perform,perform,15,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelDAGToDAG.cpp,3,"['Load', 'optimiz', 'perform']","['Load', 'optimization', 'perform']"
Performance,"// In order to preserve original lexical order for loads, keep them in the; // partition that we set up in the MemoryInstructionDependences loop.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDistribute.cpp:51,load,loads,51,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDistribute.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopDistribute.cpp,1,['load'],['loads']
Performance,"// In order to prevent the optimizer from throwing away the check, don't; // attach range metadata to the load.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp:27,optimiz,optimizer,27,interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExpr.cpp,2,"['load', 'optimiz']","['load', 'optimizer']"
Performance,"// In order to reduce live set of statepoint we might choose to rematerialize; // some values instead of relocating them. This is purely an optimization and; // does not influence correctness.; // First try rematerialization at uses, then after statepoints.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp:140,optimiz,optimization,140,interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/RewriteStatepointsForGC.cpp,1,['optimiz'],['optimization']
Performance,// In practice it's not currently possible to have a change in lane; // length for extending loads or truncating stores so both types should; // have the same scalable property.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h:93,load,loads,93,interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/BasicTTIImpl.h,2,"['load', 'scalab']","['loads', 'scalable']"
Performance,"// In presence of finite 'safelen', it may be unsafe to mark all; // the memory instructions parallel, because loop-carried; // dependences of 'safelen' iterations are possible.; // If clause order(concurrent) is specified then the memory instructions; // are marked parallel even if 'safelen' is finite.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenMP/OMPIRBuilder.cpp:198,concurren,concurrent,198,interpreter/llvm-project/llvm/lib/Frontend/OpenMP/OMPIRBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Frontend/OpenMP/OMPIRBuilder.cpp,1,['concurren'],['concurrent']
Performance,"// In reverse so user loaded modules are searched first",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Utils/PlatformWin.cpp:22,load,loaded,22,interpreter/cling/lib/Utils/PlatformWin.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Utils/PlatformWin.cpp,1,['load'],['loaded']
Performance,"// In scenarios where a function is not known to be a multiversion function; // until a later declaration, it is sometimes necessary to change the; // previously created mangled name to align with requirements of whatever; // multiversion function kind the function is now known to be. This function; // is responsible for performing such mangled name updates.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.h:323,perform,performing,323,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.h,1,['perform'],['performing']
Performance,"// In some rare cases, call instruction could be changed after being pushed; // into inline candidate queue, this is because earlier inlining may expose; // constant propagation which can change indirect call to direct call. When; // this happens, we may fail to find matching function samples for the; // candidate later, even if a match was found when the candidate was enqueued.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp:102,queue,queue,102,interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfile.cpp,1,['queue'],['queue']
Performance,"// In the ""worst"" case, we're dealing with a load for each byte. So, there; // are at most #bytes - 1 ORs.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp:45,load,load,45,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/CombinerHelper.cpp,1,['load'],['load']
Performance,"// In the ELFv2 ABI, we are not required to save all CR fields.; // If only one CR field is clobbered, it is more efficient to use; // mfocrf to selectively save just that field, because mfocrf has short; // latency compares to mfcr.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp:208,latency,latency,208,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp,1,['latency'],['latency']
Performance,"// In the IR, TFE is supposed to be used with a 2 element struct return; // type. The instruction really returns these two values in one contiguous; // register, with one additional dword beyond the loaded data. Rewrite the; // return type to use a single register result.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp:199,load,loaded,199,interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/AMDGPULegalizerInfo.cpp,1,['load'],['loaded']
Performance,"// In the Itanium C++ ABI, vtable thunks are provided by TUs that provide; // definitions of the main method. Therefore, emitting thunks with the vtable; // is purely an optimization. Emit the thunk if optimizations are enabled and; // all of the parameter types are complete.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGVTables.cpp:170,optimiz,optimization,170,interpreter/llvm-project/clang/lib/CodeGen/CGVTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGVTables.cpp,2,['optimiz'],"['optimization', 'optimizations']"
Performance,"// In the case Repl is a load or a store, we make all their GEPs; // available: GEPs are not hoisted by default to avoid the address; // computations to be hoisted without the associated load or store.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp:25,load,load,25,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,2,['load'],['load']
Performance,"// In the case of a Foreign class (loaded class without a Streamer function); // we reset fClassVersion to be -1 so that the current TVirtualStreamerInfo will not; // be confused with a previously loaded streamerInfo.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:35,load,loaded,35,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,2,['load'],['loaded']
Performance,"// In the case of a Mach-O STAB symbol, get its section only if; // the STAB symbol's section field refers to a valid section index.; // Otherwise the symbol may error trying to load a section that; // does not exist.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVELFReader.cpp:178,load,load,178,interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVELFReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVELFReader.cpp,1,['load'],['load']
Performance,"// In the case of a chain, the GetDirectory information usually does; // pertain to the Chain itself but to the currently loaded tree.; // So we can not rely on it.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreePlayer.cxx:122,load,loaded,122,tree/treeplayer/src/TTreePlayer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreePlayer.cxx,3,['load'],['loaded']
Performance,"// In the case of a latency tie, prefer an anti-dependency edge over; // other types of edges.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AggressiveAntiDepBreaker.cpp:20,latency,latency,20,interpreter/llvm-project/llvm/lib/CodeGen/AggressiveAntiDepBreaker.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AggressiveAntiDepBreaker.cpp,2,['latency'],['latency']
Performance,"// In the case of histogram with different limits; // newX(Y)Axis will now have the new found limits; // but one needs first to clone this histogram to perform the merge; // The clone is not needed when all histograms have the same limits",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/hist/hist/src/TProfileHelper.h:152,perform,perform,152,hist/hist/src/TProfileHelper.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/hist/hist/src/TProfileHelper.h,1,['perform'],['perform']
Performance,"// In the case of multiple select instructions in the same group, the order; // of non-dependent instructions (instructions of different dependence; // slices) in the true/false blocks appears to affect performance.; // Interleaving the slices seems to experimentally be the optimal approach.; // This interleaving scheduling allows for more ILP (with a natural downside; // of increasing a bit register pressure) compared to a simple ordering of; // one whole chain after another. One would expect that this ordering would; // not matter since the scheduling in the backend of the compiler would; // take care of it, but apparently the scheduler fails to deliver optimal; // ILP with a naive ordering here.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp:203,perform,performance,203,interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectOptimize.cpp,1,['perform'],['performance']
Performance,"// In the case of namespace, even if we have loaded before we need to; // load again in case there was new data member added.; // Mark the list as loaded to avoid an infinite recursion in the case; // where we have a data member that is a variable size array. In that; // case TDataMember::Init needs to get/load the list to find the data; // member used as the array size.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TListOfDataMembers.cxx:45,load,loaded,45,core/meta/src/TListOfDataMembers.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TListOfDataMembers.cxx,8,['load'],"['load', 'loaded']"
Performance,"// In the case of non-temporal gather loads and quadword gather loads there's; // only one addressing mode : ""vector + scalar"", e.g.; // ldnt1{b|h|w|d} { z0.s }, p0/z, [z0.s, x0]; // Since we do have intrinsics that allow the arguments to be in a different; // order, we may need to swap them to match the spec.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,load,loads,38,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,2,['load'],['loads']
Performance,"// In the case of non-temporal gather loads there's only one SVE instruction; // per data-size: ""scalar + vector"", i.e.; // * stnt1{b|h|w|d} { z0.s }, p0/z, [z0.s, x0]; // Since we do have intrinsics that allow the arguments to be in a different; // order, we may need to swap them to match the spec.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:38,load,loads,38,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loads']
Performance,"// In the case of stride 3 with a vector of 32 elements load the information; // in the following way:; // [0,1...,VF/2-1,VF/2+VF,VF/2+VF+1,...,2VF-1]",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp:56,load,load,56,interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InterleavedAccess.cpp,1,['load'],['load']
Performance,"// In the case where the TClass for one of ROOT's core class; // (eg TClonesArray for map<int,TClonesArray*>) is requested; // during the execution of rootcling, we could end up in a situation; // where we should have the information (since TClonesArray has; // a dictionary as part of libCore) but do not because the user; // only include a forward declaration of TClonesArray and we do not; // forcefully load the header file either (because the autoparsing; // is intentionally disabled).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx:407,load,load,407,core/meta/src/TClass.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/src/TClass.cxx,1,['load'],['load']
Performance,"// In the cases where we are behind (i.e. right of) a potential boolean optimization; // this tree variable reading may have not been executed with instance==0 which would; // result in the branch being potentially not read in.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx:72,optimiz,optimization,72,tree/treeplayer/src/TTreeFormula.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/treeplayer/src/TTreeFormula.cxx,1,['optimiz'],['optimization']
Performance,"// In the conditional case, things become more complicated. The original; // getValV() method is covering this case with very complicated logic,; // caching multiple new RooFit objects to scale the individual coefficients; // of the RooAddPdf.; //; // However, it's not complicated what we need to do mathematically:; //; // Since:; // 1. p(x, y) = p(x | y) * p(y); // 2. p(y) = Integral of p(x, y) over x; //; // We conclude:; // p(x, y); // p(x | y) = --------------------------; // Integral of p(x, y) over x; //; // What follows is the implementation of this formula in RooFit. By doing; // this here in compileForNormSet(), we don't invoke the old RooAddPdf; // projection caches (note that no conditional pdfs are on the right hand; // side of the equation).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddPdf.cxx:678,cache,caches,678,roofit/roofitcore/src/RooAddPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAddPdf.cxx,1,['cache'],['caches']
Performance,"// In the failing case, where we don't execute the store-conditional, the; // target might want to balance out the load-linked with a dedicated; // instruction (e.g., on ARM, clearing the exclusive monitor).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp:115,load,load-linked,115,interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/AtomicExpandPass.cpp,1,['load'],['load-linked']
Performance,"// In the future, check other pragmas if they're implemented (e.g. pragma; // optimize 0 will probably map to this functionality too).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaAttr.cpp:78,optimiz,optimize,78,interpreter/llvm-project/clang/lib/Sema/SemaAttr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaAttr.cpp,1,['optimiz'],['optimize']
Performance,"// In the original implementation of this library, the evaluation was done; // multi-threaded in implicit multi-threading was enabled in ROOT with; // ROOT::EnableImplicitMT().; //; // However, this multithreaded mode was not carefully validated and is; // therefore not production ready. One would first have to study the; // overhead for different numbers of cores, number of events, and model; // complexity. The, we should only consider implicit multithreading here if; // there is no performance penalty for any scenario, to not surprise the; // users with unexpected slowdows!; //; // Note that the priority of investigating this is not high, because RooFit; // R & D efforts currently go in the direction of parallelization at the; // level of the gradient components, or achieving single-threaded speedup; // with automatic differentiation. Furthermore, the single-threaded; // performance of the new CPU evaluation backend with the RooBatchCompute; // library, is generally much faster than the legacy evaluation backend; // already, even if the latter uses multi-threading.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/batchcompute/src/RooBatchCompute.cxx:79,multi-thread,multi-threaded,79,roofit/batchcompute/src/RooBatchCompute.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/batchcompute/src/RooBatchCompute.cxx,5,"['multi-thread', 'perform']","['multi-threaded', 'multi-threading', 'performance']"
Performance,"// In the other pass, load the baskets until we get to the maximum loaded so far.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx:22,load,load,22,tree/tree/src/TTreeCache.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/src/TTreeCache.cxx,2,['load'],"['load', 'loaded']"
Performance,"// In the past, this condition read `if (nintChanged && adjustProxies)`.; // However, the cache checks if the nset was already cached **by content**,; // and not by RooArgSet instance! So it can happen that the normalization; // set object is different, but the integral object is the same, in which; // case it would be wrong to not adjust the proxies. They always have to be; // adjusted when the nset changed, which is always the case when; // `syncNormalization()` is called.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsPdf.cxx:90,cache,cache,90,roofit/roofitcore/src/RooAbsPdf.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooAbsPdf.cxx,2,['cache'],"['cache', 'cached']"
Performance,"// In the prologue, the loaded (or persistent) stack pointer value is; // offset by the STDU/STDUX/STWU/STWUX instruction. For targets with red; // zone add this offset back now.; // If the function has a base pointer, the stack pointer has been copied; // to it so we can restore it by copying in the other direction.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp:24,load,loaded,24,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCFrameLowering.cpp,1,['load'],['loaded']
Performance,"// In the simple case, just pass the coerced loaded value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp:45,load,loaded,45,interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGCall.cpp,1,['load'],['loaded']
Performance,"// In the worst case, for a full 64-bit constant, a sequence of 8 instructions; // (i.e., LUI+ADDIW+SLLI+ADDI+SLLI+ADDI+SLLI+ADDI) has to be emitted. Note; // that the first two instructions (LUI+ADDIW) can contribute up to 32 bits; // while the following ADDI instructions contribute up to 12 bits each.; //; // On the first glance, implementing this seems to be possible by simply; // emitting the most significant 32 bits (LUI+ADDIW) followed by as many left; // shift (SLLI) and immediate additions (ADDI) as needed. However, due to the; // fact that ADDI performs a sign extended addition, doing it like that would; // only be possible when at most 11 bits of the ADDI instructions are used.; // Using all 12 bits of the ADDI instructions, like done by GAS, actually; // requires that the constant is processed starting with the least significant; // bit.; //; // In the following, constants are processed from LSB to MSB but instruction; // emission is performed from MSB to LSB by recursively calling; // generateInstSeq. In each recursion, first the lowest 12 bits are removed; // from the constant and the optimal shift amount, which can be greater than; // 12 bits if the constant is sparse, is determined. Then, the shifted; // remaining constant is processed recursively and gets emitted as soon as it; // fits into 32 bits. The emission of the shifts and additions is subsequently; // performed when the recursion returns.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp:560,perform,performs,560,interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/RISCV/MCTargetDesc/RISCVMatInt.cpp,3,['perform'],"['performed', 'performs']"
Performance,"// In this case we could have the operand of the binary operation; // being defined in another block, and performing the replacement; // could break the dominance relation.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp:106,perform,performing,106,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineCompares.cpp,1,['perform'],['performing']
Performance,// In this case we reduced the bound check to a comparison of the form; // (symbol or value with unsigned type) < (negative number); // which is always false. We are handling these cases separately because; // evalBinOpNN can perform a signed->unsigned conversion that turns the; // negative number into a huge positive value and leads to wildly; // inaccurate conclusions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/ArrayBoundCheckerV2.cpp:226,perform,perform,226,interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/ArrayBoundCheckerV2.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/StaticAnalyzer/Checkers/ArrayBoundCheckerV2.cpp,1,['perform'],['perform']
Performance,"// In this case, ActOnBinOp or ActOnConditionalOp performed the; // CorrectDelayedTyposInExpr check.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp:50,perform,performed,50,interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Parse/ParseExpr.cpp,1,['perform'],['performed']
Performance,// Include spiller post optimization and removing dead defs left because of; // rematerialization.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocBase.h:24,optimiz,optimization,24,interpreter/llvm-project/llvm/lib/CodeGen/RegAllocBase.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/RegAllocBase.h,1,['optimiz'],['optimization']
Performance,// Incoming chain; // Load lane and store have vector list as input.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:22,Load,Load,22,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['Load'],['Load']
Performance,"// Incorporate types by name, scanning all the types in the source module.; // At this point, the destination module may have a type ""%foo = { i32 }"" for; // example. When the source module got loaded into the same LLVMContext, if; // it had the same type, it would have been renamed to ""%foo.42 = { i32 }"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp:194,load,loaded,194,interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Linker/IRMover.cpp,1,['load'],['loaded']
Performance,// Increases the number of used scheduler queue slots of every buffered; // resource in the Buffers set.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.h:42,queue,queue,42,interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-mca/Views/SchedulerStatistics.h,1,['queue'],['queue']
Performance,// Increment the pointer if this was done before the loads in the loop.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp:53,load,loads,53,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64LoopIdiomTransform.cpp,1,['load'],['loads']
Performance,"// IncrementalExecutor has its own diagnostics (for; // now) that tries to guess which library needs to be; // loaded.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/IncrementalJIT.cpp:111,load,loaded,111,interpreter/cling/lib/Interpreter/IncrementalJIT.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/cling/lib/Interpreter/IncrementalJIT.cpp,1,['load'],['loaded']
Performance,"// Independent of whether we're targeting MIPS64 or not, the basic; // operations are the same. Also, directly use the $zero register if; // the 16 bit chunk is zero.; //; // For optimization purposes we always synthesize the splat value as; // an i32 value, then if we're targetting MIPS64, use SUBREG_TO_REG; // just before combining the values with dinsu to produce an i64. This; // enables SelectionDAG to aggressively share components of splat values; // where possible.; //; // FIXME: This is the general constant synthesis problem. This code; // should be factored out into a class shared between all the; // classes that need it. Specifically, for a splat size of 64; // bits that's a negative number we can do better than LUi/ORi; // for the upper 32bits.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelDAGToDAG.cpp:179,optimiz,optimization,179,interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Mips/MipsSEISelDAGToDAG.cpp,1,['optimiz'],['optimization']
Performance,"// Index 0, folded load and store, no alignment requirement.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,1,['load'],['load']
Performance,"// Index 0, mix of loads and stores.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp:19,load,loads,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,1,['load'],['loads']
Performance,"// Index 1, folded load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,1,['load'],['load']
Performance,"// Index 2, folded load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,1,['load'],['load']
Performance,"// Index 3, folded load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,1,['load'],['load']
Performance,"// Index 4, folded load",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp:19,load,load,19,interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86InstrFoldTables.cpp,1,['load'],['load']
Performance,"// Index needs to be lower than the minimum size of the vector, because; // for scalable vector, the vector size is known at run time.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp:80,scalab,scalable,80,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineVectorOps.cpp,2,['scalab'],['scalable']
Performance,// Indexed loads and stores are supported.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AArch64/AArch64ISelLowering.cpp,1,['load'],['loads']
Performance,// Indexed loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp:11,load,loads,11,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/DAGCombiner.cpp,1,['load'],['loads']
Performance,"// Indexes are sorted ""{ Thing, PrimaryIdx }"" arrays, so that a binary; // search can be performed by ""Thing"".",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SearchableTableEmitter.cpp:89,perform,performed,89,interpreter/llvm-project/llvm/utils/TableGen/SearchableTableEmitter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/SearchableTableEmitter.cpp,1,['perform'],['performed']
Performance,// Indicate if the base loading address is parsed from the mmap event or uses; // the preferred address,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h:24,load,loading,24,interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h,1,['load'],['loading']
Performance,// Indicate that stores don't overlap loads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LowerMemIntrinsics.cpp:38,load,loads,38,interpreter/llvm-project/llvm/lib/Transforms/Utils/LowerMemIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/LowerMemIntrinsics.cpp,4,['load'],['loads']
Performance,"// Indicate that we are performing a member access, and the cv-qualifiers; // for the base object type.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp:24,perform,performing,24,interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaCodeComplete.cpp,1,['perform'],['performing']
Performance,"// Indicate whether VFE was enabled for this module, so that the; // vcall_visibility metadata added under whole program vtables is handled; // appropriately in the optimizer.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp:165,optimiz,optimizer,165,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,1,['optimiz'],['optimizer']
Performance,// Indicates if ranges data are available; in the case of split DWARF any; // reference to ranges is valid only if the skeleton DIE has been loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/LogicalView/Readers/LVELFReader.h:141,load,loaded,141,interpreter/llvm-project/llvm/include/llvm/DebugInfo/LogicalView/Readers/LVELFReader.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/LogicalView/Readers/LVELFReader.h,1,['load'],['loaded']
Performance,"// Indicates that the chain 'fTree' went through a LoadTree",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tree/tree/inc/TFriendElement.h:51,Load,LoadTree,51,tree/tree/inc/TFriendElement.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tree/tree/inc/TFriendElement.h,1,['Load'],['LoadTree']
Performance,"// Indicates that value is a compile-time constant. Global variable; // can be 'Constant' while not being 'ReadOnly' on several occasions:; // - it is volatile, (e.g mapped device address); // - its address is taken, meaning that unlike 'ReadOnly' vars we can't; // internalize it.; // Constant variables are always imported thus giving compiler an; // opportunity to make some extra optimizations. Readonly constants; // are also internalized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h:384,optimiz,optimizations,384,interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/IR/ModuleSummaryIndex.h,1,['optimiz'],['optimizations']
Performance,// Indicates we should prefer to use a non-temporal load for this load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp:52,load,load,52,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelDAGToDAG.cpp,2,['load'],['load']
Performance,// Indicates whether the memcpy source is an in-register; // constant so it does not need to be loaded.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h:96,load,loaded,96,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,1,['load'],['loaded']
Performance,"// Indirect tail calls cannot be optimized for Thumb1 if the args; // to the call take up r0-r3. The reason is that there are no legal registers; // left to hold the pointer to the function to be called.; // Similarly, if the function uses return address sign and authentication,; // r12 is needed to hold the PAC and is not available to hold the callee; // address.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp:33,optimiz,optimized,33,interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMISelLowering.cpp,1,['optimiz'],['optimized']
Performance,"// Infer from a RooArgSet name whether this set is used internally by; // RooWorkspace to cache things.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooWorkspace.cxx:90,cache,cache,90,roofit/roofitcore/src/RooWorkspace.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooWorkspace.cxx,1,['cache'],['cache']
Performance,"// Infer instruction flags. For example, we can detect loads,; // stores, and side effects in many cases by examining an; // instruction's pattern.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenDAGPatterns.cpp:55,load,loads,55,interpreter/llvm-project/llvm/utils/TableGen/CodeGenDAGPatterns.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/TableGen/CodeGenDAGPatterns.cpp,1,['load'],['loads']
Performance,// Infer missing frames due to compiler optimizations such as tail call; // elimination.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h:40,optimiz,optimizations,40,interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/tools/llvm-profgen/ProfiledBinary.h,1,['optimiz'],['optimizations']
Performance,// Inferred alignment of the source or default value if the; // memory operation does not need to load the value.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h:98,load,load,98,interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/CodeGen/TargetLowering.h,1,['load'],['load']
Performance,"// Inform base class that observable yvar cannot be optimized away from the dataset",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooXYChi2Var.cxx:52,optimiz,optimized,52,roofit/roofitcore/src/RooXYChi2Var.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/roofit/roofitcore/src/RooXYChi2Var.cxx,1,['optimiz'],['optimized']
Performance,"// Inform the Rewriter if we have a post-increment use, so that it can; // perform an advantageous expansion.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp:75,perform,perform,75,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LoopStrengthReduce.cpp,1,['perform'],['perform']
Performance,"// Inform the solver that the called function is executable, and perform; // the merges for the arguments and return value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/CalledValuePropagation.cpp:65,perform,perform,65,interpreter/llvm-project/llvm/lib/Transforms/IPO/CalledValuePropagation.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/CalledValuePropagation.cpp,1,['perform'],['perform']
Performance,// Information about a load or store that we're scalarizing.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Scalarizer.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Transforms/Scalar/Scalarizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/Scalarizer.cpp,1,['load'],['load']
Performance,// Information on REP string instructions that require manual mitigation can; // be found here:; // https://software.intel.com/security-software-guidance/insights/deep-dive-load-value-injection#specialinstructions,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp:173,load,load-value-injection,173,interpreter/llvm-project/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp,1,['load'],['load-value-injection']
Performance,// Information on control-flow instructions that require manual mitigation can; // be found here:; // https://software.intel.com/security-software-guidance/insights/deep-dive-load-value-injection#specialinstructions,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp:175,load,load-value-injection,175,interpreter/llvm-project/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/AsmParser/X86AsmParser.cpp,1,['load'],['load-value-injection']
Performance,// Inherit any content cache data from the old source manager.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp:23,cache,cache,23,interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Basic/SourceManager.cpp,1,['cache'],['cache']
Performance,"// Init circularity ntple for performance calculations",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TVirtualPacketizer.cxx:30,perform,performance,30,proof/proof/src/TVirtualPacketizer.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TVirtualPacketizer.cxx,1,['perform'],['performance']
Performance,// Init signature cost caches,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BalancedPartitioning.cpp:23,cache,caches,23,interpreter/llvm-project/llvm/lib/Support/BalancedPartitioning.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/BalancedPartitioning.cpp,1,['cache'],['caches']
Performance,"// Init the local cache directory if the repository is remote",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TDataSetManagerFile.cxx:18,cache,cache,18,proof/proof/src/TDataSetManagerFile.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/proof/proof/src/TDataSetManagerFile.cxx,1,['cache'],['cache']
Performance,"// Initial construction must not access the cache, since it must be done; // atomically.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/PDB/Native/SymbolCache.h:44,cache,cache,44,interpreter/llvm-project/llvm/include/llvm/DebugInfo/PDB/Native/SymbolCache.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/DebugInfo/PDB/Native/SymbolCache.h,1,['cache'],['cache']
Performance,"// Initialization of extension module libcppyy.; // load commonly used python strings",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/src/CPyCppyyModule.cxx:52,load,load,52,bindings/pyroot/cppyy/CPyCppyy/src/CPyCppyyModule.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/src/CPyCppyyModule.cxx,1,['load'],['load']
Performance,// Initialization required when processing a COFF file:; // Cache the symbols relocations.; // Create a mapping for virtual addresses.; // Get the functions entry points.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVCodeViewReader.cpp:60,Cache,Cache,60,interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVCodeViewReader.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/DebugInfo/LogicalView/Readers/LVCodeViewReader.cpp,1,['Cache'],['Cache']
Performance,"// Initializations are performed ""as if by a defaulted default constructor"",; // so enter the appropriate scope.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp:23,perform,performed,23,interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Sema/SemaDeclCXX.cpp,1,['perform'],['performed']
Performance,"// Initialize CPU specific properties. We should add a tablegen feature for; // this in the future so we can specify it together with the subtarget; // features.; // TODO: Check TuneCPU and override defaults (that are for LA464) once we; // support optimizing for more uarchs.; // Default to the alignment settings empirically confirmed to perform best; // on LA464, with 4-wide instruction fetch and decode stages. These settings; // can also be overridden in initializeProperties.; //; // We default to such higher-than-minimum alignments because we assume that:; //; // * these settings should benefit most existing uarchs/users,; // * future general-purpose LoongArch cores are likely to have issue widths; // equal to or wider than 4,; // * instruction sequences best for LA464 should not pessimize other future; // uarchs, and; // * narrower cores would not suffer much (aside from slightly increased; // ICache footprint maybe), compared to the gains everywhere else.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp:178,Tune,TuneCPU,178,interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/LoongArch/LoongArchSubtarget.cpp,3,"['Tune', 'optimiz', 'perform']","['TuneCPU', 'optimizing', 'perform']"
Performance,"// Initialize a set a candidate getelementptrs. Note that we use a; // SetVector here to preserve program order. If the index computations; // are vectorizable and begin with loads, we want to minimize the chance; // of having to reorder them later.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp:175,load,loads,175,interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Vectorize/SLPVectorizer.cpp,1,['load'],['loads']
Performance,// Initialize arrays for load and store spill opcodes on supported subtargets.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h:25,load,load,25,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.h,1,['load'],['load']
Performance,// Initialize from another scalable vector of the same type.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp:27,scalab,scalable,27,interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGExprScalar.cpp,1,['scalab'],['scalable']
Performance,"// Initialize gClient in case libGui is loaded in batch mode",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGClient.cxx:40,load,loaded,40,gui/gui/src/TGClient.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/gui/gui/src/TGClient.cxx,1,['load'],['loaded']
Performance,// Initialize map that relates the PPC addressing modes to the computed flags; // of a load/store instruction. The map is used to determine the optimal; // addressing mode when selecting load and stores.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:87,load,load,87,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,['load'],['load']
Performance,// Initialize our runtime entry point cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp:38,cache,cache,38,interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/ObjCARC/ObjCARCOpts.cpp,1,['cache'],['cache']
Performance,// Initialize ready queues now that the DAG and priority data are finalized.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:20,queue,queues,20,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,2,['queue'],['queues']
Performance,"// Initialize the FileManager. We can't do this in update(), since that; // performs the initialization too late (once both target and language; // options are read).",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp:76,perform,performs,76,interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Frontend/ASTUnit.cpp,1,['perform'],['performs']
Performance,"// Initialize the basic block structure required for expansion of memcmp call; // with given maximum load size and memcmp size parameter.; // This structure includes:; // 1. A list of load compare blocks - LoadCmpBlocks.; // 2. An EndBlock, split from original instruction point, which is the block to; // return from.; // 3. ResultBlock, block to branch to for early exit when a; // LoadCmpBlock finds a difference.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp:101,load,load,101,interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ExpandMemCmp.cpp,4,"['Load', 'load']","['LoadCmpBlock', 'LoadCmpBlocks', 'load']"
Performance,"// Initialize the final register residue.; // Any residue that occupies the final by-val arg register must be; // left-justified on AIX. Loads must be a power-of-2 size and cannot be; // larger than the ByValSize. For example: a 7 byte by-val arg requires 4,; // 2 and 1 byte loads.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp:137,Load,Loads,137,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCISelLowering.cpp,2,"['Load', 'load']","['Loads', 'loads']"
Performance,// Initialize the map pointer and load the current head of the shadow stack.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ShadowStackGCLowering.cpp:34,load,load,34,interpreter/llvm-project/llvm/lib/CodeGen/ShadowStackGCLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ShadowStackGCLowering.cpp,1,['load'],['load']
Performance,"// Initialize the optimizer",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/TestOptimization.h:18,optimiz,optimizer,18,tmva/tmva/test/DNN/TestOptimization.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/tmva/tmva/test/DNN/TestOptimization.h,1,['optimiz'],['optimizer']
Performance,"// Initialize the stack printer after installing the one-shot pipe signal; // handler, so we can perform a sigaction() for SIGPIPE on Unix if requested.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/InitLLVM.cpp:97,perform,perform,97,interpreter/llvm-project/llvm/lib/Support/InitLLVM.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Support/InitLLVM.cpp,1,['perform'],['perform']
Performance,// Initialize the strategy before modifying the DAG.; // This may initialize a DFSResult to be used for queue priority.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:104,queue,queue,104,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,2,['queue'],['queue']
Performance,// Initialize the symbols cache if necessary.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/JITLink.cpp:26,cache,cache,26,interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/JITLink.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/ExecutionEngine/JITLink/JITLink.cpp,1,['cache'],['cache']
Performance,// Initialize the type cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp:23,cache,cache,23,interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CodeGenModule.cpp,1,['cache'],['cache']
Performance,"// Initializes PhysRegEntries (instead of a SmallVector, PhysRegEntries is a; // buffer of size NumPhysRegs to speed up alloc/clear for targets with large; // reg files). Calloced memory is used for good form, and quites tools like; // Valgrind too, but zero initialized memory is not required by the algorithm:; // this is because PhysRegEntries works like a SparseSet and its entries are; // only valid when there is a corresponding CacheEntries assignment. There is; // also support for when pass managers are reused for targets with different; // numbers of PhysRegs: in this case PhysRegEntries is freed and reinitialized.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterferenceCache.cpp:435,Cache,CacheEntries,435,interpreter/llvm-project/llvm/lib/CodeGen/InterferenceCache.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/InterferenceCache.cpp,1,['Cache'],['CacheEntries']
Performance,// Inject loads into all of the pred blocks.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp:10,load,loads,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/SROA.cpp,1,['load'],['loads']
Performance,"// Inline for performance, skipping one function call.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/core/meta/inc/TClass.h:14,perform,performance,14,core/meta/inc/TClass.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/core/meta/inc/TClass.h,1,['perform'],['performance']
Performance,"// Insert - store vector to stack, store scalar, load vector.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp:49,load,load,49,interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86TargetTransformInfo.cpp,1,['load'],['load']
Performance,// Insert L into the loop queue after the parent loop.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp:26,queue,queue,26,interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,1,['queue'],['queue']
Performance,// Insert Load and the value number of its memory address in VNtoLoads.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp:10,Load,Load,10,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVNHoist.cpp,1,['Load'],['Load']
Performance,// Insert a load in place of the PHI and replace all uses.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp:12,load,load,12,interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/DemoteRegToStack.cpp,1,['load'],['load']
Performance,"// Insert a new load, to preserve the saved value.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp:16,load,load,16,interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/GlobalOpt.cpp,1,['load'],['load']
Performance,// Insert a new store to null instruction before the load to indicate that; // this code is not reachable. FIXME: We could insert unreachable; // instruction directly because we can modify the CFG.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp:53,load,load,53,interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/GVN.cpp,1,['load'],['load']
Performance,"// Insert a probe before an instruction with a valid debug line number which; // will be assigned to the probe. The line number will be used later to; // model the inline context when the probe is inlined into other functions.; // Debug instructions, phi nodes and lifetime markers do not have an valid; // line number. Real instructions generated by optimizations may not come; // with a line number either.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfileProbe.cpp:351,optimiz,optimizations,351,interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfileProbe.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/IPO/SampleProfileProbe.cpp,1,['optimiz'],['optimizations']
Performance,"// Insert a read hazard before the call. This will ensure that; // any writes to the locals are performed before making the; // call. If the call throws, then this is sufficient to; // guarantee correctness as long as it doesn't also write to any; // locals.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp:96,perform,performed,96,interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/CodeGen/CGObjCMac.cpp,1,['perform'],['performed']
Performance,"// Insert canonicalizes if it's possible we need to quiet to get correct; // sNaN behavior.; // Note this must be done here, and not as an optimization combine in the; // absence of a dedicate quiet-snan instruction as we're using an; // omni-purpose G_FCANONICALIZE.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp:139,optimiz,optimization,139,interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/GlobalISel/LegalizerHelper.cpp,1,['optimiz'],['optimization']
Performance,// Insert in reverse order. loadRegFromStackSlot can insert multiple; // instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/XCore/XCoreFrameLowering.cpp:28,load,loadRegFromStackSlot,28,interpreter/llvm-project/llvm/lib/Target/XCore/XCoreFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/XCore/XCoreFrameLowering.cpp,1,['load'],['loadRegFromStackSlot']
Performance,// Insert in reverse order. loadRegFromStackSlot can insert; // multiple instructions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp:28,load,loadRegFromStackSlot,28,interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/PrologEpilogInserter.cpp,2,['load'],['loadRegFromStackSlot']
Performance,"// Insert in the inner loop body, which computes; // Res += Load(CurrentRow, K) * Load(K, CurrentColumn)",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp:60,Load,Load,60,interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Scalar/LowerMatrixIntrinsics.cpp,2,['Load'],['Load']
Performance,// Insert loop into loop nest (LoopInfo) and loop queue (LQ).,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp:50,queue,queue,50,interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LoopPass.cpp,1,['queue'],['queue']
Performance,// Insert new combined load/store + alu operation,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp:23,load,load,23,interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/Lanai/LanaiMemAluCombiner.cpp,1,['load'],['load']
Performance,// Insert newly created edges into the queue.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp:39,queue,queue,39,interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp,1,['queue'],['queue']
Performance,// Insert over-defined values into their own cache to reduce memory; // overhead.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp:45,cache,cache,45,interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/LazyValueInfo.cpp,1,['cache'],['cache']
Performance,// Insert right away in the cache to handle recursive PHIs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp:28,cache,cache,28,interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Analysis/MemoryBuiltins.cpp,1,['cache'],['cache']
Performance,// Insert stack pointer adjustment for later moving of return addr. Only; // applies to tail call optimized functions where the callee argument stack; // size is bigger than the callers.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.cpp:98,optimiz,optimized,98,interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/M68k/M68kFrameLowering.cpp,2,['optimiz'],['optimized']
Performance,// Insert start element into queue.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Format/UnwrappedLineFormatter.cpp:29,queue,queue,29,interpreter/llvm-project/clang/lib/Format/UnwrappedLineFormatter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/clang/lib/Format/UnwrappedLineFormatter.cpp,1,['queue'],['queue']
Performance,// Insert the S_CBRANCH_EXECZ instruction which will be optimized later; // during SIRemoveShortExecBranches.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp:56,optimiz,optimized,56,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SILowerControlFlow.cpp,1,['optimiz'],['optimized']
Performance,"// Insert the call to load.relative intrinsic before LOAD.; // GEP might not be immediately followed by a LOAD, like it can be hoisted; // outside the loop or another instruction might be inserted them in between.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp:22,load,load,22,interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/RelLookupTableConverter.cpp,3,"['LOAD', 'load']","['LOAD', 'load']"
Performance,// Insert the edges into the queue.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp:29,queue,queue,29,interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/Utils/CodeLayout.cpp,1,['queue'],['queue']
Performance,"// Insert the false branch. Do this even if it's a fall through branch,; // this makes it easier to do DAG optimizations which require inverting; // the branch condition.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp:107,optimiz,optimizations,107,interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/SelectionDAG/SelectionDAGBuilder.cpp,1,['optimiz'],['optimizations']
Performance,// Insert the load at the point of the original dominating load.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMParallelDSP.cpp,2,['load'],['load']
Performance,// Insert the load into the predecessor block,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp:14,load,load,14,interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/WinEHPrepare.cpp,2,['load'],['load']
Performance,// Insert the loaded element into the appropriate place in the vector.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp:14,load,loaded,14,interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/X86/X86ISelLowering.cpp,1,['load'],['loaded']
Performance,// Insert the new list into the cache.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/ImmutableList.h:32,cache,cache,32,interpreter/llvm-project/llvm/include/llvm/ADT/ImmutableList.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/ADT/ImmutableList.h,1,['cache'],['cache']
Performance,// Insert the toc load instructions into InsInstrs.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp:18,load,load,18,interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/PowerPC/PPCInstrInfo.cpp,1,['load'],['load']
Performance,// Insert this block at the end of the function. Inserting in between may; // interfere with control flow optimizer decisions.,MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ShrinkWrap.cpp:106,optimiz,optimizer,106,interpreter/llvm-project/llvm/lib/CodeGen/ShrinkWrap.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/ShrinkWrap.cpp,1,['optimiz'],['optimizer']
Performance,"// InsertedExpressions caches Values for reuse, so must track RAUW.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/ScalarEvolutionExpander.h:23,cache,caches,23,interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/ScalarEvolutionExpander.h,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/include/llvm/Transforms/Utils/ScalarEvolutionExpander.h,1,['cache'],['caches']
Performance,"// Inserting ""S_WAITCNT vmcnt(0)"" is not required because the hardware; // does not reorder memory operations with respect to preceeding buffer; // invalidate. The invalidate is guaranteed to remove any cache lines of; // earlier writes and ensures later writes will refetch the cache lines.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:203,cache,cache,203,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,4,['cache'],['cache']
Performance,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INV"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:228,cache,cache,228,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,2,['cache'],['cache']
Performance,"// Inserting a ""S_WAITCNT vmcnt(0)"" after is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a preceding ""BUFFER_INVL2"". The invalidate is guaranteed to; // remove any cache lines of earlier writes by the same wave and ensures; // later reads by the same wave will refetch the cache lines.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:230,cache,cache,230,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,2,['cache'],['cache']
Performance,"// Inserting a ""S_WAITCNT vmcnt(0)"" before is not required because the; // hardware does not reorder memory operations by the same wave with; // respect to a following ""BUFFER_WBL2"". The ""BUFFER_WBL2"" is guaranteed; // to initiate writeback of any dirty cache lines of earlier writes by the; // same wave. A ""S_WAITCNT vmcnt(0)"" is needed after to ensure the; // writeback has completed.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp:254,cache,cache,254,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIMemoryLegalizer.cpp,2,['cache'],['cache']
Performance,"// Inserts Constant Islands. Block sizes cannot be increased after this point,; // as this may push the branch ranges and load offsets of accessing constant; // pools out of range..",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetMachine.cpp:122,load,load,122,interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetMachine.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/ARM/ARMTargetMachine.cpp,1,['load'],['load']
Performance,"// Inserts the optimized s_mov_b32 / v_cmpx sequence based on the; // operands extracted from a v_cmp ..., s_and_saveexec pattern.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp:15,optimiz,optimized,15,interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Target/AMDGPU/SIOptimizeExecMasking.cpp,1,['optimiz'],['optimized']
Performance,"// InstSimplify already performed this fold if it was possible subject to; // current poison-generating flags. Check whether dropping poison-generating; // flags enables the transform.; // Try each equivalence substitution possibility.; // We have an 'EQ' comparison, so the select's false value will propagate.; // Example:; // (X == 42) ? 43 : (X + 1) --> (X == 42) ? (X + 1) : (X + 1) --> X + 1",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp:24,perform,performed,24,interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/Transforms/InstCombine/InstCombineSelect.cpp,1,['perform'],['performed']
Performance,"// Instantiate (and cache) templated methods, return method if any",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/src/TemplateProxy.cxx:20,cache,cache,20,bindings/pyroot/cppyy/CPyCppyy/src/TemplateProxy.cxx,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/bindings/pyroot/cppyy/CPyCppyy/src/TemplateProxy.cxx,1,['cache'],['cache']
Performance,"// Instantiate the selected scheduler for this target, function, and; // optimization level.",MatchSource.CODE_COMMENT,root-project,root,v6-32-06,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp:73,optimiz,optimization,73,interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/lib/CodeGen/MachineScheduler.cpp,2,['optimiz'],['optimization']
