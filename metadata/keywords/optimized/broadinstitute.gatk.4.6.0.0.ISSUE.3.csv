quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,This upgrades htsjdk to v3.0.0 which was attempted in #7867 and then reverted in #7960 in order to unblock the jukebox merge.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8025:5,upgrade,upgrades,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8025,1,['upgrade'],['upgrades']
Deployability,"This version can compute on either locally, or on the cloud. There are a few things still on the table: it can't read dbSNP from GCS yet (no point in doing that since the skeleton team's working on the same thing) and it repatriates the recalTables even if the report is meant to be on the cloud (I expect many things to change, so better to do that once things are more stable. Besides, report on the cloud is an edge case, normally we want the textual report on the client's machine. This relies on dataflow-java 0.9; it has now released so we're good.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/529:531,release,released,531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/529,1,['release'],['released']
Deployability,This was also a GATK3 issue: https://github.com/broadinstitute/gsa-unstable/issues/1624. GQ should be updated to reflect the subset PLs.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3404:102,update,updated,102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3404,1,['update'],['updated']
Deployability,This was an issue with propagating polymorphic std::exception code from the native library's logger utility and has been fixed in the [1.3.2 release ](https://mvnrepository.com/artifact/org.genomicsdb/genomicsdb/1.3.2) of the GenomicsDB library. Also note that using java option `GATK_STACKTRACE_ON_USER_EXCEPTION` with gatk will also output a C/C++ limited stacktrace as requested by @lbergelson.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6852:141,release,release,141,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6852,1,['release'],['release']
Deployability,"This was mentioned already as a comment in #562 but it should be an issue instead. BQSR on Dataflow can _theoretically_ avoid the step of saving the textual report, instead piping the recalibration analysis results directly to the ApplyBQSR phase. In practice it cannot, because it turns out that **saving a report and immediately loading it back is not a no-op**. it actually changes in some necessary way. It would be great if someone familiar with that part of the code could help factor the mutation apart from the saving/loading, so I could just call that in the pipeline. This would speed things up a bit and the code would be that much cleaner.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/799:568,pipeline,pipeline,568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/799,1,['pipeline'],['pipeline']
Deployability,This was missed in #7754 ; @droazen I even updated the all important badge...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7808:43,update,updated,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7808,1,['update'],['updated']
Deployability,This will provide a temporary fix for https://github.com/broadinstitute/gatk/issues/2793 so that we can release beta,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3118:104,release,release,104,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3118,1,['release'],['release']
Deployability,"Three major changes here.; 1. Added in logic to create the ploidy table during ingest (with necessary supporting class) and use it during extract automatically as part of the default joint workflow. Also removed a column that we won't need when creating it automatically.; 2. Rearranged the PAR checking logic to consolidate it in its own class (PloidyUtils.java). Successful run against tiny sample set ""PLOIDY_TEST"" in echo callset project:. https://app.terra.bio/#workspaces/allofus-drc-wgs-dev/GVS%20AoU%20WGS%20Echo%20Callset%20v2/job_history/a93aa2ef-9cef-451d-8cf8-b31f1c6a8407. You'll need your aou credentials to see the results. Successful integration run on XY:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/6a9a5fdf-ffaa-4dcb-af73-56a4b25e69a4. This run shows all of the OTHER integration tests running successfully except BGE, due to the test data needing an updates for BGE X and Y:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/21664810-7516-49f2-a60c-51b2e05faf06. The only difference between those two tests running was an update to the expected values for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8994:650,integrat,integration,650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8994,5,"['integrat', 'update']","['integration', 'update', 'updates']"
Deployability,Throttling integration test [VS-1076],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8545:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8545,1,['integrat'],['integration']
Deployability,"Ticks off a few straggler issues noted in #7724. @meganshand mind reviewing? Hopefully should be quick and we can get it in before @droazen cuts the next release. Note that this shouldn't change behavior in the Ultima pipeline, as the default toggle is still the same start-position resource-matching strategy inherited from VQSR, but we might want to explore the effect of choosing another strategy there.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8049:154,release,release,154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8049,3,"['pipeline', 'release', 'toggle']","['pipeline', 'release', 'toggle']"
Deployability,To remove warning about unfound logger.; Also updated latest version in gatk; Updated to latest version of gatk in wdls.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7774:46,update,updated,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7774,2,"['Update', 'update']","['Updated', 'updated']"
Deployability,"To support running CARROT tests from PR comments, it's necessary that the [carrot-publish-github-action ](https://github.com/broadinstitute/carrot-publish-github-action) be integrated following the instructions in the README for that repo, so that PR comments will be processed by the GitHub action. This also requires that secrets be set for the pubsub topic and SA key for sending the messages.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6916:173,integrat,integrated,173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6916,1,['integrat'],['integrated']
Deployability,"To upgrade ApplyBQSR for cloud execution I had to:. (i) change the input to remove reads with the unaligned flag; (ii) load the recalibration report from GCS instead of shipping it as a serialized object, because Dataflow explodes if we ship too much (error is: ""malformed JSON"").; (iii) for the case of a remote execution with a local output file name, add logic to copy the output via GCS to the client's machine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/595:3,upgrade,upgrade,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/595,1,['upgrade'],['upgrade']
Deployability,Tool.getReads(GATKSparkTool.java:212); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:68); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:184); 	... 21 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dee81497-fad3-4d70-a33e-68a5d5584d9a] entered state [ERROR] while waiting for [DONE].; ```. I'm not sure if it's a jenkins problem or a real regression but we need to investigate it either way.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2449:2232,deploy,deploy,2232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2449,6,['deploy'],['deploy']
Deployability,"Tool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls outp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:6086,deploy,deploy,6086,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['deploy'],['deploy']
Deployability,Tool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:9872,deploy,deploy,9872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['deploy'],['deploy']
Deployability,Tool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:233); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:5645,deploy,deploy,5645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['deploy'],['deploy']
Deployability,Tool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.Contai,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:43513,deploy,deploy,43513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['deploy'],['deploy']
Deployability,Tool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePip,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:11332,deploy,deploy,11332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['deploy'],['deploy']
Deployability,"Tool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:17343,deploy,deploy,17343,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['deploy'],['deploy']
Deployability,"Tools, filters and annotations for mitochondria pipeline",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5193:48,pipeline,pipeline,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5193,1,['pipeline'],['pipeline']
Deployability,Trailing SNP sites and depth intervals without read coverage were being omitted from the output.; Integration tests have been updated to test that this revision solves that problem.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8045:98,Integrat,Integration,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8045,2,"['Integrat', 'update']","['Integration', 'updated']"
Deployability,"Travis [updated](https://docs.travis-ci.com/user/build-environment-updates/2017-12-12/) the trusty images last night (seems to be ok so far). They also added a new update schedule and a new [group](https://blog.travis-ci.com/2017-12-01-new-update-schedule-for-linux-build-images) declaration. The default appears to be ""stability ensured"", but this adds an explicit declaration for that.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3953:8,update,updated,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3953,4,['update'],"['update', 'update-schedule-for-linux-build-images', 'updated', 'updates']"
Deployability,"Travis rolled out updated images that caused our builds to start failing. It looks like some of the WDL tests had been failing for a [while](https://github.com/broadinstitute/gatk/issues/3558), but that wasn't causing the **builds** to fail until the new Travis images were rolled out, at which point we started running out of space logging the errors. We're temporarily requesting to use the old Travis image (https://github.com/broadinstitute/gatk/pull/3557), but we should revert that once we address the underlying issues.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3559:18,update,updated,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3559,1,['update'],['updated']
Deployability,Travis-CI now has git-lfs baked into the images so we can delete our installation script and just use their instance. ; deleted download_lfs_files.sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3226:69,install,installation,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3226,1,['install'],['installation']
Deployability,Trivial formatting update for skip interval file parser,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2379:19,update,update,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2379,1,['update'],['update']
Deployability,Try using SAMRecord instead of model read in Spark pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/875:51,pipeline,pipeline,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/875,1,['pipeline'],['pipeline']
Deployability,Trying to write bam to dev null using PrintReadsSpark results in:. ```; org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: org.apache.hadoop.fs.ParentNotDirectoryException: Parent path is not a directory: file:/dev/null; at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:50); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1436:292,pipeline,pipelines,292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1436,1,['pipeline'],['pipelines']
Deployability,Turn on and update tests skipped in #8741,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8893:12,update,update,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8893,1,['update'],['update']
Deployability,"Turns out I've been self-consistently using SVIntervals as BED intervals, which is the wrong interpretation according to its methods `toBEDInterval(...)` and `toSimpleInterval(...)`. Note to self to update the interpretation in package `sv.discovery`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5154:199,update,update,199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5154,1,['update'],['update']
Deployability,Turns out we disabled codecov 3 years ago for a reason. We can leave it on for informational purposes with this patch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7815:112,patch,patch,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7815,1,['patch'],['patch']
Deployability,"Two .vcf.idx files used by the haplotype caller integration test had; file name lengths > 144. This is incompatible with ecryptfs, which is; commonly used for encrypted home directories on linux. Renaming the; .vcf and .vcf.idx files and updating references to them fixed the; problem. Fixes #4718.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4736:48,integrat,integration,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4736,1,['integrat'],['integration']
Deployability,"Two annotations, allele depth and total depth, consider whether reads are informative relative to the alleles in the output `VariantContext`, which is in general a subset of the alleles contains in the `ReadLikelihoods`. In PR #2185 I overlooked this and forgot to subset the likelihoods' alleles to those of the vc, which was the previous behavior (see the diff from that PR for the two annotations in this PR). This PR duplicates the old behavior. This fixes failures in the HaplotypeCaller integration tests introduced by the previous PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2239:493,integrat,integration,493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2239,1,['integrat'],['integration']
Deployability,"UPDATE: @lbergelson and I started creating this as an extension to SplitIntervals, but it quickly because very complex to fit it into that framework/abstraction so we decided to create a specialized tool for GVS. It would be valuable for SplitIntervals to be able to split intervals based not on number of genomic bases, but by using a set of weights. Ideally this new mode would read in a BED file containing the weights in the score field and attempt to produce a series of intervals that have equal total weights. . Note: ` --dont-mix-contigs` should still continue to work. ** Why? **; In the Genomic Variant Store, we have found that scattering work by ""# of genomic bases"" does not lead to even runtimes for the shards. ![image](https://user-images.githubusercontent.com/1423491/147964102-d2c83dea-8486-4699-9e7a-eef3dc759732.png). Instead we have found that an excellent proxy for runtime is the number of variants contained in a given interval:. ![image](https://user-images.githubusercontent.com/1423491/147964259-333f4058-a701-4b31-b410-242518d1b3b2.png). And furthermore, that this generalizes even when we use a subset of a different dataset. ![image](https://user-images.githubusercontent.com/1423491/147964392-17d045e9-e2f2-467b-8eae-77bd63291902.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7622:0,UPDATE,UPDATE,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7622,1,['UPDATE'],['UPDATE']
Deployability,"Ultima, New/Updated Features",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8579:12,Update,Updated,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8579,1,['Update'],['Updated']
Deployability,"Unlike the other validation rules, this does not test the validity of the VAT, but whether the pipeline completed as we expected it to--so I have added this as the singular test that runs during the pipeline. Validation Rule 2: The number of passing variants in GVS matches the number of variants in the VAT. Please note that we are counting the number of variants in GVS, not the number of sites, which may add a difficulty to this task. Another way to phrase it: ""If I were to make a sites only VCF of GVS and split each passing variant into it's own line, that number should equal the number of unique VIDs in the VAT."". Measure number of unique variants in sites only VCF that is generated. We don't want to count filtered variants so we can't count the GVS table. NOTE:. this pr also has some general cleanup as per discussion with Andrea. ; where would y'all suggest I put the template file for the custom annotations?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7374:95,pipeline,pipeline,95,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7374,2,['pipeline'],['pipeline']
Deployability,"UnpairedMapping SKIPPED; ```. This test fails because some JAR wasn't built:; ```; Running Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest); Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest > testPipeForPicardTools STANDARD_ERROR; No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: or. or; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: export GATK_LOCAL_JAR=<path_to_local_jar>. export GATK_LOCAL_JAR=<path_to_local_jar>; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. ```. etc... etc... Version: 4.6.0.0; FreeBSD 14.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:4233,Pipeline,PipelineSupportIntegrationTest,4233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,5,['Pipeline'],['PipelineSupportIntegrationTest']
Deployability,Update -autosomal-coverage description,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7165:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7165,1,['Update'],['Update']
Deployability,Update .dockstore.yml,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7553:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7553,2,['Update'],['Update']
Deployability,Update AS_RMSMappingQuality.java comments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7607:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7607,1,['Update'],['Update']
Deployability,Update AUTHORS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/494:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/494,2,['Update'],['Update']
Deployability,Update AUTHORS with new contributors.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5033:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5033,1,['Update'],['Update']
Deployability,Update AoU Documentation for GvsImportGenomes to reflect new behavior,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8571:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8571,1,['Update'],['Update']
Deployability,Update AsynchronousStreamWriterService unit test timeouts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4028:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4028,1,['Update'],['Update']
Deployability,Update Beta User WDL to use bulk ingest [VS-982],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8379:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8379,1,['Update'],['Update']
Deployability,Update Beta workspace [VS-969],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8496:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8496,1,['Update'],['Update']
Deployability,Update Broken Links in Tool Docs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7270:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7270,1,['Update'],['Update']
Deployability,Update CNV WDLs to WDL 1.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6502:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6502,1,['Update'],['Update']
Deployability,Update CNV WDLs to use PreprocessIntervals and CollectReadCounts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3661:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3661,1,['Update'],['Update']
Deployability,Update CNV tool docs to clarify log2 status of inputs and outputs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3156:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3156,1,['Update'],['Update']
Deployability,Update CNV tools and documentation to use IntervalMergingRule.NONE.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5891:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5891,1,['Update'],['Update']
Deployability,Update CRAM detector output files.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8971:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8971,1,['Update'],['Update']
Deployability,Update CommonSuffixSplitter.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4213:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4213,1,['Update'],['Update']
Deployability,Update Docker to newer version of miniconda.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5866:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5866,1,['Update'],['Update']
Deployability,Update Docs Around Callset Cleanup and Cost [VS-1107],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8976:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8976,1,['Update'],['Update']
Deployability,Update EchoCallset gatk docker to most recent build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8803:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8803,1,['Update'],['Update']
Deployability,Update GATK dependencies to patch security vulnerabilities,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8352:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8352,2,"['Update', 'patch']","['Update', 'patch']"
Deployability,Update GATK docker base image to Ubuntu 22.04,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8243:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8243,1,['Update'],['Update']
Deployability,Update GATK jar used in GvsJointVariantCalling WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8216:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8216,1,['Update'],['Update']
Deployability,Update GCS connector from 1.6.1 to 1.6.3.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4590:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4590,1,['Update'],['Update']
Deployability,"Update GKL to 0.4.1 and HTSJDK to 2.9.1. GKL 0.4.1 provides the following improvements over GKL 0.1.2 (currently used by GATK4):; - Improved performance and compression ratio for level-1 compression; - Improved performance for compression levels > 1; - Decompression acceleration with `IntelInflater`. The `IntelInflater` is enabled by default and can be disabled with the option `--use_jdk_inflater`. Resolves #2421, #2315, #2302",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423,1,['Update'],['Update']
Deployability,Update GKL to 0.8.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3865:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3865,1,['Update'],['Update']
Deployability,Update GVS Docs to cover additional use cases and troubleshooting suggestions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8917:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8917,1,['Update'],['Update']
Deployability,Update GVS docs with VETS information,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8466:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8466,1,['Update'],['Update']
Deployability,Update GVS sample QC to support multiple callsets per datasset [VS-177],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7451:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7451,1,['Update'],['Update']
Deployability,Update Gradle to 7.5.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8098:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8098,1,['Update'],['Update']
Deployability,Update GvsCalculatePrecisionAndSensitivity.wdl to allow for different scale of calibration_sensitivity vs. lod score.; Also retrieving score from JointVcfFiltering and storing that in BQ and in the VCF. (Probably don't need this long term.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8230:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8230,1,['Update'],['Update']
Deployability,Update GvsCreateFilterSet.wdl to disable excess alleles by default,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7239:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7239,1,['Update'],['Update']
Deployability,"Update GvsCreateVAT.wdl to build the subpopulation-specific files from the input ancestry file.; Have GvsCreateVAT.wdl only pull fields from the VCF for the selected subpopulations.; Update create_variant_annotation_table.py to set empty population-specific AC/AN/AF, etc. values if population is not present.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7965:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7965,2,['Update'],['Update']
Deployability,Update GvsExtractCallset.example.inputs.json,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7469:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7469,1,['Update'],['Update']
Deployability,Update GvsExtractCallset.wdl to use public weights bed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7678:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7678,1,['Update'],['Update']
Deployability,"Update GvsPrepareCallset step so that, instead of creating and inserting all the data into the `_pet_new` table in one step (which errors out with large callsets), add the data in smaller sections that correspond to each `pet_` in the GVS. Closes https://broadworkbench.atlassian.net/browse/VS-48",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7395:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7395,1,['Update'],['Update']
Deployability,Update HTSJDK to 4.1.1 and Picard to 3.2.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8900:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8900,1,['Update'],['Update']
Deployability,Update HTSJDK to 4.1.1 and Picard to 3.2.0. Included a unit test to check for the presence of the fix in HTSJDK 4.1.1 for the CRAM base corruption bug reported in https://github.com/broadinstitute/gatk/issues/8768; ; Resolves #8768,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8900:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8900,1,['Update'],['Update']
Deployability,Update Intel GKL to 0.8.10,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8181:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8181,1,['Update'],['Update']
Deployability,Update JEXL recommendations in public documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5509:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5509,1,['Update'],['Update']
Deployability,Update JexlExpression.java link,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7317:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7317,1,['Update'],['Update']
Deployability,Update LeftAlignIndels documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6177:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6177,1,['Update'],['Update']
Deployability,Update M2ArgumentCollection.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6840:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6840,1,['Update'],['Update']
Deployability,Update Main.java to reflect Picard command line parser selection changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6229:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6229,1,['Update'],['Update']
Deployability,Update MarkDuplicatesGATK/MarkDuplicatesSpark to incorporate recent Picard development,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3705:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3705,1,['Update'],['Update']
Deployability,Update MarkDuplicatesSpark OpticalDuplicate code to match Picards,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4700:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4700,1,['Update'],['Update']
Deployability,Update MitochondriaPipeline.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8395:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8395,1,['Update'],['Update']
Deployability,Update ModelSegments documentation to describe param table values,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4640:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4640,1,['Update'],['Update']
Deployability,Update MuTect2 documentation about not computing obvious germline variants,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2499:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2499,1,['Update'],['Update']
Deployability,Update Mutect2 javadoc to reflect v4.1 changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5769:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5769,1,['Update'],['Update']
Deployability,Update Mutect2 pon WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5859:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5859,1,['Update'],['Update']
Deployability,Update Mutect2's filtering,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6903:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6903,1,['Update'],['Update']
Deployability,Update Mutect2.java Documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8999:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8999,1,['Update'],['Update']
Deployability,Update Owner to latest version / new Maven release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4490:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4490,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update P + S WDL to get around input limits [VS-970],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8449:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8449,1,['Update'],['Update']
Deployability,Update POPAF retrieval,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6999:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6999,1,['Update'],['Update']
Deployability,Update PathSeq read filters and DUST transformer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2665:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2665,1,['Update'],['Update']
Deployability,Update PathSeqFilterSpark and PathSeqBuildKmers tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3115:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3115,1,['Update'],['Update']
Deployability,Update Picard to 2.25.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7075:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7075,1,['Update'],['Update']
Deployability,Update Picard to 2.27.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7766:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7766,1,['Update'],['Update']
Deployability,Update Protocol Buffer dependency to 3.0.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2437:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2437,1,['Update'],['Update']
Deployability,Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7924:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7924,2,"['Integrat', 'Update']","['Integration', 'Update']"
Deployability,Update Quickstart Integration for X/Y scaling changes [VS-464],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7881:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7881,2,"['Integrat', 'Update']","['Integration', 'Update']"
Deployability,Update README before changing repo name to GATK,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/950:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/950,1,['Update'],['Update']
Deployability,Update README to include list of popular software included in docker image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8745:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8745,1,['Update'],['Update']
Deployability,Update README to mention that we use zenhub (or migrate from zenhub to github's new equivalent),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2487:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2487,1,['Update'],['Update']
Deployability,Update README with new gatk-dev-public list address,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1093:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1093,1,['Update'],['Update']
Deployability,Update README.md,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/239:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/239,4,['Update'],['Update']
Deployability,Update README.md - adding profiling documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1612:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1612,1,['Update'],['Update']
Deployability,Update README.md - clarified that you can run dataproc from laptop,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1789:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1789,1,['Update'],['Update']
Deployability,Update README.md - recommend gradlew and use in all examples,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1511:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1511,1,['Update'],['Update']
Deployability,Update README.md on testing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/837:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/837,1,['Update'],['Update']
Deployability,Update README.md to add info about dynamic allocation on YARN,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1737:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1737,1,['Update'],['Update']
Deployability,Update README.md to remove outdated references to the Intel conda environment.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5753:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5753,1,['Update'],['Update']
Deployability,Update ReadFilters documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3128:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3128,1,['Update'],['Update']
Deployability,"Update Readme, added info on how to contribute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1061:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1061,1,['Update'],['Update']
Deployability,Update Readme.md to reflect switch from Travis.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7808:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7808,1,['Update'],['Update']
Deployability,Update RecalDatum.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1851:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1851,1,['Update'],['Update']
Deployability,Update STS Delivery Doc [VS-770],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8150:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8150,1,['Update'],['Update']
Deployability,Update SV clustering classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7243:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7243,1,['Update'],['Update']
Deployability,Update SV split-read strand validation and clustering,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8378:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8378,1,['Update'],['Update']
Deployability,Update Script to Handle Both Files and FOFN [VS-1441],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8954:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8954,1,['Update'],['Update']
Deployability,Update Spark scripts to reflect changes from #5386 and #5127.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5415:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5415,1,['Update'],['Update']
Deployability,"Update StandardSomaticAnnotation to reflect M2's default annotations, currently under M2ArgumentCollection as annotation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3123:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3123,1,['Update'],['Update']
Deployability,Update Storage API to handle no data query return,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7082:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7082,1,['Update'],['Update']
Deployability,Update StrandBiasBySample documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7283:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7283,1,['Update'],['Update']
Deployability,Update To handle if no data error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7084:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7084,1,['Update'],['Update']
Deployability,Update ValidateSamFileIntegrationTest once htsjdk #369 CRAM bug fix is available,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1138:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1138,1,['Update'],['Update']
Deployability,Update VariantRecalibrator RScript Compatibility with Newer ggplot2 Versions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8992:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8992,1,['Update'],['Update']
Deployability,Update WDL tests to Cromwell 30.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3960:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3960,1,['Update'],['Update']
Deployability,Update `AlignContigsAndCallBreakpointsSpark` with jBWA's para changing capacity,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2001:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2001,1,['Update'],['Update']
Deployability,Update `isInformative` to use the natural log,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6884:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6884,1,['Update'],['Update']
Deployability,"Update all to createVCFWriter(Path) (not File). As a result, all VCF tools should now be able to write to Cloud (including Funcolator). This commit also removes createVCFWriter(File), ensuring all new; code retains the ability to write to Cloud. This touches many files, but the changes are mostly the same everywhere. Only `FilterByOrientationBias.java` is a bit different since it also needed code to add a suffix to a file name. . Fixes #5726",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5728:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5728,1,['Update'],['Update']
Deployability,Update allele subsetting to support genotype posteriors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7390:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7390,1,['Update'],['Update']
Deployability,Update annotation args to GATK4 names,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7413:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7413,1,['Update'],['Update']
Deployability,Update artifactory URL.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3200:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3200,1,['Update'],['Update']
Deployability,Update base image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8228:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8228,1,['Update'],['Update']
Deployability,Update beta docs to tell people not to use free credits,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8184:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8184,1,['Update'],['Update']
Deployability,Update beta/gvs-quickstart.md,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8364:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8364,1,['Update'],['Update']
Deployability,Update build.gradle to make sure that tests are always rerunable.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1719:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1719,1,['Update'],['Update']
Deployability,Update bwamem-jni dependency to 1.0.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3723:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3723,1,['Update'],['Update']
Deployability,Update changelog for Beta release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8743:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8743,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update cleanup doc [VS-787],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8981:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8981,1,['Update'],['Update']
Deployability,Update command-line examples in docs to use gatk-launch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3039:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3039,1,['Update'],['Update']
Deployability,Update compareBamFiles test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/227:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/227,1,['Update'],['Update']
Deployability,Update copyright date in LICENSE.TXT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6383:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6383,1,['Update'],['Update']
Deployability,Update copyright date in license,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4018:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4018,1,['Update'],['Update']
Deployability,Update cromwell and womtool from v51 to v59.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7824:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7824,1,['Update'],['Update']
Deployability,Update cromwell and womtool. See if I feel lucky.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7824:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7824,1,['Update'],['Update']
Deployability,Update data.table R dependency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3712:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3712,1,['Update'],['Update']
Deployability,"Update dependencies to address security vulnerabilities, and add a security scanner to build.gradle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8607:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8607,1,['Update'],['Update']
Deployability,Update disq to 0.3.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6040:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6040,1,['Update'],['Update']
Deployability,Update disq to 0.3.4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6252:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6252,1,['Update'],['Update']
Deployability,Update disq to 0.3.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6323:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6323,1,['Update'],['Update']
Deployability,Update do spark 2.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2911:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2911,1,['Update'],['Update']
Deployability,"Update doc templates to reflect ""Experimental"" tag.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4033:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4033,1,['Update'],['Update']
Deployability,Update docker base,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9005:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9005,1,['Update'],['Update']
Deployability,Update docs for Funcotator. Tool documentation and GencodeFuncotationFactory / GencodeFuncotation documents (to create a proper spec. for the gencode annotations).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5611:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5611,1,['Update'],['Update']
Deployability,Update docs for Nirvana reference disk [VS-796] [VS-531],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8170:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8170,1,['Update'],['Update']
Deployability,Update docs for callset stats [VS-1094],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8591:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8591,1,['Update'],['Update']
Deployability,Update documentation for VDS/VAT [VS-1125],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8688:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8688,1,['Update'],['Update']
Deployability,Update documentation for various tools (MW),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4026:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4026,1,['Update'],['Update']
Deployability,Update documentation to point to newly released BGE exome calling list.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8407:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8407,2,"['Update', 'release']","['Update', 'released']"
Deployability,Update error message based on https://github.com/broadinstitute/gatk/issues/4669.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4678:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4678,1,['Update'],['Update']
Deployability,Update example code to correspond to Barclay arg tagging change,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5710:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5710,1,['Update'],['Update']
Deployability,Update firehose to reflect standardization of target table format,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2867:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2867,1,['Update'],['Update']
Deployability,Update for funcotator data sources,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6660:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6660,1,['Update'],['Update']
Deployability,Update forum posts for the name change in CNLoH caller,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2904:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2904,1,['Update'],['Update']
Deployability,Update forum posts to reflect standardization of target table format,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2866:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2866,1,['Update'],['Update']
Deployability,Update funcotator bundled data sources,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8296:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8296,1,['Update'],['Update']
Deployability,Update gCNV mappability track.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6591:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6591,1,['Update'],['Update']
Deployability,Update gcloud docker commands in build_docker.sh,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7078:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7078,1,['Update'],['Update']
Deployability,Update gcloud-java-nio to version 0.2.8. Addresses #2110.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2132:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2132,1,['Update'],['Update']
Deployability,Update git-lfs prerequisite message.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4678:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4678,1,['Update'],['Update']
Deployability,Update google-cloud-java dependency to a snapshot containing newly added NIO API methods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2441:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2441,1,['Update'],['Update']
Deployability,Update google-cloud-nio dependency to 0.20.4-alpha-20170727.190814-1:shaded,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3373:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3373,1,['Update'],['Update']
Deployability,Update google-cloud-nio to support underscores in bucket names,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8439:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8439,1,['Update'],['Update']
Deployability,Update gradle and build.gradle,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8998:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8998,1,['Update'],['Update']
Deployability,Update gvs workspace description [VS-1042],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8509:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8509,1,['Update'],['Update']
Deployability,Update hail version to 120 in Integration test VS 1025,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8502:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8502,2,"['Integrat', 'Update']","['Integration', 'Update']"
Deployability,Update hail_create_vat_inputs.py,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8772:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8772,1,['Update'],['Update']
Deployability,Update haplochecker dockerfile and task to match,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5760:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5760,1,['Update'],['Update']
Deployability,"Update hdf5-java-bindings to version 1.2.0-hdf5_2.11.0, which removes log4j 1.x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8908:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8908,1,['Update'],['Update']
Deployability,Update hmmUrl in install_R_packages.R,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8638:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8638,1,['Update'],['Update']
Deployability,Update htsjdk to 1.137,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/758:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/758,1,['Update'],['Update']
Deployability,Update htsjdk to 1.141.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1122:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1122,1,['Update'],['Update']
Deployability,Update htsjdk to 2.18.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5585:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5585,1,['Update'],['Update']
Deployability,Update htsjdk to 2.20.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6094:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6094,1,['Update'],['Update']
Deployability,Update htsjdk to 2.20.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6126:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6126,1,['Update'],['Update']
Deployability,Update htsjdk to 2.23.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6702:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6702,1,['Update'],['Update']
Deployability,Update htsjdk to 2.24.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7073:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7073,1,['Update'],['Update']
Deployability,Update htsjdk to 2.24.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7149:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7149,1,['Update'],['Update']
Deployability,Update htsjdk to 2.7.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2247:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2247,1,['Update'],['Update']
Deployability,Update htsjdk to 2.8.1-17-g19bd848-SNAPSHOT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2354:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2354,1,['Update'],['Update']
Deployability,Update http-nio and wire its new settings,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8611:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8611,1,['Update'],['Update']
Deployability,Update http-nio to 1.1.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8889:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8889,1,['Update'],['Update']
Deployability,Update import_gvs.py,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8617:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8617,1,['Update'],['Update']
Deployability,Update instructions for installing Java 8 in the README,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8089:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8089,2,"['Update', 'install']","['Update', 'installing']"
Deployability,Update instructions for installing R packages in README.md.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3601:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3601,2,"['Update', 'install']","['Update', 'installing']"
Deployability,Update license copyright date,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2752:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2752,1,['Update'],['Update']
Deployability,Update links to old GATK forum and website,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6382:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6382,1,['Update'],['Update']
Deployability,Update log4j to 2.17.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7624:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7624,1,['Update'],['Update']
Deployability,Update logic around missing sample data in TDM [VS-997],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8422:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8422,1,['Update'],['Update']
Deployability,"Update multiple tools to use a Path instead of File. Updated: FeatureWalker, ReadWalker, IntervalWalker, MultiVariantWalker, AnnotateIntervals, BaseRecalibrator, SelectVariants, SplitNCigarReads. Some tools will need a change to ReferenceSequenceFileWalker in htsjdk, so I couldn't change those. Same for the writers. Part of addressing issue #3709",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3921:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3921,2,['Update'],"['Update', 'Updated']"
Deployability,Update numpy\scipy\pymc3 python package,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6978:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6978,2,['Update'],['Update']
Deployability,Update ojAlgo and improve commons-math / ojAlgo integration,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3970:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3970,2,"['Update', 'integrat']","['Update', 'integration']"
Deployability,Update old docs with GATK4 name: ApplyRecalibration -> ApplyVQSR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7058:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7058,1,['Update'],['Update']
Deployability,Update orientation bias model command in mutect2.pdf,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5856:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5856,1,['Update'],['Update']
Deployability,Update our HTSJDK dependency to 4.0.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8584:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8584,1,['Update'],['Update']
Deployability,Update our google-cloud-java fork to 0.20.5-alpha-GCS-RETRY-FIX,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5099:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5099,1,['Update'],['Update']
Deployability,Update overlap calculation in FragmentUtils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6824:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6824,1,['Update'],['Update']
Deployability,Update override jar in bulk ingest staging,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8451:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8451,1,['Update'],['Update']
Deployability,Update phasingString so it is unique across contigs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6936:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6936,1,['Update'],['Update']
Deployability,Update picard and htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6462:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6462,1,['Update'],['Update']
Deployability,Update picard to 2.21.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6253:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6253,1,['Update'],['Update']
Deployability,Update picard to 3.1.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8585:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8585,1,['Update'],['Update']
Deployability,Update protobuf to 3.21.6,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8036:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8036,1,['Update'],['Update']
Deployability,Update quickstart,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7439:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7439,1,['Update'],['Update']
Deployability,Update references to old-style arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5063:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5063,1,['Update'],['Update']
Deployability,Update repo location in install_R_packages.R.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3777:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3777,1,['Update'],['Update']
Deployability,Update serializable objects to use lambda functions where appropriate after upgrading to Kryo 3.0+,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1510:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1510,1,['Update'],['Update']
Deployability,Update several dependencies to fix vulnerabilities,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8898:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8898,1,['Update'],['Update']
Deployability,Update spark-dataflow to fix Combine.Globally bug affecting ApplyWhol…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/600:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/600,1,['Update'],['Update']
Deployability,Update spark.md,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1195:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1195,1,['Update'],['Update']
Deployability,Update sqllite library to support M1 Macs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7519:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7519,1,['Update'],['Update']
Deployability,Update stand_call_conf doc string,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2332:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2332,2,['Update'],['Update']
Deployability,Update sv pipeline scripts for dataproc timed self-termination,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3574:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3574,2,"['Update', 'pipeline']","['Update', 'pipeline']"
Deployability,Update tandem duplication annotations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2567:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2567,1,['Update'],['Update']
Deployability,Update the BQSR integration tests to test without BAQ and indel qualities,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2563:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2563,2,"['Update', 'integrat']","['Update', 'integration']"
Deployability,Update the GATK base image to a newer LTS ubuntu release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8610:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8610,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update the GQ after PLs get subset,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3409:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409,1,['Update'],['Update']
Deployability,"Update the Intel GKL to the latest release, 0.8.11",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8409:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8409,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update the M2 WDL readme with Funcotator instead of Oncotator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5889:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889,1,['Update'],['Update']
Deployability,Update the Owner version with the latest release from Maven.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4490:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4490,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update the PGT FORMAT header as discussed in #6937:. PGT format field on each genotype can be interpreted as being an indicator of which of the two phased haplotypes in the sample contains the site-specific alternate allele at the site. See also #6220 and #6952,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6954:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6954,1,['Update'],['Update']
Deployability,Update the README for alpha,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1327:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1327,1,['Update'],['Update']
Deployability,Update the VAT pipeline readme to know about the VDS inputs. This will need to further be edited once George's VDS VAT pipeline changes have been completed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8090:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8090,3,"['Update', 'pipeline']","['Update', 'pipeline']"
Deployability,Update the change log in preparation for releasing version 0.3.2. of the Beta workflow.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8518:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8518,1,['Update'],['Update']
Deployability,Update the conda environment and build.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4749:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4749,1,['Update'],['Update']
Deployability,Update the gcloud package signing key during the docker build,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7180:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7180,1,['Update'],['Update']
Deployability,"Update the gradle distribution commands to include the sparkJar, gatk-launch, and settings.gradle in the zipped archive so we can have a single command to build a distributable zip.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1779:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1779,1,['Update'],['Update']
Deployability,Update the header of AS_FilterStatus,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6858:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6858,1,['Update'],['Update']
Deployability,Update the large CRAM files to v3.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8832:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8832,1,['Update'],['Update']
Deployability,Update the main project README for beta,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3158:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3158,1,['Update'],['Update']
Deployability,Update the setup_cloud github action,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8651:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8651,1,['Update'],['Update']
Deployability,Update the strand artifact filter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4500:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4500,1,['Update'],['Update']
Deployability,Update the wikis to use gatk-launch syntax,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1328:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1328,1,['Update'],['Update']
Deployability,Update to 1.1.2.2 with Linux and MacOS shared libraries packaged and added junit to check if they are in the genomicsdb-<VERSION>.jar.; Also brought back #6188 and #6190 into this branch which had to be reverted before - see #6204.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6206:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6206,1,['Update'],['Update']
Deployability,Update to Barclay 1.0.0-24-g87c3fa2-SNAPSHOT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2455:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2455,1,['Update'],['Update']
Deployability,Update to GKL 0.3.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2259:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259,1,['Update'],['Update']
Deployability,"Update to GKL 0.3.1 which provides the following features compared to GKL 0.1.2 currently used by GATK:. 1. Improved performance for Level 1 compression.; 1. OpenMP AVX PairHMM with fallback to non-OpenMP support logic in GATK.; 1. Individual shared library objects for each native binding.; 1. Flush-to-zero get/set support. Updated `VectorPairHMMUnitTest` to test all supported implementations of `VectorLoglessPairHMM` and skip the test if no implementations are supported. **Note**; The default number of threads used by OpenMP AVX PairHMM is set to 1, and the command line argument to change the number of threads is not connected yet (see #1946). So, the OpenMP and non-OpenMP PairHMM currently have the same performance. Resolves #1819; First step for #1946",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2259:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2259,2,['Update'],"['Update', 'Updated']"
Deployability,"Update to GKL 0.4.1, HTSJDK 2.9.1, and enable IntelInflater",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2423:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2423,1,['Update'],['Update']
Deployability,"Update to GKL 0.5.8, which fixes bug in AVX detection, which was beha…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3513:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3513,1,['Update'],['Update']
Deployability,Update to GKL version 0.8.8,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7203:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7203,1,['Update'],['Update']
Deployability,Update to Genomicsdb 1.1.2.2 with Linux and MacOS shared libraries packaged,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6206:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6206,1,['Update'],['Update']
Deployability,Update to Intel GKL version 0.5.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3392:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3392,1,['Update'],['Update']
Deployability,Update to Intel GKL version 0.5.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3401:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3401,1,['Update'],['Update']
Deployability,Update to Spark 3.0 and Java 11 (or 17),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6671:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6671,1,['Update'],['Update']
Deployability,Update to gatk script to contain custom jdk path parameter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8495:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8495,1,['Update'],['Update']
Deployability,Update to gcloud NIO 0.19.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3044:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3044,1,['Update'],['Update']
Deployability,Update to htsjdk 2.16.0. This only updates gatk to fix the compile warnings from deprecations. An additional PR is needed in order to support fasta.gz files. fixes #4039,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4914:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4914,2,"['Update', 'update']","['Update', 'updates']"
Deployability,Update to htsjdk 2.24.1. This fixes a gross issue where we accidentally included Junit as a runtime dependency.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7149:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7149,1,['Update'],['Update']
Deployability,Update to htsjdk 2.4.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1867:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1867,1,['Update'],['Update']
Deployability,Update to htsjdk 2.8.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2301:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2301,1,['Update'],['Update']
Deployability,"Update to latest gcloud, uncomment getAuthenticatedGcs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2581:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2581,1,['Update'],['Update']
Deployability,Update to latest version of VQSR Lite; Refactor GvsCreateFilterSet.wdl to move VQSR Classic code to its own WDL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8269:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8269,1,['Update'],['Update']
Deployability,Update to latest version of ah_var_store gatk override jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7793:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7793,1,['Update'],['Update']
Deployability,Update to new gcloud release when it's out,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2822:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2822,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update to next google-cloud-java release once it's out,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3500:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3500,2,"['Update', 'release']","['Update', 'release']"
Deployability,Update to reflect Picard version 1.130.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/332:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/332,1,['Update'],['Update']
Deployability,Update to spark-dataflow 0.1.1 and fix unit tests that are failing wh…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/574:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/574,1,['Update'],['Update']
Deployability,Update to the latest import_gvs() code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8330:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8330,1,['Update'],['Update']
Deployability,Update tool docs for Funcotator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7160:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7160,1,['Update'],['Update']
Deployability,Update tool documentation to reflect GATK4 changes (with comms team),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2474:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2474,1,['Update'],['Update']
Deployability,Update two instances of '--genotyping_mode' with '--genotyping-mode',MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5658:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5658,1,['Update'],['Update']
Deployability,Update utils.text package to work on java.nio.Path,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5747:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5747,1,['Update'],['Update']
Deployability,Update variable names in mutect2_pon.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259,1,['Update'],['Update']
Deployability,Update variable names in mutect2_pon.wdl to reflect changes in mutect2.wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4259:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4259,1,['Update'],['Update']
Deployability,Update variants base image [VS-866],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8262:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8262,1,['Update'],['Update']
Deployability,Update versions of htsjdk/picard/disq,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6637:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6637,1,['Update'],['Update']
Deployability,Update wiki pages that talk about gatk-protected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2794:0,Update,Update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2794,1,['Update'],['Update']
Deployability,"UpdateVCFSequenceDictionary replaces ""##FORMAT=<ID=GQ"" with stock text",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8629:0,Update,UpdateVCFSequenceDictionary,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8629,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,UpdateVCFSequenceDictionary tool.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2234:0,Update,UpdateVCFSequenceDictionary,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2234,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,Updated --mapping-quality-threshold and its documentation to be less confusing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7036:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7036,1,['Update'],['Updated']
Deployability,Updated COSMIC to annotate protein change strings with their counts.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5181:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5181,1,['Update'],['Updated']
Deployability,Updated Callset stats README [VS-210],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7502:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7502,1,['Update'],['Updated']
Deployability,Updated Funcotator to support ENSEMBL GTF files (and non-human species).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6477:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6477,1,['Update'],['Updated']
Deployability,"Updated GVCF ""reblocking"" with no gaps, no overlaps",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7122:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7122,1,['Update'],['Updated']
Deployability,Updated GencodeGtfCodec to be more permissive.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7166:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7166,1,['Update'],['Updated']
Deployability,Updated M2 WDL README with Funcotator info.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5892:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5892,1,['Update'],['Updated']
Deployability,Updated M2 parameters to make them consistent with HC,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8186:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8186,1,['Update'],['Updated']
Deployability,Updated M2 with latest Funcotator info.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5735:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5735,1,['Update'],['Updated']
Deployability,Updated MarkDuplicates Scoring and Comparison code to reflect mismatches to picard,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5023:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5023,1,['Update'],['Updated']
Deployability,Updated MarkDuplicates code to rely on the Picard OpticalDuplicatesFinder,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4750:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4750,1,['Update'],['Updated']
Deployability,Updated MarkDuplicates to use Picard metrics code,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4779:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4779,1,['Update'],['Updated']
Deployability,Updated Picard to version 2.18.25,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5597:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5597,1,['Update'],['Updated']
Deployability,"Updated PostprocessGermlineCNVCalls (segments VCF writing, WDL scripts, unit tests, integration tests)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4396:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4396,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,"Updated Python and PyMC, removed TensorFlow, and added PyTorch in conda environment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['Update'],['Updated']
Deployability,Updated Quickstart,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7568:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7568,1,['Update'],['Updated']
Deployability,Updated R requirement in the README,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4023:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4023,1,['Update'],['Updated']
Deployability,Updated README to mention that macOS is not supported at this time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6788:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6788,1,['Update'],['Updated']
Deployability,Updated README to state that only 64-bit Linux distributions are supported. Made this edit under the Python dependencies section. Fix issue #6786,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6788:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6788,1,['Update'],['Updated']
Deployability,"Updated RMS Mapping quality annotations to be of type long/Long instead; of int/Integer. With int types, a large cohort could overflow; Integer.MAX_VALUE when handling sum of squared mapping qualities. With; long types this should not be a problem until we have off-world; colonies. This resolves issue 5433.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5435:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5435,1,['Update'],['Updated']
Deployability,Updated VCF gencode annotation positions/alleles.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5131:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5131,1,['Update'],['Updated']
Deployability,"Updated WDL generation, upgrade to Barclay 4.0.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6800:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6800,2,"['Update', 'upgrade']","['Updated', 'upgrade']"
Deployability,Updated arg names for engine-level arguments. Fixed logging for PossibleDeNovo annotation. Also addresses #3927 -- minor bug in CalculateGenotypePosteriors,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3928:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3928,1,['Update'],['Updated']
Deployability,Updated arguments and tests to use a datasources directory.; ; Fixed the way dna repair genes get parsed.; Fixed a coding sequence bug when dealing with multiple gencode sources.; Fixed a bug in COSMIC annotaitons when GENCODE annotations are in IGRs.; ; Fixes #3999,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4020:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4020,1,['Update'],['Updated']
Deployability,Updated carrot publish github action version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8084:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8084,1,['Update'],['Updated']
Deployability,Updated code to allow for protein changes with N / IUPAC bases.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6778:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6778,1,['Update'],['Updated']
Deployability,Updated datasource version parsing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5149:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5149,1,['Update'],['Updated']
Deployability,Updated doc templates to match website updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4805:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4805,2,"['Update', 'update']","['Updated', 'updates']"
Deployability,Updated docs with usage examples and kebabed long args for:; - HaplotypeCaller; - HaplotypeCallerSpark; - CombineGVCFs; - GenomicsDBImport; - GenotypeGVCFs; - VariantFiltration; - ASEReadCounter; - SplitNCigarReads; - CalculateGenotypePosteriors; - VariantRecalibrator; - ApplyVQSR. Elaborated on/fixed docs for:; - InbreedingCoeff; - ExcessHet; - SampleList. Hid GatherTranches. Added a ReadTransformer to SplitNCigarReads to simplify the command from the old RNA best practices (https://software.broadinstitute.org/gatk/documentation/article.php?id=3891) NOTE: this slightly changes the default behavior @vdauwera . Unfortunately I squashed the SplitNCigarReads changes into the doc fixes. :( If that's a problem I can split into two commits.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3891:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3891,1,['Update'],['Updated']
Deployability,Updated documentation with FAQ section to respond to user comments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5755:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5755,1,['Update'],['Updated']
Deployability,Updated gCNV WDLs for FireCloud and changed CNV/M2 WDLs to use gatk launch script.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4071:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4071,1,['Update'],['Updated']
Deployability,Updated install_R_packages.R to fix broken ggplot2 dependency in gatkbase Docker image.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5040:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5040,1,['Update'],['Updated']
Deployability,Updated log4j to version 2.16.0 to further mitigate CVE-2021-44228,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7605:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7605,1,['Update'],['Updated']
Deployability,Updated mutect2.wdl and funcotator.wdl with latest args for funcotator.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4890:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4890,1,['Update'],['Updated']
Deployability,Updated one set of outdated forum links.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8273:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8273,1,['Update'],['Updated']
Deployability,Updated plotting for ModelSegments CNV pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3729:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3729,2,"['Update', 'pipeline']","['Updated', 'pipeline']"
Deployability,"Updated scripts for sv cluster creation, initialization and pipeline execution",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2468:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2468,2,"['Update', 'pipeline']","['Updated', 'pipeline']"
Deployability,Updated templates to fit current website logic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3165:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3165,1,['Update'],['Updated']
Deployability,Updated testing Gencode data sources to fully exercise test data set,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5423:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5423,1,['Update'],['Updated']
Deployability,"Updated the carrot github action workflow to the most recent version, which supports using #carrot_pr to trigger branch vs master comparison runs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8084:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8084,1,['Update'],['Updated']
Deployability,Updated the code that grabs a resource file to use Resource.getResour…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4723:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4723,1,['Update'],['Updated']
Deployability,Updated the experimental funcotator.wdl to work properly in cromwell.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4870:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4870,1,['Update'],['Updated']
Deployability,Updated the link to the Funcotator info/tutorial page.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6920:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6920,1,['Update'],['Updated']
Deployability,Updated the stats script to be more easily used.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7759:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7759,1,['Update'],['Updated']
Deployability,Updated the ubuntu version for the carrot github action because github dropped support for 18.04,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8299:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8299,1,['Update'],['Updated']
Deployability,Updated to a newer release of the carrot-publish-github-action,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6986:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6986,2,"['Update', 'release']","['Updated', 'release']"
Deployability,Updated top-level class documentation for Funcotator.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4655:0,Update,Updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4655,1,['Update'],['Updated']
Deployability,UpdatedConcordanceDoc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3108:0,Update,UpdatedConcordanceDoc,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3108,1,['Update'],['UpdatedConcordanceDoc']
Deployability,"Updates (EchoCallset Version):. Changes to scatter width of VCFs generated has changed the amount of data generated in tests, so need to update truth; Adding a new field to extracted VCF Header EXCESS_ALLLELES and that will break the tests.; And why not validate our VCFs for jollies.; Updates 'truth' path for data to match these changes. Integration tests failed due to different number of output VCFs now. So I cherry-picked Miguel's commit on ah_var_store that changed the scatter.; Integration tests *still* [failing](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/8f7b0cc9-4a31-404b-99b3-89e182707e8b) due to the change in VCF Header:. > 6,7d5; < ##FILTER=<ID=high_CALIBRATION_SENSITIVITY_INDEL,Description=""Site failed INDEL model calibration sensitivity cutoff (0.99)"">; < ##FILTER=<ID=high_CALIBRATION_SENSITIVITY_SNP,Description=""Site failed SNP model calibration sensitivity cutoff (0.997)"">; 9c7; < ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Genotype Filter Field"">; ---; > ##FORMAT=<ID=FT,Number=1,Type=String,Description=""Sample Genotype Filter Field"">; 3388a3387,3388; > ##high_CALIBRATION_SENSITIVITY_INDEL=Sample Genotype FT filter value indicating that the genotyped allele failed INDEL model calibration sensitivity cutoff (0.99); > ##high_CALIBRATION_SENSITIVITY_SNP=Sample Genotype FT filter value indicating that the genotyped allele failed SNP model calibration sensitivity cutoff (0.997)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8848:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8848,5,"['Integrat', 'Update', 'update']","['Integration', 'Updates', 'update']"
Deployability,Updates M2 WDLs to 1.0 and fixes NIO,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6108:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6108,1,['Update'],['Updates']
Deployability,"Updates SVAnnotate's functional consequence annotation of complex SVs.; * Introduce PREDICTED_PARTIAL_DISPERSED_DUP annotation to describe dispersed duplications of coding sequence that are not expected to behave the same way as tandem duplications; * Ignore INV intervals in dDUP events; * Modify INV intervals in dupINV and similar events to more accurately capture the overall impact of the complex SV; * Ignore complex DUP segments for promoter, noncoding, and nearest TSS annotations because these DUPs are never in tandem; * Merge relevant intervals before annotating nearest TSS for complex events containing DELs; * Update documentation. Testing; * Add unit test for CPX SV segment determination; * Add CPX SV unit test cases; * Update unit/integration test expected outputs; * All unit & integration tests for SVAnnotate ran successfully",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8516:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8516,5,"['Update', 'integrat']","['Update', 'Updates', 'integration']"
Deployability,Updates bwamem-jni depedency to 1.0.2 and adds the possibility of ali…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3474:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3474,1,['Update'],['Updates']
Deployability,Updates expectations for the X/Y scaling changes that went in a couple of weeks ago and slip in a few unrelated minor improvements.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7881:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7881,1,['Update'],['Updates']
Deployability,Updates files for integration tests that were failing because of a conflict between #3876 and #3611. (Feel free to merge if it passes review so others can continue working immediately),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3910:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3910,2,"['Update', 'integrat']","['Updates', 'integration']"
Deployability,Updates for latest data source release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5614:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614,2,"['Update', 'release']","['Updates', 'release']"
Deployability,Updates gatk-bwamem-jni dependency to 1.0.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3727:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3727,1,['Update'],['Updates']
Deployability,Updates in preparation for release 0.5.2 [VS-1196],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8659:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8659,2,"['Update', 'release']","['Updates', 'release']"
Deployability,Updates in preparation for release 0.6.0 [VS-1376],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8925:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8925,2,"['Update', 'release']","['Updates', 'release']"
Deployability,"Updates the code to pull the PS field out of gVCFs and stores it in the VET_* table(s).; Note schema change on the VET table; Successful BulkIngest [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Exome%20Beta%20Test%20ggrant/job_history/5d1f17a9-eeb2-4db3-a681-32d36d9e567e) (run on Exomes as they have PGT, PID, and PS in their gVCFs).; Successful Integration Test Run [here](https://job-manager.dsde-prod.broadinstitute.org/jobs/9ab365ff-743b-4d97-9c2a-6a09cf8728f4) - But note that the Exome Integration test failed for slight (and expected) difference in table sizes. I have updated the truth in `gs://gvs-internal-quickstart/integration/2023-07-25-quicker/exome_weighted/table_sizes_expected.csv`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8531:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8531,5,"['Integrat', 'Update', 'integrat', 'update']","['Integration', 'Updates', 'integration', 'updated']"
Deployability,Updates to VcfComparator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8973:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8973,1,['Update'],['Updates']
Deployability,Updates to `gatk` launch script to fix properties and config file definitions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4653:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4653,1,['Update'],['Updates']
Deployability,Updates to rc-vs-651-vat-from-vds to support Scattering.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8122:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8122,1,['Update'],['Updates']
Deployability,Updates to reduce size of docker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8259:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8259,1,['Update'],['Updates']
Deployability,Updates to the dockerfile to use latest intel-optimized tensorflow,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5725:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5725,1,['Update'],['Updates']
Deployability,"Updates:; - Changes to scatter width of VCFs generated has changed the amount of data generated in tests, so need to update truth; - Adding a new field to extracted VCF Header `EXCESS_ALLLELES` and that will break the tests.; - And why not validate our VCFs for jollies.; - Updates 'truth' path for data to match these changes. Integration Tests:; Passing test against Chr20/X/Y [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/985fbc06-36ed-4006-9703-0b86577f704c); Passing test against All Chromosomes [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/9d473c81-4742-4188-bc70-1e9371bfcc11)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8846:0,Update,Updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8846,4,"['Integrat', 'Update', 'update']","['Integration', 'Updates', 'update']"
Deployability,Updating Funcotator tool documentation for 4.1 release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5620:47,release,release,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5620,1,['release'],['release']
Deployability,Updating dockers to include updates to Google cloud storage libraries,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8401:28,update,updates,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8401,1,['update'],['updates']
Deployability,Updating parameters used in local assembly step in SV pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1938:54,pipeline,pipeline,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1938,1,['pipeline'],['pipeline']
Deployability,"Updating the AUTHORS file to include authors who contributed to gatk-protected who's work has been integrated into GATK by the merger. I need to find out the preferred emails for the newly listed authors. Anders Peterson; Ayman Abdel Ghany <aymana.ghany@devfactory.com>; Kenji Kaneda ; Nils Homer. @apete @AymanDF @kkaneda @nh13 Would you like to be included here and if so, what email address would you like listed? Have I spelled your name correctly?. Resolves #3048",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3330:99,integrat,integrated,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3330,1,['integrat'],['integrated']
Deployability,Updating the README.md to fix the R installation instructions which were out of date.; scripts/install_R_packages.R -> scripts/docker/gatkbase/install_R_packages.R; fixes #3601,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3602:36,install,installation,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3602,1,['install'],['installation']
Deployability,Upgrade Barclay and integrate ExperimentalFeature annotation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2744:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2744,2,"['Upgrade', 'integrat']","['Upgrade', 'integrate']"
Deployability,"Upgrade Barclay, suppress file expansion for interval arguments.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4270:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4270,1,['Upgrade'],['Upgrade']
Deployability,Upgrade Barclay.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2790:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2790,1,['Upgrade'],['Upgrade']
Deployability,Upgrade CNV WDLs to 1.0 spec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6506:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6506,1,['Upgrade'],['Upgrade']
Deployability,Upgrade GCS Connector to 1.9.17,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6135:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6135,1,['Upgrade'],['Upgrade']
Deployability,Upgrade GenomicsDB to version 0.7.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3575:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3575,1,['Upgrade'],['Upgrade']
Deployability,Upgrade Mockito from 1.10.19 -> 2.10.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3581:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3581,1,['Upgrade'],['Upgrade']
Deployability,"Upgrade Nirvana to 3.18.1, add a workflow to push GVS output VCFs [VS-661]",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8056:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8056,1,['Upgrade'],['Upgrade']
Deployability,Upgrade Oncotator version in the cromwell M2 WDL test json,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4808:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4808,1,['Upgrade'],['Upgrade']
Deployability,Upgrade Picard and remove/resolve placeholder program groups.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4034:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4034,1,['Upgrade'],['Upgrade']
Deployability,Upgrade Picard from 2.18.7 -> 2.18.13.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5173:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5173,1,['Upgrade'],['Upgrade']
Deployability,Upgrade Picard to 2.14.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3737:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3737,1,['Upgrade'],['Upgrade']
Deployability,Upgrade automatic WDL tests to use cromwell 30,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4418:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4418,1,['Upgrade'],['Upgrade']
Deployability,"Upgrade barclay, htsjdk, gatk-fermilite-jni and gatk-bwamem-jni",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3127:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3127,1,['Upgrade'],['Upgrade']
Deployability,Upgrade cromwell to v51.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6628:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6628,1,['Upgrade'],['Upgrade']
Deployability,Upgrade google-cloud-java from 0.23.1 to 0.24.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3594:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3594,1,['Upgrade'],['Upgrade']
Deployability,Upgrade google-cloud-java to 0.30.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3855:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3855,1,['Upgrade'],['Upgrade']
Deployability,Upgrade htsjdk to 1.140.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/979:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/979,1,['Upgrade'],['Upgrade']
Deployability,Upgrade htsjdk to 2.1.1.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1551:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1551,1,['Upgrade'],['Upgrade']
Deployability,Upgrade htsjdk to 2.11.0-4-g958dc6e-SNAPSHOT.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3504:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3504,1,['Upgrade'],['Upgrade']
Deployability,Upgrade htsjdk to 2.3.0 and Hadoop-BAM to 7.5.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1817:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1817,1,['Upgrade'],['Upgrade']
Deployability,Upgrade htsjdk to v3.0.0.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7867:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7867,1,['Upgrade'],['Upgrade']
Deployability,Upgrade htsjdk to v3.0.1 and picard to 2.27.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8025:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8025,1,['Upgrade'],['Upgrade']
Deployability,Upgrade log4j to 2.17.0 to mitigate another newly discovered issue,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7615:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7615,1,['Upgrade'],['Upgrade']
Deployability,Upgrade miniconda version installed on docker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5851:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5851,2,"['Upgrade', 'install']","['Upgrade', 'installed']"
Deployability,Upgrade numpy in the conda environment,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6480:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6480,1,['Upgrade'],['Upgrade']
Deployability,Upgrade numpy to version 1.17.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6494:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6494,1,['Upgrade'],['Upgrade']
Deployability,Upgrade picard version and gCNV WDLs after scatter-by-interval is added to IntervalListTools.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5174:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5174,1,['Upgrade'],['Upgrade']
Deployability,Upgrade samtools in the GATK docker image,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8460:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8460,1,['Upgrade'],['Upgrade']
Deployability,Upgrade samtools in the GATK docker image to 1.10 (at least),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7886:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7886,1,['Upgrade'],['Upgrade']
Deployability,Upgrade spark dependency to 1.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1406:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1406,1,['Upgrade'],['Upgrade']
Deployability,"Upgrade testNG to 6.11, use force dependency resolution for testNG",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2617:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2617,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Barclay 1.0.0-17-g30db73c-SNAPSHOT.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2392:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2392,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Barclay 1.2.2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3804:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3804,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Barclay 2.0.0 and Picard 2.17.2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4070:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4070,1,['Upgrade'],['Upgrade']
Deployability,"Upgrade to Barclay 3.0.0, with changes for FeatureInput discovery based on Barclay refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4523:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4523,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Barclay 4.0.1.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6864:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6864,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Barclay 4.0.2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7602:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7602,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Barclay snapshot for tagged arguments.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2388:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2388,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Disq 0.3.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5981:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5981,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to GKL 0.7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3615:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3615,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Gradle 7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7609:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Hadoop-BAM 7.9.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3991:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3991,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Picard 2.18.1.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4620:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4620,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Picard 2.18.16.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5412:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5412,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Picard 2.21.7.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6416:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6416,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Picard 2.23.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6717:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6717,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Spark 2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2220:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2220,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Spark 2.0.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2277:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2277,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Spark 2.4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5782:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5782,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to Spark 2.4.3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5990:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5990,1,['Upgrade'],['Upgrade']
Deployability,"Upgrade to Spark Dataflow 0.2.3, which has scalability and performance",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/657:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/657,1,['Upgrade'],['Upgrade']
Deployability,"Upgrade to Spark Dataflow 0.4.0, which implements Dataflow 0.4.150727.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/785:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/785,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to TestNG 7,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5787:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to a released htsjdk.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3505:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3505,2,"['Upgrade', 'release']","['Upgrade', 'released']"
Deployability,"Upgrade to gkl-0.5.6, and added log4j-1.2-api bridge to dependencies,…",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to gradle 5.4.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6007:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6007,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to htsjdk 2.11.0. Make TargetCodec indexable.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3403:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3403,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to htsjdk 2.5 and hadoop-bam 7.6.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1958:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1958,1,['Upgrade'],['Upgrade']
Deployability,Upgrade to htsjdk2.0.0 and update HadoopReferenceSequenceFileFactory for nio path changes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1243:0,Upgrade,Upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1243,2,"['Upgrade', 'update']","['Upgrade', 'update']"
Deployability,Upgraded Funcotator.wdl to WDL 1.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5589:0,Upgrade,Upgraded,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5589,1,['Upgrade'],['Upgraded']
Deployability,Upgraded Owner to 1.0.10.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4638:0,Upgrade,Upgraded,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4638,1,['Upgrade'],['Upgraded']
Deployability,Upgraded the Funcotator WDL to WDL 1.0 and implemented new features:; * Automatic disk size calculation; * More readable default parameter settings; * Use of WDL 1.0 features such as defaults and true/false operators; * Using GATK to pull the default datasource rather than wget,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5589:0,Upgrade,Upgraded,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5589,1,['Upgrade'],['Upgraded']
Deployability,"Upgrades Barclay to 4.1.0. Adds an `@DeprecatedFeature` annotation that can be applied to any `@DocumentedFeature` or `@Argument`, and updates the handful of such features that are already tagged with the standard Java annotation `@Deprecated` to use the new annotation. Mutually exclusive with `@Beta` and `@Experimental`. The output for `--use-new-qual-calculator`:; <img width=""783"" alt=""Screen Shot 2022-11-21 at 5 04 44 PM"" src=""https://user-images.githubusercontent.com/10062863/203168110-381424c7-e0e2-4f93-b51b-41f74212b669.png"">; <img width=""918"" alt=""Screen Shot 2022-11-21 at 5 14 16 PM"" src=""https://user-images.githubusercontent.com/10062863/203168311-cfc7b1b1-79d8-48c8-b965-766e424135ab.png"">; <img width=""851"" alt=""Screen Shot 2022-11-21 at 6 39 03 PM"" src=""https://user-images.githubusercontent.com/10062863/203179466-b817bba3-312b-4718-a758-9c063f41d755.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8100:0,Upgrade,Upgrades,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8100,2,"['Upgrade', 'update']","['Upgrades', 'updates']"
Deployability,"Upgrades PathSeqScoreSpark to perform abundance score calculations on the executors rather than the driver. This was crashing on inputs with a lot of pathogen reads. . This also required some minor changes to the `PSPathogenTaxonScore` class to be able to keep track of abundance score contributions that come directly from hits to that taxon and those that are from the taxon's descendents. As a result, some of the test output changed when using bitwise, exact checks on the output. So the tests now check for output equivalence, meaning parsing the scores table, checking that all the taxa are the same, and that the scores are equal to within some defined epsilon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3406:0,Upgrade,Upgrades,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3406,1,['Upgrade'],['Upgrades']
Deployability,Upgrading to Gradle 7. - removed all deprecation warnings; - upgraded shadow and download plugins for compatibility; - moved to maven-publish (existing maven plugin is deprecated); - install/uploadArtifacts are now PublishToMavenLocal/publish respectively (due to above move). Caveats. - I was unable to test signing of artifacts fully. I did test it by commenting out the requirement that we only sign release jars published and it did perform the signing. ; - I was unable to test publish to Sonatype as I do not have an account,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7609:61,upgrade,upgraded,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7609,3,"['install', 'release', 'upgrade']","['install', 'release', 'upgraded']"
Deployability,"Upon running Mutect2 using the germline and pon provided by gatk inside a pipeline while running in parallel. If I use an intervals BED file I get the following error:. ```; org.broadinstitute.hellbender.exceptions.GATKException: Error initializing feature reader for path /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getTribbleFeatureReader(FeatureDataSource.java:383); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:335); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:282); 	at org.broadinstitute.hellbender.engine.FeatureManager.addToFeatureSources(FeatureManager.java:246); 	at org.broadinstitute.hellbender.engine.FeatureManager.initializeFeatureSources(FeatureManager.java:209); 	at org.broadinstitute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:156); 	at org.broadinstitute.hellbender.engine.GATKTool.initializeFeatures(GATKTool.java:488); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:709); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.onStartup(AssemblyRegionWalker.java:79); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz (Device or resource busy), for input source: /media/AGROS/hg19/af-only-gnomad.raw.sites.vcf.gz; 	at htsjdk.tribble.TabixFea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7059:74,pipeline,pipeline,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7059,1,['pipeline'],['pipeline']
Deployability,"Use Cromwell's [noAddress](https://support.terra.bio/hc/en-us/community/posts/360060020871-noAddress-true-results-in-stalling-jobs) feature to avoid the unnecessary use of external IPs on Cromwell worker VMs on wide scatters that cause us Google quota problems. For most of the GVS code this only involves adding `noAddress: true` to the existing runtime attributes. In the PGEN code this was slightly more work to change away from `docker: ""ubuntu:22.04""` which is implicitly pulled from Docker Hub. Since `noAddress: true` means the VM can only interact with Google services, we have to switch to a GCR-hosted image as Docker Hub has become unreachable. - [Mostly successful integration test](https://job-manager.dsde-prod.broadinstitute.org/jobs/32b9e2b5-3c56-4bf8-ab5e-66fa72c7cadb), delta some existing issues with cost discrepancies documented in VS-1324.; - [Successful PGEN extract](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/1f19bea2-a9b9-4ec5-b741-e9f64bbfa35a)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8764:677,integrat,integration,677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8764,1,['integrat'],['integration']
Deployability,"Use Java 11.0.11+9 for testing, upgrade hadoop to v3.3.1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8102:32,upgrade,upgrade,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8102,1,['upgrade'],['upgrade']
Deployability,Use Spark to speed up the MarkDuplicates and SortSam steps in the single-sample pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3706:80,pipeline,pipeline,80,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3706,1,['pipeline'],['pipeline']
Deployability,Use conda env create --force rather than conda env update in localDevCondaEnv task.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5776:51,update,update,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5776,1,['update'],['update']
Deployability,Use gvs-internal project in integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7901:28,integrat,integration,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7901,1,['integrat'],['integration']
Deployability,Use the latest GKL 0.8.3 release which includes optimizations for Level 2 compression,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4311:25,release,release,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4311,1,['release'],['release']
Deployability,"User brought up that Broad's google-cloud-sdk is not up to date, so I added a note telling people to upgrade regularly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3663:101,upgrade,upgrade,101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3663,1,['upgrade'],['upgrade']
Deployability,"User has reported default settings in other countries cause tools to error because of the switch between commas and periods. The tools themselves generate the data with the commas that then error downstream tools. User reports a workaround solution of adding `-Duser.language=en -Duser.country=US` to commands but this is cumbersome. . Should our tools should be internationally compatible?. ---; Hi @shlee, @slee,. yes it is working when I replace the commas with points, but since I am using the wdl-pipeline which was provided by slee on gitHub, this is not a sufficient workaround. Due to my expectation that the problem (commas instead of points) arise from my language settings I try different thing to change my actual language on the system. To make the long story short the solution was to start CalculateTargetCoverage with the flags (I added them in the cnv_somatic_tasks.wdl):; ```; java -Duser.language=en -Duser.country=US -Xmx${default=4 mem}g -jar ${gatk_jar} CalculateTargetCoverage; ```; And now the cnv_somatic_panel_workflow.wdl runs smoothly to the end, with points instead of commas :). An other solution would be to running the pipeline in a DockerContainer wich have an english java-version installed, like the one which is provide by the gatk-team. I hope this helps other user too which are working with non-englisch java versions. Greetings EADG. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/40481#Comment_40481",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3297:502,pipeline,pipeline,502,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3297,3,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,User has reported longer runtimes in GATK4 beta3 release compared to GATK4 beta 2 release. It sounds like this is not expected. Her runtimes are below. The first post in the forum thread has her original report. . | Tool | 4.beta.2 | 4.beta.3 |; | ------------------------------------- | -----------:| -----------:|; | BaseRecalibrator | 1m 3s | 3m 3s |; | ApplyBQSR (scattered) | 4m 48s | 11m 51s |; | HaplotypeCaller (scattered) | 23m 42s | 29m 7s |; | GenotypeGVCFs (scattered) | 4m 6s | 9m 28s |; | VariantRecalibrator (for SNPs) | 4m 7s | 6m 38s |; | VariantRecalibrator (for INDELs) | 2m 7s | 4m 8s |; | ApplyVQSR (for SNPs) | 37s | 2m 36s |; | ApplyVQSR (for INDELs) | 39s | 2m 35s |. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/41669#Comment_41669,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3491:49,release,release,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3491,2,['release'],['release']
Deployability,"User has tested GATK3.7 HaplotypeCaller and GATK4 HaplotypeCaller. GATK4 takes ~35 hours while GATK3.7 takes about 18 hours. Original report is here: https://github.com/broadinstitute/gatk/issues/3631 David, I assigned you just so you could take a look. User seems satisfied that GATK4 is faster, but I am just making sure this is expected. I asked for more details on what type of data they are using and whether Spark version is faster (assuming this is from non-Spark version). . ----; User Report; ----. @Sheila,. Hi Sheila,. I repeated the experiment with GATK4.0.0 version. The performance is much better than GATK4beta5 version. Here are the logs: . $ tail -400 NA12892.HaplotypeCaller.err; Using GATK jar /gpfs/software/genomics/GATK/4.0.0/gatk-package-4.0.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -jar /gpfs/software/genomics/GATK/4.0.0/gatk-package-4.0.0.0-local.jar HaplotypeCaller --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4.0.0/NA12892/bam/NA12892.recal.bam --dbsnp /gpfs/data_jrnas1/ref_data/Hsapiens/GRCh37/variation/dbsnp_138.vcf.gz --emit-ref-confidence GVCF --read-validation-stringency LENIENT --native-pair-hmm-threads 32 --output /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4.0.0/NA12892/vcf/NA12892.raw.snps.indels.g.vcf; [January 26, 2018 1:09:58 AM AST] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: **2,133.48 minutes**.; Runtime.totalMemory()=2183659520; real 128010.56; user 436969.62; sys 3030.18. Thanks and Regards,; Naga. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/45634#Comment_45634",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4361:1139,pipeline,pipeline,1139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4361,2,['pipeline'],['pipeline']
Deployability,"User is @wleeidt and they outline their case in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/40530#Comment_40530>. **I can generate plots** with their data and so presumably they are missing some component for the tool to generate plots. Whatever these dependencies, the tool should not emit a `SUCCESS` for the run when plots are absent. User instead gets a `plotting_dump.rda` file. Data is at `/humgen/gsa-scr1/pub/incoming/bugReport_by_wleeidt.updated.zip`.; User's system is; ```; Mac OS X; 10.11.4 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_101-b13; ; ```; GATK Version:; ```; 4.beta.1; ```; Command; ```; gatk-launch PlotSegmentedCopyRatio -TN S4_tumor.pn.tsv -PTN S4_tumor.ptn.tsv -S S4_tumor.seg -O sandbox -SD hg19.dict -pre S4_gatk4_cnv_segment -LOG; ```. Tool could use better error messaging. I will hand this to @LeeTL1220 for appropriate assignment.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3301:466,update,updated,466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3301,1,['update'],['updated']
Deployability,"User provided input files that i tested and one of the AD values did get concatenated but not all AD values greater than 100 were concatenated. . ### User post; ---; I am trying to extract info from a vcf file using the following command and encountered a problem:<br />; ```<br />; gatk VariantsToTable -R $REF -V final_SNP.vcf -F CHROM -F POS -F REF -F ALT -F QUAL -GF AD -GF GQ -GF PL -GF GT -O snpPE_final.tsv<br />; ```<br />; For SNPs with its AD value less than 100, the results are fine, but for SNPs with its AD value greater than 100, VariantsToTable just concatenates the two AD values. Here is an entry in the vcf file:<br />; ```<br />; 1 15880 . G A 3785.46 PASS AC=2;AF=0.500;AN=4;BaseQRankSum=6.325;DP=296;ExcessHet=4.7712;FS=3.153;MLEAC=2;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=12.79;ReadPosRankSum=-1.165;SOR=0.888 GT:AD:DP:GQ:PL 0/1:58,35:93:99:895,0,2296 0/1:98,105:203:99:2900,0,3782<br />; ```<br />; And here is the corresponding row in the tsv file:<br />; ```<br />; CHROM POS REF ALT QUAL S1.AD S1.GQ S1.PL S1.GT S2.AD S2.GQ S2.PL S2.GT<br />; 1 15880 G A 3785.46 58,35 99 895,0,2296 G/A 98105 99 2900,0,3782 G/A<br />; ```<br />; The AD values in the S2.AD column should be 98,105, not 98105. I use GATK4-4.1.2.0-1 and openjdk 1.8.0_152-release on Ubuntu 18.04. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24368/seems-variantstotable-not-properly-handle-ad-greater-than-100/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6115:1265,release,release,1265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6115,1,['release'],['release']
Deployability,"User report:. I am using GATK 4.1.4.0 and noticed the following : if Mutect2 is run with parameter --bam-output it makes a BAM file that contains :; @PG ID:HalpotypeBAMWriter; (note the typo); If then FilterAlignmentArtifacts is run with this file as input and BAM output is asked, it says; java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!; and does not create the file. Command Used:; gatk Mutect2 --input normal.recalibrated.vcf --input tumor.recalibrated.vcf -normal mormal -tumor tumor --reference /data/Homo_sapiens.UCSC.hg38.fa --output tumor.recalibrated.mutect2/vcf --f1r2-tar-gz f1r2.tar.gz --native-pair-hmm-threads 4 --bam-output tumor.recalibrated.realigned.bam --add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:881,patch,patches,881,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['patch'],['patches']
Deployability,"User report:. I test this in 4.1.4.0 and 4.1.4.1. ```shell; gatk CountBasesSpark \; -I input_reads.bam \; -O base_count.txt; ```. When run this cmd, it is OK, and get a right output base_count.txt.; But I want compute bases located in a interval file, so:; ```; gatk CountBasesSpark \; -I input_reads.bam \; -O base_count.txt\; -L interval.file; ```. This cmd cannot run successfully, with some errors I find like this:. ```shell; ......; 9/11/28 17:44:01 INFO NewHadoopRDD: Input split: file:/disks/disk1/data_sample/19NGS14; 2/19NGS142.bam:1476395008+33554432; 19/11/28 17:44:01 INFO NewHadoopRDD: Input split: file:/disks/disk1/data_sample/19NGS14; 2/19NGS142.bam:1509949440+33554432; 19/11/28 17:44:01 INFO NewHadoopRDD: Input split: file:/disks/disk1/data_sample/19NGS14; 2/19NGS142.bam:704643072+33554432; 19/11/28 17:44:02 ERROR Executor: Exception in task 6.0 in stage 1.0 (TID 7); java.util.NoSuchElementException: next on empty iterator; at scala.collection.Iterator$$anon$2.next(Iterator.scala:39); at scala.collection.Iterator$$anon$2.next(Iterator.scala:37); at scala.collection.Iterator$$anon$13.next(Iterator.scala:469); ......; ```; The interval.file is fine because I use it for the whole GATK pipeline. ; The CountReadsSpark has the same error. Please check this. Thanks.; Chris. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24645/countbasesspark-doesnt-work-with-l-opt/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6319:1211,pipeline,pipeline,1211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6319,1,['pipeline'],['pipeline']
Deployability,"User reporting major time differences between 3.7 HaplotypeCaller and GATK4 latest beta release. . ----; User Report; ----. Dear team,. I am using GATK 4 Beta2 for testing HaplotypeCaller for our NGS workflow. . The command which I used is: . _time -p /gpfs/software/genomics/GATK/4b.2/gatk/gatk-launch HaplotypeCaller \ ; --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa \; --input NA12892.recal.bam \ ; --dbsnp /gpfs/data_jrnas1/ref_data/Hsapiens/GRCh37/variation/dbsnp_138.vcf.gz \ ; --emitRefConfidence GVCF \; --readValidationStringency LENIENT \ ; --nativePairHmmThreads 32 \; --createOutputVariantIndex true \; --output NA12892.raw.snps.indels.g.vcf_. **This execution time for GATK 4 Beta2 is: 51 Hours, 32 min**. Alternatively, I was running the same sample (NA12892) using GATK 3.7 using the following command: . _time -p java -XX:+UseParallelGC -XX:ParallelGCThreads=32 -Xmx128g \; -jar /gpfs/software/genomics/GATK/3.7/base/GenomeAnalysisTK.jar -T HaplotypeCaller \; -nct 8 -pairHMM VECTOR_LOGLESS_CACHING \ ; -R /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa \; -I NA12892.realigned.recal.bam -\ ; -emitRefConfidence GVCF \; --variant_index_type LINEAR \; --variant_index_parameter 128000 \; --dbsnp /gpfs/data_jrnas1/ref_data/Hsapiens/GRCh37/variation/dbsnp_138.vcf.gz \; -o NA12892.raw.snps.indels.g.vcf _. **This execution time for GATK 3.7 is: 18 Hours, 12 min**. I don't know, how to use multithreads (e.g. -nct) for GATK 4 version to reduce the execution time on the single node. Because, we have 32 cores per node with 512GB memory available for benchmarking. To parallelize the GATK 4 workload, I used the Spark version also. . I used **GATK 4 Beta2 Spark job on the cluster of 32 nodes** (32 nodes x 32 cores, totaling 1024 cores). The execution time is almost same as GATK 4 Beta2 ( 50 Hours, 21 min). Please help me, how to reduce the execution time for GATK 4 Beta2 HaplotypeCaller? . Please see this below Spark logs:. + /gpfs/software/spark/spark-2.1.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631:88,release,release,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631,1,['release'],['release']
Deployability,"User says beta 2 is much faster than release. . ----; User Report; ----; Hi @Geraldine_VdAuwera , thanks for the quick response!. We used CCLE WES FASTQs for the purpose of testing (6GB per paired end). We scattered the tool using a custom BED file with 86 intervals. 4.0.0.0 failed due to memory with 2GB per job. It completed with 4GB per job. Each job lasted approximately 1h 10 min. 4.beta.2 completed with 2GB per job and each job lasted approximately 5m. This is in gVCF mode, with the same settings as described above. All input files, settings and hardware are the same between versions. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/44970#Comment_44970",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4169:37,release,release,37,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4169,1,['release'],['release']
Deployability,"User would like to know if we have guidelines to provide. It would be nice to have a timeframe to tell our users or some generic guidelines in setting parameters. . ---; I find really interesting the Flagstat [chart](https://software.broadinstitute.org/gatk/resources/img_tutorials/tutorial_10060_figures/wes_increase_executors_chart.png ""chart"") and the relative [table](https://software.broadinstitute.org/gatk/resources/img_tutorials/tutorial_10060_figures/wes_increase_executors_table.png ""table""), it lets me understand that 7 is the most efficient executors-number for this tool. It's the same even for other tools? Or is there something similar (charts) for Pipelines like [BwaAndMarkDuplicatesPipelineSpark](https://software.broadinstitute.org/gatk/gatkdocs/4.beta.2/org_broadinstitute_hellbender_tools_spark_pipelines_BwaAndMarkDuplicatesPipelineSpark.php ""BwaAndMarkDuplicatesPipelineSpark""), [BQSRPipelineSpark](https://software.broadinstitute.org/gatk/gatkdocs/4.beta.3/org_broadinstitute_hellbender_tools_spark_pipelines_BQSRPipelineSpark.php ""BQSRPipelineSpark""), HaplotypeCallerSpark and [ReadsPipelineSpark](https://software.broadinstitute.org/gatk/gatkdocs/4.beta.5/org_broadinstitute_hellbender_tools_spark_pipelines_ReadsPipelineSpark.php ""ReadsPipelineSpark"") ?; And then, the ```--driver-memory``` is an important parameter? Which should be his value?. I'm waiting for a your kind answer,; Nicholas. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/43894#Comment_43894",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3822:665,Pipeline,Pipelines,665,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3822,1,['Pipeline'],['Pipelines']
Deployability,Using adaptive pruning in mitochondria pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5669:39,pipeline,pipeline,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669,1,['pipeline'],['pipeline']
Deployability,"Using https://jitpack.io/#broadinstitute/gatk for building SNAPSHOTS (independent on your jfrog repository, which has a expired date for snapshots - https://github.com/broadinstitute/gatk/issues/4565) is not working any longer due to the requirement of git-lfs (e.g., https://jitpack.io/com/github/broadinstitute/gatk/4.0.4.0/build.log). It will be nice to add a `jitpack.yml` file (see https://jitpack.io/docs/BUILDING/#custom-commands) to install dependencies - this will be useful for downstream project depending on SNAPSHOTS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4819:441,install,install,441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4819,1,['install'],['install']
Deployability,VAT Readme updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8090:11,update,updates,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8090,1,['update'],['updates']
Deployability,"VCFCommandLine true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false --minimumMappingQuality 20; [August 24, 2017 7:26:43 PM EDT] Executing as shlee@WMCF9-CB5 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_111-b14; Version: 4.beta.3; 19:26:43.243 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 19:26:43.243 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:26:43.243 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:26:43.243 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:26:43.243 INFO Mutect2 - Deflater: IntelDeflater; 19:26:43.243 INFO Mutect2 - Inflater: IntelInflater; 19:26:43.243 INFO Mutect2 - GCS max retries/reopens: 20; 19:26:43.243 INFO Mutect2 - Using google-cloud-java patch 317951be3c2e898e3916a4b1abf5a9c220d84df8; 19:26:43.243 INFO Mutect2 - Initializing engine; 19:26:43.871 INFO Mutect2 - Done initializing engine; 19:26:44.130 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 19:26:44.346 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 19:26:44.624 WARN NativeLibraryLoader - Unable to find native library: native/libgkl_pairhmm_omp.dylib; 19:26:44.624 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; 19:26:44.643 INFO NativeLibraryLoader - Loading libgkl_pairhmm.dylib from jar:file:/Applications/genomicstools/gatk/gatk-4.latest/gatk-package-4.beta.3-local.jar!/com/intel/gkl/native/libgkl_pairhmm.dylib; [WARNING] Ignoring request for 4 threads; not using OpenMP implementation; 19:26:44.664 INFO PairHMM - Using the AVX-accelerated native PairHMM implementation; 19:26:44.7",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3514:5713,patch,patch,5713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3514,1,['patch'],['patch']
Deployability,VS 1102 add vds creation wdl to integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8554:32,integrat,integration,32,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8554,1,['integrat'],['integration']
Deployability,VS 1171 Add an optional Hail whl to the integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8624:40,integrat,integration,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8624,1,['integrat'],['integration']
Deployability,VS-1001 update dockers for google storage library update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8401:8,update,update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8401,2,['update'],['update']
Deployability,VS-1003. Update documentation to point to newly released BGE exome calling list.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8407:9,Update,Update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8407,2,"['Update', 'release']","['Update', 'released']"
Deployability,VS-1090 - Update AoU Documentation for GvsImportGenomes to reflect new behavior,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8571:10,Update,Update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8571,1,['Update'],['Update']
Deployability,"VS-1092 - Fix for GvsCreateFilterSet OOD; Integration test ran [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/f2dc8eca-1fc2-4ab3-ac38-ab430fb1d60a).; One failure - in the Exome test on AssertCostIsTrackedAndExpected. Doesn't seem related to this code change, so am going to allow it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8573:42,Integrat,Integration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8573,1,['Integrat'],['Integration']
Deployability,VS-1113 Update VQSR and VETS naming,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8948:8,Update,Update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8948,1,['Update'],['Update']
Deployability,VS-1163 - Update changelog prior to doing release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8641:10,Update,Update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8641,2,"['Update', 'release']","['Update', 'release']"
Deployability,VS-1190 - Update GvsCreateVATfromVDS.wdl to use new Nirvana reference.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8755:10,Update,Update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8755,1,['Update'],['Update']
Deployability,VS-1211. Update usage documentation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8704:9,Update,Update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8704,1,['Update'],['Update']
Deployability,VS-1288 Updates to ah_var_store from echo callset.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8991:8,Update,Updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8991,1,['Update'],['Updates']
Deployability,VS-1291 fix integration test in ah var store,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8736:12,integrat,integration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8736,1,['integrat'],['integration']
Deployability,VS-1310. Update gatk docker on EchoCallset,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8803:9,Update,Update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8803,1,['Update'],['Update']
Deployability,VS-1324 Investigate integration test failures,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8787:20,integrat,integration,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8787,1,['integrat'],['integration']
Deployability,VS-1356 - Update docker from master,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8801:10,Update,Update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8801,1,['Update'],['Update']
Deployability,VS-1397 Update Tests - ah_var_store edition,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8846:8,Update,Update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8846,1,['Update'],['Update']
Deployability,VS-1397 update tests echo callset version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8848:8,update,update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8848,1,['update'],['update']
Deployability,VS-1433.; This PR adds the tool vcf-validator to our variants docker and uses it in our integration test.; It validates that the VCFs have no errors in the `AD` field (which were previously reported by AoU friends).; It also modifies the Beta integration test to only run on WGS samples (previously ran on all samples). Passing Integration test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/0c9fb830-7831-4bee-a82c-d0146b250e59).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8903:88,integrat,integration,88,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8903,3,"['Integrat', 'integrat']","['Integration', 'integration']"
Deployability,VS-1476. Removing All usage of VQSR from VDS creation. Only will accept VETS annotated data now. Passing Integration Test (small chrs) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/fa745d55-38d6-4899-a75d-474081eac013); Passing Integration Test (all chrs) [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/bb04d1a6-7cf6-45cb-b17f-5e934ab42631),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9015:105,Integrat,Integration,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9015,2,['Integrat'],['Integration']
Deployability,VS-1483 Update AoU documentation to include details for VAT delivery,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8967:8,Update,Update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8967,1,['Update'],['Update']
Deployability,VS-598 - Minor update to AoU Documentation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7994:15,update,update,15,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7994,1,['update'],['update']
Deployability,VS-695. Updates to run Precision and Sensitivity on VQSR Lite,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8230:8,Update,Updates,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8230,1,['Update'],['Updates']
Deployability,VS-776. Update to latest version of VQSR Lite.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8269:8,Update,Update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8269,1,['Update'],['Update']
Deployability,VS-857.; Change default behavior of GvsBulkIngestGenomes.wdl BACK to not dropping GQ0 ref blocks.; Updated AoU documentation to say we need to do it there (as an input). Passing Integration Test [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/c4f921d8-52e7-4a44-9e0b-9f876eac71f3),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8550:99,Update,Updated,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8550,2,"['Integrat', 'Update']","['Integration', 'Updated']"
Deployability,VS-914 Add support for vqsr lite to integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8324:36,integrat,integration,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8324,1,['integrat'],['integration']
Deployability,VS-917. Update override jar in GvsJointCalling wdl to fix support issue.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8312:8,Update,Update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8312,1,['Update'],['Update']
Deployability,VS-924 - Have pipeline automatically differentiate between VQSR Lite and Classic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8336:14,pipeline,pipeline,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8336,1,['pipeline'],['pipeline']
Deployability,VS-931 Exome integration test on bulk ingest staging,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8448:13,integrat,integration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8448,1,['integrat'],['integration']
Deployability,VS-931 exome integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8433:13,integrat,integration,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8433,1,['integrat'],['integration']
Deployability,VS_492 - Beta User Jar release,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7934:23,release,release,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7934,1,['release'],['release']
Deployability,"ValidateVariants give an `IllegalArgumentException` if a reference isn't provided. . It should be a `UserException`. I don't know but I think there may be modes that don't require the reference, so it may need to give a smart error message. ```; gatk-launch ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; Using GATK wrapper script /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk; Running:; /Users/louisb/Workspace/gatk/build/install/gatk/bin/gatk ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf; 17:43:53.119 INFO NativeLibraryLoader - Loading libgkl_compression.dylib from jar:file:/Users/louisb/Workspace/gatk/build/install/gatk/lib/gkl-0.4.1.jar!/com/intel/gkl/native/libgkl_compression.dylib; [March 21, 2017 5:43:53 PM EDT] org.broadinstitute.hellbender.tools.walkers.variantutils.ValidateVariants --variant src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleGood.vcf --doNotValidateFilteredRecords false --warnOnErrors false --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --cloudPrefetchBuffer 40 --cloudIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [March 21, 2017 5:43:53 PM EDT] Executing as louisb@WMD2A-31E on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_112-b16; Version: Version:4.alpha.2-189-g724fbd0-SNAPSHOT; 17:43:53.162 INFO ValidateVariants - Def",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2509:453,install,install,453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2509,3,['install'],['install']
Deployability,VariantFiltration update for new htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/672:18,update,update,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/672,1,['update'],['update']
Deployability,VariantRecalibrator R-script fails if `scales` v1.3.0 is installed,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664:57,install,installed,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664,1,['install'],['installed']
Deployability,VariantRecalibrator and ApplyVQSR integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2164:34,integrat,integration,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2164,1,['integrat'],['integration']
Deployability,VariantRecalibrator and ApplyVQSR port part 1 (no integration tests).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2094:50,integrat,integration,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2094,1,['integrat'],['integration']
Deployability,"VariantWalker was the only remaining Walker type that didn't support full traversal by; intervals using -L/-XL due to lack of underlying FeatureDataSource support for traversal; by intervals (it only supported individual queries by one interval at a time). This; commit updates FeatureDataSource to allow traversal across all Features overlapping a set; of intervals, and enables -L support in VariantWalker. -Added support for full traversal by a set of intervals to FeatureDataSource; (+ tests). -Enabled -L/-XL support in VariantWalker. Resolves #232",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/326:270,update,updates,270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/326,1,['update'],['updates']
Deployability,"Variants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/our/.fa \; -L /path/to/our/.interval_list \; -gvcf \; --validation-type-to-exclude ALLELES \; --dbsnp /path/to/our/.vcf.gz \; --no-overlaps; ```. #### Expected behavior. The error message should report the _first_ overlapping interval. #### Actual behavior; The error message is reporting the _last_ overlapping interval.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:1897,release,release,1897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,1,['release'],['release']
Deployability,Vat pipeline--Even more juice,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8794:4,pipeline,pipeline--Even,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8794,1,['pipeline'],['pipeline--Even']
Deployability,"Version 3.0.0 of the base image is still on 18.04 (albeit an updated version of 18.04). Although 18.04 is still supported, we should try to move to the latest LTS release, 22.04",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8243:61,update,updated,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8243,2,"['release', 'update']","['release', 'updated']"
Deployability,"Version: 2.23.0; 12:57:16.776 INFO AnalyzeCovariates - Picard Version: 2.22.8; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:57:16.776 INFO AnalyzeCovariates - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:57:16.776 INFO AnalyzeCovariates - Deflater: IntelDeflater; 12:57:16.776 INFO AnalyzeCovariates - Inflater: IntelInflater; 12:57:16.776 INFO AnalyzeCovariates - GCS max retries/reopens: 20; 12:57:16.776 INFO AnalyzeCovariates - Requester pays: disabled; 12:57:16.776 INFO AnalyzeCovariates - Initializing engine; 12:57:16.776 INFO AnalyzeCovariates - Done initializing engine; 12:57:17.333 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates17353441228865531235.csv'; 12:57:17.414 INFO AnalyzeCovariates - Generating plots file '/home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf'; 12:57:17.829 INFO AnalyzeCovariates - Shutting down engine; [December 17, 2020 at 12:57:17 PM TRT] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=633339904; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10272183847736955081';source('/tmp/BQSR.16251220439562120273.R'); /tmp/AnalyzeCovariates17353441228865531235.csv /home/detagen/Desktop/pipeline/playground/BACKUP/FMF-248_Backup/before.recal.FMF-248.table /home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Stdout: ; Stderr: Error in library(gplots) : there is no package called ‘gplots’; Calls: source -> withVisible -> eval -> eval -> library; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:3357,pipeline,pipeline,3357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['pipeline'],['pipeline']
Deployability,"Versions of Kryo less than 3.0 can't serialize Java 8 lambda functions, forcing us in some cases to replace lambdas with full function class objects even when a lambda would be simpler and easier to read. See for example https://github.com/broadinstitute/gatk/pull/1489 . This ticket is to look through the codebase after an upgrade to Kryo 3.0+ takes place and to replace unnecessary function classes with lambdas where they'd be more appropriate.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1510:325,upgrade,upgrade,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1510,1,['upgrade'],['upgrade']
Deployability,Very simple implementation of #2297 using a custom `GATKConf` class to allow both promatically (`GATKConfBuilder`) and by commons-configuration API (constructor). Only includes:. * Packages/Classes to include in the CLP on startup.; * Packages to look for codecs on startup.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2322:130,configurat,configuration,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2322,1,['configurat'],['configuration']
Deployability,Very simple patch to extract arguments that may be useful for other toolkits into `StandardArgumentDefinitions` and some in-class changes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2283:12,patch,patch,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2283,1,['patch'],['patch']
Deployability,Vs 299 spike speed up integration tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8706:22,integrat,integration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8706,1,['integrat'],['integration']
Deployability,WDL for running PathSeq pipeline with a readme and example json input.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4143:24,pipeline,pipeline,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4143,1,['pipeline'],['pipeline']
Deployability,"We are currently using numpy 1.13.1, but that version is old and has incompatibilities with gcc on some platforms (see [this forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360056743551-Problem-Installing-GATK-python-environment-SOLUTION-POSTED-?page=1#community_comment_360009536172)).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6480:215,Install,Installing-GATK-python-environment-SOLUTION-POSTED,215,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6480,1,['Install'],['Installing-GATK-python-environment-SOLUTION-POSTED']
Deployability,We are trying to replace travis CI and we must test that the push tests work as expected. This will eventually require we rebase everything. . ; We should get rid of/update our old travis tickets,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7754:166,update,update,166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7754,1,['update'],['update']
Deployability,We are two major versions behind the latest gradle release. A later version is needed for upgrading Spark (#5990) and using BigQuery (#5928).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6007:51,release,release,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6007,1,['release'],['release']
Deployability,"We are unable to run joint genotyping on the cloud when it includes a sample with a space in the name; it fails when calling GATK's GenomicsDBImport functionality (see error log below). . This practice (samples with spaces in the name) is unfortunately not uncommon, so we've had to modify our various workflows one by one to add support for this case. Please update GenomicsDBImport to handle samples with a space in the name. . Example error log: ; gsutil cat gs://broad-jg-dev-cromwell-execution/JointGenotyping/6918095f-ca06-4883-bcb5-f5c2e343bb6d/call-ImportGVCFs/shard-0/ImportGVCFs-0-stderr.log. Using GATK jar /usr/gitc/gatk-package-4.beta.6-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Xmx4g -Xms4g -jar /usr/gitc/gatk-package-4.beta.6-local.jar GenomicsDBImport --genomicsDBWorkspace genomicsdb --batchSize 50 -L chr1:1-391754 --sampleNameMap /cromwell_root/broad-jg-dev-storage/freimer_dutch_fin_wgs_v1/v1/sample_map --readerThreads 5 -ip 500; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.H9t5pC; [December 14, 2017 7:41:30 PM UTC] GenomicsDBImport --genomicsDBWorkspace genomicsdb --batchSize 50 --sampleNameMap /cromwell_root/broad-jg-dev-storage/freimer_dutch_fin_wgs_v1/v1/sample_map --readerThreads 5 --intervals chr1:1-391754 --interval_padding 500 --genomicsDBSegmentSize 1048576 --genomicsDBVCFBufferSize 16384 --overwriteExistingGenomicsDBWorkspace false --consolidate false --validateSampleNameMap false --interval_set_rule UNION --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 0 -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3979:360,update,update,360,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3979,1,['update'],['update']
Deployability,"We currently install v4.3.3, which is quite a few releases behind the current version v4.6.8. The preferred method for activating/deactivating has changed, and the method we currently suggest/use (`source activate`) is [deprecated](https://github.com/conda/conda/releases/tag/4.4.0) and has been replaced with conda commands. The readme/online doc should be modified to reflect these changes as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5851:13,install,install,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5851,3,"['install', 'release']","['install', 'releases']"
Deployability,"We don't want users to have to prepend ""file://"" to their input/output files. Let's create a class that wraps URI, and prepends ""file://"" by default when there's no explicit prefix. It should have a constructor that takes a single String to integrate with our argument parsing system.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/477:241,integrat,integrate,241,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/477,1,['integrat'],['integrate']
Deployability,We forgot to update this when travis migrated. We're seeing old output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5617:13,update,update,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5617,1,['update'],['update']
Deployability,"We found some links that needed to be fixed in the Funcotator, ASEReadCounter, and VariantRecalibrator tool docs. We updated them or removed the links as needed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7270:117,update,updated,117,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7270,1,['update'],['updated']
Deployability,"We found that many GATK4 commands accept an option to let them output ""sharded"" files. But we didn't find how those commands accept ""sharded"" data that generated from the last step. For example,. ```; gatk ReadsPipelineSpark \; -I hdfs://ip-/user/tianj/${sample}_sort.bam \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -O hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; --knownSites hdfs://ip-/genome/ref/Mills_and_1000G_gold_standard.indels.b37.vcf \; --shardedOutput true \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip- \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp"". HaplotypeCallerSpark \; -I hdfs://ip-/user/tianj/${sample}_ReadsPipelineSpark.bam \; -O hdfs://ip-/user/tianj/$sample.vcf \; -R hdfs://ip-/genome/ref/human_g1k_v37.2bit \; -- \; --sparkRunner SPARK \; --sparkMaster spark://ip-172-31-2-45:7077 \; --num-executors 2 \; --executor-cores 16 \; --executor-memory 60g \; --driver-memory 60g \; --conf ""spark.eventLog.dir=hdfs://ip-/user/tianj/tmp"" \; --conf ""spark.local.dir=hdfs://ip-/user/tianj/tmp""; ```; This pipeline makes ${sample}_ReadsPipelineSpark.bam a directory and it doesn't work. I didn't find any option to specify that the input file is ""sharded"" or not.**How should we use the ""shardedoutput"" option?**. And also, we noticed that for SV calling, there is a whole pipeline command, but for SNP&Indels calling, we only found partial pipeline such as ReadPipelineSpark. **Is there a whole SNP&Indels calling pipeline script we can use?** Though it still seems to cost some unnecessary time keeps writing and reading files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3141:1206,pipeline,pipeline,1206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3141,4,['pipeline'],['pipeline']
Deployability,"We got an email complaining that we were the only users in the world still using the beta API. This updates to the most recent version, they changed the api from `List<String> -> List<Object>`, but the documentation still says it has to be a `String`, so I've just added some explicit casts. . Fix for #2009",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2011:100,update,updates,100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2011,1,['update'],['updates']
Deployability,"We had a little bit of trouble with AllelicCNV at the Finland workshop last week. Apologies that this isn't the most complete bug report, but the hands-on portion of the workshop moved pretty fast. Soo Hee took a picture of the error with her phone:; ![image](https://user-images.githubusercontent.com/6578548/30697898-6dff7e24-9eae-11e7-8ec3-d876483fec1a.png); The rest of the relevant line is ""undefined symbol: cblas_daxpy"". The version was the GATK4 beta 4 release and the command was:; ```; gatk-launch AllelicCNV \; --tumorHets tumor_hets.tsv \; --tangentNormalized tumor_C.tn.tsv \; --segments tumor_C.seg \; --outputPrefix acnv \; --intervalThresholdCopyRatio 5.0; ```; The inputs are in the AllelicCNV workshop bundle in Google Drive: https://drive.google.com/drive/folders/0BzI1CyccGsZiU1dkcndQMkRmTTQ. I think the host institution was running Red Hat, but it might have been Ubuntu. Like I said, sorry this is a pretty sad bug report. I haven't tried to reproduce the error since it seems platform-specific, but maybe some weirdo who doesn't use a Mac (@LeeTL1220) would give it a shot?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3599:461,release,release,461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3599,1,['release'],['release']
Deployability,"We had another edge-case bug in our clipping code: when calling ReadClipper.revertSoftClippedBases(); on a read at the start of a contig (position == 1), we could end up with an empty read if the cigar; was something like ""41S59H"", since the reverted soft-clipped bases would just get hard-clipped away.; The method did not correctly handle this case, and blew up with an exception. Added a unit test for ReadClipper that fails before the fix and passes after it, as well as an; integration test for HaplotypeCaller that also fails before the fix and passes after it. Resolves #3845",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4203:479,integrat,integration,479,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4203,1,['integrat'],['integration']
Deployability,We have a lot of links to the old forum in our source code and in code documentation. All the ones I've tested are redirecting to the new locations correctly but it would be good to test them and find any dead links. . There are constants in HelpConstants that should be updated as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6382:271,update,updated,271,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6382,1,['update'],['updated']
Deployability,We have already updated those GATK3 tests to point to a newer version output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7634:16,update,updated,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7634,1,['update'],['updated']
Deployability,We have received a request to incorporate some of the picard tool improvements in newer versions of picard. This is a bump in preparation of an eventual GATK release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7255:158,release,release,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7255,1,['release'],['release']
Deployability,"We have two methods that compare variants for our tests:. `VariantContextTestUtils::assertEqualVariants` is doing string conversions to compare variants. This is not so good. `VariantContextTestUtils::assertVariantContextsAreEqual` may not do a complete comparison. Either way, these should be combined into a single method and updated to be a complete comparison.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5572:328,update,updated,328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5572,1,['update'],['updated']
Deployability,"We haven't resolved the issue with our project configuration yet, so we need to publish this ourselves.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4008:47,configurat,configuration,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4008,1,['configurat'],['configuration']
Deployability,"We need a mechanism to guarantee that all spark tools are written in a way that allows them to be composed into larger pipelines. Currently, we rely on the tool author to extract the core of their tool into a separate method or class that can be called externally, but there is nothing forcing tool authors to do this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/960:119,pipeline,pipelines,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/960,1,['pipeline'],['pipelines']
Deployability,"We need a way to install GATK Python modules onto the docker image, from repo source, in a way that doesn't assume a repo clone is present on the docker image (there currently is one, but we want to remove it to recover space), and that also doesn't make the conda environment dependent on a repo clone. This PR adds a build task that creates a zip archive of the GATK Python source; propagates that to the docker image, and then pip installs the contents of the archive into the conda environment on the docker. Since we don't have an actual python module in the repo at the moment, there is a second, temporary, commit that contains a dummy python module used only to trigger and test that the installation works.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3964:17,install,install,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3964,3,['install'],"['install', 'installation', 'installs']"
Deployability,We need to produce a script that will make it easy to evaluate what changes to the HalpotypeCallerSpark will result in the biggest performance impact. To that end we want to write wdls and associated scripts that will make it easier for us to evaluate what each incremental change to the tool will change about accuracy and runtime for the machine configurations we care about. We should probably also hammer down what the machine types we consider important are as well.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5396:348,configurat,configurations,348,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5396,1,['configurat'],['configurations']
Deployability,"We noticed a strong drop in precision for SNVs (~10% for tumor-normal mode, ~20% in tumor-only mode) between releases 4.1.7.0 and 4.2.6.0.; With more testing using the HCC1395 somatic benchmark (https://pubmed.ncbi.nlm.nih.gov/34504347/) and sequencing data provided by the Somatic Mutation Working Group (Fudan university WES tumor-normal data set, 2 x 100x coverage), the drop in performance can be traced to changes between 4.1.8.1 and 4.1.9.0. Here are the performance metrics for selected gatk releases: . ![FD_TN_4170_filter_FD_TN_4181_filter_FD_TN_4190_filter_FD_TN_4200_filter_FD_TN_4260_filter](https://user-images.githubusercontent.com/15612230/176261673-e13b9ada-0462-4cd4-b645-67459895363b.png). The calling was done with essentially default parameters:; `; tools/gatk-${version}/gatk Mutect2 --normal-sample WES_FD_N --output $outvcf --intervals $wesbed --interval-padding 0 --input $inbam_t --input $inbam_n --reference $ref ; `. ` ; tools/gatk-${version}/gatk FilterMutectCalls --output ${outvcf%.vcf}_filtered.vcf --variant $outvcf --intervals $wesbed --reference $ref --stats ${outvcf}.stats --threshold-strategy OPTIMAL_F_SCORE --f-score-beta 1.0 ; `. som.py was used for calculating performance metrics. Curiously, we do not observe a such a substantial drop in precision in WGS data, neither in tumor-only nor in tumor-normal mode.; In the foillowing, our ""v04"" corresponds to gatk 4.1.7.0 and out ""v05"" corresponds to gatk 4.2.6.0:. Tumor-normal:. ![WGS_FD_tumor-normal_reference_workflow_v04_WGS_FD_tumor-normal_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176270981-2e56bb2e-c3a6-4715-bcce-33cbe0d0cf67.png). Tumor-only:. ![WGS_FD_tumor_reference_workflow_v04_WGS_FD_tumor_reference_workflow_v05](https://user-images.githubusercontent.com/15612230/176271103-6863a6f7-26f6-4841-b066-d963221ff735.png). In my opinion, the small gains in recall between 4.1.8.1 and 4.1.9.0. do not justify the drop in precision. This and the fact that WES data is affe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7921:109,release,releases,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7921,2,['release'],['releases']
Deployability,We now want to fully update the AoU documentation so that a VDS can be created (no VCFs!),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8169:21,update,update,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8169,1,['update'],['update']
Deployability,"We recently created the ""variantcalling"" task in the travis CI test suite to reduce the runtime of our integration tests. Once we refactored the docker image we found that the the integration tests are still taking an uncomfortable amount of time to run (upwards of 57 minutes). Short of resolving the issue more permanently (#4989) we can temporarily fix the solution by just splitting off more of the integration tests to other jobs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4990:103,integrat,integration,103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4990,3,['integrat'],['integration']
Deployability,"We recently determined that the FTZ setting gets cleared during integration tests for unknown reasons. We temporarily fixed this by explicitly turning FTZ on in every call to `jniComputeLikelihoods()` (https://github.com/broadinstitute/gatk/pull/1764), but this might be inefficient, and even if it isn't it would be good to understand what's going on. Without the fix in https://github.com/broadinstitute/gatk/pull/1764, if you run `HaplotypeCallerIntegrationTest`, the ""consistent with past results"" tests will either succeed or fail depending on whether they run first or not, and the failure is definitely due to FTZ somehow getting unset between tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1771:64,integrat,integration,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1771,1,['integrat'],['integration']
Deployability,"We recently updated (PR #4858) the default Smith-Waterman parameters for realigning reads to their best haplotype. Although there is no reason for the alignment of haplotypes to the reference to use the same parameters, it seems like we are similarly favoring indels too much. There is a forum discussion to this effect:. https://gatkforums.broadinstitute.org/gatk/discussion/23230/gatk-haplotypecaller-mnp-output-problem. Here are the parameters we use:. * match: 200; * substitution: -150; * indel start: -260; * indel extend: -11. These parameters, which are essentially a prior on biological variation, prefer an indel, with a cost of 260, to a SNP, with a cost of 350. This does not seem correct. It almost never comes up because the correct alignment is usually unambiguous, but when it does, shouldn't we break the tie in favor of the SNP?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5564:12,update,updated,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5564,1,['update'],['updated']
Deployability,"We ship an Intel conda environment that uses the Intel optimized tensorflow implementation for those with AVX-enabled hardware, but the environment requires AVX, so there are no tests since we can't assume we have AVX hardware on Travis. We need to provision some kind of continuous test that ensures that the CNN tools continue to work in this environment as we upgrade our dependencies for things like tensorflow, keras, etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5255:272,continuous,continuous,272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5255,2,"['continuous', 'upgrade']","['continuous', 'upgrade']"
Deployability,"We should add a runtime check, probably at the ScriptExecutor level, to verify that the version of the python package we're running matches the version of GATK. Otherwise subtle failure modes could ensue when a user upgrades GATK but does not re-establish the conda env.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4995:216,upgrade,upgrades,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4995,1,['upgrade'],['upgrades']
Deployability,"We should audit the plugin system (and add tests) to ensure that crazy combinations of enable/disable arguments (like `--readFilter` and `--disableReadFilter`) are disallowed, while useful combinations are permitted. Here's my attempt at an initial proposal:. `--enable X --disable X`: crazy, should be an error. `--enable X --enable X`: error. `--disable X --disable X`: error. `--enable X when X is already on by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be on, even if tool defaults change over time. We should make sure that the filter is only actually applied ONCE, however. `--disable X when X is not enabled by default in the tool`: warning, but should be allowed -- this is useful for pipeline authors to guarantee that a particular filter will be off, even if tool defaults change over time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2377:490,pipeline,pipeline,490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2377,2,['pipeline'],['pipeline']
Deployability,"We should check if we can use their copy of google-cloud-sdk instead of installing our own. We added our own installation a long time ago, but maybe the version they use is now up to date enough to not have to install it ourself.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3954:72,install,installing,72,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3954,3,['install'],"['install', 'installation', 'installing']"
Deployability,"We should figure out how to fail the travis build if the before_install or install blocks fail. We have confusing test failures when earlier stages fail, it would be better to error early.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3444:75,install,install,75,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3444,1,['install'],['install']
Deployability,"We should have an installer script checked in to the repo for downloading the Funcotator datasources. Ideally the script would have a trivial Java frontend in the form of a simple GATK tool, to make it more discoverable by users.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4549:18,install,installer,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4549,1,['install'],['installer']
Deployability,We should install the `python-is-python3` package during our docker build to create a symlink from `/usr/bin/python` to `/usr/bin/python3`. This would help avoid problems such as https://github.com/broadinstitute/gatk/issues/8402,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8497:10,install,install,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8497,1,['install'],['install']
Deployability,"We should provide a way for tools to generate standardized VCF header metadata lines. Most tools start with the lines from the input header, but after that there is a variation since each tool has custom code. We should rationalize how these are created/updated, for at least:; - ## contig: Some tools use the ones from the input directly, even if a specific reference is provided on the command line, but some update them based on the reference. Since we don't require a reference, but will accept one, we should be consistent about whether we keep or destroy the old attributes in each case, and whether we use an assembly attribute, URL attribute, etc.; - ## reference: same as above; - ## source: it seems like we should always add/update this. This is all somewhat related to https://github.com/broadinstitute/gatk/issues/1116.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2112:254,update,updated,254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2112,3,['update'],"['update', 'updated']"
Deployability,We should revert the change in https://github.com/broadinstitute/gatk/pull/3766 to retry 403s on GCS as soon as Google patches its service to return the correct status code (503) instead of 403. We can do this via a straight-up:; `git revert e11d7b9410e0fcecf2d98c5ac55ce05fad21ee17`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3800:119,patch,patches,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3800,1,['patch'],['patches']
Deployability,"We should run all of our dataflow tests on spark local runner as well as the dataflow direct pipeline runner. The plan is to run these as ""optional"" tests so that spark failure doesn't block our builds but we will be aware of any differences between the spark and google implementations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/551:93,pipeline,pipeline,93,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/551,1,['pipeline'],['pipeline']
Deployability,We should try to keep on top of the latest spark version while we're still not released yet. . When we upgrade we should verify that #2545 is resolved.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2555:79,release,released,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2555,2,"['release', 'upgrade']","['released', 'upgrade']"
Deployability,"We should update all command lines in the docs to use `gatk-launch`. Running with `java -jar` and bypassing `gatk-launch` causes several important system properties to not get set, including htsjdk compression level.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3039:10,update,update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3039,1,['update'],['update']
Deployability,"We should update any public docs that contain recommendations on how to write JEXL expressions to reflect the align with the doc changes made in https://github.com/broadinstitute/gatk/pull/5422. Specifically, we should recommend that multiple simple expressions be used in place of a single compound expression. Here are @sooheelee 's notes on what docs need to change:. Here are the documents we should also update to reflect these updates:. https://gatkforums.broadinstitute.org/gatk/discussion/1255/using-jexl-to-apply-hard-filters-or-select-variants-based-on-annotation-values; https://software.broadinstitute.org/gatk/documentation/article?id=11080; Also, here is a list of documents with the jexl tag:; https://gatkforums.broadinstitute.org/gatk/discussions/tagged/jexl. I have put in a word of caution in https://gatkforums.broadinstitute.org/gatk/discussion/12350/how-to-filter-on-genotype-using-variantfiltration/p1?new=1.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5509:10,update,update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5509,3,['update'],"['update', 'updates']"
Deployability,We should update our small dependencies if they need it. - [x] gatk-native-bindings -> release it as version 1.0.0; - [x] gatk-fermilite-jni -> release the current version as 1.1.0; - [x] gatk-bwamem-jni -> release a version 1.0.4 which includes a new method; - [ ] update gatk with new libraries,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4030:10,update,update,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4030,5,"['release', 'update']","['release', 'update']"
Deployability,"We somehow missed some new deprecation warnings that gradle gives when we updated to 7.3.2. . When running `bundle`; ex:; ```; Task :sparkJar; Execution optimizations have been disabled for task ':sparkJar' to ensure correctness due to the following reasons:; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/classes/java/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/resources/main'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; - Gradle detected a problem with the following location: '/Users/louisb/Workspace/gatk/build/tmp/sparkJar/MANIFEST.MF'. Reason: Task ':sparkJar' uses this output of task ':condaStandardEnvironmentDefinition' without declaring an explicit or implicit dependency. This can lead to incorrect results being produced, depending on what order the tasks are executed. Please refer to https://docs.gradle.org/7.3.2/userguide/validation_problems.html#implicit_dependency for more details about this problem.; ```. ```; Deprecated Gradle features were used in this build, making it incompatible with Gradle 8.0. You can use '--warning-mode all' to show the individual deprecation warnings and determine if they come from your own scripts or plugins. See https://docs.g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7625:74,update,updated,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7625,1,['update'],['updated']
Deployability,"We use Gauss-Legendre integration in the strand bias model. The number of subdivisions increases with the read count and for very deep coverage this can cause a stack overflow because, unfortunately, Apache Commons has a very questionable recursive implementation. The short-term fix is to cap the number of subdivisions. The long-term fix is to write some sort of simple adaptive 1D and 2D quadrature method. This ticket is for the short-term fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3317:22,integrat,integration,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3317,1,['integrat'],['integration']
Deployability,We're seeing a high rate of failures running BQSR pipelines in production with `ExecutionException`s. . The root cause seems to be an `UnknownHostException` thrown in the google storage api client. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: www.googleapis.com; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:567); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:556); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); 	at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); 	at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:404); 	at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); 	at htsjdk.samtools.util.BinaryCodec.readByteBuffer(BinaryCodec.java:490); 	at htsjdk.samtools.util.BinaryCodec.readInt(BinaryCodec.java:501); 	at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:198); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5094:50,pipeline,pipelines,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5094,1,['pipeline'],['pipelines']
Deployability,"We've been implementing new features but the coverage in the `sv.utils` package has been non-pretty. Some of the classes like `ExternalCommandlineProgramModule.java` and `GATKSVVCFConstants.java` are OK (`ExternalCommandlineProgramModule` exists for historical reason, when we had SGA as the assembler; `GATKSVVCFConstants` is mostly holding String literals), but other classes are doing actual work and have low coverage. We should test them, not via integration tests only, but also unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5139:452,integrat,integration,452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5139,1,['integrat'],['integration']
Deployability,"We've been using 4.beta.6 to generate new callsets because it has the GenomicsDBImport batching fix and it seems to have introduced transient Auth errors that production was not seeing before. This happens a maybe one shard at every task level and when rerun usually succeeds but as you can imagine is pretty annoying. This happens across multiple tools (GenomicsDBImport, GatherVcfs). Sometimes we get this as the only response from GATK when this happens. ```; ***********************************************************************. A USER ERROR has occurred: Couldn't read file. Error was: Failure while waiting for FeatureReader to initialize with exception: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-gotc-prod-storage/pipeline/G101956/gvcfs/DDP_ATCP_42_1.4afb46bb-4009-47c4-9aa0-407e92de0db8.g.vcf.gz. ***********************************************************************; ```. and other times we get a nice stacktrace for this issue. ```; java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: 403 Forbidden; 443301511749-compute@developer.gserviceaccount.com does not have storage.objects.get access to broad-jg-dev-11k-call-set/JointGenotyping/0cb36821-b8bf-4e6d-a352-07b101f6b7d1/call-ApplyRecalibration/shard-1734/GMKF_Seidman_CHD_WGS_904.filtered.1734.vcf.gz.; 	at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); 	at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); 	at htsjdk.samtools.util.IOUtil.transferByStream(IOUtil.java:141); 	at org.broadinstitute.hellbender.tools.GatherVcfsCloud.gatherWithBlockCopying(GatherVcfsCloud.java:394); 	at org.broadinstitute.hellbender.tools.GatherVcfsCloud.doWork(GatherVcfsCloud.java:143); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3735:841,pipeline,pipeline,841,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3735,1,['pipeline'],['pipeline']
Deployability,"We've discovered a confusing git-lfs bug that effects people who have a git fork of gatk that started before the gatk/gatk-protected merge. . The symptom is that if you try to update your fork from the gatk master branch, and then push to your master, you'll see git errors like this:. ```; $ git push; open/Users/louisb/tmp/gatk-1/src/test/resources/large-protected/1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx: no such file or directory; ```. This will prevent you from updating your master. The solution is to checkout the commit `32a2b39` which will manually trigger a git-lfs download of 1000G.phase3.broad.withGenotypes.chr20.10100000.vcf.idx. Then checkout master again and push. @magicDGS This probably affects you. . We're going to open a git-lfs issue since it seems to be a pretty serious git-lfs bug, I'll update this ticket with more information when we have it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3038:176,update,update,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3038,2,['update'],['update']
Deployability,"We've had a request to retry 502's in `google-cloud-nio`, so we'll likely have to patch our custom fork again. When we do so, we should move it to a more standard location (broadinstitute org instead of my personal account).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4762:82,patch,patch,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4762,1,['patch'],['patch']
Deployability,"We've had reports of two kinds of intermittent network errors when using NIO that are not currently caught by google-cloud-nio:. * `UnknownHostException`; * 502 Bad Gateway. We should patch the library to add these errors to the list of retryable/reopenable errors. We can start by patching the GATK fork of `google-cloud-java`, and then concurrently submit a PR against `google-cloud-java` proper.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4888:184,patch,patch,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4888,2,['patch'],"['patch', 'patching']"
Deployability,"We've patched GatherVCFs to allow it to read from NIO, but since it's a picard tool it's going to be reverted when we switch to depending on the real picard. We need to either port the changes to picard or make a gatk-tool version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2745:6,patch,patched,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2745,1,['patch'],['patched']
Deployability,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:827,Configurat,Configuration,827,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['Configurat'],['Configuration']
Deployability,"What about this GATK 4 pipeline script, written by Chat-GPT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8905:23,pipeline,pipeline,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8905,1,['pipeline'],['pipeline']
Deployability,"What is the best Sparkified way to recapitulate our germline Best Practices workflow? Is there one pipeline that encompasses all steps from BWA alignment to HaplotypeCaller calling?. Please see forum thread as I have answered the user question tentatively with these two options:. [1] BwaSpark --> SortReadFileSpark --> ReadsPipelineSpark; [2] BwaAndMarkDuplicatesPipelineSpark --> SortReadFileSpark --> BQSRPipelineSpark --> HaplotypeCallerSpark. Thanks. ---; Hi @shlee ,; I am really sorry for the delay but I was busy in the last weeks. Anyway I will try to be clearer with this picture:. ![](https://us.v-cdn.net/5019796/uploads/editor/3x/9bu9fsvbgjrh.png """"). as you can see I would like to combine the tools `BwaAndMarkDuplicatesPipelineSpark` and `BQSRPipelineSpark` in one single tool, in order to improve efficiency of the pipeline (avoiding for example a disk writing). ; I tried to do it with [this](https://pastebin.com/XEqvpKmG ""this"") naive approach as I reported in previous comments, but executing this code I obtain this error (as you can see at the end of this [stack-trace](https://paste.ee/p/dMod1 ""stack-trace"") ) : ; ```; 17/11/03 13:02:14 ERROR Utils: Aborting task; java.lang.IllegalArgumentException: Reference index for 'chr11' not found in sequence dictionary.; ```. Do you think is better if I speak directly with developers in the GitHub repository?. Best regards,; Nicholas. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/44143#Comment_44143",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3878:99,pipeline,pipeline,99,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3878,2,['pipeline'],['pipeline']
Deployability,"What is the current behavior when OpenMP is not available or is the wrong version on a Linux system? If the answer is ""it blows up with a gross error"", then let's patch the code so that it gracefully falls back to the single-threaded version, as on a Mac.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1819:163,patch,patch,163,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1819,1,['patch'],['patch']
Deployability,"What this PR does: ; * implemented alignment breaker based on CIGAR gaps; ; * calls insertion, deletion and naive tandem repeat annotation in addition to inversion; ; * some further refactoring of code related to sv caller; ; * tests updated accordingly. The PR is into another branch but NOT `master` because it is based on a continuation effort, which is already reviewed in #2258 , therefore has all the changes already made there (a little Spark 2 phobia caused this derail; once changes are reviewed, will see if tool is runnable and ""pipelineable"" with previous stages in the SV pipeline under the new Spark version). @cwhelan please review.; Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2320:234,update,updated,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2320,3,"['pipeline', 'update']","['pipeline', 'pipelineable', 'updated']"
Deployability,"When I tried to run the latest build of the spark gatk I got the following error message on both dataproc and the onprem cluster.; `File Not Found: [/Users/emeryj/IdeaProjects/gatk/build/libIntelDeflater.so]`. Making the following changes to gatk-launch seems to fix it. ```; @@ -23,14 +23,13 @@ BUILD_LOCATION = script +""/build/install/"" + projectName + ""/bin/""; GATK_RUN_SCRIPT = BUILD_LOCATION + projectName; BIN_PATH = script + ""/build/libs"". -EXTRA_JAVA_OPTIONS=""-Dsamjdk.intel_deflater_so_path=libIntelDeflater.so -Dsamjdk.compression_level=1 -DGATK_STACKTRACE_ON_USER_EXCEPTION=true ""; +EXTRA_JAVA_OPTIONS=""-DGATK_STACKTRACE_ON_USER_EXCEPTION=true "". DEFAULT_SPARK_ARGS = [""--conf"", ""spark.kryoserializer.buffer.max=512m"",; ""--conf"", ""spark.driver.maxResultSize=0"",; ""--conf"", ""spark.driver.userClassPathFirst=true"",; ""--conf"", ""spark.io.compression.codec=lzf"",; ""--conf"", ""spark.yarn.executor.memoryOverhead=600"",; -""--conf"", ""spark.yarn.dist.files="" + script + ""/build/libIntelDeflater.so"",; ""--conf"", ""spark.driver.extraJavaOptions="" + EXTRA_JAVA_OPTIONS,; ""--conf"", ""spark.executor.extraJavaOptions="" + EXTRA_JAVA_OPTIONS]; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1930:329,install,install,329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1930,1,['install'],['install']
Deployability,"When I was trying to use user exceptions in a consistent way independently of the constructor (mostly related with files), I found very weird behaviour with the messages. Here I try to fix some of the things that I was struggling with:. * Support for path in constructors for `CouldNotReadInputFile`, `CouldNotCreateOutputFile`, `MalformedFile` and `MalformedBAM`, in addition to some missing constructors to have the same structure for all of them (with `File` and/or `String`).; * ~~Updated javadoc in `CommandLineException`, including extending classes to make clear that in the GATK framework is not printed out if it is thrown out of parameter validation.~~ __Edited__: this is not longer required, because `CommandLineException` is decoupled from `UserException` through barclay.; * Added a TODO into the `MalformedBAM` constructor that includes a `GATKRead` that is not used.; * __Edited__: added final to constructors.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2282:485,Update,Updated,485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2282,1,['Update'],['Updated']
Deployability,"When SelectVariants samples are subset and PLs are updated, GQ doesn't appear to get updated. This fixes some incorrect results for LeftAlignAndTrimVariants -split. Also fixes results for SelectVariants -trimAlternates -- PLs are just 0 with no called alts, so there shouldn't be GQ. Port of https://github.com/broadinstitute/gsa-unstable/pull/1625",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3409:51,update,updated,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3409,2,['update'],['updated']
Deployability,"When `GenotypeGVCFs` is sharded by `-L` interval, as it typically is in production, it needs a way to avoid emitting the same call across shards in the case of indels that span interval boundaries. The simplest way to do this is to only emit records that start in the current interval. It might make sense to do this at the VCF writer level, and hook it up to an engine-wide argument to toggle this behavior. `HaplotypeCaller` and `Mutect` currently do something similar internally, and could possibly leverage this functionality.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2735:387,toggle,toggle,387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2735,1,['toggle'],['toggle']
Deployability,"When extracting the overlapped bases of two reads, FragmentUtils miscalculates the starting position of the second read if there are softclipped bases at its head. This leads to misalignment of the two reads and incorrect updating of the base qualities. In some other cases, it may fail to detect overlapping bases if the first and second read both contain softclips. This pull request updates the position calculation and adds a unit test to demonstrate the issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6824:386,update,updates,386,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6824,1,['update'],['updates']
Deployability,"When hard clipping a read resulted in an empty read, and the read was sufficiently; close to the start of a contig, we could end up trying to set the read to a negative; start position and blowing up. This patch causes us to return an empty read earlier,; preventing us from setting an invalid start. Resolves the ""IllegalArgumentException during clipping: contig must be non-null and not; equal to *, and start must be >= 1"" error reported by many users of the GATK4; HaplotypeCaller. Resolves #3466",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4080:206,patch,patch,206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4080,1,['patch'],['patch']
Deployability,"When originally released, Funcotator had blistering speed. In the latest tests, this speed has dropped off to be a very small fraction of that original data processing rate. We need to profile Funcotator to determine the source of the slowdown, and then we must remedy it. In particular the hg38 acceptance testing dataset (`hg38_trio.vcf`) has slowed significantly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6574:16,release,released,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6574,1,['release'],['released']
Deployability,"When parsing interval arguments in GATK using the latest htsjdk, files that end in "".interval_list"" are claimed by the new IntervalListCodec introduced in https://github.com/samtools/htsjdk/pull/1327. This PR renames the one test file in GATK that has a Picard interval list file extension but isn't really a Picard interval list; without this change, CountReads and CountReadsSpark integration tests will fail when we upgrade to the next htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5879:383,integrat,integration,383,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5879,2,"['integrat', 'upgrade']","['integration', 'upgrade']"
Deployability,"When run with the current master build against our snapshot HG00514 sample, the experimental variant interpretation pipeline fails with the following exception:. ```; 18/04/11 20:27:50 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 32.0 in stage 42.0 (TID 56552, cwhelan-hg00514-1-cram-samtools-bam-feature-w-4.c.broad-dsde-methods.internal, executor 28): org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter$UnhandledCaseSeen: 1st segment is not overlapping with head alignment but it is not immediately before/after the head alignment either; AssemblyContigWithFineTunedAlignments{sourceTig=(asm022672:tig00004, [1_1430_chr9:130955309-130956738_-_1430M2216S_60_-1_-1_S, 1587_1763_chr9:130955156-130955308_-_1586S54M24I99M1883S_60_-1_-1_S, 1824_2015_chr9:130954964-130955155_-_1823S192M1631S_60_-1_-1_S, 2164_3646_chr9:130953867-130955307_-_2163H167M42I1274M_60_55_1318_O]), insertionMappings=[1963_2177_chr9:130955093-130955304_-_1962H179M3I33M1469H_19_14_138_O], hasEquallyGoodAlnConfigurations=false, saTAGForGoodMappingToNonCanonicalChromosome='NONE'}; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.extractAltHaplotypeSeq(CpxVariantCanonicalRepresentation.java:338); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantCanonicalRepresentation.<init>(CpxVariantCanonicalRepresentation.java:143); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$b3be3b47$1(CpxVariantInterpreter.java:53); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4649:116,pipeline,pipeline,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4649,1,['pipeline'],['pipeline']
Deployability,"When running PrintReads on GCS with shardedOutput false, I'm experiencing a file write failure for my output. I have verified that I can write to the bucket I specify and that when shardedOutput is true this error does not occur. @tomwhite - any thoughts?. Here's my invocation:; ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O gs://jonn-test-bucket/foo.bam --shardedOutput false -- --sparkRunner GCS --cluster jonn-test-cluster --num-executors 5 --executor-cores 4 --executor-memory 4g; ```; Here is the stack trace:; ```; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file gs://jonn-test-bucket/foo.bam because writing failed with exception jonn-test-bucket/foo.bam.parts; 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.inv",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:938,pipeline,pipelines,938,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['pipeline'],['pipelines']
Deployability,"When running Spark tools with GCS files on Dataproc the [GCS connector](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) is set up and configured for you, but this isn't the case when running with local Spark, even on a GCP VM. We should make the experience easier through documentation and/or configuration improvements.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5996:317,configurat,configuration,317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996,1,['configurat'],['configuration']
Deployability,"When running StructuralVariationDiscoveryPipelineSpark (GATK 4.0.1.1) on a hadoop cluster, the following exception occurs. The pipeline has been running fine on other cram files. **Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0**. Below is the stack and other details. . The pipeline looks like it is running through all the contigs and is not limited to chr1-chr22, ChrY, ChrX and ChrM. Would it help running the software if the extra 3000+ contig names in the GRCh38 reference are excluded? If so, what is the best way to exclude processing the extra contigs?. ```; 18/02/23 23:06:22 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 15.0 (TID 29435) in 2906 ms on scc-q04.scc.bu.edu (executor 1) (48/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 15.0 (TID 29463) in 2354 ms on scc-q04.scc.bu.edu (executor 1) (49/70); 18/02/23 23:06:23 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 15.0 (TID 29448) in 2653 ms on scc-q03.scc.bu.edu (executor 6) (50/70); 18/02/23 23:06:23 WARN scheduler.TaskSetManager: Lost task 27.0 in stage 15.0 (TID 29438, scc-q13.scc.bu.edu, executor 7): org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSplitera",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:127,pipeline,pipeline,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,2,['pipeline'],['pipeline']
Deployability,"When running StructuralVariationDiscoveryPipelineSpark, we found the local storage on our Hadoop Datanodes filling up. The pipeline was generating 2 GB of output and filling up the /var storage on the datanodes. The job would then fail. Is there a way to turnoff this extensive logging info for this pipeline and make less extensive logging the default? . Here is an example of one of the files.....; container_e2417_1520821377253_0060_01_000052/stdout. writing; 10000139 2/2 76b aligned read.; writing; 1000014 1/2 76b aligned read.; writing; 1000014 2/2 76b aligned read.; writing; 10000140 2/2 76b aligned read.; writing; 10000140 1/2 76b aligned read.; writing; 10000141 1/2 76b aligned read.; writing; 10000141 2/2 76b aligned read.; writing; 10000142 1/2 76b aligned read.; writing; 10000142 2/2 76b aligned read.; writing; 10000143 1/2 76b aligned read.; writing; 10000143 2/2 76b aligned read.; writing; 10000144 2/2 76b aligned read.; writing; 10000144 1/2 76b aligned read.; writing; 10000145 2/2 76b aligned read.; writing; 10000145 1/2 76b aligned read.; writing; 10000146 2/2 76b aligned read.; writing; 10000146 1/2 76b aligned read.; writing; 10000147 1/2 76b aligned read.; writing; 10000147 2/2 76b aligned read.; writing; 10000148 2/2 76b aligned read.; writing; 10000148 1/2 76b aligned read.; writing; 10000149 2/2 76b aligned read.; writing; 10000149 1/2 76b aligned read.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4531:123,pipeline,pipeline,123,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4531,2,['pipeline'],['pipeline']
Deployability,"When running the current version of the SV pipeline against the NA12878_PCR-_30X bam file aligned to hg19/b37, stage 10 is held up by a single assembly task that takes much longer than the others. Unfortunately I don't have exact timings yet, and haven't identified the assembly issue that's causing the problem, but I wanted to log this as an issue so we don't forget about it. It's possible that other samples will have similar outlier assemblies so it would be good to track down cases like this so that we can try to identify and remove pathological intervals. @tedsharpe if you find yourself with free time could you take a look at this? Whatever is going wrong with it might be fixed or relevant to your work on debugging assemblies and kmer gathering (if we're lucky perhaps your 'diffuse k-mer' fix will solve this issue).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3607:43,pipeline,pipeline,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3607,1,['pipeline'],['pipeline']
Deployability,"When running the following command. gatk4 BaseRecalibratorSpark -I xx_markduplicatespark.bam -knownSites dbsnp_138.b37.vcf -knownSites Mills_and_1000G_gold_standard.indels.b37.vcf -O xx_baserecalibratespark.table -R humann_g1k_v37.2bit --TMP_DIR tmp. I got error message,. Using GATK wrapper script /curr/tianj/software/gatk/build/install/gatk/bin/gatk; Running:; /curr/tianj/software/gatk/build/install/gatk/bin/gatk BaseRecalibratorSpark -I A15_markduplicatespark.bam -knownSites ref/Mills_and_1000G_gold_standard.indels.b37.vcf -O A15_baserecalibratespark.table -R /curr/tianj/data/humann_g1k_v37.2bit; 17:19:00.338 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/curr/tianj/software/gatk/build/instabgkl_compression.so; [May 17, 2017 5:19:00 PM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark --knownSites /genome/ref/db_1000G_gold_standard.indels.b37.vcf --output A15_baserecalibratespark.table --reference /curr/tianj/data/humann_g1k_v37.2bp --joinStrategy BROADCAST --mismatches_context_size 2 --indels_context_size 3 --maximum_cycle_value 500 --mismatches_defdeletions_default_quality 45 --low_quality_tail 2 --quantizing_levels 16 --bqsrBAQGapOpenPenalty 40.0 --preserve_qscores_lles false --useOriginalQualities false --defaultBaseQualities -1 --readShardSize 10000 --readShardPadding 1000 --readValid-interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --sharl[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inf; [May 17, 2017 5:19:00 PM UTC] Executing as tianj@ip-172-31-78-66 on Linux 4.4.41-36.55.amzn1.x86_64 amd64; Java HotSpot(TM:4.alpha.2-261-gb8d32ee-SNAPSHOT; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.BUFFER_SIZE : 131072; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.COMPRESSION_LEVEL : 1; 17:19:00.371 INFO BaseRecalibratorSpark - Defaults.CREATE_INDEX : false; 17:19:00.371 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2732:331,install,install,331,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2732,2,['install'],['install']
Deployability,When the changes from this https://github.com/broadinstitute/picard/pull/1442 go in and we update our picard dependency we should port the test and scaling factor from that branch.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6316:91,update,update,91,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6316,1,['update'],['update']
Deployability,"When we make enhancements to the walker engine (eg., modify the `GATKTool` base class to support CRAM, or to validate the sequence dictionaries of the inputs), it would be good if Spark tools could also reap the benefits of these changes automatically. We may need to unify (or better integrate) the `GATKTool` and `SparkCommandLineProgram` base classes somehow to make this possible, as well as classes like `ReadsDataSource` (for walkers) and `ReadsSparkSource` (for Spark tools).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/680:285,integrat,integrate,285,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/680,1,['integrat'],['integrate']
Deployability,When we refactored the docker image to no longer contain the source directory we found that the changes had the side effect of causing the tests inside of the docker image to execute more slowly (~10 minutes per docker test). We solved this problem by further splitting the tests up but this is only a temporary solution. It is worth figuring out what about this gradle configuration is slow to save everyone time.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4989:370,configurat,configuration,370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4989,1,['configurat'],['configuration']
Deployability,"While testing BwaSpark on a yarn cluster, I noticed that only a single executor (the driver) was being created and running only on the master node. The worker nodes were idle. This happened when running the following command . ```; spark-submit \; --deploy-mode client \; --class org.broadinstitute.hellbender.Main \; --master yarn \; /home/hadoop/gatk-package-4.alpha.2-269-gdce8abc-SNAPSHOT-spark.jar BwaSpark \; --bwamemIndexImage /var/tmp/hs38DH-V.fasta.img \; -I hdfs:///unaligned.bam \; -O hdfs:///aligned.bam \; -R hdfs:///hg38/hs38DH-V.fasta \; --disableSequenceDictionaryValidation true; ```. I checked the spark environment settings in the spark web UI and found that `spark.master` was set to `local` despite the `--master yarn` cmd-line argument. Next I checked the gatk source and found a suspicious call to `SparkConf.setMaster()` at line 138 in ; `src/main/java/org/broadinstitute/hellbender/engine/spark/SparkContextFactory.java`. I commented out this call, recompiled, and tried again, and the issue went away. I suspect that `setMaster()` was overriding the `--master yarn` cmd-line argument. I believe this is a bug and propose somehow avoiding `setMaster()` when the app is run with `spark-submit`. Thanks,; David",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2718:250,deploy,deploy-mode,250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2718,1,['deploy'],['deploy-mode']
Deployability,While working on #9012 I tried to update the gencode v28 datasource snippets in the Funcotator integration tests to V43. In doing so I found that it broke the MAF vs. VCF output render tests with errors of the following nature: . ```; java.lang.AssertionError: Failed Matching VCF and MAF fields:; 	VCF (Gencode_43_variantClassification): 	RNA[0]	RNA[1]	RNA[2]	RNA[3]	RNA[4]	RNA[5]	RNA[6]	RNA[7]	RNA[8]	RNA[9]	RNA[10]; 	MAF (Variant_Classification): 	LINCRNA[0]	LINCRNA[1]	LINCRNA[2]	LINCRNA[3]	LINCRNA[4]	LINCRNA[5]	LINCRNA[6]	LINCRNA[7]	LINCRNA[8]	LINCRNA[9]	LINCRNA[10]; ----; 	VCF (Gencode_43_otherTranscripts): 	[0]	[1]	[2]	[3]	[4]	[5]	[6]	[7]	[8]	[9]	[10]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[11]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[12]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK/PIK3CA-DT_ENST00000435560.1_RNA[13]	PIK3CA_ENST00000643187.1_INTRON/PIK3CA-DT_ENST00000435560.1_FIVE_PRIME_FLANK[14]	[48]	[49]	[50]	[51]	[52]	[53]	[54]	[55]	[56]	[57]	[58]	[59]	[60]	[61]	[62]	[63]	[64]	[65]	[66]	[67]	[68]	[69]	[70]	[71]	[72]	[73]	[74]	[75]	[76]	[77]	[78]	[79]	[80]	[81]	[82]	[83]	[84]	[85]	[86]	[87]	[88]	[89]	[90]	[91]	[92]	[93]	[94]	[95]	[96]	[97]	[98]	[99]	[100]	[101]	[102]	[103]; 	MAF (Other_Transcripts): 	[0]	[1]	[2]	[3]	[4]	[5]	[6]	[7]	[8]	[9]	[10]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[11]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[12]	PIK3CA_ENST00000643187.1_FIVE_PRIME_FLANK|PIK3CA-DT_ENST00000435560.1_LINCRNA[13]	PIK3CA_ENST00000643187.1_INTRON|PIK3CA-DT_ENST00000435560.1_FIVE_PRIME_FLANK[14]	[48]	[49]	[50]	[51]	[52]	[53]	[54]	[55]	[56]	[57]	[58]	[59]	[60]	[61]	[62]	[63]	[64]	[65]	[66]	[67]	[68]	[69]	[70]	[71]	[72]	[73]	[74]	[75]	[76]	[77]	[78]	[79]	[80]	[81]	[82]	[83]	[84]	[85]	[86]	[87]	[88]	[89]	[90]	[91]	[92]	[93]	[94]	[95]	[96]	[97]	[98]	[99]	[100]	[101]	[102]	[103]; ----; ```. Its unclear what is the most correct output ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/9013:34,update,update,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/9013,2,"['integrat', 'update']","['integration', 'update']"
Deployability,"Why SGA, the currently used assembler for SV pipeline, generates slightly diff results on different runs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1805:45,pipeline,pipeline,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1805,1,['pipeline'],['pipeline']
Deployability,Will fail until the Barclay upgrade.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4523:28,upgrade,upgrade,28,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4523,1,['upgrade'],['upgrade']
Deployability,Will your workflow be ready for the Jan 9 release?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3769:42,release,release,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3769,1,['release'],['release']
Deployability,"With allele-specific annotations and filtering in the new exome pipeline, this should be helpful. (I was using it for debugging merging Mutect2 perAlleleAnnotations for MT data.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5697:64,pipeline,pipeline,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5697,1,['pipeline'],['pipeline']
Deployability,"With the GATK gCNV having great performance results on the first round of evaluations it is ready to be used to call on ExAC. The following things need to be done first:. - Set up gCNV workflow to run on SGE (since exome samples are stored on prem). - Decide on target filtering strategy. . - Decide on the number of samples to use to learn the model (PoN). - Get some truth data to do QC, for example CNV calls from Genome STRiP on matched genome samples in gnomAD. - Design an interval list for samples in ExAC that do not mention one in their metadata. One possible solution could be to use cluster assignment of a sample to choose the interval list pertaining to that cluster. - (Optional) Consider importing list of common CNV regions into gCNV. To make job of gCNV inference easier we could use the list of common CNV regions that was obtained from Genome STRiP calls. To start @ldgauthier suggested using samples sequenced using latest Illumina capture protocol (Standard_Exome_Sequencing_v4) to get the ball rolling",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4738:1016,rolling,rolling,1016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4738,1,['rolling'],['rolling']
Deployability,"With the addition of denoised-copy-ratio output in #5823, there are probably enough outputs to warrant this. Might also check other CNV tools (I've personally been thinking we should do this in DenoiseReadCounts as well). Should highlight in release notes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6128:242,release,release,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6128,1,['release'],['release']
Deployability,"With the addition of the Owner configuration mechanism to GATK, we're going to need to fold (most of) `gatk-launch` into Java, so that the Spark settings can be loaded from Owner before `spark-submit` or `gcloud` are invoked. Note that we'll still need to have a thin GATK launcher script for the sake of the tab completion, which requires that we invoke GATK using a unique command name.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3503:31,configurat,configuration,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3503,1,['configurat'],['configuration']
Deployability,Without JCenter the retrieval of this artifact is failing like:. ```; #16 21.85 A problem occurred evaluating root project 'gatk'.; #16 21.85 > Could not resolve all files for configuration ':runtimeClasspath'.; #16 21.85 > Could not find biz.k11i:xgboost-predictor:0.3.0.; #16 21.85 Searched in the following locations:; #16 21.85 - https://repo.maven.apache.org/maven2/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://broadinstitute.jfrog.io/broadinstitute/libs-snapshot/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - https://oss.sonatype.org/content/repositories/snapshots/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 - file:/root/.m2/repository/biz/k11i/xgboost-predictor/0.3.0/xgboost-predictor-0.3.0.pom; #16 21.85 Required by:; #16 21.85 project :; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7830:176,configurat,configuration,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7830,1,['configurat'],['configuration']
Deployability,Work is split into two commits:. - Removed undocumented mid-p correction to p-values in exact test of Hardy-Weinberg equilibrium and updated corresponding unit tests.; - Updated expected ExcessHet values in integration test resources and added an update toggle to GnarlyGenotyperIntegrationTest. Various scout cleanups as well. We now report the same value as ExcHet in bcftools. Note that previous values of 3.0103 (corresponding to mid-p values of 0.5) will now be 0.0000. See discussion below and in linked issue for additional details. Closes #7392.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7394:133,update,updated,133,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7394,5,"['Update', 'integrat', 'toggle', 'update']","['Updated', 'integration', 'toggle', 'update', 'updated']"
Deployability,Workspace updates in release docs [VS-1202],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8661:10,update,updates,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8661,2,"['release', 'update']","['release', 'updates']"
Deployability,Write a hardcoded one-off pipeline with stubs for each tool in the read pre-processing pipeline.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/459:26,pipeline,pipeline,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/459,2,['pipeline'],['pipeline']
Deployability,Write a stub end-to-end integration test for the ReadsPreprocessingPipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/761:24,integrat,integration,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/761,1,['integrat'],['integration']
Deployability,Write a tribble codec for parsing Picard interval_list format (+ integration tests in GATK),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5788:65,integrat,integration,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5788,1,['integrat'],['integration']
Deployability,Write basic integration tests for CRAM support using PrintReads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/675:12,integrat,integration,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/675,1,['integrat'],['integration']
Deployability,Write common interfaces for the different kinds of transforms we identify in the dataflow read pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/463:110,pipeline,pipeline,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/463,1,['pipeline'],['pipeline']
Deployability,Write integration test for GermlineCNVCaller for single sample calling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3002:6,integrat,integration,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3002,1,['integrat'],['integration']
Deployability,Write stub implementations of all tools in the dataflow reads pre-processing pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/464:77,pipeline,pipeline,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/464,1,['pipeline'],['pipeline']
Deployability,"Writing output via NIO is currently broken, as reported by @jean-philippe-martin:. ```; /home/jpmartin/build/install/gatk/bin/gatk PrintReads -I inputs/CEUTrio.HiSeq.WEx.b37.NA12892.bam -L 10:1000000-2000000 -O gs://jpmartin/staging/out_tmp.bam; htsjdk.samtools.util.RuntimeIOException: Error opening file: /home/jpmartin/gs:/jpmartin/staging/out_tmp.bam; at htsjdk.samtools.SAMFileWriterFactory.makeBAMWriter(SAMFileWriterFactory.java:246); ```. Now that the read support is in good shape, let's get the writing end working -- should hopefully be an easy fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2422:109,install,install,109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2422,1,['install'],['install']
Deployability,"Writing reads was fixed in https://github.com/broadinstitute/gatk/commit/73f2a62bee52518b57a985717770ed3a64d83243, but unfortunately the same problem occurs with variants. This commit (https://github.com/broadinstitute/gatk/commit/a861a23b544012fb8f69358a3999dd587d343547) fixes the problem for variants, when deployed with a Hadoop-BAM fix (https://github.com/HadoopGenomics/Hadoop-BAM/pull/143).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3485:310,deploy,deployed,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3485,1,['deploy'],['deployed']
Deployability,"X). ![unnamed](https://cloud.githubusercontent.com/assets/15305869/26426249/ce3ffb68-40a5-11e7-8002-6ea4f8513eea.png). A naive calculation of the relative X ploidy, i.e. calculating X_pcov = (X_total_read_counts / autosome_total_read_count) for all samples, performing a 2-mean clustering, and dividing the X_pcov by the lower ploidy cluster mean reveals that indeed, the X conting has twice more coverage on _average_ in XX samples:; ![image](https://cloud.githubusercontent.com/assets/15305869/26426348/2b2d6982-40a6-11e7-8eca-e93916bfc80c.png). Further investigation shows that the wrong behavior of TargetCoverageSexGenotyper stems from the lack of robustness of Poisson regression to outliers: there are a number of targets in the X contig with anomalously high coverage (200x median!). In the absence of Y coverage data (and bias adjustment), higher ploidy genotypes are always favored (in this case, XX). Solution: either filter read counts for outliers before calculating Poisson log likelihoods, or simply use the naive median-based ploidy estimates and perform genotyping on the estimated ploidies (rather than target-resolved read counts). The latter is proven to be robust to outliers. Update: it turns out that the issue can be fixed by simply taking into account bait count as a multiplicative bias. Otherwise, the distribution of raw read counts is multimodal and far from Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516437/54da4930-4254-11e7-9093-5e5fe1e0e28e.png). Correcting for bait count yields a neat over-dispersed Poisson:; ![image](https://cloud.githubusercontent.com/assets/15305869/26516442/68d9ba4c-4254-11e7-82f0-c182f2485d67.png). Todo:; - [x] bait count target annotations; - [x] take bait count into account in TargetCoverageSexGenotyper model; - [x] PAR region blacklisting via command line in TargetCoverageSexGenotyper; - [ ] unit test for bait count functionality of TargetAnnotator; - [x] unit tests for genotyping with only X coverage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3015:1736,Update,Update,1736,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3015,1,['Update'],['Update']
Deployability,"Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:2493,toggle,toggle,2493,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['toggle'],['toggle']
Deployability,You're using an old version of ojAlgo. I upgraded it for you. There has been improvements to the SVD implementations. (Significant improvements to some users.). Also added a dependency to ojalgo-commons-math3 which gives you wrapper classes to convert back and forth between ojAlgo and Commons Math matrix classes. You no longer need to copy the resulting matrices.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3970:41,upgrade,upgraded,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3970,1,['upgrade'],['upgraded']
Deployability,"ZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.REFERENCE_FASTA : null; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.USE_CRAM_REF_DOWNLOAD : false; > > 15:10:22.794 INFO PrintReadsSpark - Deflater IntelDeflater; > > 15:10:22.794 INFO PrintReadsSpark - Initializing engine; > > 15:10:22.794 INFO PrintReadsSpark - Done initializing engine; > > 15:10:23.180 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; > > 15:10:25.800 INFO PrintReadsSpark - Shutting down engine; > > [October 18, 2016 3:10:25 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.05 minutes.; > > Runtime.totalMemory()=467140608; > > org.broadinstitute.hellbender.exceptions.GATKException: unable to write bam: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:252); > > at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); > > at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); > > at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); > > at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:3412,pipeline,pipelines,3412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['pipeline'],['pipelines']
Deployability,[10 sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/10d083c7-2c20-4339-aa2b-70945056de44); [5k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/60325b52-7040-492b-82e1-23a58850fb59); [20k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/7fee1590-d00c-4038-a575-fd06a9edba1a); [50k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/2efb51a2-105a-4f62-a1fe-26b7350706ed); [100k sample run](https://app.terra.bio/#workspaces/gvs-dev/GVS_190k_Exomes/job_history/dba1e793-51e4-47cc-9016-c478628e7252); [integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/19c865ab-69bc-45a3-a897-ff4e5f3d2a53),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8615:630,integrat,integration,630,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8615,1,['integrat'],['integration']
Deployability,"[GenomicsDB]. As part of our pipeline, we are running the below mentioned command.; What this step does is, reads the vcf files in batch of 50 so in total 4 batches will run for 156 samples, and then for each batch it writes the tables/data to the “genomicsdb” folder. ; ; Parent Command : python /gatk/gatk --java-options -Xmx4g -Xms4g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; Child Process : java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646:29,pipeline,pipeline,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646,1,['pipeline'],['pipeline']
Deployability,[Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5eb30da2-4afe-4f8e-ad72-e96ac647c588) is a passing Integration test (note - updated truth).; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/7ab3d8de-4325-4ca1-b507-1546f9b89986) is a run of GvsJointCalling using this code.; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/494b9661-b5d2-4d4e-8da7-8df84d52e162) is a run of GvsExtractCalset extracting from tables created by this code (with phasing fields in the prepare tables) ; [Here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20ggrant/job_history/dddfefce-f4c7-49bf-aabb-d9b92720b809) is a run of GvsExtractCallset extracing from (OLD) tables created before this code (without phasing fields in the prepare tables).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8655:130,Integrat,Integration,130,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8655,2,"['Integrat', 'update']","['Integration', 'updated']"
Deployability,[Integration run](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dfeb564d-5b4d-4b6b-9ffd-88e618acf5e0) in progress,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8454:1,Integrat,Integration,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8454,1,['Integrat'],['Integration']
Deployability,"[Tag 4.2.4.0](https://github.com/broadinstitute/gatk/releases/tag/) doesn't appear to address the fact that WDLs still reference older GATK Docker images and therefore are still vulnerable. This is a quick replacement of all references to outdated GATK images that I could find in this repo's WDLs and WDL-specific JSONs. Note that these changes may be breaking, especially for older workflows; I do not have the bandwidth to individually test each one. The following images were not updated as I couldn't find a suitable replacement, although I suspect several could be replaced with the standard GATK image; * pkrusche/hap.py (I'm not sure this would even be affected by the vulnerability); * broad-gotc-prod/genomes-in-the-cloud; * gatksv/sv-base-mini -- referenced as gatksv/sv-base-mini:b3af2e3 in joint_call_exome_cnvs.wdl; * broadinstitute/oncotator -- referenced as broadinstitute/oncotator:1.9.5.0-eval-gatk-protected in cnv_somatic_oncotator_workflow.wdl, but is a fallback option; * us.gcr.io/broad-dsde-methods/haplochecker; * us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.4.2-1552931386. If this PR is accepted, note that all affected WDLs should also have their default tag on Dockstore changed -- only [MitochondriaPipeline](https://dockstore.org/workflows/github.com/broadinstitute/gatk/MitochondriaPipeline:master?tab=files) defaults to master in Dockstore if I recall correctly. [gatk4-rnaseq-germline-snps-indels](https://dockstore.org/workflows/github.com/gatk-workflows/gatk4-rnaseq-germline-snps-indels/gatk4-rnaseq-germline-snps-indels:master?tab=versions) defaults to master but is in the gatk-workflows repo, not this one.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7611:53,release,releases,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7611,2,"['release', 'update']","['releases', 'updated']"
Deployability,[org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8102,Continuous,ContinuousBuildActionExecuter,8102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,[org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.i,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7250,Continuous,ContinuousBuildActionExecuter,7250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,"\* Opening on behalf of a user on an HPC cluster, my knowledge in this field is a bit limited. ### Affected tool(s) or class(es); gatk HaplotypeCaller. ### Affected version(s); Latest 4.6.0.0 release. ### Description ; When running command, ~16 hours into the run the program crashes. Below is the start of the Java error report file. ```; #; # A fatal error has been detected by the Java Runtime Environment:; #; # SIGSEGV (0xb) at pc=0x00007f06ed243291, pid=1058615, tid=1058616; #; # JRE version: OpenJDK Runtime Environment (17.0.2+8) (build 17.0.2+8-86); # Java VM: OpenJDK 64-Bit Server VM (17.0.2+8-86, mixed mode, sharing, tiered, compressed oops, compressed class ptrs, g1 gc, linux-amd64); # Problematic frame:; # C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; #; # Core dump will be written. Default location: Core dumps may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/ramadugulab/luy/SNPcallingBreeding/core.1058615); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. --------------- S U M M A R Y ------------. Command Line: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /bigdata/operations/pkgadmin/opt/linux/centos/8.x/x86_64/pkgs/gatk/4.6.0.0/gatk-package-4.6.0.0-local.jar HaplotypeCaller -R /rhome/luy/bigdata/genomes/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam -O AlignedCalToCcl_Scaffolds.vcf.gz -ERC GVCF. Host: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz, 64 cores, 20G, Rocky Linux release 8.8 (Green Obsidian); Time: Sat Sep 28 04:11:19 2024 PDT elapsed time: 58592.788414 seconds (0d 16h 16m 32s). --------------- T H R E A D ---------------. Current thread (0x00007f06e4025b70): JavaThread ""main""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988:192,release,release,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988,1,['release'],['release']
Deployability,] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.execution.TaskPathProjectEvaluator.configureHierarchy(TaskPathProjectEvaluator.java:42); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultBuildConfigurer.configure(DefaultBuildConfigurer.java:38); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.initialization.DefaultGradleLauncher$2.run(DefaultGradleLauncher.java:151); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:3387,configurat,configuration,3387,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,"_ACC5611A1_XXXXXX_consensusalign_ss_r2.bam -O mutect2/concatenated_ACC5611A1_XXXXXX_mutect2_unfiltered_ss_r2.vcf.gz; 09:39:55.358 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/proj/bin/conda/envs/D_UMI_APJ/share/gatk4-4.1.8.0-0/gatk-package-4.1.8.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jul 03, 2020 9:39:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 09:39:55.559 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.559 INFO Mutect2 - The Genome Analysis Toolkit (GATK) v4.1.8.0; 09:39:55.559 INFO Mutect2 - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:39:55.559 INFO Mutect2 - Executing as ashwini.jeggari@hasta.scilifelab.se on Linux v3.10.0-1062.4.1.el7.x86_64 amd64; 09:39:55.560 INFO Mutect2 - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 09:39:55.560 INFO Mutect2 - Start Date/Time: July 3, 2020 9:39:55 AM CEST; 09:39:55.560 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.560 INFO Mutect2 - ------------------------------------------------------------; 09:39:55.560 INFO Mutect2 - HTSJDK Version: 2.22.0; 09:39:55.561 INFO Mutect2 - Picard Version: 2.22.8; 09:39:55.561 INFO Mutect2 - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:39:55.561 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:39:55.561 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:39:55.561 INFO Mutect2 - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:39:55.561 INFO Mutect2 - Deflater: IntelDeflater; 09:39:55.561 INFO Mutect2 - Inflater: IntelInflater; 09:39:55.561 INFO Mutect2 - GCS max retries/reopens: 20; 09:39:55.561 INFO Mutect2 - Requester pays: disabled; 09:39:55.561 INFO Mutect2 - Initializing engine; 09:39:56.014 INFO FeatureManager - Using codec BE",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6695:1862,release,release-,1862,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6695,1,['release'],['release-']
Deployability,_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:35:26.517 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:35:26.517 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 18:35:26.517 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 18:35:26.517 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 18:35:26.517 INFO MarkDuplicatesSpark - Requester pays: disabled; 18:35:26.517 INFO MarkDuplicatesSpark - Initializing engine; 18:35:26.517 INFO MarkDuplicatesSpark - Done initializing engine; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/user/wup/miniconda3/envs/gatk/share/gatk4-4.1.8.1-0/gatk-package-4.1.8.1-local.jar) to method java.nio.Bits.unaligned(); WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 20/10/08 18:35:27 INFO SparkContext: Running Spark version 2.4.5; 18:35:27.640 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 20/10/08 18:35:27 INFO SparkContext: Submitted application: MarkDuplicatesSpark; 20/10/08 18:35:27 INFO SecurityManager: Changing view acls to: wup; 20/10/08 18:35:27 INFO SecurityManager: Changing modify acls to: wup; 20/10/08 18:35:27 INFO SecurityManager: Changing view acls groups to: ; 20/10/08 18:35:27 INFO SecurityManager: Changing modify acls groups to: ; 20/10/08 18:35:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(wup); groups with view permissions: Set(); users with modify permissions: Set(wup); groups with modify permissions: Set(); 20/10/08 18:35:28 INFO Utils: Suc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6875:2463,release,release,2463,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6875,1,['release'],['release']
Deployability,"_fixes) ; ; 12:52:16.269 INFO GenotypeGVCFs - Initializing engine ; ; terminate called after throwing an instance of 'VariantQueryProcessorException' ; ; what(): VariantQueryProcessorException : Could not open array genomicsdb\_array at workspace: /home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4. Hi, I used GenomicsDBImport to combined 2000 GVCFs. To speed up, I split the bed file and concatenated multiple intervals into a contig. I also met the file locking problem which can be solved by setting  TILEDB\_DISABLE\_FILE\_LOCKING=1 in my Linux system. Currently, I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000](https://sites.google.com/a/broadinstitute.org/legacy-gatk-forum-discussions/2018-04-11-2017-12-02/11184-Could-not-open-array-genomicsdbarray-at-workspace-from-GenotypeGVCFs-in-GATK-4000) , but except for using the latest version of GATK, it seems like there are no other solutions. I was wondering that how do I fix the issues with GATK 4.0.3.0? Does anyone have a better solution?. I also tried GenotypeGVCFs in GATK 4.2.1.0, but there is a problem in terms of MQ calculation. So I think it's better to stick to the same GATK version in the whole workflow. A USER ERROR has occurred: Bad input: Presence of '-RAW\_MQ' annotation is detected. ; ; This GATK version expects key RAW\_MQandDP with a tuple of sum of squared MQ values and total reads over variant genotypes as the value. ; ; This could indicate that the provided input was produced with an older version of GATK. ; ; Use the argument '--allow-old-rms-mapping-quality-annotation-data' to override and attempt the deprecated MQ calculation. ; ; There may be differences i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:5395,a/b,a/broadinstitute,5395,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,1,['a/b'],['a/broadinstitute']
Deployability,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5963:63,configurat,configuration,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963,2,['configurat'],['configuration']
Deployability,"`GATKTool` uses stricter sequence dictionary validation settings for CRAM vs. reference than for non-CRAM vs. reference:. ```; if ( hasCramInput() ) {; // Use stricter validation for CRAM vs. the reference; SequenceDictionaryUtils.validateCRAMDictionaryAgainstReference(refDict, readDict);; }; else {; // Use standard validation settings for non-CRAM reads input vs. the reference; SequenceDictionaryUtils.validateDictionaries(""reference"", refDict, ""reads"", readDict);; }; ```. `GATKSparkTool.validateToolInputs()` should be patched to do the same, AFTER https://github.com/broadinstitute/gatk/issues/966 is done and cram support is in a usable state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1179:525,patch,patched,525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1179,1,['patch'],['patched']
Deployability,"`HaplotypeCallerIntegrationTest` has two kinds of tests: exact-match tests against prior output to let us know when ANYTHING changes, and looser concordance tests against GATK3 output. The exact-match tests should stay, and are not a problem now that we've implemented an `UPDATE_EXACT_MATCH_EXPECTED_OUTPUTS` toggle to update them all at once. The GATK3 concordance tests, however, are past their usefulness. We've now diverged sufficiently from GATK3 that we need a new truth set for `HaplotypeCaller`. We should change the tests to assert concordance against this new truth set (whatever it ends up being) rather than GATK 3 output.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5532:310,toggle,toggle,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5532,2,"['toggle', 'update']","['toggle', 'update']"
Deployability,"`HaplotypeCaller` works with unphased genotypes. The unphased genotype A/B represent the phased genotpyes A/B _and_ B/A, for a combinatorial factor of 2, in contrast to A/A, which has a combinatorial factor of 1. It does not appear that the genotype likelihoods emitted by `GenotypeLikelihoodCalculator` account for this. That is, the genotype likelihoods for heterozygous diploid genotypes are too small by a factor of 2. The relevant code in `GenotypeLikelihoodCalculator` is. ``` java; private double[] genotypeLikelihoods( [arguments]) {; ...; for (int g = 0; g < genotypeCount; g++) {; result[g] = MathUtils.sum(readLikelihoodsByGenotypeIndex[g], 0, readCount) - denominator;; [ we need result += (GenotypeAlleleCounts of genotype g).log10CombinationCount() ]; }; ...; ```. @ldgauthier @vruano do you agree?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2019:71,A/B,A/B,71,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2019,2,['A/B'],['A/B']
Deployability,"`IntegrationTestSpec` currently ignores leading/trailing whitespace by default when doing its comparison against expected outputs. This is problematic given that whitespace can include things like field delimiters, leading to bugs like the one fixed in https://github.com/broadinstitute/gatk/pull/7559. We should change the default to *not* ignore leading/trailing whitespace. Tests that have a legitimate reason for ignoring it can then explicitly opt-in by calling `setTrimWhiteSpace()`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7560:1,Integrat,IntegrationTestSpec,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7560,1,['Integrat'],['IntegrationTestSpec']
Deployability,`ReadBAMTransform.getReadsFromBAMFilesSharded()` (which we rely upon to produce initial `PCollection<Read>`s in our pipelines) relies on flawed conversion from `SAMRecord` to `Read` (`ReadConverter.makeRead()`) that (among other things) does not encode attributes correctly. It also does not support unmapped reads. We should fix these issues.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/509:116,pipeline,pipelines,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/509,1,['pipeline'],['pipelines']
Deployability,"`ReadsPipelineSpark` should override `GATKSparkTool.getRecommendedNumReducers()` to provide an appropriate default value for the `numReducers` argument. The default gives us one partition per 10MB of input, which may be too small for this pipeline (it was kept small on purpose due to memory issues, but perhaps these have been resolved?)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1660:239,pipeline,pipeline,239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1660,1,['pipeline'],['pipeline']
Deployability,"`SortReadFileSpark` gives a log message like this:; ```; 17:31:00.062 INFO SortReadFileSpark - Using %s reducers2744; ```; It's trivial, but I think the [logger](https://github.com/broadinstitute/gatk/blob/c18e7800ed85c55f81387cf02fdcbf6cb3aaaf5e/src/main/java/org/broadinstitute/hellbender/tools/spark/pipelines/SortReadFileSpark.java#L41) needs a small fix.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3095:303,pipeline,pipelines,303,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3095,1,['pipeline'],['pipelines']
Deployability,"`TableReader` and `TableWriter` only work on `java.io.File`, and need to be updated to accept `java.nio.Path` so we can Path-enable the tools that have code paths that depend on this package, like FilterByOrientationBias.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5747:76,update,updated,76,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5747,1,['update'],['updated']
Deployability,"`Tribble` codecs can only read data from `Locatable` data sources (those with contig + start + end position). Recently we have found a need for reading in files that do not have locatable data (e.g. tabular data that has a `Gene Name` and a set of attributes, but no start/stop location). Tribble should be updated to have a baseline `Interface` that is generic (and not necessarily `Locatable`). Our current interface / infrastructure can inherit from that for data sources that are `Locatable`. Then a new `Codec` can be created for data sources that are not Locatable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3760:307,update,updated,307,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3760,1,['update'],['updated']
Deployability,"`ValidateVariants` performs several checks that go above and beyond what the VCF spec requires for VCF files (e.g. throwing an exception if a variant has an alt allele but has a genotype of hom ref [as found by this user](https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-)). This is good - it helps catch logic errors in our and others' pipelines. . However, we should add a flag to `ValidateVariants` that will cause it to validate solely based on the VCF spec and not the more strict guidelines.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6553:407,pipeline,pipelines,407,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6553,1,['pipeline'],['pipelines']
Deployability,"`ValidateVariants` requires a large amount of memory (>16Gb) to validate a GVCF when another GVCF is used as the interval list. This is not the case if a regular interval list is used instead. This comes up in the production `ReblockGVCFs` pipeline since we validate the reblocked GVCF using the input (unreblocked) GVCF as the interval list to validate over (with `-L`). For now we can just use larger memory machines to run this tool, but it is confusing to me why using a ~4Gb GVCF as an interval list would cause such a large increase in memory requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8608:240,pipeline,pipeline,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8608,1,['pipeline'],['pipeline']
Deployability,"``; #!/bin/bash. export HADOOP_CONF_DIR=/etc/hadoop; export HADOOP_HOME=/mnt/hadoop-latest; export JAVA_HOME=/mnt/jre1.8.0_192; export SPARK_HOME=/mnt/spark-2.3.1-bin-without-hadoop; export HADOOP_USER_NAME=hadoop. # export SPARK_DIST_CLASSPATH=$($HADOOP_HOME/bin/hadoop classpath). TEST_DIR=""hdfs://cromwellhadooptest:8020/user/hadoop/gatk/small""; COMMON_DIR=""hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common""; INPUT_DIR=""$TEST_DIR/input""; OUTPUT_DIR=""$TEST_DIR/output"". input_bam=""$INPUT_DIR/small_CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam""; output_vcf_basename=""$OUTPUT_DIR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21"". ref_fasta=""$COMMON_DIR/human_g1k_v37.20.21.fasta""; known_sites=""$COMMON_DIR/dbsnp_138.b37.20.21.vcf"". gatk ReadsPipelineSpark \; -R ${ref_fasta} \; -I ${input_bam} \; -O ${output_vcf_basename}.vcf \; --known-sites ${known_sites} \; -pairHMM AVX_LOGLESS_CACHING \; --spark-verbosity DEBUG \; -- --spark-runner SPARK --spark-master yarn-cluster \; # --conf 'spark.submit.deployMode=cluster'; ```. #### Expected behavior. ReadsPipelineSpark should be able to resolve the hdfs file path: `hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. #### Actual behavior; The tool tries to access: `file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta` even when the input is: `hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. Verified that the file is accesible through hdfs:; ```; (gatk) root@2e738717b9c1:/gatk/mnt# $HADOOP_HOME/bin/hdfs dfs -ls hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta; -rw-r--r-- 3 hadoop supergroup 113008112 2020-07-29 15:54 hdfs://cromwellhadooptest:8020/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta; ```; When I specify input as: `hdfs://cromwellhadooptest/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`, (i.e. without the port) I get the same error. **Stack trace for this**:; ```; *******************************************************************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730:1414,deploy,deployMode,1414,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730,1,['deploy'],['deployMode']
Deployability,"``; #### Actual behavior. Extra 'C'. ---. @ldgauthier commented on [Thu Oct 06 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252063664). I think this happened when there was a AC ->C deletion originally called that got subset out because it didn't meet the QUAL threshold, but the trimming that was done on the final allele set didn't work right because of the *. ---. @vdauwera commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252204232). Do we have GVCF snippets to reproduce this? . ---. @ldgauthier commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496). I don't know what intermediates we save on the cloud but maybe @yfarjoun is willing to help. ---. @yfarjoun commented on [Fri Oct 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2959:1772,pipeline,pipeline-help,1772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959,1,['pipeline'],['pipeline-help']
Deployability,"```; ......AAAAAAAAAA......; ```; Here we have a homology of exactly 10A's.; When we detect the deletion by studying the alignment signature, the alt haplotype would have two alignments mapped to the reference, one ends just before the G-block, one starts just after the G-block, with the A-block on the alt haplotype mapped to two places.; We follow the left-align/left-justify convention, and place the POS 1-bp before the left most A (hence saying `10A10G` was deleted, as opposed to right-justify which would say `10G10A` deleted, in fact without the convention any contiguous substring of 20 bp long of `10A10G10A` would be correct). However, it can be imagined the homologous sequences flanking the G's are not exactly the same, or may not be the same length (small indels), and the alignments would contain small gaps in their CIGARs. By assuming the homologous sequence are of the same length, which is what we are doing now, we could get the breakpoint location wrong. This is generally not a serious problem, but when the accumulated gap sizes are large enough, we can end up too-far off. A similar issue is when inferring SVLEN for small tandem duplications, where we are assuming the extra copies have the same length. This is not always true and when the `DUP_SEQ_CIGARS` annotation is available, it should be easily fixable. When it is not available, one could use the difference between `SEQ_ALT_HAPLOTYPE` and END-POS for that. #### Steps to reproduce; Run the `StructuralVariationDiscoveryPipelineSpark` pipeline on a site with SV event having different homology length around the breakpoints. #### Expected behavior; Breakpoint inference taking into account of small indels in the micro-homology surrounding the breakpoints. #### Actual behavior; Breakpoint inference assuming homologous sequence surrounding the breakpoints having the same length. #### What could be done:; The inference code could use CIGAR's in the alignment to infer how much to left adjust the breakpoint. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4883:2089,pipeline,pipeline,2089,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4883,1,['pipeline'],['pipeline']
Deployability,```; ./gatk-launch FlagStat; ```. gives me this. ```; Running:; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk FlagStat; ***********************************************************************. A USER ERROR has occurred: Invalid command line: Argument input was missing: Argument 'input' must be specified at least once.; ```. it needs to add something about -h to see all arguments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1285:107,install,install,107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1285,1,['install'],['install']
Deployability,```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:937,deploy,deploy,937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,```; // Suggested by the akka devs to make sure that we do not get the spark configuration error.; // http://doc.akka.io/docs/akka/snapshot/general/configuration.html#When_using_JarJar__OneJar__Assembly_or_any_jar-bundler; transform(com.github.jengelman.gradle.plugins.shadow.transformers.AppendingTransformer) {; resource = 'reference.conf'; }; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1447:77,configurat,configuration,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1447,2,['configurat'],['configuration']
Deployability,```; 18/01/17 08:24:55 INFO org.bdgenomics.adam.serialization.ADAMKryoRegistrator: Did not find Spark internal class. This is expected for Spark 1.; ``` . Appears hundreds of times in the log output since we upgraded ADAM. This is despite the fact that we're not using spark 1. Maybe there's a way to silence a specific logger?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4186:208,upgrade,upgraded,208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4186,1,['upgrade'],['upgraded']
Deployability,"```; ERROR StatusLogger No log4j2 configuration file found. Using default configuration: logging only errors to the console.; ```. shows up at the top of every run, we should fix this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/216:34,configurat,configuration,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/216,2,['configurat'],['configuration']
Deployability,```; build/install/hellbender/bin/hellbender CollectWgsMetrics -I src//test/resources/org/broadinstitute/hellbender/tools/exome/exome-read-counts-NA12878.bam -R src//test/resources/org/broadinstitute/hellbender/tools/exome/test_reference.fasta -O CollectWGSMetrics.txt; ```. ```; java.lang.ArrayIndexOutOfBoundsException: 1000; at org.broadinstitute.hellbender.tools.picard.analysis.directed.CollectWgsMetrics.doWork(CollectWgsMetrics.java:161); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:51); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); ```. note: same bug is present in picard 1.138,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/918:11,install,install,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/918,1,['install'],['install']
Deployability,```; gradle clean installSpark; ./gatk-launch FlagStatSpark -I src/test/resources/org/broadinstitute/hellbender/tools/count_bases.bam ; Missing GATK wrapper script: /Users/droazen/src/hellbender/build/install/gatk/bin/gatk; To generate the wrapper run:. /Users/droazen/src/hellbender/gradlew installDist; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1314:18,install,installSpark,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314,3,['install'],"['install', 'installDist', 'installSpark']"
Deployability,"```; spark-submit --class org.broadinstitute.hellbender.Main \; --deploy-mode client \; --master yarn-client \; --driver-memory 8G \; --conf spark.driver.maxResultSize=0 \; --conf spark.driver.userClassPathFirst=true \; --conf spark.executor.userClassPathFirst=true \; --conf spark.io.compression.codec=lzf \; --conf spark.yarn.executor.memoryOverhead=600 \; --executor-memory ${execMem}g \; --num-executors $execs \; --executor-cores $cores \; bin/cleanHellbender/gatk/build/libs/gatk-all-*-spark.jar \; ReadsPipelineSpark \; --sparkMaster yarn-client \; -I hdfs:///user/akiezun/CEUTrio.HiSeq.WEx.b37.NA12892.bam \; -R hdfs:///user/droazen/bqsr/human_g1k_v37.2bit \; --programName ${name} \; -O $bamout \; --knownSites hdfs:////user/akiezun/dbsnp_138.b37.excluding_sites_after_129.vcf \; --emit_original_quals \; --duplicates_scoring_strategy SUM_OF_BASE_QUALITIES; ```. exec=24; cores=5; execMem=25. fails with . ```; java.lang.IllegalArgumentException: SimpleInterval is 1 based, so start must be >= 1, start: 0; at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:58); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:33); at org.broadinstitute.hellbender.utils.baq.BAQ.getReferenceWindowForRead(BAQ.java:525); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:46); at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine$BQSRReferenceWindowFunction.apply(BaseRecalibrationEngine.java:41); at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithRefBases.lambda$addBases$c54addeb$1(BroadcastJoinReadsWithRefBases.java:52); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.coll",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1234:66,deploy,deploy-mode,66,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1234,1,['deploy'],['deploy-mode']
Deployability,"`ah_var_store` edition: Allows hard-filtering based on a maximum number of alt alleles [VS-1334], as well as fixing GATK Docker image building to use image IDs rather than git hashes [VS-1357]. Integration test _mostly_ successful [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/5d021859-5971-4fd7-8451-086c224fdb00). `GvsQuickstartIntegration` failed with:. ```; The bytes observed (89733530) for 'ExtractFilterTask.GvsCreateFilterSet.BigQuery Query Scanned' differ from those expected (85119360); FAIL!!! The relative difference between these is 0.0514208, which is greater than the allowed tolerance (0.05); ```. Successful tieout run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/04b840f9-9779-48d6-8faa-4425d67ddadb). [VS-1334]: https://broadworkbench.atlassian.net/browse/VS-1334?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [VS-1357]: https://broadworkbench.atlassian.net/browse/VS-1357?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8806:194,Integrat,Integration,194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8806,1,['Integrat'],['Integration']
Deployability,`com.google.cloud:google-cloud-nio:0.20.4-alpha-20171031.194208-5:shaded` -> `org.broadinstitute:google-cloud-nio-GATK4-custom-patch:0.20.4-alpha-GCS-RETRY-FIX:shaded`; This artifact should be functionally identical to our previous artifact but it's hosted in maven central instead of our artifactory. closes #4008,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4108:127,patch,patch,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4108,1,['patch'],['patch']
Deployability,"`gatk-launch DownsampleSam --help`. ```; /local/dev/akiezun/alphaTesting/gatk/build/install/gatk/bin/gatk DownsampleSam --help |; org.broadinstitute.hellbender.exceptions.GATKException$CommandLineParserInternalException: [R, reference] has already been used. |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.handleArgumentAnnotation(CommandLineParser.java:620) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.createArgumentDefinitions(CommandLineParser.java:149) |; at org.broadinstitute.hellbender.cmdline.CommandLineParser.<init>(CommandLineParser.java:134) |; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:186) |; at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:41) |; at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66) |; at org.broadinstitute.hellbender.Main.main(Main.java:81); ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1303:84,install,install,84,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1303,1,['install'],['install']
Deployability,`testGetReadsFromHadoopBam` broke when we updated dataflow. I've placed this test into the new group 'broken-spark` which will be ignored in travis. When https://github.com/cloudera/spark-dataflow/issues/49 is complete we should re-enable this test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/581:42,update,updated,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/581,1,['update'],['updated']
Deployability,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:393,install,installed,393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['install'],['installed']
Deployability,"a commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260457641). @ldgauthier Now that we have allele-specific VQSR, do we still need this ticket? . ---. @ldgauthier commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260462851). Yes we do. Sad but true. It's something we've been focusing on with the new VQSR model work, though. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260467426). Ah, ok. So keep it in the GATK3 repo then, for now? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-260642385). Yes. I believe @lucidtronix is doing VQSR development in GATK3 and we'll port later. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505). @ldgauthier @lucidtronix Any update on this since I heard VQSR got ported to GATK4?. ---. @ldgauthier commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287905381). It's unlikely the behavior has changed. For gnomad we used hard filters to; address the problem, which is probably a good global recommendation. On Mar 20, 2017 1:35 PM, ""Geraldine Van der Auwera"" <; notifications@github.com> wrote:. > @ldgauthier <https://github.com/ldgauthier> @lucidtronix; > <https://github.com/lucidtronix> Any update on this since I heard VQSR; > got ported to GATK4?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/868#issuecomment-287837505>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AGRhdLRwdezIkmt3uPqIABWLggVjRN3yks5rnrjegaJpZM4Dt4t7>; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/iss",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2508:8205,update,update,8205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2508,1,['update'],['update']
Deployability,a:109); at org.seqdoop.hadoop_bam.BCFSplitGuesser.<init>(BCFSplitGuesser.java:89); at org.seqdoop.hadoop_bam.VCFInputFormat.addGuessedSplits(VCFInputFormat.java:254); at org.seqdoop.hadoop_bam.VCFInputFormat.fixBCFSplits(VCFInputFormat.java:242); at org.seqdoop.hadoop_bam.VCFInputFormat.getSplits(VCFInputFormat.java:221); at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1910); at org.apache.spark.rdd.RDD.count(RDD.scala:1121); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:445); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:47); at org.broadinstitute.hellbender.tools.spark.pipelines.CountVariantsSpark.runTool(CountVariantsSpark.java:39); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:69); at org.broadinstitute.hellbender.Main.main(Main.java:84),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1815:1858,pipeline,pipelines,1858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1815,1,['pipeline'],['pipelines']
Deployability,"ab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remotes"", repos=""https://cloud.r-project.org/""); > library(remotes); > install_version(""scales"", version=""1.2.1"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.2.1’; > quit(); $ gatk VariantRecalibrator [arguments omitted for brevity]; $; ```. #### Expected behavior; The output rscript file is used to generate a PDF. #### Actual behavior; Generation of the PDF fails due to an deprecation in the `scales` library causing the `Rscript` command to abort.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664:1978,install,install,1978,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664,1,['install'],['install']
Deployability,add CreateVariantIngestFiles integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7071:29,integrat,integration,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7071,1,['integrat'],['integration']
Deployability,add a continuous test in jenkins to ensure that BQSR will continue to run in < 3.5 GB,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1594:6,continuous,continuous,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1594,1,['continuous'],['continuous']
Deployability,add bwa to reads pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1981:17,pipeline,pipeline,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1981,1,['pipeline'],['pipeline']
Deployability,add echocallset as an option for integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8757:33,integrat,integration,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8757,1,['integrat'],['integration']
Deployability,add in service account auth for aou; localize with service account in same vm; update disk size; fix input_vcf to work with manual localization and streaming,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7140:79,update,update,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7140,1,['update'],['update']
Deployability,add scoring strategy for mark dups in Spark. Thus fixes 1 integration…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1156:58,integrat,integration,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1156,1,['integrat'],['integration']
Deployability,add some metrics to the pipeline to see how well that scales. Metrics should come cheaply because the data RDD is loaded at that point.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1237:24,pipeline,pipeline,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1237,1,['pipeline'],['pipeline']
Deployability,"add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!. at htsjdk.samtools.SAMFileHeader.addProgramRecord(SAMFileHeader.java:202). at htsjdk.samtools.SAMTextHeaderCodec.parsePGLine(SAMTextHeaderCodec.java:158). at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:107). at htsjdk.samtools.SAMFileHeader.clone(SAMFileHeader.java:398). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriterFromFactory(ReadUtils.java:1215). at org.broadinstitute.hellbender.utils.read.ReadUtils.createCommonSAMWriter(ReadUtils.java:1163). at org.broadinstitute.hellbender.utils.haplotype.SAMFileDestination.(SAMFileDestination.java:35). at org.broadinstitute.hellbender.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:1754,patch,patches,1754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['patch'],['patches']
Deployability,"added bcftools, upgraded gcloud version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7369:16,upgrade,upgraded,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7369,1,['upgrade'],['upgraded']
Deployability,"added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO Nat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:1037,install,install,1037,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,2,['install'],"['install', 'installation']"
Deployability,adding snappy.disable=true to prevent htsjdk from using snappy; this is a temporary solution for #2026 until we can patch htsjdk to fix incompatibilities with more modern snappy. fixes #2026,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2028:116,patch,patch,116,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2028,1,['patch'],['patch']
Deployability,"addresses specops issues:; - #268 https://github.com/broadinstitute/dsp-spec-ops/issues/268; - #270 https://github.com/broadinstitute/dsp-spec-ops/issues/270. similar changes as the PR for ExtractCohort:; - added custom classes `ExtractFeaturesRecord` that implements `Locatable`; - refactored attribute building from these records; - now that the records are `Locatable`s, can use `OverlapDetector` to filter locations down to only desired intervals (including excluded sites). also:; - enable using intervals input (-L) rather than specifying min-location and max-location. updated WDL to support scattering using SplitIntervals (based off of CohortExtract); - add back AS_QD to headers (currently headers are shared between ExtractCohort and ExtractFeatures - AS_QD not needed for cohort but is needed for features)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7184:576,update,updated,576,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7184,1,['update'],['updated']
Deployability,"ader.getFeatureReader(AbstractFeatureReader.java:110); 	at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:74); 	at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:58); 	at org.broadinstitute.hellbender.tools.walkers.mutect.CreateSomaticPanelOfNormals.doWork(CreateSomaticPanelOfNormals.java:122); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:173); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); Caused by: htsjdk.tribble.TribbleException$InvalidHeader: Your input file has a malformed header: We never saw the required CHROM header line (starting with one #) for the input VCF file; 	at htsjdk.variant.vcf.VCFCodec.readActualHeader(VCFCodec.java:115); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:83); 	at htsjdk.tribble.AsciiFeatureCodec.readHeader(AsciiFeatureCodec.java:36); 	at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureReader.java:251); 	... 12 more; ```. Either we should update the documentation to point users to use the `--arguments_file` instead (let me know) or we should update the functionality to accept a list of files. Currently, the option is described as:. > --vcfsListFile,-vcfs:File VCFs for samples to include. May be specified either one at a time, or as one or more .list file containing multiple VCFs, one per line. This argument must be specified at least once. Required. . Here is the test data:; [createsomaticpanelofnormals_testcase_shlee.zip](https://github.com/broadinstitute/gatk/files/1251993/createsomaticpanelofnormals_testcase_shlee.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3510:4685,update,update,4685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3510,2,['update'],['update']
Deployability,"after the addition of travis_wait gradle check travis was executing gradle check twice; with ./gradlew and one with gradle. This was happening because it was being run onces explicitly in the install phase and once implicitly in the script phase. this was causing some strange issues. instead of using travis_wait, I made the dataflow cloud tests output status information as they go; this similarly prevents travis from timing out due to lack of output, but doesn't need the travis_wait; if we add any really long running dataflow tests, or really highly spammy ones we'll have to revisit this",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/853:192,install,install,192,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/853,1,['install'],['install']
Deployability,"age](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to speci",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1515,configurat,configuration,1515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,1,['configurat'],['configuration']
Deployability,ah update array extract tool,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6827:3,update,update,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6827,1,['update'],['update']
Deployability,ah-mito-update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6867:8,update,update,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6867,1,['update'],['update']
Deployability,alFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSpa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1505,deploy,deploy,1505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,2,['deploy'],['deploy']
Deployability,all picard tools should be synched before we go to alpha release. We are now at some version and need to sync to the latest available by the alpha release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/705:57,release,release,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/705,2,['release'],['release']
Deployability,allerSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:01:43.730 INFO HaplotypeCallerSpark - Deflater: IntelDeflater; 19:01:43.730 INFO HaplotypeCallerSpark - Inflater: IntelInflater; 19:01:43.730 INFO HaplotypeCallerSpark - GCS max retries/reopens: 20; 19:01:43.730 INFO HaplotypeCallerSpark - Requester pays: disabled; 19:01:43.730 WARN HaplotypeCallerSpark - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:4062,configurat,configuration,4062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['configurat'],['configuration']
Deployability,"allow for gatk to be overridden, update with known good jar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7758:33,update,update,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7758,1,['update'],['update']
Deployability,also updating build to extract the intelDeflater from the jar instead of downloading it; removed the htsjdkVersion property I had recently added since it was no longer necessary with the update to include the .so in the jar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1706:187,update,update,187,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1706,1,['update'],['update']
Deployability,"am.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-08 09:14 output.bam.parts/_SUCCESS; -rw-r--r-- 3 yaron yaron 62019 2017-06-08 09:14 output.bam.parts/part-r-00000.bam; -rw-r--r-- 3 yaron yaron 16 ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:6305,deploy,deploy,6305,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['deploy'],['deploy']
Deployability,am.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:10091,deploy,deploy,10091,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['deploy'],['deploy']
Deployability,am.java:119); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:233); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:5864,deploy,deploy,5864,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['deploy'],['deploy']
Deployability,am.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:43732,deploy,deploy,43732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['deploy'],['deploy']
Deployability,am.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:11551,deploy,deploy,11551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['deploy'],['deploy']
Deployability,"am.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:17562,deploy,deploy,17562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['deploy'],['deploy']
Deployability,"ample. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by proj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:1601,release,releases,1601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['releases']
Deployability,"ang.reflect.InvocationTargetException; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.lang.invoke.SerializedLambda.readResolve(SerializedLambda.java:230); at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:3744,pipeline,pipelines,3744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,1,['pipeline'],['pipelines']
Deployability,"annoyingly, spark UI and Hadoop UI claim the job succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1723:650,pipeline,pipelines,650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723,1,['pipeline'],['pipelines']
Deployability,anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349:3501,Integrat,IntegrationTestSpec,3501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349,1,['Integrat'],['IntegrationTestSpec']
Deployability,"antWalker` that inputs a bam and a vcf and outputs the bases of all alt reads in the `ReadsContext` at each variant; 2) send these to an external alignment program; 3) read in the alignments in a GATK tool and filter accordingly. The ambitious version is to write our own simple aligner, eg a kmer-based method like BLAT or BBMap but with all the messy parts for handling big indels, RNA, and proteins removed. Writing our own BWA aligner would be wildly impractical. @takutosato @LeeTL1220 keeping you in the loop. ---. @davidbenjamin commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-275483449). *Even better*: rely on someone else in the group, such as Ted, to write a Java binding for BWA in memory. See broadinstitute/gatk#2367. ---. @davidbenjamin commented on [Sun Apr 23 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296515266). So. . . given that our pipeline aligns with BWA, it might seem like this is just a redundant and laborious rehashing of the mapping quality score. *However*, the mapping quality only considers multi-mapping within the reference, and therefore doesn't account for mapping errors due to incompleteness of the reference. That is, reads from genomic regions that are not part of the reference (because they're hard to assemble, like centromeres etc) might map well to a unique regions within the reference, and therefore will have fine mapping quality even though they are artifacts. There are published ""decoy genomes"" -- essentially pseudo-contigs of regions missing from the reference, and mapping with BWA in memory to *those* might be very helpful. So, we need to: 1) get our hands on a decoy genome that will play nicely with BWA, and 2) talk to the SV team. ---. @ldgauthier commented on [Mon Apr 24 2017](https://github.com/broadinstitute/gatk-protected/issues/844#issuecomment-296675311). To be pedantic, the mapping quality also considers how well the read aligns; to its",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:1330,pipeline,pipeline,1330,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['pipeline'],['pipeline']
Deployability,"ariants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2545,update,update,2545,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['update'],['update']
Deployability,"ark.driver.userClassPathFirst=false,spark.io.compression.codec=lzf,spark.yarn.executor.memoryOverhead=600,spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 ,spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --jar gs://broad-dsde-methods/shuang/tmp/gatk-jars/gatk-spark_5710525a8758807e46bbb660ac998e63.jar -- PrintReadsSpark -I hdfs://shuang-small-m:8020/data/HG00512.cram.samtools1_9.bam -O hdfs://shuang-small-m:8020/results/temp.bam -L hdfs://shuang-small-m:8020/data/intervals.bed --spark-master yarn; Job [5838bd7dec2d4533ad090ce03ecc7c0c] submitted.; Waiting for job output...; 18/07/24 21:02:03 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 21:02:08.430 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:02:08.594 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/tmp/5838bd7dec2d4533ad090ce03ecc7c0c/gatk-spark_5710525a8758807e46bbb660ac998e63.jar!/com/intel/gkl/native/libgkl_compression.so; 21:02:08.889 INFO PrintReadsSpark - ------------------------------------------------------------; 21:02:08.890 INFO PrintReadsSpark - The Genome Analysis Toolkit (GATK) v4.0.6.0-26-g3979bdb-SNAPSHOT; 21:02:08.890 INFO PrintReadsSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:02:08.890 INFO PrintReadsSpark - Executing as root@shuang-small-m on Linux v3.16.0-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:3591,configurat,configuration,3591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['configurat'],['configuration']
Deployability,"ark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true --num-executors 5 --executor-cores 2 --executor-memory 1g /home/yaron/gatk/build/libs/gatk-spark.jar PrintReadsSpark -I NA12878.chr17_69k_70k.dictFix.bam -O /user/yaron/output.bam --sparkMaster yarn; 09:14:13.551 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yaron/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; [June 8, 2017 9:14:13 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /user/yaron/output.bam --input NA12878.chr17_69k_70k.dictFix.bam --sparkMaster yarn --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [June 8, 2017 9:14:13 AM CST] Executing as yaron@dn1 on Linux 4.4.0-31-generic amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.alpha.2-281-g752d020-SNAPSHOT; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 09:14:13.567 INFO PrintReadsSpark - HTSJDK Defaults.USE_ASYNC_IO_W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:1568,pipeline,pipelines,1568,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['pipeline'],['pipelines']
Deployability,"ark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; deploy:; mode: global; restart_policy:; condition: on-failure; tty: true #keeps the container alive; volumes:; - reference-image:/reference_image. volumes:; reference-image:. networks:; workbench:; external: true; ```; - Hadoop:; ```; version: '3'; services:; namenode:; image: bde2020/hadoop-namenode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - namenode:/hadoop/dfs/name; environment:; - CLUSTER_NAME=test; env_file:; - ./hadoop.env; deploy:; mode: replicated; replicas: 1; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50070; ports:; - 8334:50070; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/output/:/output/; - /data/ngs/:/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:1828,deploy,deploy,1828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['deploy'],['deploy']
Deployability,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:43585,deploy,deploy,43585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['deploy'],['deploy']
Deployability,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.try,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:11404,deploy,deploy,11404,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['deploy'],['deploy']
Deployability,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:17415,deploy,deploy,17415,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['deploy'],['deploy']
Deployability,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:6158,deploy,deploy,6158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['deploy'],['deploy']
Deployability,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstanc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:9944,deploy,deploy,9944,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['deploy'],['deploy']
Deployability,arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:233); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLik,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:5717,deploy,deploy,5717,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['deploy'],['deploy']
Deployability,"arkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3154,deploy,deploy,3154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['deploy'],['deploy']
Deployability,at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:4793,Continuous,ContinuousBuildActionExecuter,4793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,3,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,ata/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  Nat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:8662,pipeline,pipeline,8662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"ated a new SpanningDeletionRecord as a subclass of ReferenceRecord but allows us to store GT and GQ; 2. in handlePotentialSpanningDeletion when processing a deletion, we create a new SpanningDeletionRecord with the correct GT and GQ based on the deletion; 3. When processing reference data at a variant site, return ReferenceRecords/SpanningDeletionRecord instead of just a string ""state"" since we need more than just state now; 4. Because of the above, we are now returning an object for the inferred state instead of just a string. Since the inferred state is so, so common a singleton InferredReferenceRecord was created; 5. Processing of spanning deletions beyond. **Ugly**; 1. The construction of the singleton is ugly because it _requires_ a location even though we don't for this case. We could go to an tagging interface (like Cloneable) these all implement, but that seems ugly also. *Refactoring Changes*; One of the challenges with this PR was testing as the work is really done in the lower-level methods and it would be nice to have this as a unit test rather than an integration/end-to-end test. This motivated the following changes:. 1. don't write to VCF directly, instead have take a Consumer<VariantContext> to emit VariantContexts. This let's us provide a different consumer in unit tests to collect our result.; 2. we previously had a chain of calls createVariantsFromSortedRanges -> processSampleRecordsForLocation -> finalizeCurrentVariant that returned void and as a side effect wrote to VCF. These deeper methods now return a VariantContext and the writing (via consumer) is done higher up in the call stack; 3. made some private methods package-private so we could call them from tests. **Thinking Out Loud**. We have three different sets of datastructures for the same data, some of this is history, some is performance/memory, but could use some rethinking; 1. GenericRecord (pulling from BQ); 2. ReferenceRecord/SpanningDeletionRecord (in memory data structure for referenc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7857:1118,integrat,integration,1118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7857,1,['integrat'],['integration']
Deployability,"ation - Start Date/Time: June 15, 2018 3:42:33 PM PDT; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.116 INFO VariantFiltration - ------------------------------------------------------------; 15:42:34.117 INFO VariantFiltration - HTSJDK Version: 2.15.1; 15:42:34.118 INFO VariantFiltration - Picard Version: 2.18.2; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:42:34.118 INFO VariantFiltration - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:42:34.118 INFO VariantFiltration - Deflater: IntelDeflater; 15:42:34.119 INFO VariantFiltration - Inflater: IntelInflater; 15:42:34.119 INFO VariantFiltration - GCS max retries/reopens: 20; 15:42:34.119 INFO VariantFiltration - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:42:34.119 INFO VariantFiltration - Initializing engine; 15:42:34.634 INFO FeatureManager - Using codec VCFCodec to read file file:///Users/sherlock/dev/Bhatt_lab/crassphage/variants/6753_12-15-2015_first_pass_filtered.vcf; 15:42:34.663 INFO VariantFiltration - Done initializing engine; 15:42:34.750 INFO ProgressMeter - Starting traversal; 15:42:34.750 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 15:42:34.781 INFO VariantFiltration - Shutting down engine; [June 15, 2018 3:42:34 PM PDT] org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=342884352; java.lang.NumberFormatException: For input string: ""26.67""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Long.parseLong(Long.java:589); 	at java.lang.Long.parseLo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4921:2885,patch,patch,2885,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4921,1,['patch'],['patch']
Deployability,"ator). \--. REQUIRED for all errors and issues: ; ; a) GATK version used:v4.2.6.1  ; ; b) Exact command used: see below ; ; c) Entire program log: see below ; ; **How can I assign a temp directory and won't get the bug?**. I always got error when I assigned the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  Native",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:1322,pipeline,pipeline,1322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,aultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:4831,Continuous,ContinuousBuildActionExecuter,4831,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,3,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,b succeeds. see output here: https://console.cloud.google.com/dataproc/jobs/813fba95-8b19-4b4d-9cd1-b5bb94e4e52c?project=broad-dsde-dev&authuser=1&graph=GCE_CPU&duration=PT1H&tab=output. ```; java.lang.ArrayIndexOutOfBoundsException: 18; at org.broadinstitute.hellbender.utils.recalibration.EventType.eventFrom(EventType.java:35); at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generateReportTables(RecalUtils.java:255); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:50); at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:109); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1723:1675,deploy,deploy,1675,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1723,6,['deploy'],['deploy']
Deployability,"b10; 05:06:55.410 INFO SelectVariants - Start Date/Time: November 6, 2019 5:06:54 AM EST; 05:06:55.410 INFO SelectVariants - ------------------------------------------------------------; 05:06:55.410 INFO SelectVariants - ------------------------------------------------------------; 05:06:55.411 INFO SelectVariants - HTSJDK Version: 2.16.0; 05:06:55.411 INFO SelectVariants - Picard Version: 2.18.7; 05:06:55.411 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 05:06:55.411 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 05:06:55.412 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 05:06:55.412 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 05:06:55.412 INFO SelectVariants - Deflater: IntelDeflater; 05:06:55.412 INFO SelectVariants - Inflater: IntelInflater; 05:06:55.412 INFO SelectVariants - GCS max retries/reopens: 20; 05:06:55.412 INFO SelectVariants - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 05:06:55.412 INFO SelectVariants - Initializing engine; 05:06:55.796 INFO FeatureManager - Using codec VCFCodec to read file file:///disk/juntong/huada/V300029595_results/mutect_outputs/V300029595.merged.vcf.gatk.somatic.vcf.gz; 05:06:55.962 INFO SelectVariants - Done initializing engine; 05:06:56.099 INFO ProgressMeter - Starting traversal; 05:06:56.099 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:39:06.894 INFO SelectVariants - Shutting down engine; [November 6, 2019 6:39:06 AM EST] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 92.20 minutes.; Runtime.totalMemory()=29215424512; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.grow(ArrayList.java:265); at java.util.ArrayList.ensureExplicitCapacit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6254:2954,patch,patch,2954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6254,1,['patch'],['patch']
Deployability,"bam_index, and normal_sample_name: BAM, index and sample name for the normal sample (optional if running tumor-only); ##; ## ** Primary resources ** (optional but strongly recommended); ## pon, pon_index: optional panel of normals in VCF format containing probable technical artifacts (false positves); ## gnomad, gnomad_index: optional database of known germline variants (see http://gnomad.broadinstitute.org/downloads); ## variants_for_contamination, variants_for_contamination_index: VCF of common variants with allele frequencies fo calculating contamination; ##; ## ** Secondary resources ** (for optional tasks); ## onco_ds_tar_gz, default_config_file: Oncotator datasources and config file; ## sequencing_center, sequence_source: metadata for Oncotator; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested.; ##; ## Cromwell version support ; ## - Successfully tested on v27; ##; ## LICENSING : ; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in ; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may ; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker ; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information ; ## pertaining to the included programs. workflow Mutect2 {; # Runtime; String gatk4_jar; File picard_jar; String m2_docker; String oncotator_docker; Int preemptible_attempts; # Workflow options; Int scatter_count; File? intervals ; Array[String] artifact_modes; String? m2_extra_args; String? m2_extra_filtering_args; Boolean is_run_orientation_bias_filter; Boolean is_run_oncotator; # Primary inputs ; File ref_fasta; File ref_fasta_index; File ref_dict; File tumor_bam; File tumor_bam_index; String tumor_sample_name; File? normal_bam; File? normal_bam_index; String",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3341:2853,release,released,2853,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3341,1,['release'],['released']
Deployability,"ber 2, 2018 3:00:35 PM JST; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.945 INFO GenomicsDBImport - ------------------------------------------------------------; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Version: 2.16.0; 15:00:35.946 INFO GenomicsDBImport - Picard Version: 2.18.7; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:35.946 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:35.946 INFO GenomicsDBImport - Deflater: IntelDeflater; 15:00:35.946 INFO GenomicsDBImport - Inflater: IntelInflater; 15:00:35.946 INFO GenomicsDBImport - GCS max retries/reopens: 20; 15:00:35.946 INFO GenomicsDBImport - Using google-cloud-java fork https://github.com/broadinstitute/google-cloud-java/releases/tag/0.20.5-alpha-GCS-RETRY-FIX; 15:00:35.946 INFO GenomicsDBImport - Initializing engine; 15:00:38.360 INFO IntervalArgumentCollection - Processing 58617616 bp from intervals; 15:00:38.366 INFO GenomicsDBImport - Done initializing engine; Created workspace /work/Analysis/wgs_chr19; 15:00:38.849 INFO GenomicsDBImport - Vid Map JSON file will be written to /work/Analysis/wgs_chr19/vidmap.json; 15:00:38.849 INFO GenomicsDBImport - Callset Map JSON file will be written to /work/Analysis/wgs_chr19/callset.json; 15:00:38.849 INFO GenomicsDBImport - Complete VCF Header will be written to /work/Analysis/wgs_chr19/vcfheader.vcf; 15:00:38.850 INFO GenomicsDBImport - Importing to array - /work/Analysis/wgs_chr19/genomicsdb_array; 15:00:38.850 INFO ProgressMeter - Starting traversal; 15:00:38.850 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 15:00:39.771 INFO GenomicsDBImport - Importing batch 1 with 5 samples; Buffer resized from 28469byt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5342:2300,release,releases,2300,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5342,1,['release'],['releases']
Deployability,biz.k11i:xgboost-predictor:0.3.0 is now ported to the h2oai repo and released to Maven Central?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7839:69,release,released,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7839,1,['release'],['released']
Deployability,"bly well without this, though.; - [x] Evaluate algorithm on simulated data.; - Implemented simple Queue pipeline for running CLI on simulated ACNV segment files. Takes <2 minutes for ~1000 iterations for each sample, can run 100s of samples in parallel on the gsa clusters.; - Need to write up some scripts to automatically calculate and plot metrics.; - [x] Evaluate algorithm on real data; - Some initial runs on HCC1143 purity series show reasonable results for the clonal model, i.e., purity is recovered within credible intervals (question: what are the error bars on the purities of the samples?). Subclonal performance is a little less clear due to 1) no real ground truth, 2) events in the normal, and 3) lack of outlier absorption.; - Can we get a hold of some cleaner purity series?; - [ ] Document algorithm in technical whitepaper. ---. @samuelklee commented on [Thu Dec 08 2016](https://github.com/broadinstitute/gatk-protected/issues/750#issuecomment-265798051). The first release of this tool will most likely include the following:. - Some refactoring to MCMC package and addition of an EnsembleSampler, which implements affine-invariant ensemble sampling from Goodman & Weare 2010 (this is the same method used by the emcee python package). This method is critical for sampling our highly multimodal posterior well. - Output of 1) all population fraction / ploidy MCMC samples, and 2) average variant profile and 3) posterior summaries at the posterior mode (determined by naive binning of samples). - No plotting. Early next quarter:. - [ ] Unit tests for EnsembleSampler. - [ ] Allowing for >1 tumor population. The model already allows for this, but some performance optimization of the variant-profile sampling step will probably be required. - [ ] Evaluation on BAMs prepared with mixing scripts. - [ ] Writeup of model in technical white paper. - [ ] Tool to produce interactive plots. I think this is necessary to represent the uncertainty in ""solutions"" produced by the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2909:2507,release,release,2507,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2909,1,['release'],['release']
Deployability,"both of these are updates to the ImportGenomes wdl:; - reduce memory/cpus for the CreateImportTsvs task from 10GB to 3.75GB and 2 CPU to 1 CPU. these settings were tested on 3000 gvcfs and none errored out because of memory. this ties out spec-ops issues #211 and #233; - before loading files using `bq load`, check for existing files in the gs bucket. only run `bq load` if there are matching files in the bucket. this will prevent an error if you run a subset of samples corresponding to a larger sample map such that you've created a pet_002 table but there aren't any samples to load for pet_002 yet. this was tested in Terra and worked as expected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7121:18,update,updates,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7121,1,['update'],['updates']
Deployability,"bug reported on `AF=.`, _FilterMutectCalls_ seems to complain about MPOS fields having a value of `.`. No intermediate processing was done between _Mutect2_ and _FilterMutectCalls_. Below the error stack trace :. ```; 17:13:28.491 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data-ddn/home/anthony/sbx/mutect2/work/conda/gatk4-mutect2-nf-bcf605d6af4c0524a368d3d105898641/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:13:30.503 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.503 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 17:13:30.504 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:13:30.504 INFO FilterMutectCalls - Executing as anthony@node063 on Linux v2.6.32-220.el6.x86_64 amd64; 17:13:30.504 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 17:13:30.504 INFO FilterMutectCalls - Start Date/Time: February 17, 2019 5:13:28 PM CET; 17:13:30.504 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 17:13:30.505 INFO FilterMutectCalls - Picard Version: 2.18.25; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INFO FilterMutectCalls - Deflater: IntelDeflater; 17:13:30.506 INFO FilterMutectCalls - Inflater: IntelInflater; 17:13:30.506 INFO FilterMutectCalls - GCS max retries/reopens: 20; 17:13:30.506 INFO FilterMut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5684:1194,release,release-,1194,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684,1,['release'],['release-']
Deployability,"build will break because genomicsdb-0.6.0 hasn't been released yet and dependence on protobuf-java-format needs to be fixed!. @droazen, remember now that I added the dependence on protobuf-java-format to use protobuf.JsonFormat.printToString() method which converts a protobuf structure to JSON string. I want to use import configuration protocol buffers in this code which means this dependence will be back to bite us! Need to fix this cause I don't want to break the Spark build again",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2634:54,release,released,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2634,2,"['configurat', 'release']","['configuration', 'released']"
Deployability,build.gradle finds the tool provider with the following line:. ```; final javadocJDKFiles = files(((URLClassLoader) ToolProvider.getSystemToolClassLoader()).getURLs()); ```; ToolPrivider.getSystemToolClassLoader() returns null on jre and certain other java installations. This causes a confusing null pointer exceptions. We should have a better error message when this happens.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4532:257,install,installations,257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4532,1,['install'],['installations']
Deployability,"by a quirk of travis this includes pull request builds, so if you need a snapshot you can open a pull request and the head of it will be build into a snapshot on artifactory. the version identifier should will appear in the output of the ""after_sucess"" block on travis; this will only package the linux version of the pairHmm library, to package both versions you must do a manual snapshot. also upgrades gradle to 2.13 to use the new findProperty function. DON'T MERGE UNTIL THE NEW ARTIFACT IS CONFIRMED TO BE IN ARTIFACTORY FOR THIS PR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1828:396,upgrade,upgrades,396,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1828,1,['upgrade'],['upgrades']
Deployability,"by delegating work to ; `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark`, ; so we have a single tool running the whole pipeline. This PR also does:; * refactoring of `FindBreakpointEvidenceSpark` and `DiscoverVariantsFromContigAlignmentsSpark` to accommodate the new tool; * added three integration test (dummy in the sense that it only makes sure they run, and no correctness check on the output) for the three tools. Known differences:. * For NA12878 test sample: The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` generated VCF and `StructuralVariationDiscoveryPipelineSpark` generated VCF differ by how supplementary alignment's soft clipping is treated. The `FindBreakpointEvidenceSpark`->`DiscoverVariantsFromContigAlignmentsSpark` path has an optimization turned on that soft clipped bases for supplementary alignments are hard clipped away (no contig sequence is lost as it is always saved in the primary alignment), so the CIGARs are a little different. As a consequence, the SAM file generated by the two routes also differ in this CIGAR and sequence part.; * For CHM test sample: The differences are more delicate and even master version yields slightly different results from run to run. So I summarized them in the attached zip. @tedsharpe and @cwhelan please take a look, as `FindBreakpointEvidenceSpark` is modified (no change of logic, but how code is called).; [chm.zip](https://github.com/broadinstitute/gatk/files/919925/chm.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2595:148,pipeline,pipeline,148,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2595,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,cala:802); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595); > 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); > 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); > 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); > 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); > 	at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:38); > 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); > 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:2441,pipeline,pipelines,2441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['pipeline'],['pipelines']
Deployability,can i haz integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8757:10,integrat,integration,10,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8757,1,['integrat'],['integration']
Deployability,catesSpark.java:65); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.spark.Logging; at java.lang.ClassLoader.findClass(ClassLoader.java:530); at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34); at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:55); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 56 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [16ec1fd0-9528-4249-971e-f1447314bde4] entered state [ERROR] while waiting for [DONE]. ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2183:4976,deploy,deploy,4976,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2183,5,['deploy'],['deploy']
Deployability,"ccurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2234,INSTALL,INSTALLDIRGATK,2234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"cf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.25.1.el7.x86_64 amd64; 11:25:55.709 INFO ReblockGVCF - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 11:25:55.709 INFO ReblockGVCF - Start Date/Time: June 30, 2021 11:25:55 AM EDT; 11:25:55.710 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.710 INFO ReblockGVCF - --------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:1555,install,install,1555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['install'],['install']
Deployability,"changes to build.gradle; R package installation is now part of the gradle build; install_R_packages.R no longer reinstalls existing packages; a warning will be emitted if this fails. compilation no longer depends on R installation, installation does. test run in parallel now; this is set to use 2 cores on travis and 4 locally. adding a note about our R dependency to the readme. travis changes; adding caching to travis for dramatic R installation speedup; updating gradle download because it was using an out of date link. misc changes:; adding an additional flag to mark duplicates to avoid the garbage collection statistics while integration testing; tagging tests that depend on R for future use",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/296:35,install,installation,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/296,5,"['install', 'integrat']","['installation', 'integration']"
Deployability,changing cran repo for R installation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3443:25,install,installation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3443,1,['install'],['installation']
Deployability,changing cran repo for R installation -- sol. 2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3451:25,install,installation,25,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3451,1,['install'],['installation']
Deployability,"che.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; log4j:ERROR A ""org.apache.log4j.ConsoleAppender"" object is not assignable to a ""org.apache.log4j.Appender"" variable.; log4j:ERROR The class ""org.apache.log4j.Appender"" was loaded by; log4j:ERROR [sun.misc.Launcher$AppClassLoader@6d21714c] whereas object of type; log4j:ERROR ""org.apache.log4j.ConsoleAppender"" was loaded by [org.apache.spark.util.ChildFirstURLClassLoader@6ee12bac].; log4j:ERROR Could not instantiate appender named ""console"".; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; log4j:WARN No appenders could be found for logger (org.apache.spark.SparkContext).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 09:14:26.202 INFO PrintReadsSpark - Shutting down engine; [June 8, 2017 9:14:26 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=494927872; ***********************************************************************. A USER ERROR has occurred: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file. ***********************************************************************; org.broadinstitute.hellbender.exceptions.UserException$CouldNotCreateOutputFile: Couldn't write file /user/yaron/output.bam because writing failed with exception /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:255); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:37); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:4167,pipeline,pipelines,4167,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['pipeline'],['pipelines']
Deployability,"chedulerBackend: Granted executor ID app-20180424175501-0004/5 on hostPort xx.xx.xx.23:49023 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20180424175501-0004/6 on worker-20180424173107-xx.xx.xx.25-33478 (xx.xx.xx.25:33478) with 16 cores; 18/04/24 17:55:01 INFO NettyBlockTransferService: Server created on xx.xx.xx.16:49734; 18/04/24 17:55:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20180424175501-0004/6 on hostPort xx.xx.xx.25:33478 with 16 cores, 1024.0 MB RAM; 18/04/24 17:55:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:12806,update,updated,12806,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,3,['update'],['updated']
Deployability,"ck broadcast_0 stored as values in memory (estimated size 285.6 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.131.101.159:34044 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 14.5 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 2.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.131.101.159:34044 (size: 2.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 1 from broadcast at ReadsSparkSink.java:195; 17/10/11 14:19:18 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir; 17/10/11 14:19:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1; 17/10/11 14:19:18 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 17/10/11 14:19:18 INFO spark.SparkContext: Starting job: saveAsNewAPIHadoopFile at ReadsSparkSink.java:203; 17/10/11 14:19:18 INFO input.FileInputFormat: Total input paths to process : 1; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Registering RDD 5 (mapToPair at SparkUtils.java:157); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Got job 0 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) with 1 output partitions; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Parents of fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:10468,Configurat,Configuration,10468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['Configurat'],['Configuration']
Deployability,clarify expected behavior with test (and update WGS joint; calling WDL in appropriate repos) and update tool docs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6423:41,update,update,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6423,2,['update'],['update']
Deployability,clarify javadoc in BucketUtils regarding PipelineOptions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/923:41,Pipeline,PipelineOptions,41,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/923,1,['Pipeline'],['PipelineOptions']
Deployability,clone deep to fix continuous snapshots,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1996:18,continuous,continuous,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1996,1,['continuous'],['continuous']
Deployability,closes #329 . Removed our copy of `Locatable`; updated `getChr()` and `getSequence()` to standardized `getContig()`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/330:47,update,updated,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/330,1,['update'],['updated']
Deployability,cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19744,deploy,deploy,19744,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,"cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7954,deploy,deploy,7954,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2468,configurat,configuration,2468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,"consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1327,continuous,continuous,1327,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['continuous'],['continuous']
Deployability,"container for gatk-4.1.4.0. while preparing the gatk conda environment numpy-1.13.3 ins installed but biopython==1.70 requirement from the pip section of the gatkcondaenv.yml. removes it and install numpy-1.18.1. see relevant part of conda env create -n gatk -f gatk-4.1.4.0/gatkcondaenv.yml 2>&1 | tee log; NB full log is attached : [log.txt](https://github.com/broadinstitute/gatk/files/4091802/log.txt). ```; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... done. Downloading and Extracting Packages. keras-preprocessing- | 36 KB | ########## | 100%; astor-0.8.0 | 46 KB | ########## | 100%; setuptools-36.4.0 | 563 KB | ########## | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a string.; Multidimensional index",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:1033,install,installed,1033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,1,['install'],['installed']
Deployability,count reads in spark + integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/920:23,integrat,integration,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/920,1,['integrat'],['integration']
Deployability,"create a ""smoke test"" for cluster upgrades",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1392:34,upgrade,upgrades,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392,1,['upgrade'],['upgrades']
Deployability,"cription . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:1114,install,install,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['install'],['install']
Deployability,"cted version(s); - \[x] Latest master branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087:1073,Update,UpdateVCFSequenceDictionary,1073,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,"ction accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0hMTukUGLtk1oTOse4Oj3awHf_exks5rnq1CgaJpZM4JNjE->; > .; >. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287828851). OK well this workaround should really be moved to a ""validation stringency"" level decision, not a hardcoded hack. . @ronlevine Do you know i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:3631,update,update,3631,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['update'],['update']
Deployability,"ctures with gatk or 2) make sure gatk docker images include the native libraries and are set to use them. Logs for `MarkDuplicatesSpark` without and with native libraries, running on a Broad login server:. Without:. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx37; .NA12892.readnamesort.dupmarked.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked.bam --spark; -master local[8]; 14:40:21.800 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 14:40:21.889 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 14:40:21.989 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.990 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 14:40:21.990 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:21.991 INFO MarkDuplicatesSpark - Executing as cwhelan@gsa6.broadinstitute.org on Linux v2.6.32-696.16.1.el6.x86_64 amd64; 14:40:21.991 INFO MarkDuplicatesSpark - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 14:40:21.992 INFO MarkDuplicatesSpark - Start Date/Time: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 14:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:1650,install,install,1650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['install'],['install']
Deployability,"currently the travis build is at ~10minutes. There are several options, and we should probably do all of them:; 1. Speedup individual tests:; The top offenders are:; `SplitNCigarReadsIntegrationTest`: this 1 test takes a minute, we can probably do something about this; `MarkDuplicatesIntegrationTest`: 56 tests each taking ~1 second, unclear what we could do; `AnalyzeCovariatesIntegrationTests` 13 tests taking ~40 seconds. Some take longer than others, not sure what we can do about these; `CachingIndexedFastaSequenceFileUnitTest` 21 tests taking a minute. Lets push this to htsjdk.; 2. We lose several minutes installing R libraries (2-3). We could parallelize our travis build and split it into 2 builds, so that 1 branch of the build installs R libraries and then runs the tests that depend on those, and the other branch runs all the other tests. This would probably make the R installation effectively free provided we have sufficient worker nodes.; 3. Run tests in parallel. Travis gives us more than 1 core. I tested running with cores set to 2, and the actual test time dropped nearly in half.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/221:615,install,installing,615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/221,3,['install'],"['installation', 'installing', 'installs']"
Deployability,"currently we only do this for the variants discovered by the tool that is about to be replaced. -----. **UPDATE**; to be more specific, the attributes are `SPLIT_READS `, `READ_PAIRS` and `EXTERNAL_CNV_CALLS`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4228:105,UPDATE,UPDATE,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4228,1,['UPDATE'],['UPDATE']
Deployability,cutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:5180,Continuous,ContinuousBuildActionExecuter,5180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,2,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,cutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:5142,Continuous,ContinuousBuildActionExecuter,5142,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,2,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,"d evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvalua",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2273,configurat,configuration,2273,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,"d in [section 1.3 of the VCF specification](https://samtools.github.io/hts-specs/VCFv4.3.pdf). I've had a quick look at the code, and think the dubious value may be generated in [ReadPosition::getValueForRead](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ReadPosition.java#L57) when the result from [ReadPosRankSumTest.getReadPosition](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ReadPosRankSumTest.java#L53) is cast to an `int`. Looking at that function, it can [return `INVALID_ELEMENT_FROM_READ`](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/ReadPosRankSumTest.java#L62) which is [defined as `Double.NEGATIVE_INFINITY`](https://github.com/broadinstitute/gatk/blob/946f39/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/RankSumTest.java#L23). According to the [java documentation](https://docs.oracle.com/javase/specs/jls/se7/html/jls-5.html#jls-5.1.3), casting NEGATIVE_INFINITY to int will result in a value of `INT_MIN`. (Disclaimer: I haven't tested this, so it may be completely wrong...). #### Steps to reproduce; See attached .zip file which includes a smallish bam file that shows the problem. I ran mutect2 on it in the Docker container for the latest GATK release:; ```sh; unzip mpos_issue.zip; cd mpos_issue; ../gatk Mutect2 --input input/small.bam --reference input/small.fa --output small.vcf; grep MPOS=- small.vcf; ```. #### Expected behavior; `MPOS` should have a sensible value. #### Actual behavior; ```; ref|NC_001224|	12835	.	A	AATAT	.	.	DP=2810;ECNT=5;MBQ=20,34;MFRL=185,202;MMQ=60,29;MPOS=-2147483648;POPAF=7.30;TLOD=3.04	GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB	0|1:2804,3:1.431e-03:2807:1316,3:1412,0:0|1:12828_AATAC_A:12828:1200,1604,2,1; ```. ----. [mpos_issue.zip](https://github.com/broadinstitute/gatk/files/4014487/mpos_issue.zip)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6342:1785,release,release,1785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6342,1,['release'],['release']
Deployability,d on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. example commandline:. ```; ./gatk-launch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output --apiKey $HELLBENDER_TEST_APIKEY -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1785:1646,deploy,deploy,1646,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785,6,['deploy'],['deploy']
Deployability,d on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535122). Also increase the kmer size to 35 does the job (-kmerSize 35) I guess that that prevents non-ref paths merge back into the reference between events thus resulting in less complex graphs. ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265535498). The user can be inform of using these work arounds (change the max number of haplotypes or kmersize) but those are not good solutions in general as he would pay a CPU and sensitivity penalty in other places. . ---. @vruano commented on [Wed Dec 07 2016](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-265551340). I can see how the current best-path selection algorithm may fail to produce a good coverage of events across the active region depending on the weights on edges .... for some configurations the algorithm may dedicate too much time in exploring alternatives in one section of the graph because these are nearly equaly likely disregarding other possibilities other section just because they can only result in a relatively larger drop in the likelihood of the path. . I quick but elegant solution would be to simulate passes across the graph... first iterations would produce a quickly growing set of haplotypes but eventually repeated sampling would not produce new haplotypes. if after 100 subsequent simulations there is no new discovery or we have reached a limit (128?) we would stop there. This simulation approach could be implemented only in situations the graphs are too complex for an analytical solution. We can determine the maximum number of paths in a graph with a quick deep first traversal to decide whether to use the analytical-exact or the simulation-proximate approach. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1530#issuecomment-287813366). To be done in GATK4.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2954:4673,configurat,configurations,4673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2954,1,['configurat'],['configurations']
Deployability,"dMateCigar Reverts the original base qualities and adds the mate cigar tag to read-group BAMs; RevertSam Reverts SAM/BAM files to a previous state; SamFormatConverter Convert a SAM/BAM/CRAM file to a SAM/BAM/CRAM file; SamToFastq Converts a SAM/BAM file into a FASTQ; SortSam Sorts a SAM/BAM/CRAM file; SplitNCigarReads Split Reads with N in Cigar; SplitReads Outputs reads from a SAM/BAM/CRAM by read group, sample and library name; UnmarkDuplicates Unmark duplicates in a SAM/BAM/CRAM file; ValidateSamFile Validates a SAM/BAM/CRAM file. --------------------------------------------------------------------------------------; Spark Validation tools: Tools written in Spark to compare aspects of two different files; CompareBaseQualitiesSpark Diff qs of the BAMs; CompareDuplicatesSpark Compares two BAMs for duplicates. --------------------------------------------------------------------------------------; Spark pipelines: Pipelines that combine tools and use Apache Spark for scaling out (experimental); BQSRPipelineSpark Both steps of BQSR (BaseRecalibrator and ApplyBQSR) on Spark; ReadsPipelineSpark Takes aligned reads (likely from BWA) and runs MarkDuplicates and BQSR. The final result is analysis-ready reads. --------------------------------------------------------------------------------------; Spark tools: Tools that use Apache Spark for scaling out (experimental); ApplyBQSRSpark ApplyBQSR on Spark; BaseRecalibratorSpark BaseRecalibrator on Spark; BaseRecalibratorSparkSharded BaseRecalibrator on Spark (experimental sharded implementation); CollectBaseDistributionByCycleSpark CollectBaseDistributionByCycle on Spark; CollectQualityYieldMetricsSpark CollectQualityYieldMetrics on Spark; CountBasesSpark CountBases on Spark; CountReadsSpark CountReads on Spark; CountVariantsSpark CountVariants on Spark; CreateHadoopBamSplittingIndex create a hadoop-bam splitting index; FindBadGenomicKmersSpark find ref kmers with high copy number; FindSVBreakpointsSpark Produce small FASTQs of",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:6713,pipeline,pipelines,6713,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,2,"['Pipeline', 'pipeline']","['Pipelines', 'pipelines']"
Deployability,"dPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5975:5941,release,releases,5941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975,1,['release'],['releases']
Deployability,"data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:12:21.141 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:12:21.141 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:12:21.141 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:12:21.142 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:12:20 AM CST ; ; 00:12:21.142 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.142 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:15814,pipeline,pipeline,15814,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:12:21.140 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:12:21.141 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:12:21.141 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:12:21.141 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:15370,pipeline,pipeline,15370,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"dbSNP build, it throws this error:. \[Fri Jul 23 13:25:03 CEST 2021\] picard.vcf.CollectVariantCallingMetrics done. Elapsed time: 70.55 minutes. Runtime.totalMemory()=1623195648. To get help, see [http://broadinstitute.github.io/picard/index.html#GettingHelp](http://broadinstitute.github.io/picard/index.html#GettingHelp). java.lang.NullPointerException: Cannot invoke ""htsjdk.samtools.SAMSequenceRecord.getSequenceLength()"" because the return value of ""htsjdk.samtools.SAMSequenceDictionary.getSequence(String)"" is null. at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:163). at picard.util.DbSnpBitSetUtil.createSnpAndIndelBitSets(DbSnpBitSetUtil.java:131). at picard.vcf.CollectVariantCallingMetrics.doWork(CollectVariantCallingMetrics.java:101). at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:308). at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:37). at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160). at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203). at org.broadinstitute.hellbender.Main.main(Main.java:289). As a bit of a background, I am trying to use the latest dbSNP release (build 155, GRCh38, GCF\_000001405.39) and have tried using GATK version 4.1.9.0 and the latest 4.2.0.0, both having the same problem. To prepare the dbSNP file for use with the best practices workflow, I renamed the NCBI chromosome accession numbers  to UCSC style names using bcftools annotate, updated the vcf headers using UpdateVcfSequenceDictionary, and indexed the file using IndexFeatureFile. The dbSNP file worked well with both HaplotypeCaller and GenotypeGVCFs, with the rsids overlapping perfectly with those obtained when using the dbSNP resource bundle version. Any help with this would be greatly appreciated!<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/171466'>Zendesk ticket #171466</a>)<br>gz#171466</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7383:2185,release,release,2185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7383,3,"['Update', 'release', 'update']","['UpdateVcfSequenceDictionary', 'release', 'updated']"
Deployability,"ded. A previous user already found a similar error in ValidateVariants (https://gatk.broadinstitute.org/hc/en-us/community/posts/360061452132-GATK4-RNAseq-short-variant-discovery-SNPs-Indels-), but then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position chr\_1:1088200 are not observed at all in the sample genotypes \*\*\*\*\* ; ; chr\_1 1088200 . T \*,TAAAAAAAAAAAA 64.39 . AC=8,0;AF=0.667,0.00;AN=12;DP=118;ExcessHet=3.0103;FS=0.000;InbreedingCoeff=0.4286;MLEAC=7,7;MLEAF=0.583,0.583;MQ=58.73;QD=32.19;SOR=2.303 GT:AD:DP:GQ:PL ./.:9,0,0:9:.:0,0,0,0,0,0 0/0:9,0,0:9:0:0,0,113,0,113,113 ./.:10,0,0:10:.:0,0,0,0,0,0 ./.:5,0,0:5:.:0,0,0,0,0,0 1/1:0,0,1:1:0:225,15,0,15,0,0 ./.:0,0,0:0:.:0,0,0,0,0,0 ./.:12,0,0:12:.:0,0,0,0,0,0 ./.:8,0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:1527,pipeline,pipeline,1527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,1,['pipeline'],['pipeline']
Deployability,default configuration rather than the path. Fixes #1324.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1433:8,configurat,configuration,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1433,1,['configurat'],['configuration']
Deployability,"dependency-name=commons-io:commons-io&package-manager=gradle&previous-version=2.7&new-version=2.14.0)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot show <dependency name> ignore conditions` will show all of the ignore conditions of the specified dependency; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/broadinstitute/gatk/network/alerts). </details>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9003:1624,upgrade,upgrade,1624,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9003,3,['upgrade'],['upgrade']
Deployability,"der.tools.dragstr.ComposeSTRTableFile done. Elapsed time: 10.52 minutes.; Runtime.totalMemory()=1128792064; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.Dragstr.model -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:55:30.890 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:55:31 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:55:31.182 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13:55:31.183 INFO CalibrateDragstrModel - The Genome Analysis Toolkit (GATK) v4.2.0.0; 13:55:31.183 INFO CalibrateDragstrModel - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:55:31.183 INFO CalibrateDragstrModel - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 13:55:31.184 INFO CalibrateDragstrModel - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_121-b13; 13:55:31.184 INFO CalibrateDragstrModel - Start Date/Time: April 4, 2021 1:55:30 PM EDT; 13:55:31.184 INFO CalibrateDragstrModel - ------------------------------------------------------------; 13",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:12051,install,install,12051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['install'],['install']
Deployability,diff engine port and added hookup for our integration tests; +added PrintVariants as an example variant walker,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/619:42,integrat,integration,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/619,1,['integrat'],['integration']
Deployability,dinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:7698,pipeline,pipelines,7698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['pipeline'],['pipelines']
Deployability,"directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > So, would adding a toggle be acceptable? And more importantly, can we make; > stringent validation default, with the option to not blow up on silly exome; > files? Will production accept that?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0tUTNAAyuk3m_2cJ8j_3KYroaqB1ks5q-QpsgaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287821154). Any update on this, @yfarjoun ?. ---. @yfarjoun commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-287826525). I think we will only fix the interval list when we move exomes to; hg38....so, no. On Mon, Mar 20, 2017 at 12:45 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Any update on this, @yfarjoun <https://github.com/yfarjoun> ?; >; > —; > You are receiving this because you were mentioned.; >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:2990,toggle,toggle,2990,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['toggle'],['toggle']
Deployability,"ds but `VariantAnnotator` only has the reads. Several annotations had fallback code to annotate a likelihoods object that had no likelihoods. @vruano This is the class that you disliked so much in your recent code review of #5783 . A few issues with this state of things:. * Torturing the definition of `AlleleLikelihoods`, which forced the class to have methods like `hasLikelihoods()`.; * `VariantAnnotator` only applied the few annotations that had custom pileup-based fallback code.; * Lots more annotation code for the fallback mode. So the first step was the option that @lbergelson and @jamesemery liked most: create a regular likelihoods object in `VariantAnnotator` by hard-assigning of each read to the allele it best matches. This is exactly what all the custom fallback modes were doing in effect, but now it's implemented in one place instead of six or so. This lets us delete `UnfilledLikelihoods` and also lets `VariantAnnotator` apply any annotation. @ldgauthier Since the most non-trivial aspect is the new integration test I'm inclined to assign you the review, but a case could be made for someone on the engine team. This completely broke the `VariantAnnotator` tests, which were based on exact matches. This had been an issue before and has always been a bit of a nuisance, but now overhauling the tests became completely unavoidable. So, I rewrote all the tests and wrote a rigorous test based on concordance with annotations from `Mutect2`. If I were reviewing I would start with the new code in `VariantAnnotator` that constructs the likelihoods object from the reads and verify that it is just a more polished version of the fallback code that several annotations used to have. Then I would look at the new `VariantAnnotator` integration tests. Some of the tolerances are fairly liberal but it's worth noting that much of the old exact match ""truth"" annotations were completely bogus. This is better than what we had before by a long shot but it's still use at your own risk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6172:1269,integrat,integration,1269,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6172,2,['integrat'],['integration']
Deployability,"due to recursive implementation of Legendre abscissas in Apache Commons. @vdauwera @takutosato this is very simple; it just caps the number of subdivisions of the integral to avoid the recursive stack overflow. I tested it on absurdly high coverage (100,000) and reproduced the error with the old code. Whichever one of you gets to this first should review. While this isn't the most beautiful thing in the world, it will work reasonably while new integration code is pending.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3335:448,integrat,integration,448,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3335,1,['integrat'],['integration']
Deployability,"dules, and still being able to publish different components in different artifacts. At least I would like to have a different artifact for pure-java components separate from the rest, to be sure that python (for example) is not required. Does some of this makes sense for you? A proposed scheme will be the following:. * `common`/`engine`: this should include the engine, utils, and everything that it can be useful by itself. This should be a dependency for every other module. Components in other sub-modules might be proposed to be moved to this one if they might be useful out of their own. If the package names does not change, the interface and usage will be unmodified, and then there is no change in the API.; * `spark`: I think that this is a nice separation from other components. In this case, this can include all code related with Spark classes for removal of the huge Spark dependency in sub-projects that does not require them.; * `tools` and `spark-tools`: this can be even split in more fine grained sub-modules depending on the pipeline (e.g. CNV, Mutect, etc., if it makes sense). In addition, the separation between normal tools and spark-tools will make easier for downstream projects to support or not spark in their code.; * `experimental`: this might contain prototype code that might change in the future, and that will be nice in terms of documentation purposes (always annotated with `@BetaFeature` or `@Experimental`, etc.). In addition, code shouldn't rely on the code in this package for anything, allowing to have experimental code for play around and remove if required, without any major version bump.; * `testing`: this will contain the testing framework. It is related with #1481 and #3567. ; * `documentation`: this might be useful for code dependent on `com.sun.javadoc` to do not interact with other classes if code for documenting a downstream project is not necessary.; * Other modules might be useful for concrete components: e.g, ., the gCNV python computati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3900:1462,pipeline,pipeline,1462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3900,1,['pipeline'],['pipeline']
Deployability,"duplicate key error. Ted Brookings identified how to solve this issue:; -The exact problem is in AlleleFrequencyUtils.java, line 30.; -The solution is to skip collecting as a map, have getMaxMinorAlleleFreq take a stream and return an optional float, then return false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df · broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)). . ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7504:1147,pipeline,pipelines,1147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504,1,['pipeline'],['pipelines']
Deployability,"e > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testChimericUnpairedMapping SKIPPED; Running Test: Test method testPerfectUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testPerfectUnpairedMapping SKIPPED; ```. This test fails because some JAR wasn't built:; ```; Running Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest); Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest > testPipeForPicardTools STANDARD_ERROR; No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: or. or; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: export GATK_LOCAL_JAR=<path_to_local_jar>. export GATK_LOCAL_JAR=<path_to_local_jar>; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.Pipeli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:3820,Pipeline,PipelineSupportIntegrationTest,3820,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['Pipeline'],['PipelineSupportIntegrationTest']
Deployability,"e AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6012:1180,install,install,1180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012,1,['install'],['install']
Deployability,"e Germline Contig Ploidy step, I stumble upon this error. Please guide me to solve this problem. ### Affected tool(s) or class(es); ```; gatk DetermineGermlineContigPloidy \; -L /home/nguyen/RB1/RB1.cohort.gc.filtered.interval_list \; --interval-merging-rule OVERLAPPING_ONLY \; -I ... (63 tsv files output from CollectReadCounts); ```. ### Affected version(s); - GATK 4.1.6.1; ### Description ; Full error log:; ```; Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.380621677219090732.py"", line 119, in <module>; ploidy_task.engage(); File ""/home/nguyen/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 339, in engage; converged_continuous = self._update_continuous_posteriors(); File ""/home/nguyen/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 395, in _update_continuous_posteriors; assert not np.isnan(loss), ""The optimization step for ELBO update returned a NaN""; AssertionError: The optimization step for ELBO update returned a NaN; 11:09:59.446 DEBUG ScriptExecutor - Result: 1; 11:09:59.447 INFO DetermineGermlineContigPloidy - Shutting down engine; [April 28, 2020 11:09:59 AM ICT] org.broadinstitute.hellbender.tools.copynumber.DetermineGermlineContigPloidy done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=623902720; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /tmp/cohort_determine_ploidy_and_depth.380621677219090732.py --sample_coverage_metadata=/tmp/samples-by-coverage-per-contig8606344533091962323.tsv --output_calls_path=/home/nguyen/Exec/gatk-4.1.6.0/ploidy-calls --mapping_error_rate=1.000000e-02 --psi_s_scale=1.000000e-04 --mean_bias_sd=1.000000e-02 --psi_j_scale=1.000000e-03 --learning_rate=5.000000e-02 --adamax_beta1=9.000000e-01 --adamax_beta2=9.990000e-01 --log_emission_samples_per_round=2000 --log_emission_sampling_rounds=100 --log_emission_sampling_median_rel",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6573:1145,update,update,1145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6573,2,['update'],['update']
Deployability,"e VM ubuntu. ; I downloaded gatk-4.4.0.0. Step by step, I tried to build GATK4. (https://github.com/broadinstitute/gatk/blob/master/README.md#building). I made a gitclone using ; wget https://github.com/broadinstitute/gatk. and entered gatk folder. ; there was a gradlew.; and I entered ; ./gradlew bundle ; or; ./gradlew. but it failed to build GATK4 with following errors. . ====================================; OpenJDK 64-Bit Server VM warning: Insufficient space for shared memory file:; 30934; Try using the -Djava.io.tmpdir= option to select an alternate temp location. FAILURE: Build failed with an exception. * What went wrong:; Gradle could not start your build.; > Cannot create service of type DependencyLockingHandler using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyLockingHandler() as there is a problem with parameter #2 of type ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to cre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:1066,Configurat,ConfigurationContainerInternal,1066,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['Configurat'],['ConfigurationContainerInternal']
Deployability,"e a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:4605,release,release,4605,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,"e adopt a new default branch name and retire the use of 'master'.*. The use of 'master' as the default branch is quickly tipping into the realm of being archaic, and present the image of being increasingly tone deaf. 'main' is the commonly accepted replacement on GitHub, but I'm stopping short of suggesting the replacement name, just asking ""please retire master"". . ### 'master has a specific technical meaning' . It does. And is also an example of structural racism, which; > refers to the complex interactions of large scale societal systems, practices, ideologies, and programs that produce and and perpetuate inequities for racial minorities. The key aspect of structural or systematic racism is that these macro-level mechanisms operate independent of the intentions and actions of individuals, so that even if individual racism is not present, the adverse conditions and inequalities for racial minorities will continue to exist - [Gee & Ford, 2011](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4306458/). _And if you just felt as if you were accused of being a racist, please re-read the above definition again_ I'm addressing the bureaucracy ( which I can not realistically effect much change with, but some of you can).; ; Ultimately, a fair number of people are to varying degrees uncomfortable or threatened by this trope. And on these merits alone are a good reason to ditch master. [The process is straight forward and documentation abounds](https://www.git-tower.com/learn/git/faq/git-rename-master-to-main), [there are even tools to help automate the conversion](https://github.com/dsyer/main-branch-switch). But it will take time, and is not the most exciting work in the world. . Perhaps it's a sticky change as part of all major releases, or otherwise planned for? So, that's my vote, if I were to be asked to vote that is. And, if there are detailed plans in place to make this change, horray! Link them here, and now you have a(nother?) nice honeypot for this topic. John Major",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7621:2323,release,releases,2323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7621,1,['release'],['releases']
Deployability,e blows up:. ```; ./bin/gatk/gatk-launch PrintReadsSpark -I hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.bam -O hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.small.out.bam \; -- \; --sparkRunner SPARK --sparkMaster yarn-client \; --num-executors 5 --executor-cores 2 --executor-memory 4g \; --conf spark.yarn.executor.memoryOverhead=600; ```. blows up with . ```; java.lang.ClassCastException: org.apache.hadoop.fs.RawLocalFileSystem cannot be cast to org.apache.hadoop.fs.LocalFileSystem; at org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:350); at org.apache.spark.deploy.yarn.Client$.org$apache$spark$deploy$yarn$Client$$getQualifiedLocalPath(Client.scala:1373); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:329); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1389:1080,deploy,deploy,1080,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389,1,['deploy'],['deploy']
Deployability,"e from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since options 3 and 4 seem to be the only options with votes, let's sit down next week and discuss in detail the pain points of these two options, and make a choice between them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:6032,release,release,6032,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,3,['release'],"['release', 'released']"
Deployability,"e mapping quality also considers how well the read aligns; to its best mapping. In places where a sample has a lot of nearby SNPs; compared to the reference the mapping qualities of the reads are low; compared to reads that contain fewer SNPs. I've been mulling over the; conflation of these two aspects of mapping quality for a while because it; biases our VQSR results, but maybe the new filtering models will be able to; figure it out. The b37 reference with decoy contigs is here:; /humgen/1kg/reference/human_g1k_v37_decoy.fasta.I believe that the; reference issue that required the decoy in the b37 1000G work was resolved; in the hg38 reference. This is an excellent topic to discuss with Heng; during his office hours when he gets back from China in a few weeks, but I; expect the SV team will also be helpful in the meantime. On Sun, Apr 23, 2017 at 11:14 PM, David Benjamin <notifications@github.com>; wrote:. > So. . . given that our pipeline aligns with BWA, it might seem like this; > is just a redundant and laborious rehashing of the mapping quality score.; >; > *However*, the mapping quality only considers multi-mapping within the; > reference, and therefore doesn't account for mapping errors due to; > incompleteness of the reference. That is, reads from genomic regions that; > are not part of the reference (because they're hard to assemble, like; > centromeres etc) might map well to a unique regions within the reference,; > and therefore will have fine mapping quality even though they are artifacts.; >; > There are published ""decoy genomes"" -- essentially pseudo-contigs of; > regions missing from the reference, and mapping with BWA in memory to; > *those* might be very helpful.; >; > So, we need to: 1) get our hands on a decoy genome that will play nicely; > with BWA, and 2) talk to the SV team.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2930:3264,pipeline,pipeline,3264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2930,1,['pipeline'],['pipeline']
Deployability,"e node, 32 cores/node, 1002GB node memory) NOTE that I am able to successfully run JointGenotyping on a set of 80 gvcfs, also produced by ExomeGermlineSingleSample, in this HPC/Singularity environment with 248GB memory, 24 cores/node - this doesn't seem to be a resource issue. The only difference appears to be the number of input gvcfs, which is still quite small (345 vs 80).  The number of reader threads for GenomicsDBImport has been hard-coded to 1 because these are exome sequences; scatter count = 10, batch size = 50, gather\_vcfs = false. GenomicsDBImport appears to succeed on all 10 shards but workflow execution fails with exactly the same c++ error, see below. REQUIRED for all errors and issues: ; ; a) GATK version used: v4.2.6.1. b) Exact command used:. java -Dconfig.file=/scratch.global/lee04110/config/sing-cache.conf -jar /home/pankrat2/public/bin/gatk4/cromwell-81.jar run -i /scratch.global/lee04110/config/jg.ca\_defects.json /home/pankrat2/public/bin/gatk4/warp/pipelines/broad/dna\_seq/germline/joint\_genotyping/JointGenotyping.wdl -o  <(echo '{""final\_workflow\_outputs\_dir"" : ""/scratch.global/lee04110/tmp\_jg"", ""use\_relative\_output\_paths"" : true, ""workflow-log-temporary"" : true}'). c) Entire program log: (too big to include the whole thing). (From main process stderr, picking from SplitInterval setting status to Done). \[2022-10-18 15:38:20,88\] \[info\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.SplitIntervalList:NA:1\]: Status change from WaitingForReturnCode to Done. \[2022-10-18 15:38:25,47\] \[info\] WorkflowExecutionActor-9743b28a-3819-49a7-8598-b0c5267647ee \[9743b28a\]: Starting JointGenotyping.ImportGVCFs (10 shards). \[2022-10-18 15:38:33,03\] \[info\] Assigned new job execution tokens to the following groups: 9743b28a: 10. \[2022-10-18 15:38:33,14\] \[warn\] BackgroundConfigAsyncJobExecutionActor \[9743b28aJointGenotyping.ImportGVCFs:3:1\]: Unrecognized runtime attribute keys: preemptible, bootDiskSizeGb, disks, cpu, ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8076:1722,pipeline,pipelines,1722,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8076,1,['pipeline'],['pipelines']
Deployability,"e two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and exploratory work). Notes:. - We still need to decide about `GermlineCNVCaller` default arguments. See issue #4719.; - The case denoising and calling is unlikely to benefit from t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1810,continuous,continuous,1810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['continuous'],['continuous']
Deployability,"e(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:2683,deploy,deploy,2683,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['deploy'],['deploy']
Deployability,"e-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state. add to wdl; - fix when NON-REF has value (drop it anyway). add memory to wdl; - fix order of columns in vet; - Making ingest work on gvcfs without allele specific annotations (#6934); - fix partitioning (#6939); - more fixes for non-allele-specific gvcfs (#6945); - fix GT for ./.; - add in feature extract code. resulting vcf still has bugs (#6947); - Rudimentary support for other output types, support for scattering extract cohort (#6949); - added batch/interactive flag, removed samples (#6953); - WIP for Feature Extract code (#6958); - move wdl and cromwell_tests dirs under variantstore (#6961); - WDL to run feature extract, VQSR, and upload (#6966); - support for multiple PET/VETs (#6969); - Add filtering to ExtractCohort (#6971); - support for non-AS called data (#6975); - fixing bug in filtering (#6976); - Updating extract wdls to apply filtering (#6977); - moving from specops repo (#6983); - optimized TSV experiment, and range GQ dropping (#6987); - back to 30x defaults; - allow no filtering to be applied (#7004); - Separate bigquery table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:4710,update,updated,4710,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['update'],['updated']
Deployability,"e.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1085); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta -I gs://cclebams/hg38_wes/CDS-ce3y1s.hg38.bam -tumor HAP1_1 --germline-resource gs://gatk-best-practices/somatic-hg38/af-only-gnomad.hg38.vcf.gz -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz -L gs://fc-secure-76d1542e-1c49-4411-8268-e41e92f9f311/729d209c-0ef4-409f-b3af-2e84ff45ee36/omics_mutect2/16911ef5-efb2-4e12-86f2-f3d5a54b28c0/call-mutect2/Mutect2/4e4a27e2-6c57-40e9-8ddc-1024bdcc50c1/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF --gcs-project-for-requester-pays broad-firecloud-ccle; ```. #### Steps to reproduce. running the same pipeline as described in previous issues: #7492. But I have added ""--genotype-germline-sites true --genotype-pon-sites true --emit-ref-confidence GVCF"" as additional args. the rest of the arguments are defaults/basic from the mutect2.wdl pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7849:8220,pipeline,pipeline,8220,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7849,2,['pipeline'],['pipeline']
Deployability,"e.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:2150,update,updates,2150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,4,"['configurat', 'update']","['configuration', 'updates']"
Deployability,"e.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4828,deploy,deploy,4828,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,e.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53867,deploy,deploy,53867,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,"e.hellbender.engine.FeatureManager.<init>(FeatureManager.java:155) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:72) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:7424,pipeline,pipeline,7424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,e.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53542,deploy,deploy,53542,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,"e.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4503,deploy,deploy,4503,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,e.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:49); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 16/11/29 23:44:27 INFO akka.remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [155c3731-4071-4aa9-bed8-f0eb3426b805] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2287:2199,deploy,deploy,2199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2287,6,['deploy'],['deploy']
Deployability,e.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProg,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1926,deploy,deploy,1926,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,2,['deploy'],['deploy']
Deployability,"e0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at java.io.ObjectInputStream.resolveClass(ObjectInputStream.java:677); 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1826); 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1713); 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2000); 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); 	at com.esotericsoftware.kryo.serializers.JavaSerializer.read(JavaSerializer.java:63); 	... 20 more; 17/11/15 19:43:35 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5917b44d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/15 19:43:35 WARN org.apache.spark.ExecutorAllocationManager: No stages are running, but numRunningTasks != 0; 19:43:35.858 INFO PrintVariantsSpark - Shutting down engine; [November 15, 2017 7:43:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=823132160; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; Serialization trace:; genotypes (org.seqdoop.hadoop_bam.VariantContextWithHeader); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGSchedule",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:8052,pipeline,pipelines,8052,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['pipeline'],['pipelines']
Deployability,eArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); 	at hts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19674,deploy,deploy,19674,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,"eArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7884,deploy,deploy,7884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,eCNVCaller - HTSJDK Defaults.CREATE_MD5 : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY :; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:05:38.391 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 21:05:38.392 DEBUG ConfigFactory - Configuration file values:; 21:05:38.395 DEBUG ConfigFactory - gcsMaxRetries = 20; 21:05:38.395 DEBUG ConfigFactory - gcsProjectForRequesterPays =; 21:05:38.395 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 21:05:38.395 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 21:05:38.395 DEBUG ConfigFactory - samjdk.compression_level = 2; 21:05:38.395 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 21:05:38.395 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 21:05:38.395 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 21:05:38.395 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 21:05:38.395 DEBUG ConfigFactory - spark.executor.memoryOverhead = 600; 21:05:38.395 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; 21:05:38.395 DEBUG ConfigFactory - spark.executor.extra,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8952:3213,Configurat,Configuration,3213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8952,1,['Configurat'],['Configuration']
Deployability,"eHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.456 INFO CreateHadoopBamSplittingIndex - ------------------------------------------------------------; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Version: 2.14.1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Picard Version: 2.17.2; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Deflater: IntelDeflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Inflater: IntelInflater; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - GCS max retries/reopens: 20; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:47:53.457 INFO CreateHadoopBamSplittingIndex - Initializing engine; 11:47:53.458 INFO CreateHadoopBamSplittingIndex - Done initializing engine; 11:47:53.463 INFO CreateHadoopBamSplittingIndex - Shutting down engine; [March 7, 2018 11:47:53 AM EST] org.broadinstitute.hellbender.tools.spark.CreateHadoopBamSplittingIndex done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=1115160576; ***********************************************************************. A USER ERROR has occurred: Bad input: A splitting index is only relevant for a bam file, but a file with extension cram was specified. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4506:2726,patch,patch,2726,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4506,1,['patch'],['patch']
Deployability,"eMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPAR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4759,deploy,deploy,4759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,eMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53798,deploy,deploy,53798,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,eProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3735,deploy,deploy,3735,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"eProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5906,deploy,deploy,5906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"eProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, name: referenceSource, type: interface org.broadinstitute.hellbender.engine.datasources.ReferenceSource); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:2899,deploy,deploy,2899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['deploy'],['deploy']
Deployability,eUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1839,deploy,deploy,1839,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,2,['deploy'],['deploy']
Deployability,"eblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up somehow). . Note that so far, I've only tested on our own samples. If this is something you can't reproduce, I could potentially rerun on publicly available samples to demonstrate the issue. Let me know if this is needed. . #### Expected behavior; A large number of variants should be called as hom ref (`0/0`). #### Actual behavior; Many of these variants are left as not called / missing genotypes (`./.`).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8208:2054,release,releases,2054,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208,4,"['pipeline', 'release']","['pipeline', 'releases']"
Deployability,"ed and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But if performance tanks, then we might need to see whether I've introduced any bugs. @mwalker174 @asmirnov239 perhaps you can comment on what might be the appropriate test suite here----1kGP?. I'll also highlight again that this PR will remove TensorFlow and might require that the corresponding CNN implementations be supported by an alternate strategy, at least until the PyTorch implementation goes in.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:3028,update,update,3028,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['update'],['update']
Deployability,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6050:2145,release,release,2145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050,1,['release'],['release']
Deployability,"ed for a variant in terms of alignment overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whet",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1380,configurat,configuration,1380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['configurat'],['configuration']
Deployability,"ed for this tool by default ; ; 22:06:39.337 WARN  GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 22:06:39.544 INFO  HaplotypeCaller - Start Date/Time: March 12, 2022 10:06:39 PM CET ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Version: 2.24.1 ; ; 22:06:39.544 INFO  HaplotypeCaller - Picard Version: 2.25.4 ; ; 22:06:39.544 INFO  HaplotypeCaller - Built for Spark Version: 2.4.5 ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 22:06",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:5222,pipeline,pipelines,5222,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['pipeline'],['pipelines']
Deployability,"ed; 12:57:16.776 INFO AnalyzeCovariates - Initializing engine; 12:57:16.776 INFO AnalyzeCovariates - Done initializing engine; 12:57:17.333 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates17353441228865531235.csv'; 12:57:17.414 INFO AnalyzeCovariates - Generating plots file '/home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf'; 12:57:17.829 INFO AnalyzeCovariates - Shutting down engine; [December 17, 2020 at 12:57:17 PM TRT] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=633339904; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10272183847736955081';source('/tmp/BQSR.16251220439562120273.R'); /tmp/AnalyzeCovariates17353441228865531235.csv /home/detagen/Desktop/pipeline/playground/BACKUP/FMF-248_Backup/before.recal.FMF-248.table /home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Stdout: ; Stderr: Error in library(gplots) : there is no package called ‘gplots’; Calls: source -> withVisible -> eval -> eval -> library; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:130); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.doWork(AnalyzeCovariates.java:341); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:4025,pipeline,pipeline,4025,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['pipeline'],['pipeline']
Deployability,edFileSystem.java:641); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:641); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.deleteHadoopFile(ReadsSparkSink.java:200); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:191); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:106); at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.runTool(MarkDuplicatesSpark.java:94); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:257); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1451:2411,deploy,deploy,2411,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1451,6,['deploy'],['deploy']
Deployability,"ed](https://github.com/googlegenomics/dataflow-java/blob/master/src/main/java/com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform.java#L104) we get a **java.lang.VerifyError**. The full error looks like this:. ```; Exception in thread ""main"" java.lang.VerifyError: Bad type on operand stack; Exception Details:; Location:; com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform.getReadsFromBAMFilesSharded(Lcom/google/cloud/dataflow/sdk/Pipeline;Lcom/google/cloud/genomics/utils/GenomicsFactory$OfflineAuth;Ljava/lang/Iterable;Lcom/google/cloud/genomics/dataflow/readers/bam/ReaderOptions;Ljava/util/List;)Lcom/google/cloud/dataflow/sdk/values/PCollection; @25: invokevirtual; Reason:; Type 'com/google/cloud/dataflow/sdk/transforms/Create' (current frame, stack[2]) is not assignable to 'com/google/cloud/dataflow/sdk/transforms/PTransform'; Current Frame:; bci: @25; flags: { }; locals: { 'com/google/cloud/dataflow/sdk/Pipeline', 'com/google/cloud/genomics/utils/GenomicsFactory$OfflineAuth', 'java/lang/Iterable', 'com/google/cloud/genomics/dataflow/readers/bam/ReaderOptions', 'java/util/List', 'com/google/cloud/genomics/dataflow/readers/bam/ReadBAMTransform' }; stack: { 'com/google/cloud/dataflow/sdk/values/TupleTag', 'com/google/cloud/dataflow/sdk/Pipeline', 'com/google/cloud/dataflow/sdk/transforms/Create' }; Bytecode:; 0x0000000: bb00 0159 2db7 0002 3a05 1905 2bb6 0003; 0x0000010: b200 042a 1904 b800 05b6 0006 c000 07b8; 0x0000020: 0008 b600 09b8 000a b200 0b2a 2cb8 0005; 0x0000030: b600 06c0 0007 120c b800 0db6 0009 b600; 0x0000040: 0e3a 0619 0519 06b6 000f b0 . at org.broadinstitute.hellbender.engine.dataflow.datasources.ReadsDataflowSource.getReadPCollection(ReadsDataflowSource.java:130); at org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BadTypeRepro.ingestReadsAndGrabHeader(BadTypeRepro.java:100); at org.broadinstitute.hellbender.dev.tools.walkers.bqsr.BadTypeRepro.setupPipeline(BadTypeRepro.java:74); ```. This is the same error I saw when upgr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/791:731,Pipeline,Pipeline,731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/791,3,['Pipeline'],['Pipeline']
Deployability,"elease but other tools are not. ---. @vdauwera commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215498517). Frankly on the face of it I hate the idea of toolset-specific jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is cur",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:3195,release,release,3195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,"ellFormedReadFilter because they do not have read groups or base qualities. [test_pathseq_unmapped.bam.zip](https://github.com/broadinstitute/gatk/files/537153/test_pathseq_unmapped.bam.zip). > > ./gatk-launch PrintReadsSpark -I ~/Work/gatk/tests/test_pathseq_unmapped.bam -O ~/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > Using GATK wrapper script /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk; > > Running:; > > /Users/markw/IdeaProjects/gatk/build/install/gatk/bin/gatk PrintReadsSpark -I /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam -O /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam; > > 15:10:22.765 INFO IntelGKLUtils - Trying to load Intel GKL library from:; > > jar:file:/Users/markw/IdeaProjects/gatk/build/install/gatk/lib/gkl-0.1.2.jar!/com/intel/gkl/native/libIntelGKL.dylib; > > 15:10:22.790 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; > > [October 18, 2016 3:10:22 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark --output /Users/markw/Work/gatk/tests/test_pathseq_unmapped.output.bam --input /Users/markw/Work/gatk/tests/test_pathseq_unmapped.bam --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --verbosity INFO --QUIET false --use_jdk_deflater false --disableAllReadFilters false; > > [October 18, 2016 3:10:22 PM EDT] Executing as markw@WMC9F-819 on Mac OS X 10.11.6 x86_64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: Version:4.alpha.1-318-gcdc484c-SNAPSHOT; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.BUFFER_SIZE : 131072; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.COMPRESSION_LEVEL : 1; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_INDEX : false; > > 15:10:22.793 INFO PrintReadsSpark - Defaults.CREATE_MD5 : false; > > 15:10:22.79",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2219:1166,pipeline,pipelines,1166,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2219,1,['pipeline'],['pipelines']
Deployability,"ellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gcnvkernel. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 5, in <module>; from .continuous import get_tau_sd, Normal, Flat; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 12, in <module>; from scipy import stats; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/scipy/stats/__init__.py"", line 345, in <module>; from .morestats import *; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/scipy/stats/morestats.py"", line 12, in <module>; from numpy.testing.decorators import setastest; ModuleNotFoundError: No module named 'numpy.testing.decorators'. 	at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6467:5195,continuous,continuous,5195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6467,1,['continuous'],['continuous']
Deployability,"ellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. The user mentioned that this didn't happen on GATK 4.1, so I've been comparing both versions of the code. It turns out that the implementation of ""GenotypingEngine.java"" has changed since then, and after some digging, I noticed that the issue is that the newer versions have uninitialized instances of the class ""OneShotLogger"". The fix is simple, I've added the change myself and built GATK again. The user reports that the issue is gone. Just add the following code inside the constructor method:. ``` ; protected GenotypingEngine(final Config configuration,; final SampleList samples,; final boolean doAlleleSpecificCalcs) {; this.configuration = Utils.nonNull(configuration, ""the configuration cannot be null"");; Utils.validate(!samples.asListOfSamples().isEmpty(), ""the sample list cannot be null or empty"");; this.samples = samples;; this.doAlleleSpecificCalcs = doAlleleSpecificCalcs;; logger = LogManager.getLogger(getClass());; this.oneShotLogger = new OneShotLogger(logger); // <------ ADD THIS LINE; numberOfGenomes = this.samples.numberOfSamples() * configuration.genotypeArgs.samplePloidy;; alleleFrequencyCalculator = AlleleFrequencyCalculator.makeCalculator(configuration.genotypeArgs);; }; ```. #### Steps to reproduce; See description, but I can't provide the exact inputs used for it. #### Expected behavior; The null pointer exception shouldn't occur, there should be a warning only. #### Actual behavior; Program crashes with null pointer exception for high enough values of ploidy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8158:3686,configurat,configuration,3686,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8158,6,['configurat'],['configuration']
Deployability,ellbender.utils.SequenceDictionaryUtils.getContigNames(SequenceDictionaryUtils.java:464); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.getCommonContigsByName(SequenceDictionaryUtils.java:458); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.compareDictionaries(SequenceDictionaryUtils.java:234); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:150); at org.broadinstitute.hellbender.utils.SequenceDictionaryUtils.validateDictionaries(SequenceDictionaryUtils.java:98); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.validateToolInputs(GATKSparkTool.java:402); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:312); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:108); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:166); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:185); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2020:2129,deploy,deploy,2129,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2020,6,['deploy'],['deploy']
Deployability,"emIntegrationTest.java:49); Running Test: Test method testChimericUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testChimericUnpairedMapping SKIPPED; Running Test: Test method testPerfectUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testPerfectUnpairedMapping SKIPPED; ```. This test fails because some JAR wasn't built:; ```; Running Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest); Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest > testPipeForPicardTools STANDARD_ERROR; No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: or. or; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: export GATK_LOCAL_JAR=<path_to_local_jar>. export GATK_LOCAL_JAR=<path_to_local_jar>; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:3617,Pipeline,PipelineSupportIntegrationTest,3617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['Pipeline'],['PipelineSupportIntegrationTest']
Deployability,"ent overlap are different for taking part of PL calculation and AD/DP calculation. . Where is not totally clear what is the best way to go in practice. It seems to me that we should be consistent here and both PL and AD/DP should use the same criterion. The offending code lines:. **HaplotypeCallerGenotypingEngine.java ln171**:. ```java; ReadLikelihoods<Allele> readAlleleLikelihoods = readLikelihoods.marginalize(alleleMapper, ; new SimpleInterval(mergedVC).expandWithinContig(ALLELE_EXTENSION, header.getSequenceDictionary()));; if (configuration.isSampleContaminationPresent()) {; readAlleleLikelihoods.contaminationDownsampling(configuration.getSampleContamination());; }. ```; The code above decides the involvement in PL calculations. Notice that ```ALLELE_EXTENSION``` is set to ```2```. . For the AD/DP and so on the code responsible is in **AssemblyBasedCallerGenotypingEngine.java ln366**:. ```; // Otherwise (else part) we need to do it again.; if (configuration.useFilteredReadMapForAnnotations || !configuration.isSampleContaminationPresent()) {; readAlleleLikelihoodsForAnnotations = readAlleleLikelihoodsForGenotyping;; readAlleleLikelihoodsForAnnotations.filterToOnlyOverlappingReads(loc);; } else {; readAlleleLikelihoodsForAnnotations = readHaplotypeLikelihoods.marginalize(alleleMapper, loc);; if (emitReferenceConfidence) {; readAlleleLikelihoodsForAnnotations.addNonReferenceAllele(Allele.NON_REF_ALLELE);; }; }. ```. The ```filterToOnlyOverlappingReads(loc)``` is called then the overlap criterion is strict. (e.g. 0bp padding). This is also the case for the ```marginalize``` call if the conditional is false as the loc passed has not been padded. It seems to me that setting the ```ALLELE_EXTENSION == 2``` is a very deliberative action (so it was done for a reason) and perhaps this is the way to go... but in deed if the read really does not overlap the variant should be considered at all. . This come from a more complex discussion whether the in cases whether variants ar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5434:1431,configurat,configuration,1431,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5434,1,['configurat'],['configuration']
Deployability,"ential additional information: /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr.; Could not retrieve content: Could not read from /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr: /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/stderr; [2020-07-14 05:09:46,38] [info] WorkflowManagerActor WorkflowActor-968be82c-eef3-4bdb-a1ab-3d4e2ca70674 is in a terminal state: WorkflowFailedState; [2020-07-14 05:09:51,97] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2020-07-14 05:09:55,28] [info] Workflow polling stopped; [2020-07-14 05:09:55,30] [info] 0 workflows released by cromid-ca5c695; [2020-07-14 05:09:55,30] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2020-07-14 05:09:55,30] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2020-07-14 05:09:55,31] [info] JobExecutionTokenDispenser stopped; [2020-07-14 05:09:55,31] [info] Aborting all running workflows.; [2020-07-14 05:09:55,31] [info] WorkflowStoreActor stopped; [2020-07-14 05:09:55,31] [info] WorkflowLogCopyRouter stopped; [2020-07-14 05:09:55,31] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor All workflows finished; [2020-07-14 05:09:55,32] [info] WorkflowManagerActor stopped; [2020-07-14 05:09:55,53] [info] Connection pools shut down; [2020-07-14 05:09:55,53] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2020-07-14 05:09:55,53] [info] SubWorkflowStoreActor stopped; [2020-07-14 05:09:55,54] [info] JobStoreActor stopped; [2020-07-14 05:0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:7945,release,released,7945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['release'],['released']
Deployability,"enty of coverage and no obvious reason for such a low GQ score. Often the GQ should be 99 as the DP >40. This seems to be primarily an issue with homozygous reference calls. . The GT is accurate for the high DP sites but the inaccurate GQ is problematic for any genotype level qc on the pVCF. If the site is recoded from 0/0 to './.' for GQ <20, the result is higher missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:1254,Pipeline,Pipeline,1254,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,1,['Pipeline'],['Pipeline']
Deployability,"environment for gCNV. Even taking out TensorFlow (assuming that the CNN will not be supported by this environment), it's a difficult task:; > 1. The goal is to update Python from 3.6 to 3.10+, since Terra now requires the latter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:1111,release,released,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['release'],['released']
Deployability,"epare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage version. See GVS-332 (#7330); - Variant Store extraction - Add VCF size to output (#7329); - add WARP-style scattering to SNPsVariantRecalibrator in GvsCreateFilterSet (#7320); - added ref ranges support (#7337); - 318 Sites only filtered vcf then annotate wdl (#7305); - Replace service_account_json (file) with service_account_json_path (string) to allow call-caching (#7347); - Parallelize create filterset by breaking out the 3 filter set file creation/loads into separate tasks (#7342); - Create WDL to validate VAT and add first test (#7352); - Add task for VAT validation #3 (#7360); - Add task for VAT validation #4 (#7363); - Instructions on how to download BQ Metadata and visualize results (#7359); - don't mix contigs, rightsize memory (#7361); - Add custom annotations as ac an af (#7351); - Add task for VAT validation #8 & 9 (#7364); - added bcftools, upgraded gcloud version (#7369); - fix wdl (#7378); - Update .dockstore.yml; - Add VAT validation rule #5 [VS-16] (#7365); - Add VAT validation rule #7 [VS-14] and validation rule #6 [VS-15] (#7379); - Batching of samples for create import TSVs (#7382); - Add VAT validation rule #2 [VS-19] (#7374); - Create VAT scripts directory (#7386); - fixing SA change from file to string (#7371); - add extract_subpop script (#7387); - Add is_loaded column to sample_info and logic to populate after ingest [VS-158] (#7389); - Add Gnomad subpopulation info into the VAT (#7381); - implement GVS ID assignment (#",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:16328,upgrade,upgraded,16328,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,"['Update', 'upgrade']","['Update', 'upgraded']"
Deployability,"eported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. After sourcing the tab-completion script, some tools shown cannot be run. Maybe they exist somewhere in an experimental dev version but are not bundled for public release?. ### Affected version(s); - [x ] Latest public release version [4.1.7.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. After trying to tab complete the DepthOfCoverage, I saw a few tools not listed in the documentation. I tried running them and sure enough, there were errors:. `A USER ERROR has occurred: '*' is not a valid command.`; (* is one of the tools listed below). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. ```; cd gatk-4.1.7.0; source gatk-completion.sh; ./gatk Depth<tab>; #>DepthOfCoverage DepthPerAlleleBySample DepthPerSampleHC; ./gatk DepthPerSampleHC -h; ...; ***********************************************************************; A USER E",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6615:1424,release,release,1424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6615,1,['release'],['release']
Deployability,"er-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L140) and [**disk_space**](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L407). #### Expected behavior; Defining the input variables **default_ram_mb** and **default_disk_space_gb** allows you to specify your preferred memory and disk space configuration for the Funcotate task. #### Actual behavior; These variables do not define the runtime configuration for the task. Memory is defined by a workflow-level input that isn't clearly connected to Funcotate. #### Suggestion; Utilize the variables **default_ram_mb** and **default_disk_space_gb** that already exist in the task in such a way that modifying them has an impact on the configuration of the task VM.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:2501,configurat,configuration,2501,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,3,['configurat'],['configuration']
Deployability,er.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.B,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19525,deploy,deploy,19525,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,"er.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7735,deploy,deploy,7735,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,erlapping(SparkSharder.java:128); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.shard(SparkSharder.java:101); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.getVariants(VariantWalkerSpark.java:129); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.runTool(VariantWalkerSpark.java:160); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.VariantWalkerSpark.runPipeline(VariantWalkerSpark.java:57); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [dfac787d-19aa-4296-8078-c033cd9f440d] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:12394,deploy,deploy,12394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,6,['deploy'],['deploy']
Deployability,"ervals; reorganized copynumber packages.; -For motivation of changes in CallCopyRatioSegments, see #3825.; -I added the ability to turn off binning in PreprocessIntervals by specifying bin_length = 0.; -I removed the separation between coverage and allelic packages to make the package structure a bit simpler.; -@MartonKN should review, since he wrote PreprocessIntervals and is updating the caller. Added segmentation classes and tests for ModelSegments CNV pipeline.; -I added implementations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1106,pipeline,pipeline,1106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['pipeline'],['pipeline']
Deployability,"es - GCS max retries/reopens: 20; 12:57:16.776 INFO AnalyzeCovariates - Requester pays: disabled; 12:57:16.776 INFO AnalyzeCovariates - Initializing engine; 12:57:16.776 INFO AnalyzeCovariates - Done initializing engine; 12:57:17.333 INFO AnalyzeCovariates - Generating csv file '/tmp/AnalyzeCovariates17353441228865531235.csv'; 12:57:17.414 INFO AnalyzeCovariates - Generating plots file '/home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf'; 12:57:17.829 INFO AnalyzeCovariates - Shutting down engine; [December 17, 2020 at 12:57:17 PM TRT] org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=633339904; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.10272183847736955081';source('/tmp/BQSR.16251220439562120273.R'); /tmp/AnalyzeCovariates17353441228865531235.csv /home/detagen/Desktop/pipeline/playground/BACKUP/FMF-248_Backup/before.recal.FMF-248.table /home/detagen/Desktop/pipeline/playground/NECESSARY/FMF-248/AnalyzeCovariates.FMF-248.pdf; Stdout: ; Stderr: Error in library(gplots) : there is no package called ‘gplots’; Calls: source -> withVisible -> eval -> eval -> library; Execution halted. 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:80); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.getScriptException(RScriptExecutor.java:19); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:130); 	at org.broadinstitute.hellbender.utils.R.RScriptExecutor.exec(RScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.generatePlots(RecalUtils.java:360); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.generatePlots(AnalyzeCovariates.java:329); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.AnalyzeCovariates.doWor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7006:3934,pipeline,pipeline,3934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7006,1,['pipeline'],['pipeline']
Deployability,"es on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge doesn't solve this issue but one one related to the comp. performance of the existing code. . ---. @eitanbanks commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123482433). @ldgauthier that's a different issue. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-260465303). Moving to GATK4; decide there whether it's still applicable or not.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2915:1799,update,update,1799,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915,1,['update'],['update']
Deployability,"es to CreateFilterSet and ExtractCallset from 30K run (#7423); - Also changed file size from Int to Float in SumBytes task python (#7429); - Adding the subpopulation calculations to the VAT creation WDL (#7399); - 154 De obfuscate (#7435); - filter on gvs_ids for workflow (#7428); - update for assign ids and changes in import (#7439); - need to loop through sets when moving to done (#7440); - add option for create filter set to use sample_info with is_loaded (#7434); - remove dead branch (#7443); - Scaling the VAT -- switch the input to take in a file of vcf shard file names (#7446); - dockstore testing: move validate vat inputs (#7449); - Update GVS sample QC to support multiple callsets per datasset [VS-177] (#7451); - Update GvsImportGenomes.wdl (#7462); - Add extraction uuid BQ label to GvsPrepareCallstep from GvsExtractCohortFromSampleNames (#7458); - Add manifest summary file to GvsExtractCallset (#7457); - Create workflow to create and populate alt_allele table [VS-51] (#7426); - Added additional workflow and README updates for Quickstart [VS-183] (#7463); - formatting on sample QC README; - formatting change #2 to sample QC README; - address VS-152, remove extra headers from extract (#7466); - Update GvsExtractCallset.example.inputs.json (#7469); - Add ability to copy interval list files to gs directory [VS-191] (#7467); - add an expiration date to the temp tables (#7455); - fix the check for duplicates in import genomes (#7470); - added job ID to alt_allele population call output [VS-194] (#7473); - added steps and deliverables to GVS README [VS-181] (#7452); - Ah check the is loaded field in feature extract (#7475); - changes to put pet data directly into data table (#7478); - added override for ExtractTasks' preemptible value (#7477); - bcftools to the rescue (#7456); - execute_with_retry() refactor and error handling improvements [VS-159] (#7480); - Small updates to GvsExtractCallset from beta callset, new workflow for re-scattered shards (#7493); - add f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:18211,update,updates,18211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,"['Update', 'update']","['Update', 'updates']"
Deployability,es.ReferenceFileSource.<init>(ReferenceFileSource.java:31); 	at org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource.<init>(ReferenceMultiSource.java:49); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:394); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:360); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:351); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [1c0c33a8-53ac-4407-a452-bb8622fd3060] entered state [ERROR] while waiting for [DONE].; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:10337,deploy,deploy,10337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,6,['deploy'],['deploy']
Deployability,"es.sorted.BQSR.bam -O /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.g.vcf.gz --bam-output /juffowup2/malaria/haplotypecaller_arg_testing/PG0004-CW.haplotype_caller.fixed_bam_file.with_pileup.bamout.bam -contamination 0 --sample-ploidy 2 --linked-de-bruijn-graph --pileup-detection true --pileup-detection-enable-indel-pileup-calling true --max-reads-per-alignment-start 20 --annotate-with-num-discovered-alleles -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 50 -GQB 60 -GQB 70 -GQB 80 -GQB 90 -G StandardAnnotation -G StandardHCAnnotation -ERC GVCF --verbosity INFO; 14:14:15.323 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardAnnotation) is enabled for this tool by default; 14:14:15.328 WARN GATKAnnotationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default; 14:14:15.388 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/juffowup/gatk/build/install/gatk/lib/gkl-0.8.11.jar!/com/intel/gkl/native/libgkl_compression.so; 14:14:15.435 INFO HaplotypeCaller - ------------------------------------------------------------; 14:14:15.439 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.4.0.0-44-g1529aa1-SNAPSHOT; 14:14:15.439 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:14:15.439 INFO HaplotypeCaller - Executing as jonn@dsde-methods-jonn-juffowup on Linux v5.4.0-1104-gcp amd64; 14:14:15.439 INFO HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v17.0.7+7; 14:14:15.440 INFO HaplotypeCaller - Start Date/Time: July 26, 2023 at 2:14:15 PM UTC; ...; 22:15:34.977 INFO HaplotypeCaller - Shutting down engine; [July 26, 2023 at 10:15:34 PM UTC] org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller done. Elapsed time: 481.33 minutes.; Runtime.totalMemory()=47982837760; java.lang.NegativeArraySizeException: -896617256; at org.broadinstitut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8440:1422,install,install,1422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8440,1,['install'],['install']
Deployability,es/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/vcfMask.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/ad-bug-input.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/CEUTrio.20.21.missingIndel.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/chr21.bad.pl.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combined_genotype_gvcf_exception.nocall.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combined_genotype_gvcf_exception.original.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.3.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/gvcf.basepairResolution.gvcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/gvcfExample1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/leadingDeletion.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.combined.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.delOnly.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.depr.delOnly.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/testUpdatePGT.gvcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/MarkDuplicatesGATK/example.chr1.1-1K.markedDups.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/walkers/MarkDuplicatesGATK/example.chr1.1-1K.unmarkedDups.bam.bai,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:57206,pipeline,pipeline,57206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['pipeline'],['pipeline']
Deployability,"esPipelineSpark - ------------------------------------------------------------; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-23-g6e1cc8c-SNAPSHOT; 13:47:29.831 INFO BwaAndMarkDuplicatesPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Executing as root@973f3a3a3407 on Linux v4.4.0-124-generic amd64; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-8u131-b11-1~bpo8+1-b11; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - Start Date/Time: May 21, 2018 1:47:29 PM UTC; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.832 INFO BwaAndMarkDuplicatesPipelineSpark - ------------------------------------------------------------; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Version: 2.14.3; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - Picard Version: 2.18.2; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 13:47:29.833 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 13:47:29.834 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 13:47:29.834 WARN BwaAndMarkDuplicatesPipelineSpark - ; ```; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:20559,patch,patch,20559,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['patch'],['patch']
Deployability,"etting the following error. However, the chr6.raw.excessHet.vcf.gz does not contain that multiallelic variant site. This variant is the nearest at that location but the error is for a variant at chr6:26914009 with different alleles =[G*, GTGTA, GTGTATA, GTGTGTA] . . `chr6 26914005 . A ATG,G 15390.7 PASS AC=278,2;AF=0.04,0.0002879;AN=6948;AS_BaseQRankSum=-0.2,0.4;AS_FS=0.522,0;AS_InbreedingCoeff=-0.0012,0.1805;AS_MQ=58.75,59.2;AS_MQRankSum=0,-3.2;AS_QD=2.56,0.02;AS_ReadPosRankSum=0.1,0.3;AS_SOR=0.652,0.724;BaseQRankSum=-0.152;DP=118313;ExcessHet=2.9774;FS=0.518;InbreedingCoeff=0.0016;MLEAC=278,2;MLEAF=0.04,0.0002879;MQ=56.9;MQRankSum=-0.962;QD=2.57;ReadPosRankSum=0.193;SOR=0.712; `. ```; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chr6:26914009 [VC chr6.raw.excessHet.vcf.gz @ chr6:26914009 Q276902.75 of type=INDEL alleles=[G*, GTGTA, GTGTATA, GTGTGTA] attr={AC=[4269, 29, 5], AF=[0.620, 4.209e-03, ; #### Steps to reproduce; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode INDEL; ```. #### Expected behavior; Create recalibrated vcf file. #### Actual behavior; ```; Caused by:; Process `ApplyRecalibrationIndels` terminated with an error exit status (3). Command executed:. #!/bin/bash; /share/pkg.7/gatk/4.2.6.1/install/gatk-4.2.6.1/gatk --java-options -Xms5g ApplyVQSR -O indel.recalibrated.vcf.gz -V chr6.raw.excessHet.vcf.gz -AS --recal-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.recal --use-allele-specific-annotations --tranches-file /restricted/projectnb/kageproj/gatk/pVCF.vqsr/indels.tranches --truth-sensitivity-filter-level 99.0 --create-output-variant-index true -mode I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8054:1114,install,install,1114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8054,1,['install'],['install']
Deployability,"exFeatureFile - Start Date/Time: January 26, 2018 12:17:06 AM GMT; 00:17:06.846 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.846 INFO IndexFeatureFile - ------------------------------------------------------------; 00:17:06.847 INFO IndexFeatureFile - HTSJDK Version: 2.13.2; 00:17:06.847 INFO IndexFeatureFile - Picard Version: 2.17.2; 00:17:06.848 INFO IndexFeatureFile - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 00:17:06.849 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 00:17:06.850 INFO IndexFeatureFile - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 00:17:06.850 INFO IndexFeatureFile - Deflater: IntelDeflater; 00:17:06.855 INFO IndexFeatureFile - Inflater: IntelInflater; 00:17:06.856 INFO IndexFeatureFile - GCS max retries/reopens: 20; 00:17:06.858 INFO IndexFeatureFile - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 00:17:06.859 INFO IndexFeatureFile - Initializing engine; 00:17:06.860 INFO IndexFeatureFile - Done initializing engine; 00:17:07.292 INFO FeatureManager - Using codec VCFCodec to read file file://bad.vcf; 00:17:07.310 INFO IndexFeatureFile - Shutting down engine; [January 26, 2018 12:17:07 AM GMT] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=512229376; java.lang.IllegalStateException: the progress meter has not been started yet; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:697); at org.broadinstitute.hellbender.engine.ProgressMeter.stop(ProgressMeter.java:230); at org.broadinstitute.hellbender.utils.codecs.ProgressReportingDelegatingCodec.isDone(ProgressReportingDelegatingCodec.java:104); at htsjdk.tribble.index.IndexFactory$FeatureIterator.readNextFeature(IndexFactory.java:522); at htsjdk.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4269:2304,patch,patch,2304,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4269,1,['patch'],['patch']
Deployability,"existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK GenotypeGVCFs. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I conducted joint-call with GATK GenotypeGVCfs for two samples (proband, and mother). I have identified the maternal variant information filled with ""."" in jointcall.vcf which is the output file of GTAK GenotypeGVCfs (see, figure 1). For chromosome MT, all variant information field values were fileld with ""."", instead of mother's g.vcf. . ; ![image](https://user-images.githubusercontent.com/45510932/207542098-cd4af866-c209-405f-9553-3810275f7d8e.png); Figure 1. Jointcall.vcf of proband, and mother. redbox refers to maternal variant information. . . Except for chromosomes X, Y, and MT, these issues did not occur. In addition, I suspected the false positive variants filtering which is the advantage of GATK jointcall. however, the variants which have ""."" values do not seem to be false positive variants considering AD, DP, and variant allele frequency (VAF). #### Steps to reproduce; GATK version used: 3.8.1; ``",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8129:1283,release,release,1283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8129,1,['release'],['release']
Deployability,"f the CNV tools to be blocked for a long time because something else in the toolkit (like the `HaplotypeCaller`) is not ready for release. . There could be a `properties` file in the jar that controls which tools are exposed via the command-line -- this way we could publish a jar that exposes only the CNV tools, for example. An alternative approach would be to use branching and cherry-picking to do this kind of selective release, or split the GATK into even more repositories, but I'm not sure those approaches would be preferable. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215491991). This came out of a discussion between myself and @LeeTL1220 . ---. @lbergelson commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215493945). So a gatk release would contain different sets of tools sometimes? Wouldn't that be confusing? It seems like it would be better to always release different jars, or version sets of tools independently and release jars with the latest good release of each individual set of tools. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215494432). @lbergelson Well, we definitely still want there to be releases of the GATK toolkit in its entirety. If the CNV tools need to be released more frequently than this, they could be versioned/released separately and periodically incorporated into the toolkit-wide releases. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215495326). To be clear, though, this is very much still in the ""throwing out ideas for discussion"" phase, and alternate proposals are welcome provided they include the concept of a GATK-wide release, and make some provision for the situation where the CNV tools (or some other sub-category) are ready for release but other tools are not. -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:1146,release,release,1146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,4,['release'],['release']
Deployability,fail travis build if before_install / install blocks fail,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3444:38,install,install,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3444,1,['install'],['install']
Deployability,faultGradleLauncher$1.create(DefaultGradleLauncher.java:106); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:5024,Continuous,ContinuousBuildActionExecuter,5024,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,2,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,ference/Homo_sapiens_assembly38.2bit --aligner-index-image /mnt/1/reference/Homo_sapiens_assembly38.fasta.img --exclusion-intervals hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.kill.intervals --kmers-to-ignore hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.kill.kmers --cross-contigs-to-ignore hdfs://cw-test-m:8020/reference/Homo_sapiens_assembly38.kill.alts --breakpoint-intervals hdfs://cw-test-m:8020/output/intervals --fastq-dir hdfs://cw-test-m:8020/output/fastq --contig-sam-file hdfs://cw-test-m:8020/output/assemblies.sam --target-link-file hdfs://cw-test-m:8020/output/target_links.bedpe --exp-variants-out-dir hdfs://cw-test-m:8020/output/experimentalVariantInterpretations -- --spark-runner GCS --cluster cw-test --num-executors 20 --driver-memory 30G --executor-memory 30G --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 --conf spark.driver.userClassPathFirst=false; ```. It failed near the end of the pipeline. Here is the tail of the log:. ```; 20:38:14.368 INFO StructuralVariationDiscoveryPipelineSpark - Used 3549 evidence target links to annotate assembled breakpoints; 20:38:14.462 INFO StructuralVariationDiscoveryPipelineSpark - Called 662 imprecise deletion variants; 20:38:14.492 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 7234 variants.; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - INV: 184; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 4486; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1170; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1394; 18/01/12 20:38:16 WARN org.apache.spark.scheduler.TaskSetManager: Stage 17 contains a task of very large size (2518 KB). The maximum recommended task size is 100 KB.; 18/01/12 20:38:22 WARN org.apache.spark.scheduler.TaskSetManager: Stage 18 contains a task of very large size (2307 KB). The maximum recommended task size is 100 KB.;,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:1333,pipeline,pipeline,1333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['pipeline'],['pipeline']
Deployability,figure out how spark-submit --files works:; - [x] does it send files like it promises **yes**; - [x] where does it put them?; **I believe it puts them in the working directory of the executor.**; - [ ] does cache them between runs?; - [ ] how do updates to the files work if it caches them?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1689:246,update,updates,246,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1689,1,['update'],['updates']
Deployability,"filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to Ex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:21072,Update,Update,21072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Update'],['Update']
Deployability,"finally here. closing the main feature requests made in #2703 . Need to wait for #3752 and #3770 . Will generate a separate dummy PR about how to run the whole prototyping holistic SV interpretation tool. Some small patches need to be applied after this PR, basically for dumpster diving into ; * ambiguous events; * events where assembly contigs clearly couldn't tell a complete picture. but all parts of the diving suit are all available after this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3805:216,patch,patches,216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3805,1,['patch'],['patches']
Deployability,"fix #4649 . The cause of the exception is a new edge case that was not imagined when the reference region segmenting logic was initially written.; It is now covered, with updated tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4677:171,update,updated,171,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4677,1,['update'],['updated']
Deployability,fix SV pipeline default init script handling,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3467:7,pipeline,pipeline,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3467,1,['pipeline'],['pipeline']
Deployability,fix the script to validate-reads-spark-pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1921:39,pipeline,pipeline,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1921,1,['pipeline'],['pipeline']
Deployability,fixes #1448. MarkDuplicates spark was not writing an output bam when running with a standalone spark instance if the output path was a relative file path.; This fixes the problem by making any relative file paths into absolute file paths.; Added a check to see if no part files can be found so that errors like this will crash in the future instead of silently failing. No tests are added because it's still unclear how to test errors that only occur in certain spark configurations. `makeFilePathAbsolute()` should be redundant once #958 is finished,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1450:468,configurat,configurations,468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1450,1,['configurat'],['configurations']
Deployability,"fixes #1585 - getting IntelDeflater from htsjdk zipfile (it's not part of the jar!) and then putting the path to it as a default JVM argument. @lbergelson can you review? this is a bit of a hack, improvements welcome; Do check that, out of the box (on linux) you get the IntelDeflater. I run this:. ```; ./gradlew clean installDist; ./gatk-launch CountReads -I src/test/resources/large/NA12878.RNAseq.bam; ```. and check output for `IntelDeflater`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1587:320,install,installDist,320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1587,1,['install'],['installDist']
Deployability,fixes #1784 and #1780 which both caused problems with the intel deflater. note: this incorporates the changes in https://github.com/broadinstitute/gatk/pull/1717 because without them it fails with. ```; java.lang.UnsupportedOperationException: Directory size listing not supported on GCS.; at org.broadinstitute.hellbender.utils.gcs.BucketUtils.dirSize(BucketUtils.java:301); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.getRecommendedNumReducers(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:237); at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:313); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1785:628,pipeline,pipelines,628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1785,1,['pipeline'],['pipelines']
Deployability,fixes https://github.com/broadinstitute/gatk/issues/1903. also updated the system properties in build.gradle and gatk-launch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1964:63,update,updated,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1964,1,['update'],['updated']
Deployability,"fixes https://github.com/broadinstitute/gatk/issues/1904. 2 hacks needed to be done for now:; 1) bwa loses tags so we keep track of which original read corresponds to which aligned read by creating a subclass of `ShortRead` (the class that jBWA uses for JNI) and putting the original read in a pointer in there.; 2) bwa requires a fasta reference while reads pipeline wants a 2bit reference (in the BROADCAST mode, which is the default). The workaround for now is to pass two reference files one of each kind",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1981:359,pipeline,pipeline,359,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1981,1,['pipeline'],['pipeline']
Deployability,"follow deletion of prototype tools. (all, PR issued by 12/15)~~; - ~~(Reach) Collect all legacy code in a new package.~~; - [x] Delete old pipelines. (SL, #3935 awaiting review). ModelSegments pipeline:; - [x] Review and merge denoising PR (#3820).; - [x] Add WDL changes from @LeeTL1220, @meganshand, and @jsotobroad to dev branch. (Note that we exposed PreprocessIntervals.bin_length in these WDLs; I'm assuming that https://github.com/broadinstitute/cromwell/issues/2912 will allow this to be specified via the json, so I reverted this change.); - [x] Make simple improvements to ReCapSeg caller (#3825).; - [x] Review and merge modeling/WDL PR. (#3913 awaiting review. Note that this PR also deletes the old germline WDL.); - ~~Write MultidimensionalKernelSegmenterUnitTest.~~ (SL, punting, filed #3916); - ~~Write ModelSegmentsIntegrationTest.~~ (SL, punting, filed #3916); - [x] Preliminary PCAWG or HCC1143 purity evaluation. (@LeeTL1220) (LL, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (PR #4010 awaiting review.); - [x] Add SM tag and sequence dictionary headers to all appropriate files and sort accordingly. (SL, #3914 awaiting review); - [x] Update tutorial data. (@MartonKN); - [ ] (Reach) Add VCF output.; - [ ] (Reach) Add PG tags to all files.; - [ ] (Reach) Replace ReCapSeg caller with improved version. (@MartonKN). gCNV pipeline:; - [x] Review and merge Python code (#3838). (MB and SL, PR #3925 awaiting review.); - [x] CLI for ploidy determination (cohort). (@samuelklee); - [x] CLI for ploidy determination (case). (@samuelklee); - [x] CLI for calling (cohort). (@samuelklee); - [x] CLI for calling (case). (@samuelklee); - [ ] CLI for post-processing calls. (@asmirnov239) (AS, PR issued by 12/4); - [x] Python environment. (Update: I've verified that gCNV works on the gsa server with a manual setup of conda (python=3.6) + @mbabadi's pip install---altho",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:1365,Update,Update,1365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,1,['Update'],['Update']
Deployability,"for demonstrating how to using the prototyping tool. To run the tool:. ```bash; ./gradlew clean installAll. ./gatk-launch SvDiscoverFromLocalAssemblyContigAlignmentsSpark \; -I PATH_TO_LOCAL_ASSEMBLY_SAM \; -O OUTPUT_DIR \; -R PATH_TO_2BIT_REF \; --writeSAM; ```. This will scan through the assembly sam file, search for split alignments and output in a directory several VCF files, each for one raw type of variant.; This does not output VCF records for interpreted complex variant yet (see #3805 ), but put them in a more human-friendly format.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3806:96,install,installAll,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3806,1,['install'],['installAll']
Deployability,for the tieout of the VCF origin VAT and the VDS origin VAT we want to compare apples to apples (or at least run them both on the same nirvana version with the same nirvana annotations). TODO:; This is the branch I will want to tag so that future users can start here if they want to run VCFs thru the VAT pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8117:306,pipeline,pipeline,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8117,1,['pipeline'],['pipeline']
Deployability,"for writing BAM files. This reduces memory usage since the reads; don't need to all be stored in memory. Also add some missing calls to addDataflowRunnerArgs in the integration; tests to ensure the correct dataflow runner is being picked up. This is related to https://github.com/broadinstitute/hellbender/issues/771, for the Spark/Hadoop side.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/845:165,integrat,integration,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/845,1,['integrat'],['integration']
Deployability,"force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:1981,INSTALL,INSTALLDIRGATK,1981,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"form(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droazen @lbergelson @LeeTL1220 @ldgauthier @yfarjoun This is just one example of how using recently developed ML frameworks could make our lives orders of magnitude easier. The sooner we can develop a strategy to leverage these in Java land, the better!. I'd expect that roughly the same amount of code would be needed to specify this model using Stan. Interfaces for Stan exist for many other languages, so it might be relatively easy to come up with some Java bindings. However, one downside is that Stan is not built on top of a computational graph (e.g., theano/tensorflow), so we don't get GPU/distributed computing for free. I don't think this is a deal breaker, but it's something we should consider. ---. @samuelklee commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302429919). It should be said that I consider this a major blocker for the CNV team. I don't think it makes sense to rebuild the somatic pipeline to include the new coverage model until we move to this ADVI framework or something like it. I do think @mbabadi should continue adding features (such as common CNV calling) to his non-ADVI germline implementation, so that we can be in a position to start calling on gnomAD or other large cohorts, but that we should eventually move the germline tool over to this framework as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:4137,pipeline,pipeline,4137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['pipeline'],['pipeline']
Deployability,"from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:27422,update,update,27422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['update'],['update']
Deployability,from snapshots to released versions.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3127:18,release,released,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3127,1,['release'],['released']
Deployability,"ft/gatk-4.4.0.0# conda --version; conda 23.9.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: - Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.vn0sukco.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:1483,Install,Installing,1483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"g to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:6871,update,update,6871,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['update'],['update']
Deployability,"g.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:34); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:290); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:271); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:893); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289). My command looks like this:; ~/bioinformatics/programs/GATK/gatk-4.0.3.0/gatk --java-options ""-Xmx4g"" Mutect2 -R /Users/loeblabm11/bioinformatics/reference/human/hg19/hg19.fa -I 20171027_BN31_python.dcs.filt.no_overlap.bam -tumor BN31 -O 20171027_BN31_python.dcs.MuTect2.vcf -bamout 20171027_BN31_python.dcs.MuTect2.bam --max-reads-per-alignment-start 0 --max-population-af 1 --disable-tool-default-read-filters. and I'm running on MacOSX Sierra (10.12.6). java -version returns ; java version ""1.8.0_151""; Java(TM) SE Runtime Environment (build 1.8.0_151-b12); Java HotSpot(TM) 64-Bit Server VM (build 25.151-b12, mixed mode). The input file is position-sorted output from Duplex Sequencing pipelines, and has been through IndelRealigner (from GATK 3.7), and ClipOverlappingReads from FgBio. It has been indexed. . I can put together a full bug report, complete with my input file, if necessary; I just want an idea of what might be happening. . Brendan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11790/error-in-mutect2/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4665:7199,pipeline,pipelines,7199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4665,1,['pipeline'],['pipelines']
Deployability,"g.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3006,deploy,deploy,3006,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['deploy'],['deploy']
Deployability,g/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/selectVariantsInfoField.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/test.dup.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/tetra-diploid.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/tetraploid-multisample-sac.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/tetraploid-multisample.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/vcfexample2DiscordanceConcordance.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/vcfexample2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/UpdateVCFSequenceDictionary/exampleBAM.sam; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/UpdateVCFSequenceDictionary/exampleFASTA.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/expected.soap_gatk_annotated.AMD.table; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/multiallelic_gt.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/multiallelic.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/soap_gatk_annotated.noChr_lines.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/soap_gatk_annotated.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/vcfexample2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/vcfexample.noSamples.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:65684,Update,UpdateVCFSequenceDictionary,65684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,gCNV in the CASE mode now fills in all hidden DenoisingModelConfig and CopyNumberCallingConfig arguments from the input model configuration. . This addresses issue #6994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7464:126,configurat,configuration,126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7464,1,['configurat'],['configuration']
Deployability,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); GATK CalibrateDragstrModel. ### Affected version(s); - [x] Latest public release version [4.3.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; When running CalibrateDragstrModel in parallel mode, the supplied reference isn't detected correctly causing the following error stack trace:. ```bash; Using GATK jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx72g -jar /usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar CalibrateDragstrModel --input input.cram --output input.txt --reference hg38.fa --str-table-path hg38.zip --threads 12 --intervals fasta_bed.bed --tmp-dir .; 10:24:21.117 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/share/gatk4-4.3.0.0-0/gatk-package-4.3.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:21.289 INFO CalibrateDragstrModel - --------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:1291,release,release,1291,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['release'],['release']
Deployability,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6772:1306,release,release,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6772,8,['release'],['release']
Deployability,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5719:1306,release,release,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5719,1,['release'],['release']
Deployability,gatk-launch complains when running a spark tool if installSpark has been run but installDist hasn't,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1314:51,install,installSpark,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1314,2,['install'],"['installDist', 'installSpark']"
Deployability,"gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar --help; Error: Invalid or corrupt jarfile /home/athchu/bin/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; `; Next, I moved on to git clone the gatk repository, trying to build gatk. Again, I stay in the java ""1.7.0_91"" gatk env that I already created. But I got this error msg this time:; `; ./gradlew localJar; Gradle 7.5.1 requires Java 1.8 or later to run. You are currently using Java 1.7.; `; When I switch back to the server default java (1.8.0_292-b10), i got another error msg.; `; java -version; openjdk version ""1.8.0_292""; OpenJDK Runtime Environment (build 1.8.0_292-b10); OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode). ./gradlew localJar. > Configure project :; Warning: using Java 1.8 but only Java 17 has been tested. FAILURE: Build failed with an exception. * Where:; Build file '/home/athchu/bin/gatk/build.gradle' line: 141. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > A Java 17 compatible (Java 17 or later) version is required to build GATK, but 1.8 was found. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; `; So to sum up, my issues are :; 1) downloaded gatk-4.4.0.0 but it contained invalid jar file and i cannot run GATK; 2) following the github instruction, I cannot build gatk under java1.7.0_91, because it is incompatible to GRADLE7.5.1.; 3) built gatk using java 1.8.0_292 failed because gatk is java 17 compatible. Would you please advise on what I shall do? The docker installed in my sever doesn't not work....",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8432:2272,install,installed,2272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8432,1,['install'],['installed']
Deployability,gatk-protected should tag releases using annotated tags rather than unannotated tags.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2953:26,release,releases,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2953,1,['release'],['releases']
Deployability,gatkcondaenv.yml ; - uses python 3.6.10. how to update the python version?. I did try with:; conda update --all. but it does not work. . Thankyou,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8127:48,update,update,48,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8127,2,['update'],['update']
Deployability,ge of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleTable task volatile (#8011); - Restore withdrawn [VS-581] (#8006); - Km gvs add storage cost and cleanup doc (#8012); - Updating documentation to reflect the changed outputs [VS-565] (#8014); - File of callset samples -> samples marked as 'withdrawn' in GVS [VS-436] (#8009); - fix quota guidelines for CPUs (#8016); - Add in ability to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script gener,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:27558,Update,Update,27558,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Update'],['Update']
Deployability,"ge tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __SAMPLES tables to Prepare [VS-697] (#8079); - ""build-base"" Docker image for faster variantstore image builds [VS-712] (#8085); - GVS / Hail VDS integration test [VS-639] (#8086); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:30397,integrat,integration,30397,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,"['integrat', 'update']","['integration', 'updates']"
Deployability,ger.scala:1029); 	at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:792); 	at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1350); 	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:122); 	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88); 	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); 	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56); 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1488); 	at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:650); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:191); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:316); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5680:6737,pipeline,pipelines,6737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680,1,['pipeline'],['pipelines']
Deployability,"google-cloud-java: update to the official 0.59.0 release, and move off of our custom fork",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5135:19,update,update,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5135,2,"['release', 'update']","['release', 'update']"
Deployability,"gradle uploadArchives will perform a maven release (currently a snapshot release). Unfortunately the maven plugin has an ""install"" task which installs to the local repo, so it's now necessary to specify `installApp` or `installDist` instead of just `gradle install`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/482:43,release,release,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/482,7,"['install', 'release']","['install', 'installApp', 'installDist', 'installs', 'release']"
Deployability,"gradlew install script failed ungracefully with ""disk quota exceeded""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:8,install,install,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['install'],['install']
Deployability,gram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53579,deploy,deploy,53579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,"gram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_km",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4540,deploy,deploy,4540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,"gram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); ```; and; ```; java.lang.IllegalStateException: Allele in genotype G* not in the variant context [C*, T]; 	at htsjdk.variant.variantcontext.VariantContext.validateGenotypes(VariantContext.java:1360); 	at htsjdk.variant.variantcontext.VariantContext.validate(VariantContext.java:1298); 	at htsjdk.variant.variantcontext.VariantContext.<init>(VariantContext.java:401); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:494); 	at htsjdk.variant.variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:488); 	at org.broadinstitute.hellbender.tools.exome.orientationbiasvariantfilter.OrientationBiasFilterer.annotateVariantContextsWithFilterResults(OrientationBiasFilterer.java:216); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalSuccess(FilterByOrientationBias.java:211); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:840); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); ```. I tested using `gatk-package-4.beta.1-local.jar` and pre-release alpha version `gatk-4.alpha.2-1134-ga9d9d91-SNAPSHOT/gatk-package-4.alpha.2-1134-ga9d9d91-SNAPSHOT-local.jar`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3291:3587,release,release,3587,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3291,1,['release'],['release']
Deployability,"gram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-08 09:14 output.bam.parts/_SUCCESS; -rw-r--r-- 3 yaron yaron 62019 2017-06-08 09:14 output.bam.parts/part-r-00000.bam; -rw-r--r-- 3 yaron yaron 16 2017-06-08 09:14 output.bam.parts/part-r-00000.bam.splitting-bai; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:6374,deploy,deploy,6374,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['deploy'],['deploy']
Deployability,gram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.conc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:10160,deploy,deploy,10160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['deploy'],['deploy']
Deployability,gram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:233); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.appl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:5933,deploy,deploy,5933,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['deploy'],['deploy']
Deployability,gram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:11620,deploy,deploy,11620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['deploy'],['deploy']
Deployability,gram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:43801,deploy,deploy,43801,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['deploy'],['deploy']
Deployability,"gram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:17631,deploy,deploy,17631,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['deploy'],['deploy']
Deployability,"gram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx56g -Djava.io.tmpdir=./tmp -jar /opt/gatk-4.2.0.0/gatk-package-4.2.0.0-local.jar GenotypeGVCFs -R ../../01.ref/MS/genome.fasta -V gendb://genomeDB.Chr23 -all-sites -O Chr23.raw.vcf.gz. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?] V4.2.0.0; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; There are no results generated.; ![image](https://github.com/broadinstitute/gatk/assets/103233242/2c505328-c0a0-473a-bd64-63b2137e0f06). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. 1:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx48g -Djava.io.tmpdir=./tmp"" HaplotypeCaller -R ../../01.ref/MS/genome.fasta -I ../../02.mapping/MS/F10.sort.markdup.bam -L Chr01 -ERC GVCF -O F10/F10.Chr01.g.vcf.gz ; 2:singularity exec ~/biosoft/resequence/Reseq_genek.sif gatk --java-options ""-Xmx56g -Djava.io.tmpdir=./tmp -DGATK_STACKTRACE_ON_USER_EXCEPTION=true"" GenomicsDBImport --sample-name-map gvcf",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8415:6453,release,release,6453,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8415,1,['release'],['release']
Deployability,"gs://depmapomicsdata/1000g_pon.hg38.vcf.gz; 20:59:55.629 INFO Mutect2 - Shutting down engine; [October 4, 2021 8:59:55 PM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 0.12 minutes.; Runtime.totalMemory()=876609536; code: 403; message: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; reason: forbidden; location: null; retryable: false; com.google.cloud.storage.StorageException: pet-102022583875839491351@broad-firecloud-ccle.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:229); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:406); at com.google.cloud.storage.StorageImpl$4.call(StorageImpl.java:217); ...; ```. This happens while it runs the command:. ```; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx15500m\ ; -jar /root/gatk.jar Mutect2 -R gs://genomics-public-data/resources/broad/hg38/v0/Homo_sapiens_assembly38.fasta\ ; -I gs://cclebams/hg38_wes/CDS-00rz9N.hg38.bam -tumor BC1_HAEMATOPOIETIC_AND_LYMPHOID_TISSUE --germline-resource gs://gcp-public-data--gnomad/release/3.0/vcf/genomes/gnomad.genomes.r3.0.sites.vcf.bgz\ ; -pon gs://gatk-best-practices/somatic-hg38/1000g_pon.hg38.vcf.gz\ ; -L gs://fc-secure-d2a2d895-a7af-4117-bdc7-652d7d268324/7a157f4a-7d93-4a3e-aaf4-c41833463f5a/Mutect2/3be8ce8e-1075-4063-bc43-6f61e386c3f5/call-SplitIntervals/cacheCopy/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list\ ; -O output.vcf.gz --f1r2-tar-gz f1r2.tar.gz --gcs-project-for-requester-pays broad-firecloud-ccle; ```. But I gave read (both regular and legacy) access to gs://cclebams (this is a requester pays bucket). This was done on GATK 4.2.2 docker. Best,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7492:1992,release,release,1992,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7492,1,['release'],['release']
Deployability,"guments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. I see some changed in master (probably fixing this issue too), but no description: https://github.com/broadinstitute/gatk/pull/8610/files#diff-5c3c54d49d09fd7ab0957b7c3185e22c6161e225b9e3ed65e72716fa2a635a96",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:3640,Install,Installing,3640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,"h/supportingMultiA.vcf; > ; > which outputs:; > INFO field at 1:768589 .. INFO tag [AC=1] expected different number of; > values (expected 2, found 1),INFO tag [AF=0.00047] expected different; > number of values (expected 2, found 1); > Notes; > ; > Currently, all the validation modes call out to HTSJDK. Do we want to put; > the new functionality there as well?; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1053. ---. @ldgauthier commented on [Fri Jul 17 2015](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-122308040). Today I learned that the way we currently build GATK, you can't point to a local htsjdk jar anymore, so this task will be two-fold:; 1) Make a PR to htsjdk with a new function in the VariantContext class for validateInfoFieldCounts(VCFInfoHeaderLine headerLine) or similar; add a test to VariantContextUnitTest.java; 2) After change 1) is merged, update ValidateVariants accordingly to use the new function and add a test to its integration tests. ---. @vdauwera commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222213763). @ldgauthier is this still a thing? (in the sense of not having been addressed in htsjdk). ---. @ldgauthier commented on [Fri May 27 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-222214083). Still a thing. No work has been done here AFAIK. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-260465013). This seems like fairly low-hanging fruit -- @ronlevine . ---. @ronlevine commented on [Wed Nov 23 2016](https://github.com/broadinstitute/gsa-unstable/issues/1053#issuecomment-262613152). @ldgauthier Shouldn't a locus without genotypes bypass `AC` validation, given it's defined as: `Allele count in genotypes, for each ALT allele, in the same order as listed`?. ---. @ldgauthier commented on [Wed No",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2507:4514,update,update,4514,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2507,2,"['integrat', 'update']","['integration', 'update']"
Deployability,hail updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8762:5,update,updates,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8762,1,['update'],['updates']
Deployability,hardcode in less partitions so that we can test where things are slowing down/breaking in the VAT pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8780:98,pipeline,pipeline,98,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8780,1,['pipeline'],['pipeline']
Deployability,hardening standard quickstart pipeline against errors when a location is specified,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8047:30,pipeline,pipeline,30,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8047,1,['pipeline'],['pipeline']
Deployability,"hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733); > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); > Caused by: java.nio.file.ProviderNotFoundException: Provider ""maprfs"" not found; > 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); > 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); > 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); > 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); > 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:178); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:177); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69); > 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); > 	at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:3838,deploy,deploy,3838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['deploy'],['deploy']
Deployability,"help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilter; s false --minimumMappingQuality 20; [16 November 2017 4:51:50 PM] Executing as vlad@spartan.hpc.unimelb.edu.au on Linux 4.13.12-1.el7.elrepo.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_121-b15; Version: 4.beta.5; 16:51:50.648 INFO HaplotypeCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 16:51:50.648 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 16:51:50.649 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 16:51:50.649 INFO HaplotypeCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 16:51:50.649 INFO HaplotypeCaller - Deflater: IntelDeflater; 16:51:50.649 INFO HaplotypeCaller - Inflater: IntelInflater; 16:51:50.649 INFO HaplotypeCaller - GCS max retries/reopens: 20; 16:51:50.649 INFO HaplotypeCaller - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 16:51:50.649 INFO HaplotypeCaller - Initializing engine; 16:51:51.056 INFO FeatureManager - Using codec BEDCodec to read file file:///home/vlad/tmp/debug_gatk/bad_87-88.bed; 16:51:51.063 INFO IntervalArgumentCollection - Processing 1 bp from intervals; 16:51:51.068 INFO HaplotypeCaller - Done initializing engine; 16:51:51.075 INFO HaplotypeCallerEngine - Disabling physical phasing, which is supported only for reference-model confidence output; 16:51:51.293 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 16:51:51.509 WARN PossibleDeNovo - Annotation will not be calculated, must provide a valid PED file (-ped) from the command line.; 16:51:51.762 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/vlad/bcbio/anaconda/share/gatk4-4.0b5-0/gatk-package-4.beta.5-local.jar!/com/intel/gkl/native/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3845:7462,patch,patch,7462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3845,1,['patch'],['patch']
Deployability,"her missing rate due to the inaccurate GQ=0. . Directly calling the VCF with HaplotypeCaller without the gVCF intermediate gVCF file calculates the correct GQ score. Freebayes also calculates a correct GQ on these samples.; [rs429358_gq_dp.pdf](https://github.com/broadinstitute/gatk/files/2612419/rs429358_gq_dp.pdf). #### Steps to reproduce. I am seeing this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:1581,Pipeline,Pipeline,1581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,1,['Pipeline'],['Pipeline']
Deployability,"hes, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:3745,release,release-ready,3745,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release-ready']
Deployability,"hi, I downloaded the hg38 database from the following URL two months ago, which contains some known sites of vcf files such as snp and indel;; https://gatk.broadinstitute.org/hc/en-us/articles/360035890811；GATK resource bundle; But now I found that the contents of these vcf files cannot be viewed. I think the vcf files should be viewed under normal circumstances; now I want to restart download the data; but the official website was updated two days ago, can you give me a website to download the data?; thanks. the error for view is:; 1000G_phase1.snps.high_confidence.hg38.vcf may be a binary file;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6988:436,update,updated,436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6988,1,['update'],['updated']
Deployability,"htsjdk does not support the latest VCF/BCF specs, and it's starting to hurt us (see, eg., https://github.com/broadinstitute/gatk/issues/2056). Let's fix this. The changes to the spec from 4.2 -> 4.3 can be seen by cloning https://github.com/samtools/hts-specs and running:. ```; latexdiff VCFv4.2.tex VCFv4.3.tex > diff.tex; pdflatex diff.tex; ```. and then examining `diff.pdf` (note that you must have latex installed for this to work). . To build full pdfs of all the specs documents, run `make`. The major htsjdk classes involved are:. `VCFCodec` (handles VCF reading -- must be updated while retaining backwards compatibility with previous VCF 4.x versions). `VCFWriter` (handles VCF writing -- only needs to support writing the latest version of the spec). Note that `VCFCodec` shares a lot of code with `VCF3Codec` via the `AbstractVCFCodec` -- we may need to refactor this to better isolate the legacy v3 codec from the v4 codec. @cmnbroad has already started working on updating BCF support in the branch https://github.com/cmnbroad/htsjdk/tree/cn_bcf2. Before starting implementation, we should come up with an itemized summary of how we intend to deal with each change in the spec, and make sure we agree on the approach. This code is extremely performance-sensitive, so we need to trade off on performance vs. strict fidelity to the spec. For each spec change that requires a code change in htsjdk, we should be sure to add a good unit test. We should also add tests proving that support for older versions of the spec is not broken.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2092:410,install,installed,410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2092,2,"['install', 'update']","['installed', 'updated']"
Deployability,"htsjdk has had a bug for a while where it permissively would read any BCF 2.x version if x was greater than or equal to 1 (it only supports BCF 2.1, not version 2.2). In practical terms this didn't matter because BCF 2.2 uses ""IDX"" fields in some header lines, and those were rejected by htsjdk anyway. However, a recently change in htsjdk causes it to accept additional fields in header lines, so BCF 2.2 is no longer rejected due to the presence of these fields. . IndexFeatureFile `testUncompressedBCF2_2Index` is a negative test that relies on the fact that BCF2.2 is rejected, so its currently disabled. The BCF version checking is fixed in later versions of htsjdk (https://github.com/samtools/htsjdk/issues/1323), so at the next htsjdk upgrade, `testUncompressedBCF2_2Index` should be reenabled (though it might throw a different exception).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5838:743,upgrade,upgrade,743,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5838,1,['upgrade'],['upgrade']
Deployability,htsjdk has updated to a newer version of snappy (https://github.com/samtools/htsjdk/pull/872) which makes it compatible with the rest of the world's snappy. this means we can re-enable snappy usage in htsjdk which should give performance improvements in tools that use `SortingCollection`. this is a permanent solution to #2026 that replaces the changes we made in #2028,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3635:11,update,updated,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3635,1,['update'],['updated']
Deployability,"htsjdk versions older than 2.1.1 would remove NM and MD tags on bam->cram compression, and then automatically regenerate NM tags when reading cram. Starting with 2.1.1, in order to ensure lossless round-tripping, it no longer does either, and restores only the tags present in the compressed file . As a result, any cramfile read with 2.1.1+ that was generated with older htsjdk versions (or samtools) will fail validation due to missing NM tags. So this PR contains an updated cram file that contains NM tags for the SAMFileValidation tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1551:470,update,updated,470,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1551,1,['update'],['updated']
Deployability,"http://www.hammerlab.org/2015/02/27/monitoring-spark-with-graphite-and-grafana/. the task here to set this up and, if it's useful, update the readme on how to set it up and use. @laserson do you have experience with Graphite and Grafana",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1377:131,update,update,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1377,1,['update'],['update']
Deployability,"http://www.hammerlab.org/2015/07/25/spree-58-a-live-updating-web-ui-for-spark/. the task is to set this up and, if it's useful, update the readme on how to set it up and use. @laserson do you have experience with Spree?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1376:128,update,update,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1376,1,['update'],['update']
Deployability,"https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-filter-process.1.gz; git-lfs usr/share/man/man1/git-lfs-fsck.1.gz; git-lfs usr/share/man/man1/git-lfs-install.1.gz; git-lfs usr/share/man/man1/git-lfs-lock.1.gz; git-lfs usr/share/man/man1/git-lfs-locks.1.gz; git-lfs usr/share/man/man1/git-lfs-logs.1.gz; git-lfs usr/share/man/man1/git-lfs-ls-files.1.gz; git-lfs usr/share/man/man1/git-lfs-merge-driver.1.gz; git-lfs usr/share/man/man1/git-lfs-migrate.1.gz; git-lfs usr/share/man/man1/git-lfs-pointer.1.gz; git-lfs usr/share/man/man1/git-lfs-post-checkout",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8320:1513,install,install,1513,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320,1,['install'],['install']
Deployability,https://github.com/broadinstitute/gatk/blob/b4cba377e0aff179dbff615783506913e7fe3aa4/src/main/java/org/broadinstitute/hellbender/tools/LocalAssembler.java#L1272. The program can potentially fail to release a system resource.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7377:198,release,release,198,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7377,1,['release'],['release']
Deployability,"https://github.com/broadinstitute/gatk/blob/c6daf7dd02b866907fbfebad150baeb540c35bce/src/main/java/org/broadinstitute/hellbender/tools/walkers/sv/JointGermlineCNVSegmentation.java#L701. I'm running into a recurrent issue in JointGermlineCNVSegmentation, running after PostprocessGermlineCNVCalls in a gCNV pipeline. A number of batches are being merged in parallel - some of those succeed, some fail. It's not clear just yet if this is a deterministic failure, I'll re-run a few times and see if I can answer that. . ```text; org.broadinstitute.hellbender.exceptions.GATKException: Exception thrown at chrX:6383391 [VC SAMPLE_ID.segments.vcf.gz @ chrX:6383391-17732942 Q3076.53 of type=NO_VARIATION alleles=[N*] attr={END=17732942} GT=GT:CN:NP:QA:QS:QSE:QSS	0:1:581:1:3077:4:20 filters=. ... Caused by: java.lang.IllegalStateException: Encountered genotype with ploidy 1 but 2 alleles.; 	at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); 	at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.correctGenotypePloidy(JointGermlineCNVSegmentation.java:701); 	at org.broadinstitute.hellbender.tools.walkers.sv.JointGermlineCNVSegmentation.prepareGenotype(JointGermlineCNVSegmentation.java:682); ```. The VCF row in question is . ```text; chrX	6383391	CNV_chrX_6383391_17732942	N	.	3076.53	.	END=17732942	GT:CN:NP:QA:QS:QSE:QSS	0:1:581:1:3077:4:20; ```. The characterisation of this row as `type=NO_VARIATION alleles=[N*]` seems... partially correct? There is no variation at this locus, but I'm not sure why alleles is `N*`. In this situation, as I read it, the first clause should be satisfied: 1 allele, and allele is no-call. Instead the variant process is dying in the else side of the condition. Could you clarify if I'm interpreting this correctly?. Relevant versioning:; ```; 13:18:38.320 INFO JointGermlineCNVSegmentation - ------------------------------------------------------------; 13:18:38.321 INFO JointGermlineCNVSegmentation - The Genome Analy",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8834:306,pipeline,pipeline,306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8834,1,['pipeline'],['pipeline']
Deployability,"https://github.com/broadinstitute/gatk/issues/1411. The pipeline takes an readname sorted BAM and does MarkDuplicates and 2 steps of BQSR and saves sharded output. ; - legacy: SortSam + MarkDuplicates + BaseRecalibrator + PrintReads; - new: ReadsPipelineSpark. The test file is 35GB, readname sorted.; - [x] make a querynamesorted file and copy to cluster; `hdfs:///user/akiezun/data/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam`; - [x] run new code on it, measure time; - [x] run legacy code (picard + gatk3) on coordinate sorted bam, pinned to 1 CPU, 7GB ram",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1559:56,pipeline,pipeline,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1559,1,['pipeline'],['pipeline']
Deployability,https://github.com/broadinstitute/gatk/issues/1673 was fixed with the htsjdk update; this re-enables the test.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1962:77,update,update,77,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1962,1,['update'],['update']
Deployability,"hutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.Tas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:6984,deploy,deploy,6984,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"ian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. #### Expected behavior; Program runs :). #### Actual behavior; Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6053:1894,install,install,1894,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053,1,['install'],['install']
Deployability,"igFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '18570' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; 23:44:42.124 DEBUG ScriptExecutor - Result: 0; 23:44:42.124 INFO GermlineCNVCaller - Done initializing engine; 23:44:42.126 INFO GermlineCNVCaller - Intervals specified...; 23:44:42.534 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:44:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:5356,release,release,5356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['release'],['release']
Deployability,"igarReads done. Elapsed time: 99.33 minutes.; Runtime.totalMemory()=5453119488; htsjdk.samtools.util.RuntimeIOException: Attempt to add record to closed writer.; at htsjdk.samtools.util.AbstractAsyncWriter.write(AbstractAsyncWriter.java:57); at htsjdk.samtools.AsyncSAMFileWriter.addAlignment(AsyncSAMFileWriter.java:53); at org.broadinstitute.hellbender.utils.read.SAMFileGATKReadWriter.addRead(SAMFileGATKReadWriter.java:21); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.writeReads(OverhangFixingManager.java:358); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.flush(OverhangFixingManager.java:338); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.closeTool(SplitNCigarReads.java:192); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1091); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). This looks similar to issue #8232, but I've not been able to solve this using any of the fixes suggested on that page. I'm using Java version 8, the input BAMs were mapped using STAR via nfcore rnaseq without issue and I have plenty of space on my disk drive, even when specifying a temp directory. It also doesn't matter if I run the tool independently using the command above or as part of a pre-configured pipeline, and I get the same issue with SplitNCigarReads acting as if it has running out of space. . How do I fix this? I need this step to run variant calling on my rnaseq samples (I'm using the GATK best practices pipeline).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8522:10556,pipeline,pipeline,10556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8522,2,['pipeline'],['pipeline']
Deployability,"ignore, testing GitZen integration<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/4401'>Zendesk ticket #4401</a>)<br>gz#4401</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6380:23,integrat,integration,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6380,1,['integrat'],['integration']
Deployability,ileup - Defaults.CUSTOM_READER_FACTORY : ; 15:04:36.349 INFO Pileup - Defaults.EBI_REFERENCE_SERVICE_URL_MASK : http://www.ebi.ac.uk/ena/cram/md5/%s; 15:04:36.349 INFO Pileup - Defaults.NON_ZERO_BUFFER_SIZE : 131072; 15:04:36.349 INFO Pileup - Defaults.REFERENCE_FASTA : null; 15:04:36.349 INFO Pileup - Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:04:36.349 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 15:04:36.350 INFO Pileup - Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:04:36.350 INFO Pileup - Defaults.USE_CRAM_REF_DOWNLOAD : false; 15:04:36.350 INFO Pileup - Deflater IntelDeflater; 15:04:36.350 INFO Pileup - Initializing engine; WARNING: BAM index file /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bai is older than BAM /home/lichtens/broad_oncotator_configs/hcc_purity/SM-74NEG.bam; 15:04:38.560 INFO IntervalArgumentCollection - Processing 999914 bp from intervals; 15:04:38.630 INFO Pileup - Done initializing engine; 15:04:38.635 INFO ProgressMeter - Starting traversal; 15:04:38.636 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; JProfiler> Protocol version 49; JProfiler> Using JVMTI; JProfiler> JVMTI version 1.1 detected.; JProfiler> 64-bit library; JProfiler> Listening on port: 31757.; JProfiler> Attach mode initialized; JProfiler> Instrumenting native methods.; JProfiler> Can retransform classes.; JProfiler> Can retransform any class.; JProfiler> Retransforming 8 base class files.; JProfiler> Base classes instrumented.; JProfiler> Native library initialized; JProfiler> Using dynamic instrumentation; JProfiler> Time measurement: elapsed time; JProfiler> CPU profiling enabled; JProfiler> Initializing configuration.; JProfiler> Retransforming 3697 class files.; JProfiler> Configuration updated. ```. ![oncobuntu_mk3](https://cloud.githubusercontent.com/assets/2152339/22307273/583f61a8-e310-11e6-87ef-e87eaba7cf93.png),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2356:3935,configurat,configuration,3935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2356,3,"['Configurat', 'configurat', 'update']","['Configuration', 'configuration', 'updated']"
Deployability,"ility to tweak sample-every-Nth-variant parameter for SNP model creation (#8019); - add initial notebook copy pasta (#8008); - add sample_table_timestamp to GetNumSamplesLoaded (#8022); - Batched Avro export [VS-630] (#8020); - Updating references to old GATK for VS-620 (#8023); - VS-517 Use standard version of GetBQTableLastModifiedDatetime in GvsValidateVat (#8024); - Fix bug in GvsWithdrawSamples.wdl (#8026); - Ah 617 exposing the drop_state parameter to the GvsJointVariantCalling wdl used for beta (and internal customer) (#8032); - Expose maximum-training-variants VQSR parameter [VS-634] (#8029); - Callset statistics [VS-560] (#8018); - Check for withdrawn before exporting to AVRO files [VS-646] (#8039); - Small updates to GVS Integration WDL [VS-618] (#8042); - Rework Hail script generation [VS-616] (#8034); - Alpine based Variant Store Docker image [VS-648] (#8044); - update warp version (#7906); - Fail Avro extract and callset stats on bad filter name [VS-655] (#8046); - Vs 629 failure to retrieve job information during ingest (#8047); - Restore accidentally removed bcftools [VS-661] (#8051); - Allowing our pipeline to function with a sample size of one (#8055); - Vs 665 re create vcf for cd 68 po 52339 with ad padding fixed (#8057); - VS-665 and VS-620 updating code to use latest docker images containing Rori's AD calculation changes in extract (#8061); - updating the beta workflow to use the latest jar, representing the version of GATK George tested against the workflow (#8062); - VS-637 Address a couple of issues in SampleLoadStatus handling in GVSImportGenomes. (#8052); - Revert Alpinizing of apt dependent task [VS-688] (#8065); - Fix missing vat schema JSONs [VS-699] (#8072); - Fix integration expectations for fixed AD [VS-689] (#8066); - VS-698 Remove unnecessary columns from Call set statistics (#8073); - Fix Dockerfile nits that break 20.10.21 (#8078); - Nirvana 3.18.1 Docker images support [VS-661] (#8082); - Add option to not prepare __REF_DATA or __",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:28978,update,updates,28978,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,8,"['Integrat', 'pipeline', 'update']","['Integration', 'pipeline', 'update', 'updates']"
Deployability,"imple improvements to ReCapSeg caller (#3825).; - [x] Review and merge modeling/WDL PR. (#3913 awaiting review. Note that this PR also deletes the old germline WDL.); - ~~Write MultidimensionalKernelSegmenterUnitTest.~~ (SL, punting, filed #3916); - ~~Write ModelSegmentsIntegrationTest.~~ (SL, punting, filed #3916); - [x] Preliminary PCAWG or HCC1143 purity evaluation. (@LeeTL1220) (LL, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (PR #4010 awaiting review.); - [x] Add SM tag and sequence dictionary headers to all appropriate files and sort accordingly. (SL, #3914 awaiting review); - [x] Update tutorial data. (@MartonKN); - [ ] (Reach) Add VCF output.; - [ ] (Reach) Add PG tags to all files.; - [ ] (Reach) Replace ReCapSeg caller with improved version. (@MartonKN). gCNV pipeline:; - [x] Review and merge Python code (#3838). (MB and SL, PR #3925 awaiting review.); - [x] CLI for ploidy determination (cohort). (@samuelklee); - [x] CLI for ploidy determination (case). (@samuelklee); - [x] CLI for calling (cohort). (@samuelklee); - [x] CLI for calling (case). (@samuelklee); - [ ] CLI for post-processing calls. (@asmirnov239) (AS, PR issued by 12/4); - [x] Python environment. (Update: I've verified that gCNV works on the gsa server with a manual setup of conda (python=3.6) + @mbabadi's pip install---although I do get an ""install mkl"" warning from theano. We can discuss autoloading of this environment after release, but should at least have some clear documentation.); - [x] WDL and Cromwell tests. (SL, PR issued by 12/1); - [x] Preliminary evaluation. (MB, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (all, PE #3925 awaiting review.). Miscellaneous:; - [x] Update PreprocessIntervals behavior for WES. (Issue #3981, PR #4027 awaiting review.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:2211,Update,Update,2211,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,6,"['Update', 'install', 'release']","['Update', 'install', 'install---although', 'release']"
Deployability,"in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2372,INSTALL,INSTALLDIRGATK,2372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"in htsjdk 2.5.0, the deflater factory needs to be set up by hand. Hadoop-BAM needs to be updated to allow this and when it is (https://github.com/HadoopGenomics/Hadoop-BAM/issues/109), gatk should use the API to specify the IntelDeflater",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1969:89,update,updated,89,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1969,1,['update'],['updated']
Deployability,"ine.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:12:20.992 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:14826,pipeline,pipeline,14826,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:6049,deploy,deploy,6049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['deploy'],['deploy']
Deployability,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:9835,deploy,deploy,9835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['deploy'],['deploy']
Deployability,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:233); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:5608,deploy,deploy,5608,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['deploy'],['deploy']
Deployability,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:43476,deploy,deploy,43476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['deploy'],['deploy']
Deployability,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.str,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:11295,deploy,deploy,11295,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['deploy'],['deploy']
Deployability,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:17306,deploy,deploy,17306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['deploy'],['deploy']
Deployability,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53461,deploy,deploy,53461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4422,deploy,deploy,4422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:546); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721) ; ```. When I specify input as: `hdfs:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`, the tool tries to access `hdfs://cromwellhadooptest:-1/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`. **Stack trace for this:**; ```; java.lang.IllegalArgumentException: Wrong FS: hdfs://cromwellhadooptest:-1/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta, expected: hdfs://cromwellhadooptest; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:776); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:247); at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1725); at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1722); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1737); at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730:4724,deploy,deploy,4724,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730,1,['deploy'],['deploy']
Deployability,"ineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733); > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); > Caused by: java.nio.file.ProviderNotFoundException: Provider ""maprfs"" not found; > 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); > 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); > 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); > 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); > 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:178); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:177); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69); > 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); > 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); > 	at org.apache.spark.rdd.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:3910,deploy,deploy,3910,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['deploy'],['deploy']
Deployability,"ineProgram.runTool(CommandLineProgram.java:119); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733); > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); > Caused by: java.nio.file.ProviderNotFoundException: Provider ""maprfs"" not found; > 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); > 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); > 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); > 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); > 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:178); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:177); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69); > 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:3764,deploy,deploy,3764,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['deploy'],['deploy']
Deployability,"info: https://gatk.broadinstitute.org/hc/en-us/community/posts/360072797951--GenomicsDBException-Duplicate-sample-name-found-?page=1#community_comment_360012681791. `gatk --java-options ""-Xmx16g -Xms16g"" GenomicsDBImport --batch-size 24 --reader-threads 12 --genomicsdb-update-workspace-path /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/CPRs_100_proto/DB_chr1 --intervals chr1:118739963-147510543 --verbosity DEBUG -V /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/phase2_CPRs/SSC00007_CPR/SSC00007.haplotypeCalls.CPR.er.raw.vcf.gz`. ```; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16g -Xms16g -jar /afs/genomecenter.ucdavis.edu/software/gatk/4.1.6.0/lssc0-linux/gatk-package-4.1.6.0-local.jar GenomicsDBImport --batch-size 24 --reader-threads 12 --genomicsdb-update-workspace-path /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/CPRs_100_proto/DB_chr1 --intervals chr1:118739963-147510543 --verbosity DEBUG -V /rooted3/langley/work/home/chuck/rad/SFARI/SSC_hg38/WGS/phase2_CPRs/SSC00007_CPR/SSC00007.haplotypeCalls.CPR.er.raw.vcf.gz; 16:16:35.954 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/afs/genomecenter.ucdavis.edu/software/gatk/4.1.6.0/lssc0-linux/gatk-package-4.1.6.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:16:36.003 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /tmp/libgkl_compression5245166187604030095.so; Aug 28, 2020 4:16:36 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 16:16:36.284 INFO GenomicsDBImport - ------------------------------------------------------------; 16:16:36.285 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.6.0; 16:16:36.285 INFO GenomicsDBImport - For support and documentation go to https://software.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:1345,update,update-workspace-path,1345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['update'],['update-workspace-path']
Deployability,"ing wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=8095375e139fa0729c7a41c8f5e8a43281fc1b6859b6d3951d3bfba7296ee349; Stored in directory: /tmp/pip-ephem-wheel-cache-ecx6e_m0/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environment, use; #; # $ conda deactivate. ```. #### Actual behavior; ```sh; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:2039,Install,Installing,2039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"ining CachingIndexedFastaSequenceFile/overloads; - [ ] Update tools in the pathseq package (PathSeqBwaSpark, PathSeqScoreSpark) that do directory manipulation. [Edit] Somewhat tangentially, PathSeqBwaSpark currently rejects read inputs specified through `--inputs` and uses separate args to allow the user to identify inputs as paired or unpaired. Once this is using `GATKPathSpecifier` this could be changed to use ""--inputs"" annotated with tags instead. Might be a problem for WDL gen though (which doesn't support tags).; - [ ] Test utilities (createTempFile/Dir, etc. that return GATKPath); - [ ] Add a `toHadoopPath` method to `GATKPath` that returns a `org.apache.hadoop.fs.Path`.; - [ ] Change tools that generate multiple output files using a stem (SplitReads, etc) to use the `resolve` methods listed above once they're available.; - [ ] All usages of `PrintStream` should be replaced with `OutputStreamWriter` (code that requires printf-style formatting can use `write` with `String.format` instead of the `printf` methods). `PrintStream` doesn't propagate IOExceptions and instead requires calls to `checkError`, but almost all usages of `PrintStream` don't call it.; - [ ] Update `org.broadinstitute.hellbender.utils.report` (`GATKReport` and friends) to eliminate `File` references and `PrintStream` usages.; - [ ] Update `org.broadinstitute.hellbender.utils.recalibration` (`RecalUtils` and friends) to eliminate `File` references and `PrintStream` usages.; - [ ] Fix cases where we have a tool with a `File` that needs to be accessible to R code (determine if the code can handle non-local file paths). i.e.`VariantRecalibrator` TRANCHES_FILE.; - [ ] Fix cases where we have a tool with a `File` that needs to be accessible to Python (determine if the code can handle non-local file paths).; - [ ] `FeatureInput` should have all of it's String constructors removed, and only take GATKPath inputs. The constructor overloads that take tag Maps can be removed, and all call sites updated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6610:1935,Update,Update,1935,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6610,3,"['Update', 'update']","['Update', 'updated']"
Deployability,initial documentation update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4019:22,update,update,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4019,1,['update'],['update']
Deployability,install lfs with --force,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7682:0,install,install,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7682,1,['install'],['install']
Deployability,installing git-lfs and downloading large files on travis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/907:0,install,installing,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/907,1,['install'],['installing']
Deployability,"institute/hellbender/blob/c6b41e6da8c9ea3f03206a25ce4ad74312b154f0/src/test/java/org/broadinstitute/gatk/CommandLineProgramTest.java). I'm assuming this is because we have not settled on a way to `Assert` that outputs are similar after running a hellbender command line. This issue should resolve with a definition how far one should test before a pull request is accepted. After an arbitrary low level patch to the codebase, I believe the GATK [`MD5DB`](https://github.com/broadgsa/gatk/blob/3b67b448072e24c80779b2e1cbc9dcfcb5dce4cf/public/gatk-tools-public/src/test/java/org/broadinstitute/gatk/utils/MD5DB.java) and [`DiffEngine`](https://github.com/broadgsa/gatk/blob/3b67b448072e24c80779b2e1cbc9dcfcb5dce4cf/public/gatk-tools-public/src/main/java/org/broadinstitute/gatk/engine/walkers/diffengine/DiffEngine.java) are considered too hard to verify-and-update en masse. This limitation would also apply to external framework test utilities, such as TestNG's `FileAssert.assertLength()`. A 2009 discussion of file comparators is archived [here](http://stackoverflow.com/questions/466841/comparing-text-files-w-junit). Ultimately, I believe the biggest pain point with the `MD5DB` is that there does not exist a quick way to a) diagnose what has changed and b) to then update all hundreds of expected outputs. As in `DiffEngine`, we could define a way to regression test that only certain aspects of common file types aren't changing (exact number of reads in BAMs, or exact number of variants in BCF), or that values are falling within a certain range (number of quality scores all above 30 under 60), etc. As for updating results, instead of embedding the expected `MD5DB` outputs in a hundreds of java test files, one could also externalize _all_ of the expected outputs to another file (json, flat text, etc.) such that this singular sorted file for the entire test suite may be updated once. Or, we can decide that none of this is required at all and just delete `CommandLineProgramTest.java`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/69:1379,update,update,1379,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/69,2,['update'],"['update', 'updated']"
Deployability,integration test for BQSR Plotting,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/420:0,integrat,integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/420,1,['integrat'],['integration']
Deployability,integration test for CollectQualityYieldMetrics,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/842:0,integrat,integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/842,1,['integrat'],['integration']
Deployability,integration tests for SplitNCigarReads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1378:0,integrat,integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1378,1,['integrat'],['integration']
Deployability,integration tests to check min-bq argument is hooked up to HC and M2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4192:0,integrat,integration,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4192,1,['integrat'],['integration']
Deployability,"ion of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable---but perhaps he can confirm. Also noting here that I think determinism is still currently broken as of this commit---there have been some changes to PyTensor/PyMC seeding so that our previous theano/PyMC3 hack no longer applies.). So I think the next step is to just go to scientific-level testing and see what the fallout is. Ideally, we'd still get good performance (or perhaps better! at least on the runtime side, hopefully...) and we can just update the numerical tests. But i",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:2054,release,released,2054,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['release'],['released']
Deployability,"ior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it may be desirable to allow tracking of arbitrary RVs and deterministics via command line args (for debugging and explorator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1561,continuous,continuous,1561,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['continuous'],['continuous']
Deployability,is this how a gradle update is done?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1586:21,update,update,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1586,1,['update'],['update']
Deployability,"issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_; MarkDuplicates. ### Affected version(s); - [ ] Latest public release version [version?]; 4.6.0.0 GATK and Picard 3.2.0; - [ ] Latest master branch as of [date of test?]; 3 Jul 2023. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._; I'm trying to use gatk for finding snps in exome capture project. I get an error when trying to use MarkDuplicates - I tried using it from picard and from gatk. The screen output is:; ```; picard MarkDuplicates I=WA02_i5-537_i7-98_S11819_L004.bam O=test.dup.bam M=marked_dup_metrics.txt; INFO 2024-07-03 15:25:31 MarkDuplicates. ********** NOTE: Picard's command line syntax is changing.; **********; ********** For more information, please see:; **********; https://github.com/broadinstitute/picard/wiki/Command-Line-Syntax-Transition-For-Users-(Pre-Transition); **********; ********** The command line looks like this in the new syntax:; **********; ********** MarkDuplicates -I WA02_i5-537_i7-98_S11819_L004.bam -O test.dup.bam -M marked_dup_metric",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8904:1322,release,release,1322,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8904,1,['release'],['release']
Deployability,"ithub.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about moving forward with GATK4. Do you want me to move this issue to GATK or do you already have an issue for this there?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288598906). The new qual score doesn't subset alleles at all because it doesn't need to. `AlleleSubsettingUtils` handles this upstream of the new qual. We're waiting on the HaplotypeCaller tie-out to eliminate the old qual from GATK 4, however. ---. @vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288599388). Ah, do I understand correctly that if the new qual checks out and the old one can be eliminated, this issue no longer applies?. ---. @davidbenjamin commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950#issuecomment-288601472). Well, it's possible th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:4787,update,update,4787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,1,['update'],['update']
Deployability,"iver.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 17:54:55.320 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 17:54:55.321 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 17:54:55.321 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 17:54:55.321 DEBUG ConfigFactory - createOutputBamIndex = true; 17:54:55.321 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 17:54:55.321 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 17:54:55.321 INFO PathSeqPipelineSpark - Deflater: IntelDeflater; 17:54:55.321 INFO PathSeqPipelineSpark - Inflater: IntelInflater; 17:54:55.321 INFO PathSeqPipelineSpark - GCS max retries/reopens: 20; 17:54:55.321 INFO PathSeqPipelineSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 17:54:55.321 INFO PathSeqPipelineSpark - Initializing engine; 17:54:55.321 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 18/04/24 17:54:55 INFO SparkContext: Running Spark version 2.2.0; 18/04/24 17:54:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18/04/24 17:54:56 INFO SparkContext: Submitted application: PathSeqPipelineSpark; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls to: userx; 18/04/24 17:54:56 INFO SecurityManager: Changing view acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: Changing modify acls groups to:; 18/04/24 17:54:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:7270,patch,patch,7270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['patch'],['patch']
Deployability,"jars, because it increases entropy on the distribution & support side of things. I would much prefer to see this resolved by project development branches. With the possibility of making project-specific nightly builds off of those branches, to enable pointing people to hot fixes for a specific toolset without taking in whatever else is going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pic",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:3446,release,release,3446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,javadoc checks in `./gradlew install` should be turned off for now because it's just cluttering the output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1955:29,install,install,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1955,1,['install'],['install']
Deployability,jbwa updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1914:5,update,updates,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1914,1,['update'],['updates']
Deployability,"jdk.samtools.util.TempStreamFactory.wrapTempOutputStream(TempStreamFactory.java:74); at htsjdk.samtools.util.SortingCollection.spillToDisk(SortingCollection.java:223); at htsjdk.samtools.util.SortingCollection.add(SortingCollection.java:166); at htsjdk.samtools.SAMFileWriterImpl.addAlignment(SAMFileWriterImpl.java:192); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:36); at htsjdk.samtools.AsyncSAMFileWriter.synchronouslyWrite(AsyncSAMFileWriter.java:16); at htsjdk.samtools.util.AbstractAsyncWriter$WriterRunnable.run(AbstractAsyncWriter.java:117); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.ClassNotFoundException: org.xerial.snappy.LoadSnappy; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 11 more. We can find snappy-java in <INST_DIR>/build/install/gatk/lib/snappy-java-1.1.1.7.jar, but it does not have a LoadSnappy class. Renaming the snappy-java jar file so gatk cannot find it allows FastqToSam to run through. ---. @akiezun commented on [Thu Jun 30 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-229843043). thanks for the report. Can you provide the whole commandline you used?. ---. @huangk3 commented on [Thu Sep 15 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-247467619). Hi @akiezun I experience the same error when running gate-launch FastqToSam. My command line is:; ""./gatk_launch FastqToSam -SM ""test"" -F1 $fq1 -F2 $fq2 -O test.spark.sam -SO coordinate -R $ref --STRIP_UNPAIRED_MATE_NUMBER true --VALIDATION_STRINGENCY LENIENT -PL ILLUMINA --CREATE_INDEX true"". My Spark version is 2.0.0; Thanks!. ---. @lbergelson commented on [Mon Sep 19 2016](https://github.com/broadinstitute/gatk-protected/issues/587#issuecomment-248086238). @huangk3 Unfortunately ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2868:2145,install,install,2145,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2868,1,['install'],['install']
Deployability,"k - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-small-m.c.broad-dsde-methods.internal, exec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:7126,configurat,configuration,7126,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['configurat'],['configuration']
Deployability,"k --java-options ""-Xmx48g -Xms48G"" GenomicsDBImport -V C1_sentieon_gvcf.gz .......... -V SCAU-106.gvcf.gz -V SCAU-107.gvcf.gz -V SCAU-108.gvcf.gz -V SCAU-128.gvcf.gz --genomicsdb-workspace-path my_database.chr01 -R IRGSP-1.0_genome.fasta --genomicsdb-vcf-buffer-size 16384000 --intervals chr01. 11:48:08.245 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/ayu/anaconda3/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.5.1; 11:48:09.327 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:48:09.327 INFO GenomicsDBImport - Executing as ayu@ayu on Linux v5.15.90.1-microsoft-standard-WSL2 amd64; 11:48:09.327 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 11:48:09.327 INFO GenomicsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Version: 2.15.1; 11:48:09.327 INFO GenomicsDBImport - Picard Version: 2.18.2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:48:09.327 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:48:09.327 INFO GenomicsDBImport - Inflater: IntelInflater; 11:48:09.327 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:48:09.327 INFO GenomicsDBImport - Us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:1887,release,release-,1887,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,1,['release'],['release-']
Deployability,k.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349:3577,Integrat,IntegrationTestSpec,3577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349,1,['Integrat'],['IntegrationTestSpec']
Deployability,k/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5869:1467,configurat,configuration,1467,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869,1,['configurat'],['configuration']
Deployability,"ker.traverse(VariantWalker.java:102); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```; ### User Question [(LINK)](https://gatkforums.broadinstitute.org/gatk/discussion/24446/genomicsdbimport-not-completing-for-mixed-ploidy-samples/p1); ----------; I'm attempting to call variants on whole genomes for about 500 illumina paired-end samples with varying ploidy (haploid to tetraploid). I'm running a fairly standard uBam to GVCF pipeline with HaplotypeCaller passed the ploidy information (1,2,3, or 4) in -ERC GVCF mode. I then try to collect the GVCFs using GenomicsDBImport in a batch size of 50 and use GenotypeGVCFs on the combined database. My interval list that is passed to GenomicsDBImport is just each chromosome on a separate line. I'm using GATK v4.1.1.0<br />; <br />; Command:<br />; ```<br />; ${GATK_DIR}/gatk GenomicsDBImport \<br />; --java-options ""-Xmx110g -Xms110g"" \<br />; -R ${REF} \<br />; --variant ${FILE_LIST} \<br />; -L ${SCRIPT_DIR}/GATK_Style_Interval.list \<br />; --genomicsdb-workspace-path ${WORK_DIR}/GenomicsDB_20190912 \<br />; --batch-size 50 \<br />; --tmp-dir=${WORK_DIR}/<br />; ```<br />; <br />; GenomicsDBImport appears to run without error, but only shows progress for the first 6000 bp before moving onto the next batch. When I run select variants on the created database, I only get variants up to position 6716 in the first interval. When I try to run GenotypeGVCF on it, I get a strange error:<br />; h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6275:3232,pipeline,pipeline,3232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6275,1,['pipeline'],['pipeline']
Deployability,killed. Any suggestions for this? . Below is some stage info.... Stage 7	. collect at FindBreakpointEvidenceSpark.java:738 +details. org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerIntervals(FindBreakpointEvidenceSpark.java:738); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getKmerAndIntervalsSet(FindBreakpointEvidenceSpark.java:532); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.addAssemblyQNames(FindBreakpointEvidenceSpark.java:489); org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.gatherEvidenceAndWriteContigSamFile(FindBreakpointEvidenceSpark.java:174); org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark.runTool(StructuralVariationDiscoveryPipelineSpark.java:147); org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:159); org.broadinstitute.hellbender.Main.mainEntry(Main.java:202); org.broadinstitute.hellbender.Main.main(Main.java:288); sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); java.lang.reflect.Method.invoke(Method.java:498); org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:637),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4635:2131,deploy,deploy,2131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4635,1,['deploy'],['deploy']
Deployability,"kka.frameSize=512 --conf spark.akka.threads=10 --conf spark.executor.memory=50g --conf spark.driver.memory=150g --conf spark.local.dir=/gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2Spark/1024cores/tmp --class org.broadinstitute.hellbender.Main /gpfs/software/genomics/GATK/4b.2/gatk/build/libs/hellbender-spark.jar HaplotypeCaller --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2/bam//NA12892.recal.bam --dbsnp /gpfs/projects/NAGA/naga/SparkTest/SPARKCALLER/REF/dbsnp_138.vcf --emitRefConfidence GVCF --readValidationStringency LENIENT --nativePairHmmThreads 1024 --createOutputVariantIndex true --output NA12892.raw.snps.indels.g.vcf; [August 9, 2017 10:13:02 AM AST] HaplotypeCaller --nativePairHmmThreads 1024 --dbsnp /gpfs/projects/NAGA/naga/SparkTest/SPARKCALLER/REF/dbsnp_138.vcf --emitRefConfidence GVCF --output NA12892.raw.snps.indels.g.vcf --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2/bam//NA12892.recal.bam --readValidationStringency LENIENT --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --createOutputVariantIndex true --group StandardAnnotation --group StandardHCAnnotation --GVCFGQBands 1 --GVCFGQBands 2 --GVCFGQBands 3 --GVCFGQBands 4 --GVCFGQBands 5 --GVCFGQBands 6 --GVCFGQBands 7 --GVCFGQBands 8 --GVCFGQBands 9 --GVCFGQBands 10 --GVCFGQBands 11 --GVCFGQBands 12 --GVCFGQBands 13 --GVCFGQBands 14 --GVCFGQBands 15 --GVCFGQBands 16 --GVCFGQBands 17 --GVCFGQBands 18 --GVCFGQBands 19 --GVCFGQBands 20 --GVCFGQBands 21 --GVCFGQBands 22 --GVCFGQBands 23 --GVCFGQBands 24 --GVCFGQBands 25 --GVCFGQBands 26 --GVCFGQBands 27 --GVCFGQBands 28 --GVCFGQBands 29 --GVCFGQBands 30 --GVCFGQBands 31 --GVCFGQBands 32 --GVCFGQBands 33 --GVCFGQBands 34 --GVCFGQBands 35 --GVCFGQBands 36 --GVCFGQBands 37 --GVCFGQBands 38 --GVCFGQBands 39 --GVCFGQBands 40 --GVCFGQBands 41 --GVCFGQBands 42 --GVCFGQBands 43 --GVCFGQBands 44 --GVCFGQBands 45 -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631:3577,pipeline,pipeline,3577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631,1,['pipeline'],['pipeline']
Deployability,"l.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on each executor which may be a bit of a trick on it'",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312:1998,install,installedProviders,1998,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312,1,['install'],['installedProviders']
Deployability,"l.doWork(GATKTool.java:1048); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); ```. ### Second Example; This user is running multiple chromosomes at a time in parallel; Please see this link for more info: https://gatk.broadinstitute.org/hc/en-us/community/posts/360072732791-Import-GVCFs-using-GenomicsDBImport-one-chromosome-at-a-time-and-parallel-the-jobs-encounter-a-Duplicate-Sample-Name-Error?page=1#community_comment_360012681711. `time ${gatk} --java-options ""-Xmx8g -Xms2g"" GenomicsDBImport --tmp-dir /paedwy/disk1/yangyxt/test_tmp --genomicsdb-update-workspace-path ${probe_dir}/genomicdbimport_chr${1} -R ${ref_gen}/ucsc.hg19.fasta --batch-size 0 --sample-name-map ${gvcf}/batch_cohort.sample_map --reader-threads 5 --intervals chr${1}`. ```; 01:07:01.704 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/yangyxt/software/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Aug 29, 2020 1:07:01 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 01:07:02.001 INFO GenomicsDBImport - ------------------------------------------------------------; 01:07:02.002 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.8.1; 01:07:02.002 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 01:07:02.002 INFO GenomicsDBImport - Executing as yangyxt@paedwy01 on Linux v3.10.0-957.10.1.el7.x86_6",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:10049,update,update-workspace-path,10049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['update'],['update-workspace-path']
Deployability,l.writeReads(GATKSparkTool.java:259); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:39); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:730); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 17/10/11 14:19:38 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.; 17/10/11 14:19:38 INFO util.ShutdownHookManager: Shutdown hook called; 17/10/11 14:19:38 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.; 17/10/11 14:19:38 INFO util.ShutdownHookManager: Deleting directory /tmp/hdfs/spark-8c88439f-dcb0-48b2-86f3-fc82cef4c438,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:37916,deploy,deploy,37916,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,6,['deploy'],['deploy']
Deployability,laria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; errno=5(Input/output error); 01:25:43.661 INFO GenomicsDBImport - Starting batch input file preload; 01:26:19.244 INFO GenomicsDBImport - Finished batch preload; 01:26:19.244 INFO GenomicsDBImport - Importing batch 1 with 2 samples; 01:30:20.226 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.226 INFO GenomicsDBImport - Done importing batch 1/1; 01:30:20.227 INFO ProgressMeter - unmapped 5.3 1 0.2; 01:30:20.227 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 5.3 minutes.; 01:30:20.227 INFO GenomicsDBImport - Import of all batches to GenomicsDB completed!; 01:30:20.227 INFO GenomicsDBImport - Shutting down engine; [10 December 2021 01:30:20 UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 7.76 minutes.; Runtime.totalMemory()=16078340096; ```. #### Steps to reproduce. Not sure if it repr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:4478,update,update,4478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,"latest master 4353d54fd2d64ea1b4c8429986f83eb873a4d687. `/gatk-launch CountReadsSpark -I src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam`. ```; [May 18, 2016 5:10:55 PM EDT] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=318242816; java.net.BindException: Failed to bind to: /10.1.5.39:0: Service 'sparkDriver' failed after 16 retries!; at org.jboss.netty.bootstrap.ServerBootstrap.bind(ServerBootstrap.java:272); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:393); at akka.remote.transport.netty.NettyTransport$$anonfun$listen$1.apply(NettyTransport.scala:389); at scala.util.Success$$anonfun$map$1.apply(Try.scala:206); at scala.util.Try$.apply(Try.scala:161); at scala.util.Success.map(Try.scala:206); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.co",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1839:233,pipeline,pipelines,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1839,1,['pipeline'],['pipelines']
Deployability,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:119); 	at org.broadinstitute.hellb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503:1406,deploy,deploy,1406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503,1,['deploy'],['deploy']
Deployability,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.MapReferenceResolver.addWrittenObject(MapReferenceResolver.java:41); 	at co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:4574,deploy,deploy,4574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['deploy'],['deploy']
Deployability,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:61); 	at org.broadinstitute.he,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:2231,deploy,deploy,2231,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['deploy'],['deploy']
Deployability,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:2128,deploy,deploy,2128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['deploy'],['deploy']
Deployability,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(HttpTransportOptions.java:157); 	at shaded.cloud_nio.com.google.api.client.htt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3402,deploy,deploy,3402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,1,['deploy'],['deploy']
Deployability,lbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.ne,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12643,deploy,deploy,12643,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['deploy'],['deploy']
Deployability,"le ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2733,INSTALL,INSTALLDIRGATK,2733,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"le.map -R /scratch/PI/boip/Reference/Human_genome/GRCh37/hs37d5.fa --batch-size 400 --reader-threads 5; 14:48:08.923 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hcaoad/miniconda2/share/gatk4-4.2.0.0-0/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 16, 2021 2:48:09 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:48:09.080 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.2.0.0; 14:48:09.081 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:48:09.081 INFO GenomicsDBImport - Executing as hcaoad@hhnode-ib-46 on Linux v3.10.0-1062.el7.x86_64 amd64; 14:48:09.081 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:48:09.081 INFO GenomicsDBImport - Start Date/Time: April 16, 2021 2:48:08 PM HKT; 14:48:09.081 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - ------------------------------------------------------------; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Version: 2.24.0; 14:48:09.081 INFO GenomicsDBImport - Picard Version: 2.25.0; 14:48:09.081 INFO GenomicsDBImport - Built for Spark Version: 2.4.5; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:48:09.081 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:48:09.082 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:48:09.082 INFO GenomicsDBImport - Deflater: IntelDeflater; 14:48:09.082 INFO GenomicsDBImport - Inflater: IntelInflater; 14:48:09.082 INFO GenomicsDBImport - GCS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7218:5673,release,release-,5673,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7218,1,['release'],['release-']
Deployability,"ler - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:56:25.355 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:56:25.356 INFO GermlineCNVCaller - Deflater: IntelDeflater; 10:56:25.357 INFO GermlineCNVCaller - Inflater: IntelInflater; 10:56:25.358 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 10:56:25.358 INFO GermlineCNVCaller - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:56:25.360 WARN GermlineCNVCaller -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GermlineCNVCaller is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 10:56:25.361 INFO GermlineCNVCaller - Initializing engine; 10:56:54.347 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 10:56:55.287 INFO GermlineCNVCaller - Retrieving intervals from first read-count file (hdf5/grexome0426.hdf5)...; 10:56:55.384 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 10:56:55.482 INFO GermlineCNVC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:2367,patch,patch,2367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['patch'],['patch']
Deployability,"lizeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:2969,deploy,deploy,2969,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['deploy'],['deploy']
Deployability,"llbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 IN",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:5487,install,install,5487,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['install'],['install']
Deployability,"llectFragmentOverlaps - same filters as CollectFragmentCounts, but adding counts to all bins overlapping each fragment. Note that we need to implement a filter on maximum fragment length, otherwise we get some strange artifacts from (incorrectly mapped?) extremely long fragments; I arbitrarily chose a cutoff of 10000bp. This recovered events 1 and 2. Event 3 seemed to be the most difficult to recover. Plotting the copy ratios surrounding this event (which spans ~15 100bp bins) yields some insights:. CollectFragmentCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244188-317a7f1e-2453-11e8-937d-f7239354316e.png). CollectReadCounts:; ![image](https://user-images.githubusercontent.com/11076296/37244228-ad24908c-2453-11e8-91dd-a978578e77f4.png). CollectFragmentOverlaps:; ![image](https://user-images.githubusercontent.com/11076296/37244230-b25b9cee-2453-11e8-8646-f9c95365b355.png). The increased statistical noise in the CollectFragmentCounts result (due to the lower overall count because of the pairing of reads) probably causes us to miss this event. Also, although CollectFragmentOverlaps initially looks pretty good, I think the bin-to-bin correlations that are evident here negatively affect segmentation. This is not an extremely rigorous evaluation, but it suggests that we should consider switching over to a CollectReadCounts-like strategy. To appease CGA (in case they still feel strongly, which they might not), we can make this a separate tool, or perhaps just have a single tool called CollectCounts that can toggle between the two (we just have to be careful about filters in the latter case). We should evaluate further once more rigorous automatic evaluations are in place. @mbabadi @LeeTL1220 @asmirnov239 @sooheelee might find this of interest. Also, I have a question for engine team, @droazen. Is there a read filter I should be using for fragment length, and if not, can we add one (or is there already a preferred way to do this type of filtering)?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4519:3520,toggle,toggle,3520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4519,1,['toggle'],['toggle']
Deployability,"llegalArgumentException: Wrong FS: hdfs://user/local/print_reads.sorted.bam, expected: hdfs://dataflow01.broadinstitute.org:8020; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:654); at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:474); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:163); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:281); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:261); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:252); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.main(Main.java:86); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. This `IllegalArgumentException` should be converted to a `UserException` instead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1257:2405,deploy,deploy,2405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1257,6,['deploy'],['deploy']
Deployability,"lot of threading output writers through the codebase and perhaps this is better handled by the ""--debug"" argument like it used to? Thoughts? . Notes: ; - It should be noted that by design all of the added changes to HaplotypeCaller are opt-in, barring errors in implementation.; - This code is measurably slower than vanilla HaplotypeCaller. In particular FRD is a very expensive step that corresponds to ~5-7% of the runtime. This is in part because it has to duplicate many of the steps in the genotyper based on the number of unique mapping qualities present at a site as well as the fact that it performs an O(n^2) number of operations at sites with many possible alleles. There are options to cut down on the cost of this algorithm that moderately impact the results relative to DRAGEN. . This implementation is intended to produce results close to the results on DRAGEN 3.4.12 without stripping away the major improvements made in GATK4, as a result there are a number of areas in which we know we are producing different results: ; - In GATK4 variants that overlap with an upstream deletion will have added to their alleles list a sybmolic '*' deletion alleles which are genotyped as part of the allele array in the genotyeper. This is not the case in DRAGEN and it interacts with FRD in such a way as to produce a number of variants that in DRAGEN would have been called as 0/1 heterozygous calls with capped QUAL scores, in gatk they are called as 1/2 calls with uncapped quality scores.; - While we have added the option to use the legacy assembly region creation code, it is not part of the expected pipeline for running DRAGEN. This includes a number of arguments that were done away with in the recent refactoring pass. ; - I have added hooks to revert the assembly engine to approximately its state in gatk3. While I don't stand by that change I think we should bundle the minor assembly engine changes together with our other assembly engine work to try to make a more convincing case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6634:5268,pipeline,pipeline,5268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6634,1,['pipeline'],['pipeline']
Deployability,ltGradleLauncher.java:106); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:91); at org.gradle.internal.progress.DefaultBuildOperationExecutor.run(DefaultBuildOperationExecutor.java:63); at org.gradle.initialization.DefaultGradleLauncher.doBuild(DefaultGradleLauncher.java:106); at org.gradle.initialization.DefaultGradleLauncher.run(DefaultGradleLauncher.java:92); at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.ResetDeprecationLogger.execute(ResetDeprecationLogger.java:26); at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); at org.gradle.launcher.daemon.server.exec.RequestStopIfSingleUsedDaemon.execute(RequestStopIfSingleUsedDaemon.java:34); at org.gradle.launcher.da,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4155:5062,Continuous,ContinuousBuildActionExecuter,5062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4155,2,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,"ltiply (add in log space) this prior by the genotype likelihoods to get the posterior probabilities on genotypes. It appears to me that we have double-counted the input data, once to get its AC field and once to get its GLs. I believe the correct thing to do is use only the resources to define a prior which is then combined with the GLs to get the posterior. ---. @ldgauthier commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219778100). I will take the blame (both figuratively and the literal git blame) for PosteriorLikelihoodsUtils nomenclature problems. I either initiated them or didn't fix them when I refactored. I also intuitively prefer resources only without using the input AC, but that being said we've seen better results using both, specifically for a Finnish cohort with 100 founders. In the DSDE/ATGU meetings the use of the input AC was discussed as being analogous to a single step of EM. Would the true EM apply a different update for each sample in the callset?. ---. @davidbenjamin commented on [Tue May 17 2016](https://github.com/broadinstitute/gsa-unstable/issues/1185#issuecomment-219833193). The better results using the double-counting might have something to do with the incorrect prior -- if the prior is skewing toward homozygosity, then double-counting your variant data might counteract this and rescue some variant genotypes, which will be mainly hets. The EM model that people implicitly seem to have in mind is alternating E steps on each sample to get genotype posteriors with M steps to learn the allele frequencies. So let's work out what happens if you do just one iterations:. 0) Initialize allele frequencies to the mean of the Dirichlet heterozygosity prior; i.e. ~1 for ref, ~1/1000 for each alt, plus any allele counts from the resources. Genotype priors come from the multinomial distribution (one genotype is a draw of 2 alleles) of these allele frequencies.; 1) (E step) genotype posteriors are th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2918:4059,update,update,4059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2918,1,['update'],['update']
Deployability,"make a list of all Picard metrics tools used in the production pipeline (WGS, WES)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/422:63,pipeline,pipeline,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/422,1,['pipeline'],['pipeline']
Deployability,make sv pipeline script copy_sv_results.sh able to take bucket path that starts with gs://. currently the result-copying step would fail if I run the script by providing the `GCS_SAVE_PATH` argument that starts with `gs://`.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4223:8,pipeline,pipeline,8,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4223,1,['pipeline'],['pipeline']
Deployability,making the version number depend on the git hash using a gradle git plugin from https://github.com/ajoberstar/gradle-git. It seems like the top gradle-git integration library. There are lots of pre-baked things in it to help with releases and such that we can grow into.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/196:155,integrat,integration,155,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/196,2,"['integrat', 'release']","['integration', 'releases']"
Deployability,manage_sv_pipeline checks version from gatk-spark.jar and compares it; to the current git hash (to ensure the correct version is run). Newer; gatk versions had a slightly different file name format and caused; errors parsing the hash. This updates the hash check and produces; more comprehensible error messages when it fails. Resolves: #3593,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3595:240,update,updates,240,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3595,1,['update'],['updates']
Deployability,"mark duplicates in dataflow - based on the code by garrickevans . The main work is done in; `private static final class MarkDuplicatesDataflowTransform extends PTransform<PCollection<Read>, PCollection<Read>>` - the sigrature conforms to the main read processing pipeline. Limitations:; - no optical duplicates; - only integration tests (would be good to have unit tests that check dup detection logic on very specific reads - ideally those from picard's tests). @droazen please review",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/541:263,pipeline,pipeline,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/541,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,mdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:2192); 	at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:346); 	at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:205); 	... 23 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:2270,deploy,deploy,2270,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['deploy'],['deploy']
Deployability,"me: May 7, 2018 2:40:21 PM EDT; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 14:40:21.992 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 14:40:21.992 INFO MarkDuplicatesSpark - Picard Version: 2.18.2; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:40:21.993 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:40:21.993 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 14:40:21.993 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 14:40:21.993 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 14:40:21.993 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:40:21.994 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:40:21.994 INFO MarkDuplicatesSpark - Initializing engine; 14:40:21.994 INFO MarkDuplicatesSpark - Done initializing engine; 14:40:22.338 WARN NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:3332,patch,patch,3332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['patch'],['patch']
Deployability,"me: May 7, 2018 9:47:47 PM EDT; 21:47:48.270 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Version: 2.14.3; 21:47:48.271 INFO MarkDuplicatesSpark - Picard Version: 2.18.2; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 21:47:48.271 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 21:47:48.272 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 21:47:48.272 INFO MarkDuplicatesSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 21:47:48.272 INFO MarkDuplicatesSpark - Deflater: IntelDeflater; 21:47:48.272 INFO MarkDuplicatesSpark - Inflater: IntelInflater; 21:47:48.272 INFO MarkDuplicatesSpark - GCS max retries/reopens: 20; 21:47:48.272 INFO MarkDuplicatesSpark - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 21:47:48.272 WARN MarkDuplicatesSpark -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: MarkDuplicatesSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:47:48.273 INFO MarkDuplicatesSpark - Initializing engine; 21:47:48.273 INFO MarkDuplicatesSpark - Done initializing engine; 22:29:27.746 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 22:43:29.758 INFO ReadsSparkSink - Finished merging shards into a single output bam; 22:43:36.475 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 10:43:36 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 55.82 minutes.; Runtime.totalMemory()=1243086848",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:7178,patch,patch,7178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,1,['patch'],['patch']
Deployability,"mentations of copy-ratio, allele-fraction, and ""multidimensional"" (joint) segmentation. All implementations are pretty boilerplate; they simply partition by contig and then call out to KernelSegmenter. Note that there is some logic in multidimensional segmentation that only uses the first het in each copy-ratio interval and if any are available, and imputes the alt-allele fraction to 0.5 if not.; -Makes sense for @mbabadi to review this, since he reviewed the KernelSegmenter PR. Added modeling classes and tests for ModelSegments CNV pipeline.; -Most of this code is copied from the old MCMC code. However, I've done some overall code cleanup and refactoring, especially to remove some overextraction of methods in the allele-fraction likelihoods (see #2860). I also added downsampling and scaling of likelihoods to cut down on runtime. Tests have been simplified and rewritten to use simulated data.; -@LeeTL1220 do you think you could take a look?. Added ModelSegments CLI.; -Mostly control flow to handle optional inputs and validation, but there is some ugly and not well documented code that essentially does the GetHetCoverage step. We'll refactor later, I filed #3915.; -@asmirnov239 can review. This is lower priority than the gCNV VCF writing. Deleted gCNV WDL and Cromwell tests.; -Trivial to review. Added WDL and Cromwell tests for ModelSegments CNV pipeline.; -This includes the cost optimizations from @meganshand and @jsotobroad (sorry guys, I wasn't sure how to track your contributions while fixing up commits!) I also added tests for both GC/no-GC pair workflows.; -@MartonKN should review to gain familiarity with the WDL. Note that this WDL has already been through many revisions from @meganshand, @jsotobroad, and @LeeTL1220, so hopefully there shouldn't be too much for you to find serious fault with. Note that I punted on adding MultidimensionalKernelSegmenterUnitTest and ModelSegmentsIntegrationTest. Filed #3916. Closes #2858. (FINALLY!); Closes #3825.; Closes #3661.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3913:1934,pipeline,pipeline,1934,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3913,1,['pipeline'],['pipeline']
Deployability,merge prototyping SV breakpoint and type inference tool into SV spark pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3890:70,pipeline,pipeline,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3890,2,['pipeline'],['pipeline']
Deployability,methods like BucketUtils.createFile take a PipelineOptions argument. That argument can be null if the path is not on a cloud but the doc does not mention that fact.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/923:43,Pipeline,PipelineOptions,43,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/923,1,['Pipeline'],['PipelineOptions']
Deployability,"metimes yield the homologuos sequence to a shorter alignment and take that away from a longer alignment; when it comes to a later filtering step that filters out alignments purely based on its unique read span, because of the deoverlapping step, those short alignment that received the homologuous sequence because of the left-aligning convention now survived, compared to if there were no upfront deoverlap. The drop in number of DUP is due to a similar reason:; the upfront overlap removal made some contigs that would be classified as having incomplete picture unfairly now becomes a contig with complete picture (see [this](http://www.genomeribbon.com/?perma=AvOvbThN4z) for example). IMO removal of the upfront deoverlapping step is good, because the number of calls would not be affected by which convention we follow (convention should only affect representation, not if a variant is real); on the other hand, the convention definitely hurts some other small alignments. -------. The drop in number of variants detected from stable-versioned tool to the experimental tool without this fix are mainly from the imprecise deletion calls that are not hooked up in the experiemental tool yet, and many deletion and duplications are now incoporated into CPX variants. -------. Alternatively, we can turn off the filter based on unique alignment length all together, and annotate the calls with unique alignment length for later stage filter. I did experiment with that, and as expected, the number of BND calls and deletion calls increased, whereas the numer of DUP calls dropped, when comparing only-turn-off-length-filter, turn-off-upfront-deoverlap-and-length-filter. The drop in number of DUP calls are due to the fact that without upfront deoverlap, more contigs are classified as having incomplete picture. -------; Also proposing an ""improvement"" to the script by adding more checks in the pipeline bash script:; the problem it is trying to address is documented in the changes to the script.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4282:6299,pipeline,pipeline,6299,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4282,1,['pipeline'],['pipeline']
Deployability,"micsDBImport - Start Date/Time: November 26, 2023 11:48:08 AM CST; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - ------------------------------------------------------------; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Version: 2.15.1; 11:48:09.327 INFO GenomicsDBImport - Picard Version: 2.18.2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 11:48:09.327 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 11:48:09.327 INFO GenomicsDBImport - Deflater: IntelDeflater; 11:48:09.327 INFO GenomicsDBImport - Inflater: IntelInflater; 11:48:09.327 INFO GenomicsDBImport - GCS max retries/reopens: 20; 11:48:09.327 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 11:48:09.328 INFO GenomicsDBImport - Initializing engine; 11:48:14.819 INFO IntervalArgumentCollection - Processing 43270923 bp from intervals; 11:48:14.846 INFO GenomicsDBImport - Done initializing engine; Created workspace /mnt/g/ubuntushare/sequence/C271_sentieon_gvcf/my_database.chr01; 11:48:14.919 INFO GenomicsDBImport - Vid Map JSON file will be written to my_database.chr01/vidmap.json; 11:48:14.919 INFO GenomicsDBImport - Callset Map JSON file will be written to my_database.chr01/callset.json; 11:48:14.919 INFO GenomicsDBImport - Complete VCF Header will be written to my_database.chr01/vcfheader.vcf; 11:48:14.919 INFO GenomicsDBImport - Importing to array - my_database.chr01/genomicsdb_array; 11:48:14.924 INFO ProgressMeter - Starting traversal; 11:48:14.924 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 11:48:19.709 INFO GenomicsD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8593:2920,patch,patch,2920,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8593,1,['patch'],['patch']
Deployability,migrate code from googlegenomics/genomics-pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/427:42,pipeline,pipeline,42,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/427,1,['pipeline'],['pipeline']
Deployability,mmandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSpa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2076,deploy,deploy,2076,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,"mmandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4406,deploy,deploy,4406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,mmandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:23144,deploy,deploy,23144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,mmandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:2566,deploy,deploy,2566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"mmandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:7302,deploy,deploy,7302,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"mmandLineProgram.java:38); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733); > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); > Caused by: java.nio.file.ProviderNotFoundException: Provider ""maprfs"" not found; > 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); > 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); > 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); > 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); > 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:178); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:177); > 	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); >",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:3685,deploy,deploy,3685,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['deploy'],['deploy']
Deployability,"most recent call last):; File ""/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/cohort_denoising_calling.1650827882847090378.py"", line 143, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 754, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 515, in Var; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 869, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 250, in logp; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/tensor/var.py"", line 155, in __mul__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1586, in cthunk_factory; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1118, in module_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1017, in _get_from_ke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:19826,continuous,continuous,19826,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['continuous'],['continuous']
Deployability,"moving settings from SparkCommandLineProgram to SparkContextFactory; the settings that were being applied to SparkContext.getConf were pointless since getConf is a copy of the configuration; instead they're applied to the SparkConfig in SparkContextFactory.getSparkContext. fixes #1096. <!-- Reviewable:start -->. [<img src=""https://reviewable.io/review_button.png"" height=40 alt=""Review on Reviewable""/>](https://reviewable.io/reviews/broadinstitute/gatk/1097). <!-- Reviewable:end -->",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1097:176,configurat,configuration,176,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1097,1,['configurat'],['configuration']
Deployability,"mpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; sudo apt-get -qq update > /dev/null; sudo apt-get -qq install -y gradle. echo ""Downloading binaries for Spark""; wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz; tar -xzf spark-1.5.1-bin-hadoop2.6.tgz; export SPARK_HOME=spark-1.5.1-bin-hadoop2.6. echo ""Set up Spark for standalone mode processing""; $SPARK_HOME/sbin/start-master.sh -h localhost; $SPARK_HOME/sbin/start-slave.sh spark://localhost:7077. echo ""Downloading source for GATK4""; wget https://github.com/broadinstitute/gatk/archive/4.alpha.tar.gz; tar -xvzf 4.alpha.tar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3859,install,installed,3859,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['install'],['installed']
Deployability,must contain active span.; at org.broadinstitute.hellbender.utils.Utils.validate(Utils.java:814); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:104); at org.broadinstitute.hellbender.engine.AssemblyRegion.<init>(AssemblyRegion.java:80); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popNextReadyAssemblyRegion(ActivityProfile.java:332); at org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile.popReadyAssemblyRegions(ActivityProfile.java:277); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.loadNextAssemblyRegion(AssemblyRegionIterator.java:159); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:112); at org.broadinstitute.hellbender.engine.AssemblyRegionIterator.next(AssemblyRegionIterator.java:35); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:192); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:173); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1058); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. see attachement. ```; gatk HaplotypeCaller -I jeter.bam -L jeter.bed --seconds-between-progress-updates 600 --minimum-mapping-quality 30 -R hs37d5_all_chr.fasta -O jeter.vcf.gz; ```. #### Expected behavior. no exception. #### Actual behavior. ```; java.lang.IllegalStateException: Padded span must contain active span; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7289:2122,update,updates,2122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7289,1,['update'],['updates']
Deployability,"my pipeline(from fastq ) is 👍 ; . 1. FastqToSam . 2. ConvertHeaderlessHadoopBamShardToBam. 3. BwaAndMarkDuplicatesPipelineSpark; ; ...... but in the step 3 （BwaAndMarkDuplicatesPipelineSpark），the pipeline crash; ```; Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, com1, executor 1): # **java.lang.IllegalArgumentException: Reference name for '1853452901' not found in sequence dictionary.; at htsjdk.samtools.SAMRecord.resolveNameFromIndex(SAMRecord.java:569); at htsjdk.samtools.SAMRecord.setMateReferenceIndex(SAMRecord.java:506); at htsjdk.samtools.BAMRecord.<init>(BAMRecord.java:94); at htsjdk.samtools.DefaultSAMRecordFactory.createBAMRecord(DefaultSAMRecordFactory.java:42); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:210); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.<init>(BAMFileReader.java:963); at htsjdk.samtools.BAMFileReader.getIterator(BAMFileReader.java:491)**; at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:182); at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:211); at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:180); at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:179); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:134); at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4179:3,pipeline,pipeline,3,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4179,2,['pipeline'],['pipeline']
Deployability,"n(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:8118,pipeline,pipeline,8118,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"nScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:205); at org.broadinstitute.hellbender.utils.python.StreamingPythonScriptExecutor.start(StreamingPythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.walkers.vqsr.CNNScoreVariants.onTraversalStart(CNNScoreVariants.java:302); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1046); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gatktool. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'gatktool'. at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.checkPythonEnvironmentForPackage(PythonScriptExecutor.java:198); when I checked gatktool python package, it is installed in the python packages by conda. after activate gatk4 , I checked with pip install gatktool, and it says the package already installed. Anyone experienced this error?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7397:3400,install,installed,3400,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7397,3,['install'],"['install', 'installed']"
Deployability,"nce context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updated test suite to use full, checked-in references. - Updated many tests to use regression test data sources rather than small data; sources specifically for PIK3CA and MUC16. - Removed some now unused data files (primarily old data sources). - Updated regression tests to work on local data sources and references. - Now `FuncotatorIntegrationTest::regressionTest` works on local, checked-out copies of the data sources so that; they can be run from any checkout. - Fixes #4344 ; - Fixes #5295",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:4526,Update,Updated,4526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,6,['Update'],['Updated']
Deployability,nch PrintReadsSpark -I gs://hellbender/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.clus,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1012,deploy,deploy,1012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"nder.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVaria",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:8070,install,installed,8070,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['install'],['installed']
Deployability,"ne(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf sha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3078,deploy,deploy,3078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['deploy'],['deploy']
Deployability,"neProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```; #### Steps to reproduce; `gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz`. The cram is HG0096.final.cram found here:. https://www.internationalgenome.org/data-portal/data-collection/30x-grch38. #### Expected behavior; When I run an earlier version v4.1.7.0, it runs without an error.... ```; gatk HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar HaplotypeCaller -L chr22 -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -I cram/HG00096.final.cram -O test.g.vcf.gz; 14:40:45.497 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.1.7.0/install/bin/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 10, 2021 2:40:45 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:40:45.786 INFO HaplotypeCaller - ------------------------------------------------------------; 14:40:45.787 INFO HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.1.7.0; 14:40:45.787 INFO HaplotypeCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:40:45.788 INFO HaplotypeCaller - Executing as farrell@scc-hadoop.bu.edu on Linux v3.10.0-1160.6.1.el7.x86_64 amd64; 14:40:45.788 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7076:5732,install,install,5732,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7076,1,['install'],['install']
Deployability,neProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); 	at org.broadinstitute.hellbender.Main.main(Main.java:275); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFuncti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:12566,deploy,deploy,12566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['deploy'],['deploy']
Deployability,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:111); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:169); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:188); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:120); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:141); 	at org.broadinstitute.hellbender.Main.main(Main.java:196); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:728); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Invalid splitting BAM index: should contain at least 1 offset and the file size; 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.readIndex(SplittingBAMIndex.java:69); 	at org.seqdoop.hadoop_bam.SplittingBAMIndex.<init>(SplittingBAMIndex.java:49); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeSplittingBaiFiles(SAMFileMerger.java:117); 	at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:87); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.datasources.Rea,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2503:1329,deploy,deploy,1329,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2503,1,['deploy'],['deploy']
Deployability,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NegativeArraySizeException; 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.resize(IdentityObjectIntMap.java:447); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:245); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.putStash(IdentityObjectIntMap.java:246); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.push(IdentityObjectIntMap.java:239); 	at com.esotericsoftware.kryo.util.IdentityObjectIntMap.put(IdentityObjectIntMap.java:135); 	at com.esotericsoftware.kryo.uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:4497,deploy,deploy,4497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['deploy'],['deploy']
Deployability,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:220); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: jonn-test-bucket/foo.bam.parts; 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:575); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.FileTreeWalker.getAttributes(FileTreeWalker.java:219); 	at java.nio.file.FileTreeWalker.visit(FileTreeWalker.java:276); 	at java.nio.file.FileTreeWalker.walk(FileTreeWalker.java:322); 	at java.nio.file.FileTreeIterator.<init>(FileTreeIterator.java:72); 	at java.nio.file.Files.walk(Files.java:3574); 	at java.nio.file.Files.walk(Files.java:3625); 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.getFilesMatching(NIOFileUtil.java:91); 	at org.seqdoop.hadoop_bam.ut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2793:2154,deploy,deploy,2154,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2793,1,['deploy'],['deploy']
Deployability,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); 	at org.broadinstitute.hellbender.Main.main(Main.java:218); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: No enum constant com.google.cloud.storage.StorageClass.DURABLE_REDUCED_AVAILABILITY; 	at java.lang.Enum.valueOf(Enum.java:238); 	at com.google.cloud.storage.StorageClass.valueOf(StorageClass.java:22); 	at com.google.cloud.storage.BlobInfo.fromPb(BlobInfo.java:940); 	at com.google.cloud.storage.Blob.fromPb(Blob.java:779); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:189); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:197); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:571); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.isDirectory(Files.java:21,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2517:2051,deploy,deploy,2051,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2517,1,['deploy'],['deploy']
Deployability,neProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:233); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.IOException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified.; 	at shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials.refreshAccessToken(ComputeEngineCredentials.java:137); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at shaded.cloud_nio.com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at shaded.cloud_nio.com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at com.google.cloud.http.HttpTransportOptions$1.initialize(H,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3591:3325,deploy,deploy,3325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3591,1,['deploy'],['deploy']
Deployability,"neProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo ""Installing Gradle""; sudo add-apt-repository -y ppa:cwchien/gradle; su",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3294,deploy,deploy,3294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['deploy'],['deploy']
Deployability,"nel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 12, in <module>; from .sampling import *; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/sampling.py"", line 14, in <module>; from .plots.traceplot import traceplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/__init__.py"", line 1, in <module>; from .autocorrplot import autocorrplot; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/pymc3/plots/autocorrplot.py"", line 2, in <module>; import matplotlib.pyplot as plt; File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/pyplot.py"", line 113, in <module>; _backend_mod, new_figure_manager, draw_if_interactive, _show = pylab_setup(); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/__init__.py"", line 60, in pylab_setup; [backend_name], 0); File ""/Users/markw/anaconda/envs/gatk/lib/python3.6/site-packages/matplotlib/backends/backend_macosx.py"", line 19, in <module>; from matplotlib.backends import _macosx; RuntimeError: Python is not installed as a framework. The Mac OS X backend will not be able to function correctly if Python is not installed as a framework. See the Python documentation for more information on installing Python as a framework on Mac OS X. Please either reinstall Python as a framework, or try one of the other backends. If you are using (Ana)Conda please install python.app and replace the use of 'python' with 'pythonw'. See 'Working with Matplotlib on OSX' in the Matplotlib FAQ for more information.; ```; As it states, there is more information here: https://matplotlib.org/faq/osx_framework.html#osxframework-faq. I can resolve the issue by running `conda install -c anaconda python.app` and changing this line in `PythonExecutorBase.java`:; ```; PYTHON(""python""),; ```; to; ```; PYTHON(""pythonw""),; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4743:1625,install,installed,1625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4743,5,['install'],"['install', 'installed', 'installing']"
Deployability,"nfigFactory - spark.driver.maxResultSize = 0; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; > 21:13:04.230 DEBUG ConfigFactory - spark.io.compression.codec = lzf; > 21:13:04.230 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; > 21:13:04.230 DEBUG ConfigFactory - spark.driver.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; > 21:13:04.230 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; > 21:13:04.230 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; > 21:13:04.231 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; > 21:13:04.231 DEBUG ConfigFactory - createOutputBamIndex = true; > 21:13:04.231 INFO GenotypeGVCFs - Deflater: IntelDeflater; > 21:13:04.231 INFO GenotypeGVCFs - Inflater: IntelInflater; > 21:13:04.231 INFO GenotypeGVCFs - GCS max retries/reopens: 20; > 21:13:04.231 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; > 21:13:04.231 INFO GenotypeGVCFs - Initializing engine; > 21:13:11.834 INFO GenotypeGVCFs - Done initializing engine; > 21:13:11.950 DEBUG MathUtils$Log10Cache - cache miss 2 > 0 expanding to 12; > 21:13:11.992 INFO ProgressMeter - Starting traversal; > 21:13:11.992 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; > 21:14:17.635 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:1->2; > 21:14:17.858 DEBUG GenotypeLikelihoodCalculators - Expanding capacity ploidy:2->2 allele:2->3; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 13 > 12 expanding to 26; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 27 > 26 expanding to 54; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 55 > 54 expanding to 110; > 21:14:17.872 DEBUG MathUtils$Log10Cache - cache miss 111 > 110 expanding to 222; > 21:14:17.873 D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4161:4914,patch,patch,4914,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4161,1,['patch'],['patch']
Deployability,"nflater; 16:36:22.399 INFO Funcotator - GCS max retries/reopens: 20; 16:36:22.399 INFO Funcotator - Requester pays: disabled; 16:36:22.399 INFO Funcotator - Initializing engine; 16:36:22.624 INFO FeatureManager - Using codec VCFCodec to read file file:///home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz; 16:36:22.842 INFO Funcotator - Done initializing engine; 16:36:22.842 INFO Funcotator - Validating sequence dictionaries...; 16:36:22.856 INFO Funcotator - Processing user transcripts/defaults/overrides...; 16:36:22.857 INFO Funcotator - Initializing data sources...; 16:36:22.859 INFO DataSourceUtils - Initializing data sources from directory: /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s; 16:36:22.871 INFO DataSourceUtils - Data sources version: 1.7.2020429s; 16:36:22.871 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.871 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.7.20200429s.tar.gz; 16:36:22.891 INFO Funcotator - Shutting down engine; [January 10, 2024 at 4:36:22 PM GMT] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=285212672; ***********************************************************************. A USER ERROR has occurred: ERROR: Directory contains more than one config file: file:///home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s/gencode_xrefseq/hg38/. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--java-options '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace. Any guidance to resolve the issue is appreciated.; Thank you!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:4903,pipeline,pipelines,4903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['pipeline'],['pipelines']
Deployability,"ng libgkl\_compression.so from jar:file:/mnt/clinix1/Analysis/mongol/phenomata/Tools/Anaconda3/envs/gatk4/share/gatk4-4.1.4.1-1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so** ; **Mar 05, 2020 9:27:44 AM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine** ; **INFO: Failed to detect whether we are running on Google Compute Engine.** ; **09:27:44.733 INFO PathSeqPipelineSpark - ------------------------------------------------------------** ; **09:27:44.733 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.4.1** ; **09:27:44.734 INFO PathSeqPipelineSpark - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/)** ; **09:27:44.734 INFO PathSeqPipelineSpark - Executing as phenomata@cm132 on Linux v2.6.32-573.18.1.el6.x86\_64 amd64** ; **09:27:44.734 INFO PathSeqPipelineSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_152-release-1056-b12** ; **09:27:44.734 INFO PathSeqPipelineSpark - Start Date/Time: 2020년 3월 5일 (목) 오전 9시 27분 43초** ; **09:27:44.734 INFO PathSeqPipelineSpark - ------------------------------------------------------------** ; **09:27:44.734 INFO PathSeqPipelineSpark - ------------------------------------------------------------** ; **09:27:44.735 INFO PathSeqPipelineSpark - HTSJDK Version: 2.21.0** ; **09:27:44.735 INFO PathSeqPipelineSpark - Picard Version: 2.21.2** ; **09:27:44.735 INFO PathSeqPipelineSpark - HTSJDK Defaults.COMPRESSION\_LEVEL : 2** ; **09:27:44.735 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false** ; **09:27:44.735 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true** ; **09:27:44.735 INFO PathSeqPipelineSpark - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false** ; **09:27:44.735 INFO PathSeqPipelineSpark - Deflater: IntelDeflater** ; **09:27:44.735 INFO PathSeqPipelineSpark - Inflater: IntelInflat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:2762,release,release-,2762,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['release'],['release-']
Deployability,"ng this bug for 57 samples of 5000 crams at snp rs429358 but I would expect it is not unique to this site. . Select two crams with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99:0,159,1744; 0/0:45,0:45:99:0,135,1529; 0/0:44,0:44:99:0,132,1419; 0/0:38,0:38:99:0,114,1299; 0/0:37,0:37:99:0,111,1205; 0/0:34,0:34:99:0,102,1151; 0/0:57,0:57:99:0,171,1826; 0/0:27,1:28:49:0,49,904; 0/0:41,0:41:99:0,123,1364; 0/0:28,0:28:84:0,84,933; 0/0:36,0:36:99:0,108,1171; 0/0:29,0:29:87:0,87,987; 0/0:31,0:31:93:0,93,997; 0/0:37,0:37:99:0,111,126",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:1932,Pipeline,Pipeline,1932,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,ng.TestNG.run(TestNG.java:1018); at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:111); at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:204); at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:175); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:125); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140); Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.dataflow.sdk.Pipeline.run(Pipeline.java:166); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.runPipeline(DataflowCommandLineProgram.java:145); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:107); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:151); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:78); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:75); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); ... 33 more; Caused by: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.genomics.dataflow.readers.bam.Reader.process(Reader.java:93); at com.google.cloud.genomics.dataflow.readers.bam.ReadBAMTransform$ReadFn.processElement(ReadBAMTransform.java:68); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866:3786,Integrat,IntegrationTestSpec,3786,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866,2,['Integrat'],['IntegrationTestSpec']
Deployability,"ng: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:6928,configurat,configuration,6928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['configurat'],['configuration']
Deployability,"nges from @LeeTL1220, @meganshand, and @jsotobroad to dev branch. (Note that we exposed PreprocessIntervals.bin_length in these WDLs; I'm assuming that https://github.com/broadinstitute/cromwell/issues/2912 will allow this to be specified via the json, so I reverted this change.); - [x] Make simple improvements to ReCapSeg caller (#3825).; - [x] Review and merge modeling/WDL PR. (#3913 awaiting review. Note that this PR also deletes the old germline WDL.); - ~~Write MultidimensionalKernelSegmenterUnitTest.~~ (SL, punting, filed #3916); - ~~Write ModelSegmentsIntegrationTest.~~ (SL, punting, filed #3916); - [x] Preliminary PCAWG or HCC1143 purity evaluation. (@LeeTL1220) (LL, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (PR #4010 awaiting review.); - [x] Add SM tag and sequence dictionary headers to all appropriate files and sort accordingly. (SL, #3914 awaiting review); - [x] Update tutorial data. (@MartonKN); - [ ] (Reach) Add VCF output.; - [ ] (Reach) Add PG tags to all files.; - [ ] (Reach) Replace ReCapSeg caller with improved version. (@MartonKN). gCNV pipeline:; - [x] Review and merge Python code (#3838). (MB and SL, PR #3925 awaiting review.); - [x] CLI for ploidy determination (cohort). (@samuelklee); - [x] CLI for ploidy determination (case). (@samuelklee); - [x] CLI for calling (cohort). (@samuelklee); - [x] CLI for calling (case). (@samuelklee); - [ ] CLI for post-processing calls. (@asmirnov239) (AS, PR issued by 12/4); - [x] Python environment. (Update: I've verified that gCNV works on the gsa server with a manual setup of conda (python=3.6) + @mbabadi's pip install---although I do get an ""install mkl"" warning from theano. We can discuss autoloading of this environment after release, but should at least have some clear documentation.); - [x] WDL and Cromwell tests. (SL, PR issued by 12/1); - [x] Preliminary evaluation. (MB, should be do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:1616,Update,Update,1616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,1,['Update'],['Update']
Deployability,no reason to depend on spark when we depend on spark ML lib anyway. Simpler to update 1 dependency than two. @lbergelson wdyt?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2035:79,update,update,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2035,1,['update'],['update']
Deployability,no-one should be calling close() by themselves. All Closables are Autoclosables and all should be converted to the modern style. Code migrated from Java 6 is full of the old idiom and needs to be updated.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/340:196,update,updated,196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/340,1,['update'],['updated']
Deployability,"nomicsDBImport - Start Date/Time: January 10, 2018 7:29:01 PST PM; 19:29:01.392 INFO GenomicsDBImport - ------------------------------------------------------------; 19:29:01.392 INFO GenomicsDBImport - ------------------------------------------------------------; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Version: 2.13.2; 19:29:01.392 INFO GenomicsDBImport - Picard Version: 2.17.2; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 19:29:01.392 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:29:01.393 INFO GenomicsDBImport - Deflater: IntelDeflater; 19:29:01.393 INFO GenomicsDBImport - Inflater: IntelInflater; 19:29:01.393 INFO GenomicsDBImport - GCS max retries/reopens: 20; 19:29:01.393 INFO GenomicsDBImport - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 19:29:01.393 INFO GenomicsDBImport - Initializing engine; 19:29:23.214 INFO IntervalArgumentCollection - Processing 1000000 bp from intervals; 19:29:23.216 INFO GenomicsDBImport - Done initializing engine; 19:29:23.332 INFO GenomicsDBImport - Shutting down engine; [January 10, 2018 7:29:23 PST PM] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.37 minutes.; Runtime.totalMemory()=2804940800; Exception in thread ""main"" java.lang.ExceptionInInitializerError; at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.overwriteOrCreateWorkspace(GenomicsDBImport.java:706); at org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport.onTraversalStart(GenomicsDBImport.java:448); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:891); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4124:1875,patch,patch,1875,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4124,1,['patch'],['patch']
Deployability,nonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1190); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). **This is the stack I get when the test completes but fails (note that the expected line count appears to not match the line count of the expected output file in the repo): **. java.lang.AssertionError: line counts expected [2629] but found [507]; 	at org.testng.Assert.fail(Assert.java:94); 	at org.testng.Assert.failNotEquals(Assert.java:496); 	at org.testng.Assert.assertEquals(Assert.java:125); 	at org.testng.Assert.assertEquals(Assert.java:372); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:211); 	at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.assertEqualTextFiles(IntegrationTestSpec.java:190); 	at org.broadinstitute.hellbender.tools.examples.ExampleAssemblyRegionWalkerSparkIntegrationTest.testExampleAssemblyRegionWalker(ExampleAssemblyRegionWalkerSparkIntegrationTest.java:29); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); 	at org.testng.internal.TestMethodWorker.invok,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2349:3460,Integrat,IntegrationTestSpec,3460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2349,1,['Integrat'],['IntegrationTestSpec']
Deployability,note I still need to update the docker image and utils,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8728:21,update,update,21,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8728,1,['update'],['update']
Deployability,"now that we have the logic updated for per-assembly-contig view when it comes to SV type and breakpoint location inference, the next step is to take the comprehensive view, that is. * inter-contig view, i.e. mappings of other contigs nearby a potential breakpoint when inferring types ; * depth/coverage information; * evidence target links, i.e pair and split read information. To illustrate: consider a dispersed duplication, where the reference has sequence blocks `A z B` and the sample has sequence blocks arranged as `A z A B`.; Having assembled across the novel adjacency across the `A B` block, simply looking at the contig's alignment may fool us into calling a deletion&mdash;exactly what we are doing now&mdash;while it apparently is the wrong type.; On the other hand, if we have assembled across the `z A` block, without checking depth information, we might be thinking the blocks `A z` is being duplicated (we are not currently, but rather emit `BND` novel adjacency between the reference locations instead of emitting a duplication).; In the better (but not best, which we would have assembled across the whole event and the code currently would correctly emit the correct interpretation) scenario, we could have assembled across both novel adjacencies, and the increased coverage at `A` block, as well as outties pair linking `B` and `A` block, would allow us to formulate the whole picture.; Though difficult, it is what we must do. This ticket is to remind us that when we code our logic in any of these units, be open to other units for a comprehensive model on inference.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4189:27,update,updated,27,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4189,1,['update'],['updated']
Deployability,now that we're on htsjdk 2.2.1 we should switch to asyncIO for bams (not tribble); - [x] switch to asyncIO for bams (build.gradle); - [x] switch tests to use asyncIO for bams (build.gradle); - [x] update readme to say that we're using async IO; - [x] update startup message to clarify which IO is sync/async. measure and report performance impact on (using JdkDeflater and IntelDeflater); - [x] PrintReads ; - [x] BaseRecalibrator,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1653:197,update,update,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1653,2,['update'],['update']
Deployability,"nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.NoSuchFileException: /user/yaron/output.bam.parts/_SUCCESS: Unable to find _SUCCESS file; at org.seqdoop.hadoop_bam.util.SAMFileMerger.mergeParts(SAMFileMerger.java:53); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:250); ... 18 more; ```; However, I can find that _SUCCESS file exists in output.bam.parts. Could someone tell me what may be the cause? Thanks!; ```; $ hdfs dfs -ls output.bam.parts; Found 3 items; -rw-r--r-- 3 yaron yaron 0 2017-06-08 09:14 output.bam.parts/_SUCCESS; -rw-r--r-- 3 yaron yaron 62019 2017-06-0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3066:6234,deploy,deploy,6234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3066,1,['deploy'],['deploy']
Deployability,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:171); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:190); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:743); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: java.lang.IllegalStateException: unread block data; at java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:10020,deploy,deploy,10020,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['deploy'],['deploy']
Deployability,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); at org.broadinstitute.hellbender.Main.main(Main.java:233); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:736); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: observedValue must be non-negative; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); at org.broadinstitute.hellbender.tools.spark.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:5793,deploy,deploy,5793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['deploy'],['deploy']
Deployability,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:755); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:43661,deploy,deploy,43661,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['deploy'],['deploy']
Deployability,nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:153); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:277); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: org.broadinstitute.hellbender.exceptions.GATKException: Erred when inferring breakpoint location and event type from chimeric alignment:; asm010450:tig00000 1_189_chrUn_JTFH01000312v1_decoy:663-851_-_189M512H_60_8_149_O 153_701_chrUn_JTFH01000312v1_decoy:1-549_+_152S549M_60_0_549_O; at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:51); at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$null$0(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$Wrappin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:11480,deploy,deploy,11480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['deploy'],['deploy']
Deployability,"nstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RD",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:17491,deploy,deploy,17491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['deploy'],['deploy']
Deployability,nternal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfiguration(ProjectScriptTarget.java:77); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl.apply(DefaultScriptPluginFactory.java:181); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:38); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.BuildScriptProcessor.execute(BuildScriptProcessor.java:25); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.ConfigureActionsProjectEvaluator.evaluate(ConfigureActionsProjectEvaluator.java:34); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.project.LifecycleProjectEvaluator.evaluate(LifecycleProjectEvaluator.java:55); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:573); 22:05:55.971 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.api.internal.project.DefaultProject.evaluate(DefaultProject.java:125); 22:05:55.971 [ERROR] [org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:2837,configurat,configuration,2837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['configurat'],['configuration']
Deployability,"nternally by discrete RV posterior update routines (""callers"") as a safety measure to stabilize self-consistency loops. For example, consider the mean-field treatment of two coupled Markov chains: the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum valu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1275,continuous,continuous-only,1275,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['continuous'],['continuous-only']
Deployability,ntextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3410,deploy,deploy,3410,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"ntextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5581,deploy,deploy,5581,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,ntextSelector.createContext(ClassLoaderContextSelector.java:171); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.locateContext(ClassLoaderContextSelector.java:145); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:70); at org.apache.logging.log4j.core.selector.ClassLoaderContextSelector.getContext(ClassLoaderContextSelector.java:57); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:140); at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:41); at org.apache.logging.log4j.LogManager.getContext(LogManager.java:182); at org.apache.logging.log4j.LogManager.getLogger(LogManager.java:455); at org.broadinstitute.hellbender.utils.Utils.<clinit>(Utils.java:77); at org.broadinstitute.hellbender.Main.<clinit>(Main.java:45); at java.lang.Class.forName0(Native Method); at java.lang.Class.forName(Class.java:348); at org.apache.spark.util.Utils$.classForName(Utils.scala:230); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:739); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.appender.AbstractAppender; at java.net.URLClassLoader.findClass(URLClassLoader.java:381); at java.lang.ClassLoader.loadClass(ClassLoader.java:424); at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); at java.lang.ClassLoader.loadClass(ClassLoader.java:357); ... 43 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12624/java-lang-noclassdeffounderror-org-apache-logging-log4j-core-appender-abstractappender/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5126:5540,deploy,deploy,5540,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5126,6,['deploy'],['deploy']
Deployability,"ntig-Ploidy/22.Contig_Ploidy_Dir --output-prefix ploidy --verbosity DEBUG. .............................................................(BUG 002)..........................................................; Stderr: Traceback (most recent call last):; File ""/tmp/segment_gcnv_calls.3402406683372415608.py"", line 9, in <module>; import gcnvkernel; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 5, in <module>; from .continuous import get_tau_sd, Normal, Flat; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 16, in <module>; from pymc3.theanof import floatX; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/theanof.py"", line 89, in <module>; empty_gradient = tt.zeros(0, dtype='float32'); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 2558, in zeros; return alloc(np.array(0, dtype=dtype), *shape); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 3091, in __call__; ret = super(Alloc, self).__call__(val, *shapes, **kwargs); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:10214,continuous,continuous,10214,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['continuous'],['continuous']
Deployability,"ntimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:22317,update,update,22317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['update'],['update']
Deployability,"nv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5714:30206,update,update-convergence-threshold,30206,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714,1,['update'],['update-convergence-threshold']
Deployability,"o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_samtools.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/cram_stats.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/files.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/mFILE.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/open_trace_file.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/pooled_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/rANS_static.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/sam_header.o build/temp.macosx-10.7-x86_64-3.6/htslib/cram/string_alloc.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_libcurl.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_gcs.o build/temp.macosx-10.7-x86_64-3.6/htslib/hfile_s3.o -Lpysam -L. -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -Lbuild/lib.macosx-10.7-x86_64-3.6/pysam -L/Users/markw/anaconda/envs/gatk/lib -lz -llzma -lbz2 -lz -lcurl -o build/lib.macosx-10.7-x86_64-3.6/pysam/libchtslib.cpython-36m-darwin.so -dynamiclib -rpath @loader_path -Wl,-headerpad_max_install_names -Wl,-install_name,@rpath/libchtslib.cpython-36m-darwin.so -Wl,-x; gcc: error: @loader_path: No such file or directory; gcc: error: unrecognized command line option ‘-rpath’; error: command 'gcc' failed with exit status 1. ----------------------------------------; Command ""/Users/markw/anaconda/envs/gatk/bin/python -u -c ""import setuptools, tokenize;__file__='/private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"" install --record /var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-z_8e2y_r-record/install-record.txt --single-version-externally-managed --compile"" failed with error code 1 in /private/var/folders/x6/k5tc9mwd2z7dm1dthy4hv9nxmt8q9j/T/pip-build-aybz1ucp/pysam/. CondaValueError: pip returned an error; ```; This appears to have been an issue with pysam in the past: https://github.com/pysam-developers/pysam/issues/323",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4742:3476,install,install,3476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4742,2,['install'],"['install', 'install-record']"
Deployability,oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:23436,deploy,deploy,23436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.b,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2368,deploy,deploy,2368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,"oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4698,deploy,deploy,4698,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.pathExists(SparkUtils.java:100); 	at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:2858,deploy,deploy,2858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"oadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. ***********************************************************************; org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:7594,deploy,deploy,7594,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,oadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFact,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3666,deploy,deploy,3666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"oadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Clie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5837,deploy,deploy,5837,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"oadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.datasources.ReferenceMultiSource, name: referenceSource, type: interfac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:2830,deploy,deploy,2830,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['deploy'],['deploy']
Deployability,ocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkComman,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1602,deploy,deploy,1602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,2,['deploy'],['deploy']
Deployability,"ocker image is this:; ```; FROM bde2020/spark-master:2.2.0-hadoop2.8-hive-java8. MAINTAINER Jhonattan Loza <toro.ryan.jcl@gmail.com>. COPY picard.jar /; COPY GenomeAnalysisTK_v3.8-0-ge9d806836.jar /. RUN curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash; RUN apt-get install -y git-lfs; RUN git lfs install; RUN apt-get install unzip; RUN apt-get install wget; RUN apt-get install git. RUN mkdir /gatk; RUN apt-get update && apt-get install -y python git mlocate htop && export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8 && \; wget https://github.com/broadinstitute/gatk/releases/download/4.0.4.0/gatk-4.0.4.0.zip && unzip gatk-4.0.4.0.zip -d tmp && mv tmp/gatk-4.0.4.0/* /gatk && cp /spark/conf/spark-defaults.conf.template /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.enabled true"" >> /spark/conf/spark-defaults.conf && \; echo ""spark.eventLog.dir file:///spark/logs/"" >> /spark/conf/spark-defaults.conf. ENV PATH=""$PATH:/spark/bin""; ```; I have this configurations for docker-compose:; - Spark. ```; version: '3'; services:; spark-master:; image: atahualpa/spark-master:GATK4.0.4; networks:; - workbench; deploy:; replicas: 1; mode: replicated; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8080; env_file:; - ./hadoop.env; ports:; - 8333:8080; - 4040:4040; - 6066:6066; - 7077:7077; volumes:; - /data0/reference/hg19-ucsc/:/reference/hg19-ucsc/; - /data0/fastq/:/fastq/; - /data0/NGS-SparkGATK/NGS-SparkGATK/:/NGS-SparkGATK/; - /data/ngs/:/ngs/; - /data0/output/:/output/; spark-worker:; image: bde2020/spark-worker:2.2.0-hadoop2.8-hive-java8; networks:; - workbench; environment:; - SPARK_MASTER=spark://spark-master:7077; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 8081. env_file:; - ./hadoop.env; volumes:; - reference-image:/reference_image. reference:; image: vzzarr/reference:hg19_img; networks:; - workbench; de",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:1101,configurat,configurations,1101,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['configurat'],['configurations']
Deployability,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:3372,INSTALL,INSTALLDIRGCC,3372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,2,"['INSTALL', 'install']","['INSTALLDIRGCC', 'installed']"
Deployability,"oft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: | Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.i9brvcrk.requirements.txt', '--exists-action=b']; Pip subprocess output:. Pip subprocess error:; /opt/miniconda/envs/gatk/bin/python: No module named pip. failed. CondaEnvException: Pip failed. ```; ---; It can be fixed with setting classic colver:; ```; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda --version; conda 23.10.0; root@d12ac7710afc:/soft/gatk-4.4.0.0# conda config --set solver classic; root@d12ac7710afc:/soft/gatk-4.4.0.0# ""$CONDA"" env create -n gatk -f ""$SOFT/gatk-${GATK_VERSION}/gatkcondaenv.yml""; ...; Preparing transaction: done; Verifying transaction: done; Executing transaction: done; Installing pip dependencies: \ Ran pip subprocess with arguments:; ['/opt/miniconda/envs/gatk/bin/python', '-m', 'pip', 'install', '-U', '-r', '/soft/gatk-4.4.0.0/condaenv.rtsyg5rl.requirements.txt', '--exists-action=b']; Pip subprocess output:; Processing ./gatkPythonPackageArchive.zip; Building wheels for collected packages: gatkpythonpackages; Building wheel for gatkpythonpackages (setup.py): started; Building wheel for gatkpythonpackages (setup.py): finished with status 'done'; Created wheel for gatkpythonpackages: filename=gatkpythonpackages-0.1-py3-none-any.whl size=117686 sha256=f2165b43e412c95ff9a788022d355279e5434032fb8c9cf82fbd71779acd1a76; Stored in directory: /tmp/pip-ephem-wheel-cache-5a9zdytx/wheels/06/f7/e1/87cb7da6f705baa602256a58c9514b47dc313aade8809a01da; Successfully built gatkpythonpackages; Installing collected packages: gatkpythonpackages; Successfully installed gatkpythonpackages-0.1. done; #; # To activate this environment, use; #; # $ conda activate gatk; #; # To deactivate an active environmen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8618:2817,Install,Installing,2817,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8618,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"ogress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. After sourcing the tab-completion script, some tools shown cannot be run. Maybe they exist somewhere in an experimental dev version but are not bundled for public release?. ### Affected version(s); - [x ] Latest public release version [4.1.7.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. After trying to tab complete the DepthOfCoverage, I saw a few tools not listed in the documentation. I tried running them and sure enough, there were errors:. `A USER ERROR has occurred: '*' is not a valid command.`; (* is one of the tools listed below). #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. ```; cd gatk-4.1.7.0; source gatk-completion.sh; ./gatk Depth<tab>; #>DepthOfCoverage DepthPerAlleleBySample DepthPerSampleHC; ./gatk DepthPerSampleHC -h; ...; ***********************************************************************; A USER ERROR has occurred: 'DepthPerSampleHC' is not a valid command.; *******************",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6615:1480,release,release,1480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6615,1,['release'],['release']
Deployability,"ok so we will want to set the limit to 100 and update the VCF header, but this is a start at what I want for the filter model creation update",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8843:47,update,update,47,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8843,2,['update'],['update']
Deployability,"olicy; 18/04/24 17:55:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/1 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/2 is now RUNNING; 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/3 is now RUNNING; 18/04/24 17:55:01 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.16:49734 with 366.3 MB RAM, BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.16, 49734, None); 18/04/24 17:55:01 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/4 is now RUNNING; 18/04/24 17:55:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/24 17:55:03 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/24 17:55:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/5 is now RUNNING; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 276.0 KB, free 366.0 MB); 00:10 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/24 17:55:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 366.0 MB); 18/04/24 17:55:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.16:49734 (size: 23.1 KB, free: 366.3 MB); 18/04/24 17:55:05 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/24 17:55:06 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180424175501-0004/0 is now RU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:13557,update,updated,13557,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['update'],['updated']
Deployability,"ols.github.io/hts-specs/VCFv4.2.pdf) doesn't actually say what format should the SB field be, so overriding it seems to be a bug in htsjdk?. #### Steps to reproduce; ```; cat sb-good-tiny.vcf; ##fileformat=VCFv4.2; ##INFO=<ID=SB,Number=.,Type=Integer,Description="""">; ##contig=<ID=chr1,length=248956422,assembly=GRCh38>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO; chr1	10146	.	AC	A	.	.	SB=5,2,18,29. gatk ApplyVQSR -O sb-recalibrated-tiny.vcf -V sb-good-tiny.vcf --recal-file recalibration. cat sb-recalibrated-tiny.vcf; ##fileformat=VCFv4.2; ##FILTER=<ID=LOW_VQSLOD,Description=""VQSLOD < 0.0"">; ##FILTER=<ID=PASS,Description=""Site contains at least one allele that passes filters"">; ##GATKCommandLine=<ID=ApplyVQSR,CommandLine=""ApplyVQSR --recal-file /Users/vlad/tmp/sb/recalibration --output sb-recalibrated-tiny-renamed4.vcf --variant sb-good-tiny-renamed4.vcf --use-allele-specific-annotations false --ignore-all-filters false --exclude-filtered false --mode SNP --interval-set-rule UNION --interval-padding 0 --interval-exclusion-padding 0 --interval-merging-rule ALL --read-validation-stringency SILENT --seconds-between-progress-updates 10.0 --disable-sequence-dictionary-validation false --create-output-bam-index true --create-output-bam-md5 false --create-output-variant-index true --create-output-variant-md5 false --lenient false --add-output-sam-program-record true --add-output-vcf-command-line true --cloud-prefetch-buffer 40 --cloud-index-prefetch-buffer -1 --disable-bam-index-caching false --sites-only-vcf-output false --help false --version false --showHidden false --verbosity INFO --QUIET false --use-jdk-deflater false --use-jdk-inflater false --gcs-max-retries 20 --gcs-project-for-requester-pays --disable-tool-default-read-filters false"",Version=""4.1.9.0"",Date=""31 May 2021 12:07:54 PM"">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ##INFO=<ID=NEGATIVE_TRAIN_SITE,Number=0,Type=Flag,Description=""This variant was used to build the negative",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7280:2358,update,updates,2358,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7280,1,['update'],['updates']
Deployability,"omaticPanelOfNormals. ### Affected version; Tested on version 4.1.8.0 (likely commit 3e921c6, GenomicsDB 1.3.0 #6654). ### Description ; Panel of normals generated from version 4.1.8.0 has some ~28% less records (~52% less ALT alleles) than one created with 4.1.7.0 (tested at commit 9cc92e3) with all input data and arguments unchanged. The GenomicsDB version does not seem to matter as PoN created running CreateSomaticPanelOfNormals on 4.1.8.0 has the result is about the same regardless of whether GenomicsDBImport was run on 4.1.7.0 or 4.1.8.0. CreateSomaticPanelOfNormals on 4.1.7.0 fails to run on the new GenomicsDBs. |Mutect2|GenomicsDB|CreateSomaticPanelOfNormals|Output|; |---|---|---|---|; |4.1.7.0|4.1.7.0|4.1.7.0|100% alleles (reference)|; |4.1.8.0|4.1.8.0|4.1.7.0|expected error|; |4.1.7.0|4.1.7.0|4.1.8.0|48% alleles|; |4.1.8.0|4.1.8.0|4.1.8.0|48% alleles|. #### Steps to reproduce; The PoN was created with GRCh38, scattered over chromosomes. Mutect command:; ```; $gatk/gatk --java-options ""-Xmx4G"" Mutect2 \; 	-R $reference -L $chr \; 	-I $bam --max-mnp-distance 0 \; 	-O @out1@; ```. GenomicsDBImport command:; ```; $gatk/gatk --java-options ""-Xms8G -Xmx8G"" GenomicsDBImport \; 	-R $reference -L $chr \; 	--sample-name-map ${inputGenomicsDB.out1} \; 	--genomicsdb-workspace-path @folder1@; ```. CreateSomaticPanelOfNormals:; ```; $gatk/gatk --java-options ""-Xms8G"" CreateSomaticPanelOfNormals \; 	-R $reference -V gendb://@folder1@ -O @out1@ \; 	--germline-resource $gnomad \; 	--max-germline-probability 0.5; ```. #### Expected behavior; Based on description of the GenomicsDB 1.3.0 update, CreateSomaticPanelOfNormals is expected to behave similarly in 4.1.8.0 as before with the output PoN containing a similar number of variants. #### Actual behavior; 28% of PoN records (52% alleles) are missing in 4.1.8.0 compared to 4.1.7.0. Although all spanning deletions are dropped in the new version, they account for only a small portion of the loss (6.8% / 52% missing ALT alleles).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6744:1645,update,update,1645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6744,1,['update'],['update']
Deployability,"ome; - cds_end_NF: the coding region end could not be confirmed; - cds_start_NF: the coding region start could not be confirmed; - mRNA_end_NF: the mRNA end could not be confirmed; - mRNA_start_NF: the mRNA start could not be confirmed.; - basic: the transcript is part of the gencode basic geneset. Comments. Lines may be commented out by the addition of a single # character at the start. These; lines should be ignored by your parser. Pragmas/Metadata. GTF files can contain meta-data. In the case of experimental meta-data these are ; noted by a #!. Those which are stable are noted by a ##. Meta data is a single key,; a space and then the value. Current meta data keys are:. * genome-build - Build identifier of the assembly e.g. GRCh37.p11; * genome-version - Version of this assembly e.g. GRCh37; * genome-date - The date of this assembly's release e.g. 2009-02; * genome-build-accession - The accession and source of this accession e.g. NCBI:GCA_000001405.14; * genebuild-last-updated - The date of the last genebuild update e.g. 2013-09. ------------------; Example GTF output; ------------------. #!genome-build GRCh38; 11 ensembl_havana gene 5422111 5423206 . + . gene_id ""ENSG00000167360""; gene_version ""4""; gene_name ""OR51Q1""; gene_source ""ensembl_havana""; gene_biotype ""protein_coding"";; 11 ensembl_havana transcript 5422111 5423206 . + . gene_id ""ENSG00000167360""; gene_version ""4""; transcript_id ""ENST00000300778""; transcript_version ""4""; gene_name ""OR51Q1""; gene_source ""ensembl_havana""; gene_biotype ""protein_coding""; transcript_name ""OR51Q1-001""; transcript_source ""ensembl_havana""; transcript_biotype ""protein_coding""; tag ""CCDS""; ccds_id ""CCDS31381"";; 11 ensembl_havana exon 5422111 5423206 . + . gene_id ""ENSG00000167360""; gene_version ""4""; transcript_id ""ENST00000300778""; transcript_version ""4""; exon_number ""1""; gene_name ""OR51Q1""; gene_source ""ensembl_havana""; gene_biotype ""protein_coding""; transcript_name ""OR51Q1-001""; transcript_source ""ensembl_havana""; transcript_bioty",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6488:6208,update,updated,6208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6488,2,['update'],"['update', 'updated']"
Deployability,"omething similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); gatk-4.1.8.1; - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I was running Concordance analysis via:. gatk Concordance -R human_g1k_v37.fasta -eval new.vcf --truth old.vcf --summary summary.tsv. my summary.tsv contains only this:. type TP FP FN RECALL PRECISION; SNP 285 1876867 2535060 0.0 0.0; INDEL 0 0 8542 0.0 0.0. Can you please tell me how I can interpret this?. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6795:1320,release,release,1320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6795,1,['release'],['release']
Deployability,omicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 01:22:35.483 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 01:22:35.483 INFO GenomicsDBImport - Deflater: IntelDeflater; 01:22:35.483 INFO GenomicsDBImport - Inflater: IntelInflater; 01:22:35.483 INFO GenomicsDBImport - GCS max retries/reopens: 20; 01:22:35.483 INFO GenomicsDBImport - Requester pays: disabled; 01:22:35.484 INFO GenomicsDBImport - Initializing engine; 01:24:58.683 INFO FeatureManager - Using codec BEDCodec to read file file:///lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/intervals.bed; 01:24:58.801 INFO IntervalArgumentCollection - Processing 11500 bp from intervals; 01:24:58.803 INFO GenomicsDBImport - Done initializing engine; 01:24:59.055 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.3.2-e18fa63; 01:25:02.076 INFO GenomicsDBImport - Vid Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vidmap.json; 01:25:02.077 INFO GenomicsDBImport - Callset Map JSON file will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/callset.json; 01:25:02.077 INFO GenomicsDBImport - Complete VCF Header will be written to /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb/vcfheader.vcf; 01:25:02.077 INFO GenomicsDBImport - Importing to workspace - /lustre/scratch118/malaria/team112/personal/vr6/pf8-update/work/8e/c9ed494e9cd5d45835890fff4fa34c/Pf3D7_08_v3_33.bed.gdb; 01:25:02.078 INFO ProgressMeter - Starting traversal; 01:25:02.078 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; [TileDB::FileSystem] Error: (write_to_file) Cannot write to file; File writing error; path=/lustre/scratch118/malaria/team112/per,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7598:3238,update,update,3238,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7598,1,['update'],['update']
Deployability,"ommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:6082,pipeline,pipelines,6082,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['pipeline'],['pipelines']
Deployability,ompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory.java:206); at org.gradle.launcher,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:4937,Continuous,ContinuousBuildActionExecuter,4937,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,3,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,"on;; import java.nio.file.Files;; import java.nio.file.Path;; import java.nio.file.Paths;; import java.nio.file.spi.FileSystemProvider;; import java.util.ArrayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!fou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312:1813,install,installedProviders,1813,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312,1,['install'],['installedProviders']
Deployability,"onActor [968be82cValidateBamsWf.ValidateBAM:0:1]: Unrecognized runtime attribute keys: disks, memory; [2020-07-14 05:09:41,71] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: /gatk/gatk \; ValidateSamFile \; --INPUT /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/inputs/-1942028726/test.bam \; --OUTPUT test.validation_.txt \; --MODE SUMMARY; [2020-07-14 05:09:41,76] [info] BackgroundConfigAsyncJobExecutionActor [968be82cValidateBamsWf.ValidateBAM:0:1]: executing: # make sure there is no preexisting Docker CID file; rm -f /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid \; -i \; \; --entrypoint /bin/bash \; -v /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0:delegated \; broadinstitute/gatk@sha256:18146e79d06787483310e5de666502090a480e10ac0fad06a36a5e7a5c9bb1dc /cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/script. # get the return code (working even if the container was detached); rc=$(docker wait cat /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid). # remove the container after waiting; docker rm cat /gatk/my_data/tools/cromwell-executions/ValidateBamsWf/968be82c-eef3-4bdb-a1ab-3d4e2ca70674/call-ValidateBAM/shard-0/execution/docker_cid. # return exit code; exit $rc; [2020-07-14 05:09:45,29] [info] BackgroundConfigAsyncJobExecutionActor [968be8",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:5347,configurat,configuration,5347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['configurat'],['configuration']
Deployability,"onCommandExecution.java:120); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.StartBuildOrRespondWithBusy$1.run(StartBuildOrRespondWithBusy.java:50); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.DaemonStateCoordinator$1.run(DaemonStateCoordinator.java:293); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Caused by: org.gradle.api.GradleException: Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.984 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$_resolveLargeResourceStubFiles_closure36.doCall(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:102); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.resolveLargeResourceStubFiles(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:116); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17$resolveLargeResourceStubFiles$0.callCurrent(Unknown Source); 22:05:55.985 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at build_2s1dokgyqm2mnf3n5hcv2kf17.ensureBuildPrerequisites(/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle:140); 22:05:55.985 [E",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:12791,install,installed,12791,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['install'],['installed']
Deployability,"onScriptExecutorException for some of the **random** sub-projects: . .............................................................(BUG 001).......................................................... Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.3351404099122294482.py"", line 8, in <module>; import gcnvkernel; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 5, in <module>; from .continuous import get_tau_sd, Normal, Flat; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 16, in <module>; from pymc3.theanof import floatX; File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/pymc3/theanof.py"", line 89, in <module>; empty_gradient = tt.zeros(0, dtype='float32'); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 2558, in zeros; return alloc(np.array(0, dtype=dtype), *shape); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/tensor/basic.py"", line 3091, in __call__; ret = super(Alloc, self).__call__(val, *shapes, **kwargs); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; no_recycling=[]); File ""/usr/local/Anaconda/envs_app/gatk/4.1.2.0/lib/python3.6/site-packages/theano/gof/op.py"", line 955, in make_thunk; no_recycling); File ""/usr/local/Anaconda/envs_app/gatk/4.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6235:2147,continuous,continuous,2147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6235,1,['continuous'],['continuous']
Deployability,once a new htsjdk release comes out we should switch off relying on since #1153,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1154:18,release,release,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1154,1,['release'],['release']
Deployability,"oncotator that was fixed in Funcotator involving cdna; strings for - strand indels. In Oncotator the positions reported are off by 1 (they; should be one less) and the base reported is also wrong.; This is now fixed. - Removed some old code that had been taken out of the main codepath. - Fixed a bug in how the gencode reference contexts are created.; - Fixed a bug in how the end points for the gencode annotations are; created. - Ref context field is now consistent for indels.; The reference context will give WINDOW bases before and after the; logical reference allele for a variant. This is NOT the allele in the; input VCF, but rather the allele that actually has changed. For; insertions, the logical allele is the SPACE BETWEEN TWO BASES (and; therefore the resulting string will always be 2xWINDOW bases long).; For deletions, the logical allele is the given ref allele without the; required preceding base. For MNPs the logical allele is the given ref; allele.; Updated some tests and test data to reflect this change. - Added a small HG38 regression test set. - Fixed a boundary bug with codon strings.; Now codon change strings have an alternate (correct) form for insertions; that involve the start codon on the - strand, and the stop codon on the; + strand. This form eliminates any overrun/out of bounds exceptions. - Fixed an issue involving variants that overrun the end of the coding sequence. - Added in additional required files for regression test gencode data source. - Added a helpful script and modified test data set to be correct. - Updated part of Gencode to prepare for fixing the exon boundary issue. - Updated FuncotatorIntegrationTests to use environment-variable paths; more safely. - Updated `FuncotatorUtils::getCodingSequenceChangeString` to use; base data types rather than those in `SequenceComparison`. - Refactored; `GencodeFuncotationFactory::createCodingRegionFuncotationForNonProteinCodingFeature`; to remove the use of `SequenceComparison` objects. - Updat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5302:3939,Update,Updated,3939,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5302,1,['Update'],['Updated']
Deployability,"oogle.com/apt cloud-sdk-bionic InRelease [6786 B] ; Get:6 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1426 kB] ; Err:5 http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease ; The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2295 kB] ; Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] ; Get:9 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] ; Get:10 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB] ; Get:11 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB] ; Get:13 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB] ; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2200 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [575 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2731 kB]; Get:18 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:19 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Reading package lists... Done ; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY FEEA9169307EA071 NO_PUBKEY 8B57C5C2836F4BEB; E: The repository 'http://packages.cloud.google.com/apt cloud-sdk-bionic InRelease' is not signed.; N: Updating from such a repository can't be done securely, and is therefore disabled by default.; N: See apt-secure(8) manpage for repository creation and user configuration detai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7447:2060,update,updates,2060,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7447,1,['update'],['updates']
Deployability,"ool will have the exact same functionality as `CollectAllelicCounts`, to the point where I can re-use the integration tests. However, the integration tests fail. When I dig deeper in `CollectAllelicCountsSpark`, I see that only 8 RDDs (correct amount: 11) are being passed to processAlignments... Consider the following code:. ```; @Override; protected void processAlignments(JavaRDD<LocusWalkerContext> rdd, JavaSparkContext ctx) {; final String sampleName = SampleNameUtils.readSampleName(getHeaderForReads());; final SampleMetadata sampleMetadata = new SimpleSampleMetadata(sampleName);; final Broadcast<SampleMetadata> sampleMetadataBroadcast = ctx.broadcast(sampleMetadata);. final AllelicCountCollector finalAllelicCountCollector =; rdd.mapPartitions(distributedCount(sampleMetadataBroadcast.getValue(), minimumBaseQuality)); .reduce((a1, a2) -> combineAllelicCountCollectors(a1, a2, sampleMetadataBroadcast.getValue()));; final List<LocusWalkerContext> tmp = rdd.collect();; ....snip....; ```. In this case `tmp` will have a size of 8. However, the integration test would indicate a size of 11 is correct, since 11 intervals are being passed in. Note that `emitEmptyLoci()` returns `true`, so 11 is the correct number as seen in `CollectAllelicCountsSparkIntegrationTest` . . Additionally, in (at least) one result, the counts are wrong. `CollectAllelicCounts` (non-spark) passes the integration test. I have tried a couple of tests to gather more information:. - Is `emitEmptyLoci()` causing an issue? ; Does not appear to be causing the issue. I say this because when set to `false`, I get (essentially) the same error.; - The code uses `mapPartition` and not `map`, does this cause the issue? Why are you doing this?; This does not cause the issue. I refactored the code to use `map` and got the exact same issue. I use `mapPartition` in order to instantiate only one instance of `AllelicCountCollector` per partition, instead of per locus. Assigning to @tomwhite by request of @droazen ...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3823:1165,integrat,integration,1165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3823,2,['integrat'],['integration']
Deployability,"or driver) (4/4); 17/05/05 06:03:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool ; 17/05/05 06:03:53 INFO DAGScheduler: ResultStage 2 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:202) finished in 10.796 s; 17/05/05 06:03:53 INFO DAGScheduler: Job 1 finished: saveAsNewAPIHadoopFile at ReadsSparkSink.java:202, took 17.010114 s; 17/05/05 06:03:53 INFO SparkUI: Stopped Spark web UI at http://172.30.0.122:35794; 17/05/05 06:03:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/05/05 06:03:53 INFO MemoryStore: MemoryStore cleared; 17/05/05 06:03:53 INFO BlockManager: BlockManager stopped; 17/05/05 06:03:53 INFO BlockManagerMaster: BlockManagerMaster stopped; 17/05/05 06:03:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 17/05/05 06:03:53 INFO SparkContext: Successfully stopped SparkContext; [May 5, 2017 6:03:53 AM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.40 minutes.; Runtime.totalMemory()=799080448; 17/05/05 06:03:53 INFO ApplicationMaster: Final app status: FAILED, exitCode: 16, (reason: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Unregistering ApplicationMaster with FAILED (diag message: Shutdown hook called before final status was reported.); 17/05/05 06:03:53 INFO ApplicationMaster: Deleting staging directory hdfs://ip-172-30-0-86.ec2.internal:8020/user/hadoop/.sparkStaging/application_1493961816416_0002; 17/05/05 06:03:53 INFO ShutdownHookManager: Shutdown hook called; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt1/yarn/usercache/hadoop/appcache/application_1493961816416_0002/spark-23bc1c26-8133-475d-8418-1def58e976fd; 17/05/05 06:03:53 INFO ShutdownHookManager: Deleting directory /mnt/yarn/usercache/hadoop/appcache/application_1493961816416_0002/spark-edb79d30-a7d3-4ed2-ad61-2dc8ea95c5b1; ```. Thanks in helping ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2666:4216,pipeline,pipelines,4216,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2666,1,['pipeline'],['pipelines']
Deployability,"or.onTraversalSuccess(VariantRecalibrator.java:680); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1062); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms100g -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar VariantRecalibrator -V /rprojectnb2/kageproj/gatk/pVCF/chr1/chr1.raw.excessHet.sites.vcf.gz -O snps.recal --tranches-file snps.tranches --trust-all-polymorphic -tranche 100.0 -tranche 99.95 -tranche 99.9 -tranche 99.8 -tranche 99.6 -tranche 99.5 -tranche 99.4 -tranche 99.3 -tranche 99.0 -tranche 98.0 -tranche 97.0 -tranche 90.0 -an AS_QD -an AS_ReadPosRankSum -an AS_MQRankSum -an AS_FS -an AS_MQ -an AS_SOR -an AS_MQ --use-allele-specific-annotations -mode SNP --output-model snps.model --max-gaussians 6 -resource:hapmap,known=false,training=true,truth=true,prior=15 /rprojectnb2/kageproj/gatk/bundle/hapmap_3.3.hg38.vcf.gz -resource:omni,known=false,training=true,truth=true,prior=12 /rprojectnb2/kageproj/gatk/bundle/1000G_omni2.5.hg38.vcf.gz -resource:1000G,known=false,training=true,truth=false,prior=10 /rprojectnb2/kageproj/gatk/bundle/1000G_phase1.snps.high_confidence.hg38.vcf.gz -resource:dbsnp,known=true,training=false,truth=false,prior=7 /rprojectnb2/kag",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7380:10326,install,install,10326,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7380,1,['install'],['install']
Deployability,org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.low_quality_tail_5.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.mismatches_context_size_4.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.quantizing_levels_6.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.HiSeq.1mb.1RG.2k_lines.alternate.recalibrated.DIQ.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.HiSeq.1mb.1RG.2k_lines.bqsr.qq-1.alternate.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.HiSeq.1mb.1RG.2k_lines.bqsr.qq6.alternate.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.alternate_allaligned.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.alternate.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.alternate.recalibrated.DIQ.sharded.bam/part-r-00001.bam; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/human_b36_both.chr1_1k.dict; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/human_b36_both.chr1_1k.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.chr17_69k_70k.dictFix.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.oq.read_consumes_zero_ref_bases.chr20.bam.bai; src/test/resources/org/broadinstitute/hellb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:21165,pipeline,pipeline,21165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['pipeline'],['pipeline']
Deployability,orrentBroadcast.scala:268); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1303); 	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:269); 	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:126); 	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88); 	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); 	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56); 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1411); 	at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:650); 	at org.broadinstitute.hellbender.engine.spark.BroadcastJoinReadsWithVariants.join(BroadcastJoinReadsWithVariants.java:27); 	at org.broadinstitute.hellbender.engine.spark.AddContextDataToReadSpark.add(AddContextDataToReadSpark.java:68); 	at org.broadinstitute.hellbender.tools.spark.pipelines.BQSRPipelineSpark.runTool(BQSRPipelineSpark.java:108); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:131); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:152); 	at org.broadinstitute.hellbender.Main.main(Main.java:230); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegati,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3303:3276,pipeline,pipelines,3276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3303,1,['pipeline'],['pipelines']
Deployability,"ory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for ac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:1137,upgrade,upgraded,1137,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['upgrade'],['upgraded']
Deployability,"ossibility is not ported yet. Regarding alt allele reduction in AF calculator, has [this](https://github.com/broadinstitute/gatk/pull/1918) been ported back to GATK3?. ---. @SHuang-Broad commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260688221). By ""possible place"" I mean they don't always remove alt alleles, just when certain conditions are met, and are independent. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-260781482). I don't think https://github.com/broadinstitute/gatk/pull/1918 has been backported, no. . ---. @SHuang-Broad commented on [Mon Dec 19 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-268016064). Looking more closely, isn't this done already in #1377 ? @vruano ?. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287822737). @SHuang-Broad @vruano Status update on this?. ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287916452). @SHuang-Broad this is not fixed by #1377 as this makes reference to the selection executed by the AFCalculator... it might be that @davidbenjamin now AF calculator addressed the issue, but is also possible that he avoid it it entirely and just focused in the new QUAL calculation. . ---. @vruano commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287921195). Looking at the code I made reference in GATK3, it seem that it is still faulty... I guess we need to take a look on whether in GATK4 has been fixed and then back-ported if people are interested. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1376#issuecomment-287952396). Alright, thanks for the update. At this point we don't care too much about fixing it in GATK3; we're all about m",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:3882,update,update,3882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,1,['update'],['update']
Deployability,"otationPluginDescriptor - Redundant enabled annotation group (StandardHCAnnotation) is enabled for this tool by default ; ; 22:06:39.383 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/home/gvandeweyer/miniconda3/envs/ELPREP/share/gatk4-4.2.5.0-0/gatk-package-4.2.5.0-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; Mar 12, 2022 10:06:39 PM shaded.cloud\_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine ; ; INFO: Failed to detect whether we are running on Google Compute Engine. ; ; 22:06:39.543 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.543 INFO  HaplotypeCaller - The Genome Analysis Toolkit (GATK) v4.2.5.0 ; ; 22:06:39.543 INFO  HaplotypeCaller - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 22:06:39.543 INFO  HaplotypeCaller - Executing as [gvandeweyer@ngsvm-pipelines.uza.be](mailto:gvandeweyer@ngsvm-pipelines.uza.be) on Linux v4.4.0-210-generic amd64 ; ; 22:06:39.543 INFO  HaplotypeCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0\_312-b07 ; ; 22:06:39.544 INFO  HaplotypeCaller - Start Date/Time: March 12, 2022 10:06:39 PM CET ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - ------------------------------------------------------------ ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Version: 2.24.1 ; ; 22:06:39.544 INFO  HaplotypeCaller - Picard Version: 2.25.4 ; ; 22:06:39.544 INFO  HaplotypeCaller - Built for Spark Version: 2.4.5 ; ; 22:06:39.544 INFO  HaplotypeCaller - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 22:06:39.545 INFO  HaplotypeCaller - HTSJDK Defaults.USE\_ASYNC\",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7741:5265,pipeline,pipelines,5265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7741,1,['pipeline'],['pipelines']
Deployability,"ound.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:1728,INSTALL,INSTALLDIRGATK,1728,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5941:2354,configurat,configuration,2354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941,1,['configurat'],['configuration']
Deployability,"own=false,training=true,truth=false,prior=10 ~/db/mutect2_support/b37/Axiom_Exome_Plus.genotypes.all_populations.poly.b37.vcf.gz. 14:58:10.389 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:~/bin/gatk-4.1.9.0/gatk-package-4.1.9.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Nov 12, 2020 2:58:10 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 14:58:10.555 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.555 INFO VariantRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.9.0; 14:58:10.555 INFO VariantRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:58:10.555 INFO VariantRecalibrator - Executing as y@c001 on Linux v3.10.0-957.el7.x86_64 amd64; 14:58:10.555 INFO VariantRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 14:58:10.556 INFO VariantRecalibrator - Start Date/Time: November 12, 2020 2:58:10 PM CST; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - ------------------------------------------------------------; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Version: 2.23.0; 14:58:10.556 INFO VariantRecalibrator - Picard Version: 2.23.3; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:58:10.556 INFO VariantRecalibrator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:58:10.556 INFO VariantRecalibrator - Deflater: IntelDeflater; 14:58:10.556 INFO VariantRecalibrator - Inflater: IntelInflater; 14:58:10.556 INFO VariantRecalibrator - GCS max retries/reopens: 20; 14:5",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6963:3790,release,release-,3790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6963,1,['release'],['release-']
Deployability,"pache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; 11:00:54.078 INFO AbstractConnector - Stopped Spark@2f829853{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}; 11:00:54.091 INFO SparkUI - Stopped Spark web UI at http://172.20.19.130:4040; 11:00:54.122 INFO MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!; 11:00:54.175 INFO MemoryStore - MemoryStore cleared; 11:00:54.175 INFO BlockManager - BlockManager stopped; 11:00:54.193 INFO BlockManagerMaster - BlockManagerMaster stopped; 11:00:54.211 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!; 11:00:54.302 INFO SparkContext - Successfully stopped SparkContext; 11:00:54.303 INFO SortSamSpark - Shutting down engine; [August 11, 2024 at 11:00:54 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.SortSamSpark done. Elapsed time: 27.81 minutes.; Runtime.totalMemory()=1926292832256; org.apache.spark.SparkException: Job aborted.; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106); at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopDataset$1(PairRDDFunctions.scala:1078); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:406); at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1076); at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopFile$2(PairRDDFunctions.scala:995); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:24697,pipeline,pipelines,24697,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['pipeline'],['pipelines']
Deployability,pareLocalResources(Client.scala:422); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:635); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:124); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:523); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:61); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:149); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1389:2563,deploy,deploy,2563,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1389,6,['deploy'],['deploy']
Deployability,pe ConfigurationContainerInternal.; > Cannot create service of type ConfigurationContainerInternal using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createConfigurationContainer() as there is a problem with parameter #13 of type DefaultConfigurationFactory.; > Cannot create service of type DefaultConfigurationFactory using DefaultConfigurationFactory constructor as there is a problem with parameter #2 of type ConfigurationResolver.; > Cannot create service of type ConfigurationResolver using method DefaultDependencyManagementServices$DependencyResolutionScopeServices.createDependencyResolver() as there is a problem with parameter #1 of type ArtifactDependencyResolver.; > Cannot create service of type ArtifactDependencyResolver using method DependencyManagementBuildScopeServices.createArtifactDependencyResolver() as there is a problem with parameter #4 of type List<ResolverProviderFactory>.; > Could not create service of type VersionControlRepositoryConnectionFactory using VersionControlBuildSessionServices.createVersionControlSystemFactory().; > Failed to create parent directory '/home/jdjdj0202/gatk/.gradle' when creating directory '/home/jdjdj0202/gatk/.gradle/vcs-1'. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 754ms. FAILURE: Build failed with an exception. * What went wrong:; Could not update /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin; > /home/jdjdj0202/gatk/.gradle/7.5.1/fileChanges/last-build.bin (No such file or directory). * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org; * ; BUILD FAILED in 761ms; ====================================. How can I build GATK4? . Thanks a lot.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8346:2523,update,update,2523,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8346,1,['update'],['update']
Deployability,pe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:41.557 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.558 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:1948,pipeline,pipeline,1948,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"peCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:4292,release,release,4292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,pick and update changes from tws_ideas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2677:9,update,update,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2677,1,['update'],['update']
Deployability,"piens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; 00:11:11.683 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.697 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.700 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:11:11.700 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:11:11.812 INFO  BaseRecalibrator - ------------------------------------------------------------ ; ; 00:11:11.813 INFO  BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.2.6.1 ; ; 00:11:11.813 INFO  BaseRecalibrator - For support and documentation go to [https://software.broadinstitute.org/gatk/](https://software.broadinstitute.org/gatk/) ; ; 00:11:11.813 INFO  BaseRecalibrator - Executing as xieduo@pbs-master on Linux v3.10.0-1160.41.1.el7.x86\_64 amd64 ; ; 00:11:11.813 INFO  BaseRecalibrator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v18+36-2087 ; ; 00:11:11.813 INFO  BaseRecalibrator - Start Date/Time: August 21, 2022 at 12:11:11 AM CST ; ; 00:11:11.813 INFO  BaseRecalibrator - --------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:9531,pipeline,pipeline,9531,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,pin hail version in the integration test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8424:24,integrat,integration,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8424,1,['integrat'],['integration']
Deployability,"piped beta variables through to high-level beta workflow.; Also updated the gatk jar so it succeeds, as it didn't before. [Successful run of beta workflow on quickstart data](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Tiny%20Quickstart%20hatcher/job_history/e9a1af96-8c1a-463a-8063-ae455d0ba6b3)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8200:64,update,updated,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8200,1,['update'],['updated']
Deployability,"plotypes in the sample contains the site-specific alternate allele at the site (ie. excluding `*` which represents variation that beings upstream of the current variant. NB that this results in cases where `PGT` is not the same as the phased `GT` field. For example, in the case of a spanned SNP site with REF allele `A` and alt alleles `C` and `*`, `GT` may be set to `1|2` to represent the spanned SNP, while PGT would be set to `1|0` to represent the fact that it is the first haplotype in the pair of phased haplotypes that contains the site-specific alt allele (in this case `C`). If reviewers agree with this interpretation, I think we should create a new ticket to clarify documentation around the PGT and PID tags to reflect it. . After discussions with @ldgauthier I believe that there may be downstream issues in preserving phasing after passing gVCFs through CombineGVCFs, GenomicsDBImport, and/or GenotypeGVCFs, especially if the gVCFs are emitted without GT fields. In that case, `GenotypeGVCFs` should probably have logic to reconstruct the phased genotype for each sample based on the PGT and PID tags when possible. I will create a new ticket describing the issue. There still may be cases where HaplotypeCaller does not emit phasing information for spanning deletions due to the presence of extra haplotypes that contradict diploid phasing, as in https://github.com/broadinstitute/gatk/issues/6845. A fix to that issue would likely reduce the number of those cases. The integration test result file `src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/withOxoGReadCounts.vcf` does not have any changes that have to do with this PR -- it was automatically updated by GenotypeGVCFsIntegrationTest, which included some new jitter in QUAL scores as described in https://github.com/broadinstitute/gatk/pull/6859, but never got checked in with that PR. I figure that it's best to update it now so that the results reflect the current expected behavior of the tool.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6937:1856,integrat,integration,1856,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6937,3,"['integrat', 'update']","['integration', 'update', 'updated']"
Deployability,"previously CommandLineParser would accept non-ambiguous abbreviations for longopts; this caused issues where -v would have different meanings in different tools. I patched joptSimple to add an option to disallow abbreviations, and updated to 5.0-beta-1 which incorporates the necessary changes.; fixes #1347",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1549:164,patch,patched,164,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1549,2,"['patch', 'update']","['patched', 'updated']"
Deployability,"previously avx code was sometimes included if installDist had been run prior to running sparkJar, now sparkJar will always contain the native code; fixes #1576. also changing download of inteldeflator.so to only happen if it doesn't exist already",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1681:46,install,installDist,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1681,1,['install'],['installDist']
Deployability,ps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:107); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:980); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /gatk/build/libs/gatk-package-4.0.5.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5g -Xms5g -jar /gatk/build/libs/gatk-package-4.0.5.0-local.jar GenotypeGVCFs -R /cromwell-executions/JointGenotyping/3a733624-60a0-4e57-b7f5-7b7e99795738/call-GenotypeGVCFs/shard-25/inputs/data1/bill/pipeline/cromwell/hg38/Homo_sapiens_assembly38.fasta -O output.vcf.gz -D /cromwell-executions/JointGenotyping/3a733624-60a0-4e57-b7f5-7b7e99795738/call-GenotypeGVCFs/shard-25/inputs/data1/bill/pipeline/cromwell/hg38/Homo_sapiens_assembly38.dbsnp138.vcf -G StandardAnnotation --only-output-calls-starting-in-intervals --use-new-qual-calculator -V gendb://genomicsdb -L chrY:10821870-11717714; ```; This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/12283/running-260-wes-samples-through-joint-discovery-and-vqsr/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4975:4477,pipeline,pipeline,4477,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4975,2,['pipeline'],['pipeline']
Deployability,"ps://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L177); [gcnvkernel metadata.py SampleMetadataCollection class](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/structs/metadata.py#L215); [gcnvkernel model_denoising_calling.py](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/models/model_denoising_calling.py); [gcnvkernel io_metadata.py write_sample_coverage_metadata function](https://github.com/broadinstitute/gatk/blob/4e1741896bcd04d70493f94b082dd0d27023f14c/src/main/python/org/broadinstitute/hellbender/gcnvkernel/io/io_metadata.py#L16); [theano scan_op.py](https://github.com/Theano/Theano/blob/master/theano/scan_module/scan_op.py). ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm getting a strange error (see below) when running a nf-core module test. I am using test files, which are obviously smaller as for short testing times i.e. the provided bam file only provides mapped reads for a small section of the genome. #### Steps to reproduce; Run the following to create and interactive container and mount the required zip folder ([gatk_test.tar.gz](https://github.com/broadinstitute/gatk/files/10022295/gatk_test.tar.gz)):; ```docker run -it -v /path/to/gatk_test_dir:/mnt/gatk_test broadinstitute/gatk bash```; If you bash the `gatk_germlinecnvcaller.sh` within the provided zip folder in a gatk4 Docker container. #### Expected behavior; gatk GermlineCNVCaller should run as expected. #### Actual behavior; ```TypeError: ('The following error happened while compiling the node', forall_inplace,cpu,scan_fn}(Elemwise{Maximum}[(0, 0)].0, Subtensor{int64:int64:int8}.0, Subtensor{int64:int64:in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8097:1093,release,release,1093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8097,1,['release'],['release']
Deployability,publishing a master-snapshot update on every push to master,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2403:29,update,update,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2403,1,['update'],['update']
Deployability,quick pr to update the beta workflow to use the latest ploidy changes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8349:12,update,update,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8349,1,['update'],['update']
Deployability,"quified samples. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260475512). @ldgauthier Will this tool be ported to GATK4? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260637796). ¯_(ツ)_/¯. I wasn't going to port it myself. It's not under active development, but GTEx used it a little in the past. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260713724). Hmm. Who would be the right person to ask whether GTex would need this ported? . ---. @ldgauthier commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260739166). Last I checked, Xiao Li was using the tool for the work he was doing with Ayellet Segre. ---. @vdauwera commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-260778577). Thanks, I emailed them to ask about their use of the tool. . ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1208#issuecomment-287823398). Response from Xiao Li:. > The “CombineSampleData” tool is initially developed by Laura to perform integrated variant calling when we have both WES and WGS data for same individuals. Use GTEx release v6 data, we have found that it helps generating better genotype calls and improves calls from older technologies (e.g.: HiSeq2000 vs. HiSeqX, Agilent vs. ICE). In GTEx, all samples will be genotyped with both WGS and WES, and because of this, in our final release next year, we want to use this tool to generate a call set that integrates WGS and WES. Prior to this, we plan to publish this method that we could cite it in the final release paper. I will expect this method very useful for other big consortiums where both WGS and WES are available for same samples. . > Hope you could keep it in GATK.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2485:4110,integrat,integrated,4110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2485,5,"['integrat', 'release']","['integrated', 'integrates', 'release']"
Deployability,"r (MappingQualityAvailableReadFilter) is not enabled by this tool. 07:33:15.003 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/nobackupp16/swbuild/hsp/COVID19/anaconda3/envs/COVIRT_GATK/share/gatk4-4.1.7.0-0/gatk-package-4.1.7.0-local.jar!/com/intel/gkl/native/libgkl_compression.so. Sep 20, 2020 7:33:15 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine. INFO: Failed to detect whether we are running on Google Compute Engine. 07:33:15.360 INFO FilterMutectCalls - ------------------------------------------------------------. 07:33:15.361 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.7.0. 07:33:15.361 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/. 07:33:15.361 INFO FilterMutectCalls - Executing as lnsingh@pfe26 on Linux v4.12.14-122.23.1.20200609-nasa amd64. 07:33:15.361 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12. 07:33:15.361 INFO FilterMutectCalls - Start Date/Time: September 20, 2020 7:33:14 AM PDT. 07:33:15.361 INFO FilterMutectCalls - ------------------------------------------------------------. 07:33:15.361 INFO FilterMutectCalls - ------------------------------------------------------------. 07:33:15.362 INFO FilterMutectCalls - HTSJDK Version: 2.21.2. 07:33:15.362 INFO FilterMutectCalls - Picard Version: 2.21.9. 07:33:15.362 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2. 07:33:15.362 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false. 07:33:15.362 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true. 07:33:15.362 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false. 07:33:15.362 INFO FilterMutectCalls - Deflater: IntelDeflater. 07:33:15.362 INFO FilterMutectCalls - Inflater: IntelInflater. 07:33:15.363 INFO FilterMutectCalls - GCS max retries/reopens: 20. 07:33:15.363 INFO Filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6850:2519,release,release-,2519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6850,1,['release'],['release-']
Deployability,"r i post this kind of question elsewhere, please let me know. My lab creates a large dataset of macaque variant data. We regularly add new samples to a dataset that currently has ~2300 WGS/WXS datasets. We largely follow the GATK short variant calling pipeline. Our gVCF data are aggregated into a GenomicsDb workspace, followed by GenotypeGVCFs. As is, whenever we get new samples, we append them to this growing GenomicsDb workspace, and then re-call all of the genotypes. These steps are getting slower and slower (even when scatter/gathered on a cluster), and I'm concerned it's going to become untenable. Plus it's just really inefficient to constantly re-call 1000s of datasets at 40m genome-wide sites. My question is: do you have any experience with analogous datasets, where you have a large base of ""static"" datasets with regular additions of new data? It would be quite nice to avoid constantly re-genotyping the existing datasets. We could in theory just run GenotypeGVCFs on the incoming data and do a simple merge with the existing data. Are you aware of anyone running a process that looks more like this?. There are some caveats to this: 1) for the incoming batches of data, we could run GenotypeGVCF where we force it to call genotypes from every site that exists in the current dataset. This would promote consistent calling across a common set of sites, 2) after we genotype the incoming batch, we could compare the sites present in that against the sites in the current data. It's likely there would be a handful of novel sites. We could re-run GenotypeGVCFs on the existing data specifically on those new sites (presumably the existing animals are largely WT at those positions), and merge those new sites with the existing data, 3) we then merge the incoming data with the updated core data, which should each have genotypes called at the identical set of sites. Are there any discussions happening about managing/updating large variant datasets like this? Thanks for any ideas.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7526:1809,update,updated,1809,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7526,1,['update'],['updated']
Deployability,r the second pass.; 44	GetPileupSummaries	beta; helper tool for CalculateContamination	6/5/2017	https://github.com/broadinstitute/gatk-protected/blob/2bf35790393332da5414b42ec6dca813fcc63202/src/main/java/org/broadinstitute/hellbender/tools/walkers/contamination/GetPileupSummaries.java	scripts/mutect2_wdl/mutect2.wdl	https://github.com/broadinstitute/gatk/pull/3006	yes	; 33	AnnotateVcfWithBamDepth	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithBamDepth.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 34	AnnotateVcfWithExpectedAlleleFraction	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/AnnotateVcfWithExpectedAlleleFraction.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 37	CalculateMixingFractions	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/CalculateMixingFractions.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes	; 47	RemoveNearbyIndels	internal (DB)	5/30	https://github.com/broadinstitute/gatk-protected/blob/e6278def94038d76339d0fd95ce2badb3bc44a22/src/main/java/org/broadinstitute/hellbender/tools/walkers/validation/RemoveNearbyIndels.java	scripts/mutect2_wdl/unsupported/hapmap_sensitivity_truth.wdl	https://github.com/broadinstitute/gatk-protected/pull/1131	yes		; ```. This is also at <https://docs.google.com/a/broadinstitute.org/spreadsheets/d/15xviLwYUjU82MtYwxxPINiJAkovmaHpRGhqkghiEATQ/edit?usp=sharing>.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3055:14025,a/b,a/broadinstitute,14025,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3055,1,['a/b'],['a/broadinstitute']
Deployability,"r to the new contamination tool, about 3 weeks. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286453446). I would like us to consider this part of the somatic short variants discovery pipeline (which is probably what you mean by Mutect wdl). I'm not sure how urgently we need it, probably ""not very urgent in the scheme of things, but should be done eventually"". I assume @LeeTL1220 has a better idea of what we need when on the somatic side of things. . And you're right that this creates a loss of sensitivity, not false positives. I'm being distracted by a tiny human. . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286457270). We should do it. Generally, I agree with Geraldine's statement about; urgency, though. We can meet to discuss prioritization relative to other; auxiliary tools. On Tue, Mar 14, 2017 at 11:15 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > I would like us to consider this part of the somatic short variants; > discovery pipeline (which is probably what you mean by Mutect wdl). I'm not; > sure how urgently we need it, probably ""not very urgent in the scheme of; > things, but should be done eventually"". I assume @LeeTL1220; > <https://github.com/LeeTL1220> has a better idea of what we need when on; > the somatic side of things.; >; > And you're right that this creates a loss of sensitivity, not false; > positives. I'm being distracted by a tiny human.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286453446>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk6kK5A_Mhh9S3s4xX_EEZQlwtKvGks5rlq8hgaJpZM4K3d-Q>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2919:2441,pipeline,pipeline,2441,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2919,1,['pipeline'],['pipeline']
Deployability,"rImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at java.io.ObjectStreamClass.invokeReadResolve(ObjectStreamClass.java:1104); ... 31 more; Caused by: java.lang.IllegalAccessError: no such method: org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(GATKRead)FlagStatus/invokeVirtual; at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:448); at org.broadinstitute.hellbender.tools.spark.pipelines.FlagStatSpark.$deserializeLambda$(FlagStatSpark.java:20); ... 40 more; Caused by: java.lang.LinkageError: loader constraint violation: when resolving method ""org.broadinstitute.hellbender.tools.FlagStat$FlagStatus.add(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/tools/FlagStat$FlagStatus;"" the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) of the current class, org/broadinstitute/hellbender/tools/spark/pipelines/FlagStatSpark, and the class loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) for the method's defining class, org/broadinstitute/hellbender/tools/FlagStat$FlagStatus, have different Class objects for the type org/broadinstitute/hellbender/utils/read/GATKRead used in the signature; at java.lang.invoke.MethodHandleNatives.resolve(Native Method); at java.lang.invoke.MemberName$Factory.resolve(MemberName.java:965); at java.lang.invoke.MemberName$Factory.resolveOrFail(MemberName.java:990); at java.lang.invoke.MethodHandles$Lookup.resolveOrFail(MethodHandles.java:1387); at java.lang.invoke.MethodHandles$Lookup.linkMethodHandleConstant(MethodHandles.java:1739); at java.lang.invoke.MethodHandleNatives.linkMethodHandleConstant(MethodHandleNatives.java:442); ... 41 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1315:4225,pipeline,pipelines,4225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1315,1,['pipeline'],['pipelines']
Deployability,"ra commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it improves calls once filtered with VQSR. ---. @vdauwera commented on [Mon Mar 20 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-287822174). Is there a PR associated with this issue? Will there be a new feature to release? Need to know for release scheduling purposes.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2519:2596,release,release,2596,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519,2,['release'],['release']
Deployability,"ract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - add fields for uncompressed imputed data; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - Tool for arrays QC metrics calculations (#6812); - ah update array extract tool (#6827); - fix enum (#6834); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:6628,update,update,6628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['update'],['update']
Deployability,radle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 11:54:40.435 [ERROR] [org.gradle.internal.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8337,Continuous,ContinuousBuildActionExecuter,8337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,radle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7485,Continuous,ContinuousBuildActionExecuter,7485,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,ram.doWork(SparkCommandLineProgram.java:30; ); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.jav; a:179); at; org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at; org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:152); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:195); at org.broadinstitute.hellbender.Main.main(Main.java:275); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at; sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at; sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at; org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at; org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:928); at; org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203); at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90); at; org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.ClassNotFoundException: scala.Product$class; at java.lang.ClassLoader.findClass(ClassLoader.java:523); at; org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35); at java.lang.ClassLoader.loadClass(ClassLoader.java:418); at; org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40); at; org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:48); at java.lang.ClassLoader.loadClass(ClassLoader.java:351); ... 55 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6644:4969,deploy,deploy,4969,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6644,7,['deploy'],['deploy']
Deployability,ram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:23366,deploy,deploy,23366,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,ram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2298,deploy,deploy,2298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,"ram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4628,deploy,deploy,4628,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,ram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1436); 	at org.broadinstitute.hellbender.utils.spark.Spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:2788,deploy,deploy,2788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"ram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsde-dev/reference/Homo_sapiens_assembly38.fasta) does not exist. *********",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:7524,deploy,deploy,7524,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,ram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:23294,deploy,deploy,23294,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,ram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContex,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2226,deploy,deploy,2226,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,"ram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:150); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:82); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4556,deploy,deploy,4556,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,ram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); 	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1448); 	at org.apache.hadoop.fs.FileSystem.exists(Fi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:2716,deploy,deploy,2716,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"ram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also provided with a GCS path, we see this:. ```; ***********************************************************************. A USER ERROR has occurred: The specified fasta file (gs://sv-data-dsd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:7452,deploy,deploy,7452,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"ranscript as one linked to a CCDS record; - seleno: Flags this transcript has a Selenocysteine edit. Look for the Selenocysteine; feature for the position of this on the genome; - cds_end_NF: the coding region end could not be confirmed; - cds_start_NF: the coding region start could not be confirmed; - mRNA_end_NF: the mRNA end could not be confirmed; - mRNA_start_NF: the mRNA start could not be confirmed.; - basic: the transcript is part of the gencode basic geneset. Comments. Lines may be commented out by the addition of a single # character at the start. These; lines should be ignored by your parser. Pragmas/Metadata. GTF files can contain meta-data. In the case of experimental meta-data these are ; noted by a #!. Those which are stable are noted by a ##. Meta data is a single key,; a space and then the value. Current meta data keys are:. * genome-build - Build identifier of the assembly e.g. GRCh37.p11; * genome-version - Version of this assembly e.g. GRCh37; * genome-date - The date of this assembly's release e.g. 2009-02; * genome-build-accession - The accession and source of this accession e.g. NCBI:GCA_000001405.14; * genebuild-last-updated - The date of the last genebuild update e.g. 2013-09. ------------------; Example GTF output; ------------------. #!genome-build GRCh38; 11 ensembl_havana gene 5422111 5423206 . + . gene_id ""ENSG00000167360""; gene_version ""4""; gene_name ""OR51Q1""; gene_source ""ensembl_havana""; gene_biotype ""protein_coding"";; 11 ensembl_havana transcript 5422111 5423206 . + . gene_id ""ENSG00000167360""; gene_version ""4""; transcript_id ""ENST00000300778""; transcript_version ""4""; gene_name ""OR51Q1""; gene_source ""ensembl_havana""; gene_biotype ""protein_coding""; transcript_name ""OR51Q1-001""; transcript_source ""ensembl_havana""; transcript_biotype ""protein_coding""; tag ""CCDS""; ccds_id ""CCDS31381"";; 11 ensembl_havana exon 5422111 5423206 . + . gene_id ""ENSG00000167360""; gene_version ""4""; transcript_id ""ENST00000300778""; transcript_version ""4""; exon_nu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6488:6071,release,release,6071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6488,1,['release'],['release']
Deployability,"rated by GvsExtractCallset. . It ignores basically everything except genotypes, because PGENs do not store all the other fields and annotations that the VCFs might have. It will also skip over any sites in the VCFs with >254 alleles because those will not be present in the PGEN files. Any differences are written to diff files, in the form of the differing lines in the VCFs being compared. The code for this comparison tool lives [here](https://github.com/KevinCLydon/pgen_vcf_comparator) in a repo I created under my GitHub account. (I didn't create it under the Broad org because it's sort of half-baked and bad and not actually meant to be used by anyone other than me.) I don't know if y'all want to continue using this tool, but I'm happy to discuss it more if it would actually be useful to you. ## To-dos / caveats. ### PGEN-JNI; The version of PGEN-JNI I'm referencing in the current build.gradle file is a beta version that is hosted on artifactory. Functionally, this is totally fine, but we want to get a 1.0 version of it hosted publicly. Chris Norman, who developed the tool is currently very working on this and very close to done. Once he's completed this, I want to run a sanity test or two against a small subset of the Delta callset just to make sure everything is functioning as intended. ### Merging by chromosome arm; Right now, the last step of the PGEN extract workflow merges the PGEN files by contig name, so the final result is one trio of files (.pgen, .psam, and .pvar.zst) per chromosome. There was discussion about changing this to merge instead by chromosome arm. I want to make this change, but it's not super simple, so I've prioritized getting this version of the code ready for merging before tackling that. ### The PGEN format; As I mentioned above, Plink 2.0 and the PGEN file format are still not in full release, so the format could be subject to change in the future, which will require updates to our PGEN writing code and could possibly introduce problems.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8708:13877,release,release,13877,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8708,2,"['release', 'update']","['release', 'updates']"
Deployability,"rather than the default boot HDD.; Thanks to @mwalker174 for discovering this. In terms of runtime, this didn't change runtime for the SV pipeline much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3076:138,pipeline,pipeline,138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3076,1,['pipeline'],['pipeline']
Deployability,"rayList;; import java.util.List;; import java.util.ServiceLoader;. @CommandLineProgramProperties(summary = ""test"", oneLineSummary = ""testthing"", programGroup = SparkProgramGroup.class); public class TestGCS extends GATKSparkTool {; private static final long serialVersionUID = 1L;. @Override; protected void runTool(JavaSparkContext ctx) {; try {; modifyProviders();; } catch (IllegalAccessException | NoSuchFieldException e) {; throw new RuntimeException(""Couldn't reset FilesystemProviders"");; }; try {; final Path index = Paths.get(new URI(""gs://hellbender/test/build_reports/1626.1/tests/index.html""));; System.out.println(""Count:"" + Files.lines(index).count());; } catch (URISyntaxException | IOException e) {; throw new RuntimeException(""Couldn't read file"");; }; }; }. private void modifyProviders() throws IllegalAccessException, NoSuchFieldException {; final Field installedProviders = FileSystemProvider.class.getDeclaredField(""installedProviders"");; installedProviders.setAccessible(true);; installedProviders.set(null, loadInstalledProviders());; installedProviders.setAccessible(false);; }. //copied from FileSystemProvider, modified to use TestGCS.classLoader() instead of systemClassloader; private static List<FileSystemProvider> loadInstalledProviders() {; List<FileSystemProvider> list = new ArrayList<FileSystemProvider>();. ServiceLoader<FileSystemProvider> sl = ServiceLoader; .load(FileSystemProvider.class, TestGCS.class.getClassLoader());. // ServiceConfigurationError may be throw here; for (FileSystemProvider provider: sl) {; String scheme = provider.getScheme();. // add to list if the provider is not ""file"" and isn't a duplicate; if (!scheme.equalsIgnoreCase(""file"")) {; boolean found = false;; for (FileSystemProvider p: list) {; if (p.getScheme().equalsIgnoreCase(scheme)) {; found = true;; break;; }; }; if (!found) {; list.add(provider);; }; }; }; return list;; }; }; ```. We'd have to add an initial action to GATKSparkTool that would run `modifyProviders` once on e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2312:1941,install,installedProviders,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2312,1,['install'],['installedProviders']
Deployability,"re; Using GATK jar /gatk/gatk-package-4.5.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.5.0.0-local.jar CombineGVCFs -R ./test/test.fna -V ./gvcf_all.list -L NC_038255.2 -O ./NC_038255.2.merged.g.vcf.gz; total 2.3G; '""-rw-rw-rw- 1 root root  3.6K Dec 13 23:32 GATKConfig.EXAMPLE.properties""; drwxr-xr-x 2 root root  4.0K Mar 13 06:26 GCF_000004515.6_Glycine_max_v4.0; '""-rw-r--r-- 1 root root  1.6G Mar 13 06:47 NC_038255.2.merged.g.vcf.gz""; '""-rw-r--r-- 1 root root   24K Mar 13 06:47 NC_038255.2.merged.g.vcf.gz.tbi""; '""-rw-rw-rw- 1 root root   40K Dec 13 23:32 README.md""; '""-rwxrwxrwx 1 root root   21K Dec 13 23:32 gatk""; '""-rw-rw-rw- 1 root root 1016K Dec 13 23:32 gatk-completion.sh""; '""-rw-rw-rw- 1 root root  422M Dec 13 23:32 gatk-package-4.5.0.0-local.jar""; '""-rw-rw-rw- 1 root root  320M Dec 13 23:32 gatk-package-4.5.0.0-spark.jar""; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk-spark.jar -> /gatk/gatk-package-4.5.0.0-spark.jar; lrwxrwxrwx 1 root root    36 Dec 13 23:33 gatk.jar -> /gatk/gatk-package-4.5.0.0-local.jar; '""-rw-rw-rw- 1 root root  117K Dec 13 23:32 gatkPythonPackageArchive.zip""; '""-rw-rw-rw- 1 root root  4.2K Dec 13 23:32 gatkcondaenv.yml""; '""-rw-r--r-- 1 root root    53 Dec 13 23:37 gatkenv.rc""; '""-rw-r--r-- 1 root root  2.3K Mar 13 06:26 gvcf_all.list""; '""-rw-r--r-- 1 root root   866 Dec 13 23:33 run_unit_tests.sh""; drwxrwxrwx 5 root root  4.0K Dec 13 23:32 scripts; '""-rw-r--r-- 1 root root  1.3K Mar 13 06:26 wgs_jcalling_combine_gvcf_job.sh""; Filesystem      Size  Used Avail Use% Mounted on; overlay         100G   13G   88G  13% /; tmpfs            64M     0   64M   0% /dev; tmpfs           7.7G     0  7.7G   0% /sys/fs/cgroup; /dev/nvme0n1p1  100G   13G   88G  13% /etc/hosts; shm              64M     0   64M   0% /dev/shm; wgs-pipeline    1.0P     0  1.0P   0% /mnt. Thank you very much",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8735:28511,pipeline,pipeline,28511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8735,1,['pipeline'],['pipeline']
Deployability,read FASTQ files directly in spark pipelines,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4086:35,pipeline,pipelines,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4086,1,['pipeline'],['pipelines']
Deployability,"read api bytes logging, upgrade bigquery client versions",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7601:24,upgrade,upgrade,24,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7601,1,['upgrade'],['upgrade']
Deployability,read name mangling should use `replaceAll` instead of `replace`. Discovered in the process of creating test data for more comprehensive SV integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5107:139,integrat,integration,139,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5107,1,['integrat'],['integration']
Deployability,reads pipeline tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1197:6,pipeline,pipeline,6,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1197,1,['pipeline'],['pipeline']
Deployability,"rect.github.com/protocolbuffers/protobuf/issues/18375"">#18375</a>)</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/8a60b6527a976cfd0028153da3ad8e4ed280e0de""><code>8a60b65</code></a> Merge pull request <a href=""https://redirect.github.com/protocolbuffers/protobuf/issues/17704"">#17704</a> from protocolbuffers/cp-segv</li>; <li><a href=""https://github.com/protocolbuffers/protobuf/commit/94a26630e362a4771b5ec80eac49f494988ca408""><code>94a2663</code></a> Fixed a SEGV when deep copying a non-reified sub-message.</li>; <li>Additional commits viewable in <a href=""https://github.com/protocolbuffers/protobuf/compare/v3.23.4...v3.25.5"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.google.protobuf:protobuf-java&package-manager=gradle&previous-version=3.23.4&new-version=3.25.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by c",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9004:2763,update,updates,2763,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9004,1,['update'],['updates']
Deployability,release alpha build to maven central,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1188:0,release,release,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1188,1,['release'],['release']
Deployability,"removing redundant builds:; we will now have:; openJDK builds for cloud, integration, and unit tests; docker builds for integration and unit tests; an oracleJDK build for integration tests",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2770:73,integrat,integration,73,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2770,3,['integrat'],['integration']
Deployability,removing required FASTA reference input in SV discovery pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3673:56,pipeline,pipeline,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3673,1,['pipeline'],['pipeline']
Deployability,removing the non-docker unit and integration test matrix entries because; they were redundant with the docker ones,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2804:33,integrat,integration,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2804,1,['integrat'],['integration']
Deployability,rename integration tests to use IntegrationTest in name,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/191:7,integrat,integration,7,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/191,2,"['Integrat', 'integrat']","['IntegrationTest', 'integration']"
Deployability,replace calls to getChr() with getContig() after htsjdk update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/259:56,update,update,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/259,1,['update'],['update']
Deployability,"res (#7735); - Add withdrawn and is_control columns [VS-70] [VS-213] (#7736); - Allow interval lists that require the SA to see (#7743); - allow for gatk to be overridden, update with known good jar (#7758); - VS-361 Add GvsWithdrawSamples wdl (#7765); - Extract Performance Improvements (#7686); - Don't put withdrawn sample data in alt_allele table [VS-369] (#7762); - remove PET code (#7768); - Adding AD for scale testing VS 225 add AD (#7713); - Deterministic Sample ID assignments [VS-371] (#7770); - remove R scripts from filtering (#7781); - Remove an old ""temp table"" dataset (#7780); - Clean up LocalizeFile [VS-314] (#7771); - Remove pet code from CreateVariantIngestFiles and friends [VS-375] (#7773); - 317 remove excess header values in VCF extract (#7786); - correct auth in split intervals (#7790); - Add code to (optionally) zero pad the vcf filename. (#7783); - LoadData `maxRetries` parameterized, default increased [VS-383] (#7791); - Update to latest version of ah_var_store gatk override jar (#7793); - GvsUnified WDL to wrap the 6 core GVS WDLs [VS-382] (#7789); - Pinned typing_extensions python package to 4.1.1 to fix conda environment. (#7802); - WeightedSplitInterval fixes [VS-384] [VS-332] (#7795); - Replace Travis with GithubActions (#7754); - Docker build only lfs pulls main/src/resources/large (#7727); - Clean up gatk jars -- looks like we are not passing them properly in the extract (#7788); - Fix typo that broke git lfs pull (#7806); - Document AoU SOP (up to the VAT) [VS-63] (#7807); - Incident VS 365 clinvar classification fix (#7769); - VS-390. Add precision and sensitivity wdl (#7813); - Quickstart based integration test [VS-357] (#7812); - 365 vat python testing additions (#7756); - VS 396 clinvar grabs too many values (#7823); - Added a test to validate WDLs in the scripts directory. (#7826) (#7829); - VAT Performance / Reliability Improvements (#7828); - VAT naming conventions [VS-410] (#7827); - Rc remove ad from vat (#7832); - bugfix, we were",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:23100,Update,Update,23100,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Update'],['Update']
Deployability,"res-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5802:1606,Install,Installers,1606,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802,1,['Install'],['Installers']
Deployability,resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.bam -O output -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnCli,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1048,deploy,deploy,1048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"ressMeter - 1:192716273 0.6 21000 33893.7; 16:57:19.009 INFO ProgressMeter - 2:24820506 0.8 31000 38957.0; 16:57:29.031 INFO ProgressMeter - 2:94856959 1.0 44000 45700.0; 16:57:39.223 INFO ProgressMeter - 2:136329636 1.1 59000 52089.5; 16:57:49.667 INFO ProgressMeter - 2:233747942 1.3 65000 49742.4; 16:58:01.608 INFO ProgressMeter - 3:57654674 1.5 71000 47152.6; 16:58:12.449 INFO ProgressMeter - 3:179974096 1.7 84000 49809.3; 16:58:23.282 INFO ProgressMeter - 4:82276408 1.9 98000 52491.1; 16:58:33.462 INFO ProgressMeter - 5:20304602 2.0 106000 52046.3; 16:58:44.217 INFO ProgressMeter - 5:141241407 2.2 114000 51446.4; 16:58:54.298 INFO ProgressMeter - 6:28447738 2.4 122000 51176.3; 16:59:03.028 INFO CollectReadCounts - Shutting down engine; [October 6, 2020 at 4:59:03 PM EDT] org.broadinstitute.hellbender.tools.copynumber.CollectReadCounts done. Elapsed time: 2.55 minutes.; Runtime.totalMemory()=981467136; java.lang.ArrayIndexOutOfBoundsException; at java.base/java.util.zip.CRC32.update(CRC32.java:76); at htsjdk.samtools.cram.io.CRC32InputStream.read(CRC32InputStream.java:54); at htsjdk.samtools.cram.io.InputStreamUtils.readFully(InputStreamUtils.java:75); at htsjdk.samtools.cram.structure.block.Block.read(Block.java:283); at htsjdk.samtools.cram.structure.SliceBlocks.<init>(SliceBlocks.java:75); at htsjdk.samtools.cram.structure.Slice.<init>(Slice.java:155); at htsjdk.samtools.cram.structure.Container.<init>(Container.java:154); at htsjdk.samtools.cram.build.CramSpanContainerIterator$Boundary.next(CramSpanContainerIterator.java:97); at htsjdk.samtools.cram.build.CramSpanContainerIterator.next(CramSpanContainerIterator.java:57); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:97); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:204); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.getNextRecord(CRAMFileReader.java:527); at htsjdk.samtools.CRAMFileReader$CRAMIntervalIteratorBase.next(CRAMFileReader.java:521); at htsjdk.samtools.CRAM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6865:4323,update,update,4323,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6865,1,['update'],['update']
Deployability,rg.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:61); 	at org.apache.spark.api.java.JavaPairRDD.sortByKey(JavaPairRDD.scala:936); 	at org.broadinstitute.hellbender.utils.spark.SparkUtils.sortUsingElementsAsKeys(SparkUtils.java:164); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.sortSamRecordsToMatchHeader(ReadsSparkSink.java:382); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:289); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:206); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:307); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:295); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:461); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMet,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:18227,pipeline,pipelines,18227,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['pipeline'],['pipelines']
Deployability,rg.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.api.internal.artifacts.ivyservice.DefaultCacheLockingManager.close(DefaultCacheLockingManager.java:48); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$ManagedObjectProvider.stop(DefaultServiceRegistry.java:552); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$OwnServices.stop(DefaultServiceRegistry.java:519); at org.gradle.internal.concurrent.CompositeStoppable.stop(Compo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:2962,release,release,2962,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,2,['release'],['release']
Deployability,rg.gradle.cache.internal.DefaultFileLockManager$DefaultFileLock.writeFile(DefaultFileLockManager.java:163); at org.gradle.cache.internal.DefaultCacheAccess$UnitOfWorkFileAccess.writeFile(DefaultCacheAccess.java:404); at org.gradle.cache.internal.DefaultMultiProcessSafePersistentIndexedCache.close(DefaultMultiProcessSafePersistentIndexedCache.java:76); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.cache.internal.DefaultCacheAccess.closeFileLock(DefaultCacheAccess.java:136); at org.gradle.cache.internal.DefaultCacheAccess.close(DefaultCacheAccess.java:162); at org.gradle.cache.internal.DefaultPersistentDirectoryStore.close(DefaultPersistentDirectoryStore.java:77); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.close(DefaultCacheFactory.java:141); at org.gradle.cache.internal.DefaultCacheFactory$DirCacheReference.release(DefaultCacheFactory.java:130); at org.gradle.cache.internal.DefaultCacheFactory$ReferenceTrackingCache.close(DefaultCacheFactory.java:159); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.plugin.use.resolve.service.internal.PersistentCachingPluginResolutionServiceClient.close(PersistentCachingPluginResolutionServiceClient.java:124); at org.gradle.plugin.use.resolve.service.internal.InMemoryCachingPluginResolutionServiceClient.close(InMemoryCachingPluginResolutionServiceClient.java:87); at org.gradle.plugin.use.resolve.service.internal.DeprecationListeningPluginResolutionServiceClient.close(DeprecationListeningPluginResolutionServiceClient.java:82); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:16629,release,release,16629,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,1,['release'],['release']
Deployability,rg/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/haploid-multisample.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/selectVariantsInfoField.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/test.dup.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/tetra-diploid.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/tetraploid-multisample-sac.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/tetraploid-multisample.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/vcfexample2DiscordanceConcordance.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/SelectVariants/vcfexample2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/UpdateVCFSequenceDictionary/exampleBAM.sam; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/UpdateVCFSequenceDictionary/exampleFASTA.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/expected.soap_gatk_annotated.AMD.table; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/multiallelic_gt.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/multiallelic.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/soap_gatk_annotated.noChr_lines.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/soap_gatk_annotated.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/VariantsToTable/vcfexample2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/variantutils/V,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:65564,Update,UpdateVCFSequenceDictionary,65564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,"right now you get this which is bogus on many levels (duplicated and confusing categories, confusing tool names etc). We need to put more order into this. @vdauwera can you help come up with a better scheme of how to organize tools?; Compare to the ADAM project (much much smaller scope of course but very clean UI: https://github.com/bigdatagenomics/adam). ```; /gatk-launch --list; Running:; /Users/akiezun/IdeaProjects/gatk/build/install/gatk/bin/gatk --help; USAGE: <program name> [-h]. Available Programs:; --------------------------------------------------------------------------------------; Copy Number Analysis: Tools to analyze copy number data.; CalculateTargetCoverage Count overlapping reads target by target. --------------------------------------------------------------------------------------; Fasta: Tools for analysis and manipulation of files in fasta format; CreateSequenceDictionary Creates a dict file from reference sequence in fasta format; NormalizeFasta Normalizes lines of sequence in a fasta file to be of the same length. --------------------------------------------------------------------------------------; Intervals: Tools for processing intervals and associated overlapping records; BedToIntervalList Converts a BED file to an Picard Interval List; ExampleIntervalWalker Print intervals with optional contextual data; IntervalListTools General tool for manipulating interval lists; LiftOverIntervalList Lifts over an interval list between genome builds. --------------------------------------------------------------------------------------; QC: Tools for Diagnostics and Quality Control; AnalyzeCovariates Tool to analyze and evaluate base recalibration tables for BQSR; CalculateHsMetrics Produces Hybrid Selection-specific metrics for a SAM/BAM file; CollectAlignmentSummaryMetrics Produces from a SAM/BAM/CRAM file containing summary alignment metrics; CollectBaseDistributionByCycle Produces metrics about nucleotide distribution per cycle in a SAM/BAM/CRAM fi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1669:433,install,install,433,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1669,1,['install'],['install']
Deployability,rk.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:985); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:985); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:800); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.saveAsShardedHadoopFiles(ReadsSparkSink.java:202); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:229); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:152); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:247); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:35); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:21943,pipeline,pipelines,21943,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['pipeline'],['pipelines']
Deployability,rk.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:985); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:985); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:800); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.saveAsShardedHadoopFiles(ReadsSparkSink.java:203); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReadsSingle(ReadsSparkSink.java:230); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:153); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:259); 	at org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark.runTool(PrintReadsSpark.java:39); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); 	at org.broadinstitute.hellbender.Main.main(Main.java:239); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMet,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:36810,pipeline,pipelines,36810,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['pipeline'],['pipelines']
Deployability,rkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.ja,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3595,deploy,deploy,3595,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"rkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5766,deploy,deploy,5766,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"rkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource@3c82e6f4); - field (class: org.broadinstitute.hellbender.engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:2759,deploy,deploy,2759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['deploy'],['deploy']
Deployability,rkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/04/27 18:49:12 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1231); at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229); at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); at org.apache.spark.SparkContext.<init>(SparkContext.scala:602); at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); at org.broadinstitute.hellbende,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:3519,deploy,deploy,3519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"rkContextFactory.java:81); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:155); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:174); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.main(Main.java:82); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileU",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:5690,deploy,deploy,5690,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,roadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:977); 	at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); 	at htsjdk.samt,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:19602,deploy,deploy,19602,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['deploy'],['deploy']
Deployability,"roadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:7812,deploy,deploy,7812,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['deploy'],['deploy']
Deployability,"roadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.main(Main.java:81); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/01/21 14:55:33 INFO ShutdownHookManager: Shutdown hook called; ```. Attached is a small BAM file that I used to reproduce the error (If memory serves, I've seen this issue on other BAM files as well):. [NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip](https://github.com/broadinstitute/gatk/files/101575/NA12878.chrom20.100kb.ILLUMINA.bwa.CEU.exome.20121211.bam.zip). (This issue may be related to one posted here: https://github.com/broadinstitute/gatk/issues/1417.). Here is some information on what I installed:. ```; echo ""Installing Java""; sudo add-apt-repository -y ppa:webupd8team/java; sudo apt-get -qq update; echo debconf shared/accepted-oracle-license-v1-1 select true | sudo debconf-set-selections; echo debconf shared/accepted-oracle-license-v1-1 seen true | sudo debconf-set-selections; sudo apt-get -qq install -y oracle-java8-installer. java -version. echo """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1444:3225,deploy,deploy,3225,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1444,1,['deploy'],['deploy']
Deployability,roadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:451); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:439); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:135); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:180); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:199); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [91a5d7391a4647a89e50717b96eb50e0] entered state [ERROR] while waiting for [DONE]. ```. #### Steps to reproduce; Run a tool in the following way. ```; gatk ToolNameSpark \; -I hdfs://path/to/bam/test.bam \; -L hdfs://path/to/interval/file/interval.bed \; -O hdfs://path/to/output \; ....; ```. #### Expected behavior; Intervals to be parsed correctly. #### Actual behavior; Engine tries to interpret the file name as an actual interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4852:2754,deploy,deploy,2754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4852,6,['deploy'],['deploy']
Deployability,roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16/11/29 16:21:01 ERROR org.apache.spark.util.Utils: Uncaught exception in thread main; java.lang.NullPointerException; 	at org.apache.spark.network.shuffle.ExternalShuffleClient.close(ExternalShuffleClient.java:152); 	at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1286); 	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:96); 	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756); 	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219); 	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755); 	at org.apache.spark.SparkContext.<init>(SparkContext.scal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:2039,deploy,deploy,2039,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,"roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 16:21:01.561 INFO MarkDuplicatesSpark - Shutting down engine; [November 29, 2016 4:21:01 PM UTC] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8232370176; org.apache.spark.SparkException: Could not parse Master URL: 'yarn'; 	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2735); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:522); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:59); 	at org.broadinstitute.hellbender.engine.spark.S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2289:4369,deploy,deploy,4369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2289,1,['deploy'],['deploy']
Deployability,roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:95); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:102); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:115); 	at org.broadinstitute.hellbender.Main.main(Main.java:157); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$78/237665701.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:23107,deploy,deploy,23107,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['deploy'],['deploy']
Deployability,roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: Pathname /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta from /tmp/da63aa3c-e3bc-4893-9f40-42921719a343/hdfs:/svdev-caller-m:8020/reference/Homo_sapiens_assembly38.fasta is not a valid DFS filename.; 	at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:213); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1436); 	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1433); 	at org.apache.hadoop.fs.FileSystemLinkResol,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:2529,deploy,deploy,2529,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"roadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:112); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:96); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:103); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:116); 	at org.broadinstitute.hellbender.Main.main(Main.java:158); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.NullPointerException; 	at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106); 	at org.broadinstitute.hellbender.engine.AuthHolder.getOfflineAuth(AuthHolder.java:79); 	at org.broadinstitute.hellbender.engine.AuthHolder.makeStorageClient(AuthHolder.java:94); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSource.getHeader(ReadsSparkSource.java:177); 	... 20 more; ERROR: (gcloud.dataproc.jobs.submit.spark) Job [bd000687-f538-4201-b888-668612d46bad] entered state [ERROR] while waiting for [DONE].; ```. =========================. On a third note, if the reference is also ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2382:7265,deploy,deploy,7265,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2382,1,['deploy'],['deploy']
Deployability,"rom Lee [VS-210] (#7502); - Export the VAT into GCS (#7472); - addresses VS-219 (#7508); - small fix to MergeVCFs (#7517); - small fixes to GVS pipeline (#7522); - make sure ExtractTask is run on all interval files; - Revert ""make sure ExtractTask is run on all interval files""; - make sure ExtractTask is run on all interval files (#7527); - Remove Sites only step from the VAT creation WDL (#7510); - fix bad argument processing for bool (#7529); - Support for TDR DRS URIs in Import (#7528); - Match format of filename output in GvsRescatterCallsetInterval (#7539); - Reference block storage and query support (#7498); - update docs (#7540); - Kc fix rr load bug (#7550); - Update .dockstore.yml (#7553); - Ah add reblocking wdl (#7544); - Scatter over all interval files, not just scatter count (#7551); - fixed docker (#7558); - take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:20601,upgrade,upgrade,20601,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['upgrade'],['upgrade']
Deployability,"rsion of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model prior that were necessitated by the API changes, or some bugs introduced in other code updates. (Also note that I believe Andrey's PR in item 4 already broke these tests, although the numerical differences were much smaller and more reasonable--",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:1570,update,update,1570,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['update'],['update']
Deployability,"rstly, I run `./gradle localJar`. ```; Downloading https://services.gradle.org/distributions/gradle-7.5.1-bin.zip; ...........10%............20%...........30%............40%...........50%............60%...........70%............80%...........90%............100%. Welcome to Gradle 7.5.1!. Here are the highlights of this release:; - Support for Java 18; - Support for building with Groovy 4; - Much more responsive continuous builds; - Improved diagnostics for dependency resolution. For more details see https://docs.gradle.org/7.5.1/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/build/gatk/src/gatk/build.gradle' line: 104. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; > Run with --stacktrace option to get the stack trace.; > Run with --info or --debug option to get more log output.; > Run with --scan to get full insights. * Get more help at https://help.gradle.org. BUILD FAILED in 17s; ```; However, I already install git-lfs; ```; git-lfs usr/; git-lfs usr/bin/; git-lfs usr/bin/git-lfs; git-lfs usr/share/; git-lfs usr/share/licenses/; git-lfs usr/share/licenses/git-lfs/; git-lfs usr/share/licenses/git-lfs/LICENSE; git-lfs usr/share/man/; git-lfs usr/share/man/man1/; git-lfs usr/share/man/man1/git-lfs-checkout.1.gz; git-lfs usr/share/man/man1/git-lfs-clean.1.gz; git-lfs usr/share/man/man1/git-lfs-clone.1.gz; git-lfs usr/share/man/man1/git-lfs-dedup.1.gz; git-lfs usr/share/man/man1/git-lfs-env.1.gz; git-lfs usr/share/man/man1/git-lfs-ext.1.gz; git-lfs usr/share/man/man1/git-lfs-fetch.1.gz; git-lfs usr/share/man/man1/git-lfs-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8320:1158,install,installed,1158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8320,1,['install'],['installed']
Deployability,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:32071,Update,Update,32071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,4,['Update'],['Update']
Deployability,"run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240); - Add a test exclusion",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8251:32071,Update,Update,32071,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8251,4,['Update'],['Update']
Deployability,"s - Start Date/Time: February 7, 2018 10:10:10 AM GMT; 10:10:10.529 INFO FilterMutectCalls - ------------------------------------------------------------; 10:10:10.529 INFO FilterMutectCalls - ------------------------------------------------------------; 10:10:10.529 INFO FilterMutectCalls - HTSJDK Version: 2.14.1; 10:10:10.529 INFO FilterMutectCalls - Picard Version: 2.17.2; 10:10:10.529 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 10:10:10.530 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:10:10.530 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:10:10.530 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:10:10.530 INFO FilterMutectCalls - Deflater: IntelDeflater; 10:10:10.530 INFO FilterMutectCalls - Inflater: IntelInflater; 10:10:10.530 INFO FilterMutectCalls - GCS max retries/reopens: 20; 10:10:10.530 INFO FilterMutectCalls - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 10:10:10.530 INFO FilterMutectCalls - Initializing engine; 10:10:10.877 INFO FeatureManager - Using codec VCFCodec to read file file:///scratch/dberaldi/projects/20180204_combine_callers/gatk/TT001T03.snv.vcf.gz; 10:10:10.981 INFO FilterMutectCalls - Done initializing engine; 10:10:11.035 INFO ProgressMeter - Starting traversal; 10:10:11.036 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 10:10:11.331 INFO FilterMutectCalls - Shutting down engine; [February 7, 2018 10:10:11 AM GMT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.02 minutes.; Runtime.totalMemory()=933756928; org.broadinstitute.hellbender.exceptions.GATKException: INFO annotation 'MFRL' contains a non-int value '7.97254e+06'; 	at org.broadinstitute.hellbender.utils.GATKProtectedVariantContextUtils.lambda$attributeValueToIntArray$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4363:3298,patch,patch,3298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4363,1,['patch'],['patch']
Deployability,"s going on in other projects. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215757315). Alright, to give an overview of where this stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:3838,release,release,3838,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,"s stands, we have several options on the table for solving this problem:; 1. Split the GATK into even more repos (a CNV-only repo, a HaplotypeCaller repo) that are versioned separately. GATK release X would then consist of CNV version Y, HaplotypeCaller version Z, gatk-public version P, etc. This is probably the most ""correct"" solution from a software engineering perspective, but might be a nightmare to work with.; 2. Have the ability to release jars with a subset of the tools exposed to the user (eg., CNV-only jars). Geraldine hates this one, and it does seem like a bad idea to have these incomplete jars floating out in the wild.; 3. Everyone develops on separate branches, and merges to master only when everything in a branch is ""release-ready"". In this scenario master itself is always (theoretically, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:3883,release,release,3883,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,"s users pulling from all docker repos (regardless of the tier for the owner of the repository being pulled from). This might or might not affect us since it looks like travis is pulling from our GCR repo for builds but we should be mindful of workflows that might rely on pulling hundreds of docker images from docker-hub through anonymous web VMs:; ```; On Monday, November 2, 2020 at 9am Pacific Standard Time, Docker will begin enforcing rate limits on container pulls for Anonymous and Free users. Anonymous (unauthenticated) users will be limited to 100 container image pulls every six hours, and Free (authenticated) users will be limited to 200 container image pulls every six hours, when enforcement is fully implemented. Docker Pro and Team subscribers can pull container images from Docker Hub without restriction, as long as the quantities are not excessive or abusive.; In addition, we are pausing enforcement of the changes to our image-retention policies until mid-2021, when we anticipate incorporating them into usage-based pricing. Two months ago, we announced an update to Docker image-retention policies. As originally stated, this change, which was set to take effect on November 1, 2020, would result in the deletion of images for free Docker account users after six months of inactivity. Today's announcement means Docker will not enforce image expiration on November 1, 2020.; ```; This is farther clarified on their FAQ https://www.docker.com/pricing/resource-consumption-updates:; ```; Rate limits for Docker image pulls are based on the account type of the user requesting the image - not the account type of the image’s owner. These are defined on the pricing page.; The highest entitlement a user has, based on their personal account and any orgs they belong to, will be used. Unauthenticated pull requests are “anonymous” and will be rate limited based on IP address rather than user ID. For more information on authenticating image pulls, please see this docs page.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6922:1138,update,update,1138,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6922,2,['update'],"['update', 'updates']"
Deployability,"s with a Passed site with:; cram 1. Call with GT='0/0, GQ=0 and DP >40.; cram 2. Call with GT='0/1' or '1/1' and DP>20. . Create vcf with two approaches:. Pipeline 1. HaplotypeCaller-->vcf. module load gatk/4.0.11.0; gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I gq0_cram.list\; -L chr19:44907684-44909822\; --use-new-qual-calculator\; -O good.vcf.gz. Good GQ scores were also estimated with Freebayes on these samples also. Pipeline 2 HaplotypeCaller --> bvcf--->ImportVCF-->GenotypeVCF-->VCF with 2 samples. gatk HaplotypeCaller -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa\; -I $sample.cram\; --use-new-qual-calculator\; -L chr19:44907684-44909822\; -ERC GVCF\; -O bad.g.vcf.gz. Followed by import and GenotypeVCF. . #### Expected behavior; Pipeline 2 should generate accurate GQ scores that match the GQ in the HaplotypeCaller vcf output of pipeline 1. Instead GQ=0. . This is the output for the 57 GQ=0 samples with pipeline 1 which is accurate. AC=7;AF=0.061;AN=114;BaseQRankSum=-6.147;DP=1846;ExcessHet=3.8592;FS=0.000;InbreedingCoeff=-0.0640;MLEAC=6;MLEAF=0.053;MQ=60.00;MQRankSum=0.000;QD=4.52;ReadPosRankSum=-0.781;SOR=2.833; GT:AD:DP:GQ:PL; 0/0:37,0:37:99:0,111,1236; 0/0:40,0:40:99:0,120,1357; 0/0:34,0:34:99:0,102,1161; 0/0:49,0:49:99:0,147,1673; 0/0:33,0:33:99:0,99,1036; 0/0:48,0:48:99:0,144,1728; 0/0:42,0:42:99:0,126,1410; 0/0:37,0:37:99:0,111,1215; 0/0:39,0:39:99:0,117,1311; 0/0:42,0:42:99:0,126,1419; 0/0:53,0:53:99:0,159,1744; 0/0:45,0:45:99:0,135,1529; 0/0:44,0:44:99:0,132,1419; 0/0:38,0:38:99:0,114,1299; 0/0:37,0:37:99:0,111,1205; 0/0:34,0:34:99:0,102,1151; 0/0:57,0:57:99:0,171,1826; 0/0:27,1:28:49:0,49,904; 0/0:41,0:41:99:0,123,1364; 0/0:28,0:28:84:0,84,933; 0/0:36,0:36:99:0,108,1171; 0/0:29,0:29:87:0,87,987; 0/0:31,0:31:93:0,93,997; 0/0:37,0:37:99:0,111,1266; 0/0:28,2:30:70:0,70,914; 0/0:36,0:36:99:0,108,1230; 0/0:49,0:49:99:0,147,1613; 0/0:38,1:39:82:0,82,1231; 0/0:26,0:26:78:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5445:2109,pipeline,pipeline,2109,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5445,1,['pipeline'],['pipeline']
Deployability,"s). ValidateVariants: `--fail-gvcf-on-overlap` / `-no-overlaps`. ### Affected version(s); - [x] Latest public release version: 4.2.6.1; - [ ] ~Latest master branch as of~ [did not test, but affected file hasn't changed since August 2021]. ### Description . If there are overlapping reference blocks when running ValidateVariants with the `-no-overlaps` option, a USER ERROR is outputted after the entire tool finishes running, as shown below:. ```; ***********************************************************************. A USER ERROR has occurred: This GVCF contained overlapping reference blocks. The first overlapping interval is [genomic coordinates here]. ***********************************************************************; ```. This error should be generally helpful, but it appears that the interval that is reported in the error message is the _last_ overlapping interval, not the _first_. I'm not super familiar with java, but I'm guessing that `firstOverlap` might be continuously replaced by `refInterval` if there are multiple overlaps, which is inconsistent with expected behavior. . Potentially relevant lines of code: ; - `-no-overlaps` argument description ([lines 192-201](; https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L192-L201)); - `firstOverlap = refInterval` ([line 275](https://github.com/broadinstitute/gatk/blob/ca33bc953abfa7050b791f049285f5262675cf84/src/main/java/org/broadinstitute/hellbender/tools/walkers/variantutils/ValidateVariants.java#L275)). #### Steps to reproduce. Running ValidateVariants with the `-no-overlaps` flag on a .g.vcf with overlapping intervals will cause this error. More specifically, we're running this within WARP's Exome Germline Single Sample v.3.1.7 WDL release. Our command is as follows:. ```; gatk --java-options ""-Xms6000m -Xmx6500m"" \; ValidateVariants \; -V /path/to/our/.g.vcf.gz \; -R /path/to/ou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8103:1029,continuous,continuously,1029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8103,1,['continuous'],['continuously']
Deployability,s/coveragemodel/learning_sample_bias_latent.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/learning_sample_read_depth.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/learning_sample_sex_genotypes.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_contig_anots.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_HMM_priors_table.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model/mean_bias_covariates_matrix.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model/target_specific_mean_log_bias.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_model/target_specific_unexplained_variance.tsv; src/test/resources/org/broadinstitute/hellbender/tools/coveragemodel/sim_targets.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/acnv-segments-from-allelic-integration.seg; src/test/resources/org/broadinstitute/hellbender/tools/exome/af-params-from-allelic-integration.af.param; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-1.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-2.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-3.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/allelic-pon-test-pulldown-4.tsv; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/dupReadsMini.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12778.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12872.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/exome/calculatetargetcoverage/exome-read-counts-NA12878.bam,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:29037,integrat,integration,29037,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['integrat'],['integration']
Deployability,"s; 18/01/09 18:31:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 18/01/09 18:31:26 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/01/09 18:31:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/01/09 18:31:26 INFO memory.MemoryStore: MemoryStore cleared; 18/01/09 18:31:26 INFO storage.BlockManager: BlockManager stopped; 18/01/09 18:31:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/01/09 18:31:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/01/09 18:31:26 INFO spark.SparkContext: Successfully stopped SparkContext; 18:31:26.896 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [January 9, 2018 6:31:26 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 0.89 minutes.; Runtime.totalMemory()=881328128; ***********************************************************************. A USER ERROR has occurred: Input files reference and reads have incompatible contigs: No overlapping contigs found.; reference contigs = [chrM, chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chr1_gl000191_random, chr1_gl000192_random, chr4_ctg9_hap1, chr4_gl000193_random, chr4_gl000194_random, chr6_apd_hap1, chr6_cox_hap2, chr6_dbb_hap3, chr6_mann_hap4, chr6_mcf_hap5, chr6_qbl_hap6, chr6_ssto_hap7, chr7_gl000195_random, chr8_gl000196_random, chr8_gl000197_random, chr9_gl000198_random, chr9_gl000199_random, chr9_gl000200_random, chr9_gl000201_random, chr11_gl000202_random, chr17_ctg5_hap1, chr17_gl000203_random, chr17_gl000204_random, chr17_gl000205_random, chr17_gl000206_random, chr18_gl000207_random,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:31165,pipeline,pipelines,31165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['pipeline'],['pipelines']
Deployability,"s=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa.img,file:///restricted/projectnb/casa/ref/GRCh38_ignored_kmers.txt --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 /share/pkg/gatk/4.1.0.0/install/bin/gatk-package-4.1.0.0-spark.jar StructuralVariationDiscoveryPipelineSpark -R file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img --kmers-to-ignore GRCh38_ignored_kmers.txt --contig-sam-file hdfs:///project/casa/gcad/adsp.cc/sv//A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file -I hdfs:///project/casa/gcad/adsp.cc/cram/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.cram -O hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.sv.vcf --spark-master yarn. ```. #### Expected behavior. Run to completion with SV vcf output. #### Actual behavior. ```; 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 85.0 in stage 5.0 (TID 1031) in 28293 ms on scc-q09.scc.bu.edu (executor 30) (74/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_1031 on scc-q09.scc.bu.edu:40204 in memory (size: 5.4 MB, free: 42.5 GB); 2019-02-17 16:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:2385,install,install,2385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['install'],['install']
Deployability,"sDBImport: ; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms2G -Xmx20G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenomicsDBImport --genomicsdb-workspace-path 007_Database_DBImport_VCFref/database_interval_9 --sample-name-map sample_name_map --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --reader-threads 5 --batch-size 60 --tmp-dir TMPDIR --max-num-intervals-to-import-in-parallel 3 --merge-input-intervals`. GenotypeGVCFs:; `java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms4G -Xmx16G -XX:+UseParallelGC -XX:ParallelGCThreads=2 -jar MySoftwares/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar GenotypeGVCFs -R PigeonBatch5/000_DataLinks/000_RefSeq/Cliv2.1_genomic.fasta --intervals 006_IntervalsSplit_DBImport_VCFref/interval_9.list --force-output-intervals PigeonBatch4/008_RawVcfGz/MergeVcf/pigeonBatch1234_filtered.vcf.gz -V gendb://007_Database_DBImport_VCFref/database_interval_9 -O 008_RawVcfGz_DBImport_VCFref/001_DividedIntervals/interval_9.vcf.gz --tmp-dir TMPDIR --allow-old-rms-mapping-quality-annotation-data --only-output-calls-starting-in-intervals --verbosity ERROR`. #### **User Description of the Issue:**; ""I'm using the GenotypeGVCFs function based on GenomicsDBImport database. I've divided the reference into 50 intervals. Some intervals seems ok, but some reports error as following. I used a VCF file in ""--force-output-intervals"" for down stream analysis. I've never seen this error without ""--force-output-intervals"". I've searched for the error message and changed my GATK version to 4.2.6.1 since similar error has been solved as a bug in recent update, but it still not works on my dataset..."". @droazen and @samuelklee , any insight on this?. Thank you,. Anthony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7938:5450,update,update,5450,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7938,1,['update'],['update']
Deployability,sSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144); at org.apache.spark.SparkContext.<init>(SparkContext.scala:530),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:7098,deploy,deploy,7098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"sVariantWalker.java:40); 2019-10-29T18:18:04.002731707Z 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 2019-10-29T18:18:04.002740306Z 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 2019-10-29T18:18:04.002745164Z 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 2019-10-29T18:18:04.002777218Z 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 2019-10-29T18:18:04.002785268Z 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 2019-10-29T18:18:04.002855927Z 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 2019-10-29T18:18:04.002867030Z 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```; I am using ExAC lifted to hg38 as a germline resource in mutect2 with only a tumor sample, and getting the above error in filtermutectcalls. I recently updated to v4.1.3.0 to have the latest changes to mutect2. I was not having this issue with v4.0.5.1. Here is extracted information from the VCF which caused the issue. . ```; DP=1;ECNT=2;FS=0.000;MBQ=0,20;MFRL=0,91;MMQ=60,46;MPOS=6;MQ=46.00;POPAF=5.08;TLOD=4.20	GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB	0|1:0,1:0.667:1:0,1:0,0:0|1:11155815_C_T:11155815:0,0,1,0; DP=1;ECNT=2;FS=0.000;MBQ=0,20;MFRL=0,91;MMQ=60,46;MPOS=16;MQ=46.00;POPAF=5.08;TLOD=4.20	GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB	0|1:0,1:0.667:1:0,1:0,0:0|1:11155815_C_T:11155815:0,0,1,0; DP=1;ECNT=2;FS=0.000;MBQ=0,34;MFRL=0,272;MMQ=60,30;MPOS=25;MQ=30.00;POPAF=4.13;TLOD=4.20	GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB	0|1:0,1:0.667:1:0,1:0,0:0|1:11350899_C_T:11350899:0,0,1,0; DP=1;ECNT=2;FS=0.000;MBQ=0,32;MFRL=0,272;MMQ=60,30;MPOS=15;MQ=30.00;POPAF=4.23;TLOD=4.20	GT:AD:AF:DP:F1R2:F2R1:PGT:PID:PS:SB	0|1:0,1:0.667:1:0,1:0,0:0|1:11350899_C_T:11350899:0,0,1,0; ```. Additionally, I tried to re-run this sample without the germline resource and encounter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:5318,update,updated,5318,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,1,['update'],['updated']
Deployability,"sa-unstable/issues/1425#issuecomment-232150240). Comparison using KB NA12878 on PCR+ PCR- data across 20:10M-25M:. <img width=""550"" alt=""screen shot 2016-07-12 at 3 15 45 pm"" src=""https://cloud.githubusercontent.com/assets/791104/16780183/9042fcda-4843-11e6-99e5-cb9b39e5dfdc.png"">. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260494000). @vruano Are you currently working on this? Or can this be moved into the GATK4 repo for future work? . ---. @vruano commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260503444). Working on it on GATK3 but I could merge it into GATK4 whenever is ready if you prefer. ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-260509332). Ok great, totally fine to do in 3 but please do port to 4 when it's ready. Do you have an order of magnitude sense of when it might be ready? Meaning days/weeks/months (for release scheduling purposes). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285721454). In the last methods meeting I presented the results of our first effort to improve accuracy calling in STR. As far as unfiltered single and trio calls are concerned the recommendation is to apply the new model with PCR+ data. However, for PCR- dataset one either can choose not apply any correction or to apply the new model train on PCR- data... the latter seems to have slightly F1 values however for the sake of simplicity it might just make sense no to apply any correct; either way is good. The presentation I gave can be found [here](https://drive.google.com/open?id=0Bzt9p0vCNxlHWlZVUHZfdXR5MTg). ---. @vruano commented on [Fri Mar 10 2017](https://github.com/broadinstitute/gsa-unstable/issues/1425#issuecomment-285722791). It seems that at some point Planatir will take a look and see whether it im",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2519:1390,release,release,1390,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2519,1,['release'],['release']
Deployability,"se reference name = *.; at htsjdk.samtools.SAMUtils.processValidationErrors(SAMUtils.java:439); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:643); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:628); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:598); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:544); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:518); at htsjdk.samtools.util.PeekIterator.peek(PeekIterator.java:67); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.skipAnyNotprimary(SecondaryOrSupplementarySkippingIterator.java:36); at htsjdk.samtools.SecondaryOrSupplementarySkippingIterator.advance(SecondaryOrSupplementarySkippingIterator.java:31); at org.broadinstitute.hellbender.utils.read.SamComparison.compareCoordinateSortedAlignments(SamComparison.java:111); at org.broadinstitute.hellbender.utils.read.SamComparison.compareAlignments(SamComparison.java:68); at org.broadinstitute.hellbender.utils.read.SamComparison.<init>(SamComparison.java:44); at org.broadinstitute.hellbender.tools.picard.sam.CompareSAMs.doWork(CompareSAMs.java:34); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:94); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:144); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgram.instanceMain(PicardCommandLineProgram.java:51); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:77); at org.broadinstitute.hellbender.Main.main(Main.java:92); ```. Same command on original picard passes validation (though claims the bam is different from itself: https://github.com/broadinstitute/picard/issues/160). Note to whoever fixes this: once this is fixed, re-enable code in BaseRecalibratorIntegrationTest.java. ```; //IntegrationTestSpec.compareBamFiles(actualHiSeqBam_recalibrated, expectedHiSeqBam_recalibrated);; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/419:2419,Integrat,IntegrationTestSpec,2419,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/419,1,['Integrat'],['IntegrationTestSpec']
Deployability,"seRecalibrator - Inflater: IntelInflater ; ; 00:12:21.144 INFO  BaseRecalibrator - GCS max retries/reopens: 20 ; ; 00:12:21.144 INFO  BaseRecalibrator - Requester pays: disabled ; ; 00:12:21.144 INFO  BaseRecalibrator - Initializing engine ; ; 00:12:21.485 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz ; ; 00:12:21.565 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz ; ; 00:12:21.688 INFO  FeatureManager - Using codec VCFCodec to read file file:///data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz ; ; 00:12:21.797 WARN  IndexUtils - Feature file ""file:///data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file ; ; 00:12:21.895 WARN  IntelInflater - Zero Bytes Written : 0 ; ; 00:12:21.966 INFO  BaseRecalibrator - Done initializing engine ; ; 00:12:21.969 INFO  BaseRecalibrationEngine - The covariates being used here: ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ReadGroupCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     QualityScoreCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     ContextCovariate ; ; 00:12:21.969 INFO  BaseRecalibrationEngine -     CycleCovariate ; ; 00:12:22.016 INFO  ProgressMeter - Starting traversal ; ; 00:12:22.017 INFO  ProgressMeter -        Current Locus  Elapsed Minutes       Reads Processed     Reads/Minute. **How can I assign a temp directory and won't get the bug?**. I set the gatk environment using conda:. /data/xieduo/WES\_pipe/pipeline/bin/Miniconda3/bin/conda env create -n gatk\_4.2.6.1 -f gatkcondaenv.yml. Thank you!. Best,. Duo<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/293634'>Zendesk ticket #293634</a>)<br> gz#293634</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:19230,pipeline,pipeline,19230,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"sed on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - resolved rebase conflicts; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691); - refactored to common ProbeInfo, support compressed data on ingest, support local CSV probe info; - update exome ingest; - minor mods; - change structure, add compressed option to ingest; - add imputed tsv creator and refactor; - Adding a test and small features to var store branch (#6761); - upgraded to new google bigquery libraries and storage api v1; used storage api for probe info; synced encoded gt definitions; - added support for probe_id ranges (#6806); - ah - use new GT encoding (#6822); - updating ArrayCalculateMetrics for new genotype counts table (#6843); - Ability to filter variants based on QC in ArrayExtractCohort (#6844); - switch from ExcessHet back to HWE (#6848); - Moving the WDL for importing array manifest to BQ (#6860); - fix up after rebase; - Moving and testing ingest scripts from variantstore (#6881); - optionally provide sample-map-file instead of sample-map-table (#6872); - Moving extract wdls from variantstore repo (#6902); - update for genomes (#6918); - update paths; - update field name; - consolidate exome and genome code; - missing comma; - allow null for drop state.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:2739,upgrade,upgraded,2739,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,16,"['update', 'upgrade']","['update', 'upgraded']"
Deployability,"ser-images.githubusercontent.com/61913000/87845904-eea14f80-c8e0-11ea-90bd-235c9205f72f.png"">. (gatk) root@bc3c6aca6231:/gatk/my_data/tools# java -jar cromwell-51.jar run /gatk/my_data/seq-format-validation/validate-bam.wdl --inputs /gatk/my_data/seq-format-validation/validate-bam.inputs.json; [2020-07-14 05:09:22,78] [info] Running with database db.url = jdbc:hsqldb:mem:f10b64bd-d8ca-4428-917b-311fca24c372;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-07-14 05:09:29,37] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-07-14 05:09:29,47] [info] Running with database db.url = jdbc:hsqldb:mem:e337a356-2f0c-4389-92c5-255465180f24;shutdown=false;hsqldb.tx=mvcc; [2020-07-14 05:09:29,89] [info] Slf4jLogger started; [2020-07-14 05:09:30,10] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-ca5c695"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2020-07-14 05:09:30,23] [info] Metadata summary refreshing every 1 second.; [2020-07-14 05:09:30,23] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2020-07-14 05:09:30,25] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2020-07-14 05:09:30,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2020-07-14 05:09:30,36] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2020-07-14 05:09:30,46] [info] SingleWorkflowRunnerActor: Version 51; [2020-07-14 05:09:30,48] [info] SingleWorkflowRunnerActor: Submitting workflow; [2020-07-14 05:09:30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6710:1858,configurat,configuration,1858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6710,1,['configurat'],['configuration']
Deployability,"ses where applicable; 15:24:12.735 INFO ReadsSparkSink - Finished sorting the bam file and dumping read shards to disk, proceeding to merge the shards into a single file using the master thread; 15:41:27.766 INFO ReadsSparkSink - Finished merging shards into a single output bam; 15:41:34.351 INFO MarkDuplicatesSpark - Shutting down engine; [May 7, 2018 3:41:34 PM EDT] org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark done. Elapsed time: 61.21 minutes.; Runtime.totalMemory()=13635682304; ```. With native libraries (note the lack of the usual warning):. ```; $ ${GATK_DIR}/gatk MarkDuplicatesSpark --java-options ""-Djava.library.path=${HADOOP_DIR}/hadoop-2.6.5-src/hadoop-common-project/hadoop-common/target/hadoop-common-2.6.5/lib/native"" -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam -- --spark-runner LOCAL --spark-master local[8]; Using GATK wrapper script ${GATK_DIR}/gatk/build/install/gatk/bin/gatk; Running:; ${GATK_DIR}/gatk/build/install/gatk/bin/gatk MarkDuplicatesSpark -I CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam -O CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.dupmarked_native.bam --spark-master local[8]; 21:47:47.494 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 21:47:47.827 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:${GATK_DIR}/gatk/build/install/gatk/lib/gkl-0.8.5.jar!/com/intel/gkl/native/libgkl_compression.so; 21:47:48.268 INFO MarkDuplicatesSpark - ------------------------------------------------------------; 21:47:48.268 INFO MarkDuplicatesSpark - The Genome Analysis Toolkit (GATK) v4.0.4.0-7-g46a8661-SNAPSHOT; 21:47:48.268 INFO MarkDuplicatesSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 21:47:48.270 INFO MarkDuplicatesSpark - Executing as cwh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4746:4950,install,install,4950,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4746,2,['install'],['install']
Deployability,set Spark reducer chunk size to 64MB by default - speeds up our pipelines,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1874:64,pipeline,pipelines,64,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1874,1,['pipeline'],['pipelines']
Deployability,several very minor updates to help improve coverage,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/174:19,update,updates,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/174,1,['update'],['updates']
Deployability,shlee redo ts_print_reads_docs and make more doc updates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4084:49,update,updates,49,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4084,1,['update'],['updates']
Deployability,small fixes to GVS pipeline,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7522:19,pipeline,pipeline,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7522,1,['pipeline'],['pipeline']
Deployability,some things in M2 pipeline don't need to be beta any longer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6215:18,pipeline,pipeline,18,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6215,1,['pipeline'],['pipeline']
Deployability,"sor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.16.0+computecanada ; tbb 2021.1.1+computecanada; Theano 1.0.4 ; tqdm 4.19.5+computecanada ; wheel 0.34.2 ; ----. I used pyt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4872,install,install,4872,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['install'],['install']
Deployability,spark things are falling over when they encounter the new hg38 contig names that include `:` and `-` . We need to update hadoop bam to understand these since they are an unfortunate fact of life. ```; [ameyner2@node2c15 read_counts]$ /exports/igmm/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-launch SparkGenomeReadCounts -I ../../bcbio/final/WW00247b/WW00247b-ready.bam -o WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa; Using GATK jar /gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar SparkGenomeReadCounts -I ../../bcbio/final/WW00247b/WW00247b-ready.bam -o WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa; 16:51:57.743 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gpfs/igmmfs01/eddie/bioinfsvice/ameynert/software/gatk-4.beta.2/gatk-package-4.beta.2-local.jar!/com/intel/gkl/native/libgkl_compression.so; [21 July 2017 16:51:57 BST] SparkGenomeReadCounts --outputFile WW00247b.prop_cov --reference /exports/igmm/eddie/bioinfsvice/ameynert/bcbio/data/genomes/Hsapiens/hg38/seq/hg38.fa --input ../../bcbio/final/WW00247b/WW00247b-ready.bam --keepXYMT false --binsize 10000 --readValidationStringency SILENT --interval_set_rule UNION --interval_padding 0 --interval_exclusion_padding 0 --bamPartitionSize 0 --disableSequenceDictionaryValidation false --shardedOutput false --numReducers 0 --sparkMaster local[*] --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --disableToolDefaultReadFilters false; [21 ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360:114,update,update,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360,1,['update'],['update']
Deployability,"spec ops issue #248. process implemented here:; - new task `SetLoadLock` is called at the beginning of `ImportGenomes` - it generates a UUID for the submission, writes that run_uuid to a lock file, and uploads that lock file to the output_directory (where the tsvs will be generated). ; - CreateImportTsvs and LoadTables take the run_uuid as an input, compare it against the contents of the lock file in the bucket, and only proceed if the uuids match. otherwise they exit out.; - after all LoadTables tasks have completed, a new task `ReleaseLoadLock` is called that removes the lock file from the bucket (again only if the uuid in the lockfile matches this run). tested and confirmed that:; - the `loadlock` file is created and removed: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/b0b9c7a1-70fd-4d44-a76e-b5604a5068f0; - the task fails if the lock file is present: https://app.terra.bio/#workspaces/broad-dsp-spec-ops-fc/1000G-high-coverage-2019_specops_mmt_test_memory/job_history/293687f9-e7b9-474b-bfe8-e50f4c555199",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7138:536,Release,ReleaseLoadLock,536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7138,1,['Release'],['ReleaseLoadLock']
Deployability,"specops issue #265 https://github.com/broadinstitute/dsp-spec-ops/issues/265. in addition to renaming the metadata table (in both CreateVariantIngestFiles tool and ImportGenomes wdl), this PR:; - removes interval_list_blob from the metadata/sample_info table; - adds missing QUALapprox field to the vet schema defaults (in ImportGenomes wdl). this was tested by running ImportGenomes.wdl in Terra and the sample_info table gets created & populated as expected. note: ImportArrays.wdl and array tooling were not updated",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7196:511,update,updated,511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7196,1,['update'],['updated']
Deployability,"ss.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); Funcotator; gatk Funcotator --variant test.somatic.vcf --reference ucsc.hg19.fasta --ref-version hg19 --data-sources-path funcotator_dataSources.v1.7.20200521s --output test.maf --output-file-format MAF; ### Affected version(s); gatk4.1.8.1 (installed using conda). ### Description ; I want to use Funcotator to annotate the VCF file given by Illumina TruSight Oncology 500 pipeline. But when I run the command above, it throws out an error, seems something related with malformat. I check my VCF file and think it should be OK. So I wonder if you can kindly tell me how to fix this bug?; The ERROR is:; `Using GATK jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator --variant /home/shiyang/Project/BGB900_101/TSO_result/TSO_somatic_vcf/112-0005-0031-B1_L1.UP12.tmb.tsv.tso.somatic.vcf --reference /storage01/ref_genome/hg19/bwa/ucsc.hg19.fasta --ref-version hg19 --data-sources-path /home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s --outpu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:1460,install,installed,1460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['install'],['installed']
Deployability,"ssing / not called genotypes (`./.`). These variants seem to have coverages that are good enough to successfully call variants — and, genotypes are called at these sites as hom refs (`0/0`) when we run these ***same samples*** through the ***same pipeline*** (WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7)) ***without the reblocking step***. . It also seems as if we lose the PL field for these variants when working with reblocked gvcfs (which could explain why GenotypeGVCF isn’t giving us calls for these variants). I've heard that support for hom-refs with no PLs was implemented in CombineGVCFs as of Sept 2021, but I'm still seeing the issue with CombineGVCFs 4.3.0.0. To provide more info:. - We are seeing these issues regardless of if reblocked gvcfs are analyzed together with or separate from non-reblocked gvcfs. (For reference, the downstream steps in our pipeline are GenomicsDBImport & GenotypeGVCFs, but we’re seeing the same results with CombineGVCFs & GenotypeGVCFs on a smaller set of test gvcfs.); - I have a test set of samples that I've run with and without ReblockGVCF, and have used CombineGVCFs 4.3.0.0 & GenotypeGVCFs 4.3.0.0, and we're still seeing this issue.; - I have rerun ReblockGVCF including the `--allow-missing-home-ref-data` and `--all-site-pls` flags, but neither of these seem to solve the issue either. . #### Steps to reproduce. Run WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline. With the relocked gvcfs, run CombineGVCFs, then GenotypeGVCFs. ; Running WARP's [ExomeGermlineSingleSample 3.1.7](https://github.com/broadinstitute/warp/releases/tag/ExomeGermlineSingleSample_v3.1.7) pipeline ***but skipping the reblocking step*** and running CombineGVCFs and GenotypeGVCFs results in these same variants being called as hom-ref (which makes me think that reblocking is messing these up someh",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8208:1469,pipeline,pipeline,1469,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8208,1,['pipeline'],['pipeline']
Deployability,"staFile(BwaMemIndex.java:227); at org.broadinstitute.hellbender.utils.bwa.BwaMemIndex.createIndexImageFromFastaFile(BwaMemIndex.java:196); at org.broadinstitute.hellbender.BwaMemIntegrationTest.loadIndex(BwaMemIntegrationTest.java:49); Running Test: Test method testChimericUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testChimericUnpairedMapping SKIPPED; Running Test: Test method testPerfectUnpairedMapping(org.broadinstitute.hellbender.BwaMemIntegrationTest). Gradle suite > Gradle test > org.broadinstitute.hellbender.BwaMemIntegrationTest > testPerfectUnpairedMapping SKIPPED; ```. This test fails because some JAR wasn't built:; ```; Running Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest); Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: No local jar was found, please build one by running. Gradle suite > Gradle test > org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest > testPipeForPicardTools STANDARD_ERROR; No local jar was found, please build one by running; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err:. Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar. /disk-samsung/ports/biology/gatk/work/gatk-4.6.0.0/gradlew localJar; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: or. or; Test: Test method testPipeForPicardTools(org.broadinstitute.hellbender.engine.PipelineSupportIntegrationTest) produced standard out/err: export GATK_LOCAL_JAR=<path_to_local_jar>. export GATK_LOC",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8940:3439,Pipeline,PipelineSupportIntegrationTest,3439,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8940,1,['Pipeline'],['PipelineSupportIntegrationTest']
Deployability,"ster branch as of 8/3/18. ### Description ; I am running UpdateVCFSequenceDictionary on a vcf which should have a hg38 header but which (for unrelated unpleasant reasons) instead has a hg19 header. Using an hg38 dictionary as source dict to try to fix the header. If I ask to output a .vcf file, everything works fine. If I ask to output a .vcf.gz file, gatk crashes with; ```; java.lang.ArrayIndexOutOfBoundsException: 12922; 	at htsjdk.samtools.BinningIndexBuilder.processFeature(BinningIndexBuilder.java:89); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); 	at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); 	at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:146); 	at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:212); 	at org.broadinstitute.hellbender.tools.walkers.variantutils.UpdateVCFSequenceDictionary.closeTool(UpdateVCFSequenceDictionary.java:174); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:983); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:137); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:182); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:201); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); ``` . #### Steps to reproduce; Works: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/ckachulis/UpdateVCFSequenceDictionary_Bug/na12878_hg38_giab_pg_hybrid_happy.vcf.gz -O corrected.dictionary.vcf --source-dictionary /seq/references/Homo_sapiens_assembly38/v0/Homo_sapiens_assembly38.dict --replace ``. Crashes: ``gatk UpdateVCFSequenceDictionary -V /dsde/working/cka",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5087:1111,Update,UpdateVCFSequenceDictionary,1111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5087,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,"ster spark://nsnode11:6311 --driver-java-options -Dsamjdk.use_async_io_read_samtools=false,-Dsamjdk.use_async_io_write_samtools=true,-Dsamjdk.use_async_io_write_tribble=false,-Dsamjdk.compression_level=1 --conf spark.io.compression.codec=snappy --conf spark.yarn.executor.memoryOverhead=6000 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.userClassPathFirst=true --conf spark.driver.maxResultSize=0 --conf spark.executor.cores=1024 --conf spark.reducer.maxSizeInFlight=100m --conf spark.shuffle.file.buffer=512k --conf spark.akka.frameSize=512 --conf spark.akka.threads=10 --conf spark.executor.memory=50g --conf spark.driver.memory=150g --conf spark.local.dir=/gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2Spark/1024cores/tmp --class org.broadinstitute.hellbender.Main /gpfs/software/genomics/GATK/4b.2/gatk/build/libs/hellbender-spark.jar HaplotypeCaller --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2/bam//NA12892.recal.bam --dbsnp /gpfs/projects/NAGA/naga/SparkTest/SPARKCALLER/REF/dbsnp_138.vcf --emitRefConfidence GVCF --readValidationStringency LENIENT --nativePairHmmThreads 1024 --createOutputVariantIndex true --output NA12892.raw.snps.indels.g.vcf; [August 9, 2017 10:13:02 AM AST] HaplotypeCaller --nativePairHmmThreads 1024 --dbsnp /gpfs/projects/NAGA/naga/SparkTest/SPARKCALLER/REF/dbsnp_138.vcf --emitRefConfidence GVCF --output NA12892.raw.snps.indels.g.vcf --input /gpfs/projects/NAGA/naga/NGS/pipeline/GATK_Best_Practices/GATK4b2/bam//NA12892.recal.bam --readValidationStringency LENIENT --reference /gpfs/data_jrnas1/ref_data/Hsapiens/hs37d5/hs37d5.fa --createOutputVariantIndex true --group StandardAnnotation --group StandardHCAnnotation --GVCFGQBands 1 --GVCFGQBands 2 --GVCFGQBands 3 --GVCFGQBands 4 --GVCFGQBands 5 --GVCFGQBands 6 --GVCFGQBands 7 --GVCFGQBands 8 --GVCFGQBands 9 --GVCFGQBands 10 --GVCFGQBands 11 --GVCFGQBands 12 --GVCFGQBands 13 -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3631:3033,pipeline,pipeline,3033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3631,1,['pipeline'],['pipeline']
Deployability,"stitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); ```. #### Steps to reproduce. This is test script (test.sh) that is used.; ```; module load gatk; CRAM=$1; SAMPLE=$(basename $CRAM); SAMPLE=${SAMPLE/\.cram/}; mkdir -p gvcf.STR/$SAMPLE; mkdir -p gvcf.STR/$SAMPLE/tmp; gatk --java-options ""-Xmx16G"" ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/$SAMPLE/$SAMPLE.STR.table -I $CRAM; gatk --java-options ""-Xmx16G"" CalibrateDragstrModel -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --str-table-path gvcf.STR/$SAMPLE/$SAMPLE.STR.table -O gvcf.STR/$SAMPLE/$SAMPLE.Dragstr.model -I $CRAM. ```; The script runs the ComposeSTRTableFile to produce the table that is then read by CalibrateDragstrModel. ; ```; ./test.sh /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx16G -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ComposeSTRTableFile -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -O gvcf.STR/ADNI_002_S_0413.hg38.realign.bqsr/ADNI_002_S_0413.hg38.realign.bqsr.STR.table -I /restricted/projectnb/casa/wgs.hg38/adni/cram/ADNI_002_S_0413.hg38.realign.bqsr.cram; 13:44:55.228 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Apr 04, 2021 1:44:55 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 13:44:55.4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7182:3483,install,install,3483,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7182,1,['install'],['install']
Deployability,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5644:2422,deploy,deploy,2422,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644,6,['deploy'],['deploy']
Deployability,successful integration run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/802f9314-2b8b-4007-9c8d-6832a5687f22),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8770:11,integrat,integration,11,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8770,1,['integrat'],['integration']
Deployability,"successful run:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. failing run with the bug:; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/f1c952fc-7f05-4468-ae20-1c1cc5b9bf38. AC is:. Cohort builder subcohort extract in AoU and our extract workflow work with both VETS and VQSR callsets, including past callsets. (Note--I did not test in AoU, just on quickstart since the issue doesn't seem to be permission or scale related--see failure reproduced above). Full extract with past callset & VQSR; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Quickstart%20v3%20cremer/job_history/649ee4c9-1afc-473b-b460-2fc88d5f49d4. Subcohort extract with past callset & VQSR. Full extract with new callset & VETS; https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/50ef3073-f618-42ee-b207-73712a783a8a; (note this failed but only on one of the 4 runs and it's based on query cost). <img width=""1202"" alt=""Screenshot 2023-08-25 at 1 22 58 PM"" src=""https://github.com/broadinstitute/gatk/assets/6863459/39468ed8-fe2b-4bf8-9326-3bfcf6dabbb1"">. Kevin is able to run latest extract on Delta (still waiting on Kevin, but otherwise the above are all set). note that there was briefly no ""score"" col but I dont _think_ we need to be backwards compatible for that as there was no release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8488:1393,release,release,1393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8488,1,['release'],['release']
Deployability,t -- --sparkRunner GCS --cluster dataproc-cluster-3 --project broad-dsde-dev; ```. fails with . ```; 16/04/27 18:49:12 ERROR org.apache.spark.SparkContext: Error initializing SparkContext.; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57); at org.apache.spark.scheduler.Tas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:1120,deploy,deploy,1120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,"t convergence to 1% was achieved after about 250 iterations. I also did not initialize with PCA. However, upping to T = 10^6 causes out of memory. Not sure if this could be naively alleviated by setting theano flags appropriately, but I think we will probably want to minibatch in T instead. Note also that this model uses the exact Poisson likelihood. Composing with an HMM segmentation step, perhaps alternating for a few iterations, would give the gCNV PoN without the Gaussian approximation we use. ---. @samuelklee commented on [Wed May 17 2017](https://github.com/broadinstitute/gatk-protected/issues/1038#issuecomment-302234920). The same run of T = 10^5 and N = 100 took <4 minutes on the gsa5 Tesla K40c GPU---about a 3x speedup over my home CPU. A slightly larger run of T = 1.5 * 10^5 and N = 200 took 10 minutes and 6GB of the GPU's 12GB memory. (I did start running into some weird theano/pymc3 errors when I tried to go bigger, unfortunately.) Moving to the GPU does require a bit of extra configuration but is relatively trivial. The real business goes down in exactly 11 lines of code, which cleanly specify the gCNV probabilistic model for read counts:. ```; with pm.Model() as model:; alpha_u = Uniform(name='alpha_u', lower=alpha_min, upper=alpha_max, shape=D); m_t = Uniform(name='m_t', lower=m_min, upper=m_max, shape=T); psi_t = Uniform(name='psi_t', lower=psi_min, upper=psi_max, shape=T); depth_s = Uniform(name='depth_s', lower=depth_min, upper=depth_max, shape=N); ; z_su = Normal(name='z_us', mu=0., sd=1., shape=(N, D)); W_tu = Normal(name='W_tu', mu=0., sd=1. / sqrt(alpha_u), shape=(T, D)); mu_st = Deterministic(name='mu_st', var=z_su.dot(W_tu.T) + m_t); b_st = Normal(name='b_st', mu=mu_st, sd=sqrt(psi_t), shape=(N, T)); n_ts = Poisson(name='n_ts', mu=depth_s * exp(b_st).T, observed=n_ts_data); ; fit_pm = pm.variational.advi(model=model, n=num_iterations, learning_rate=learning_rate, random_seed=random_seed, eval_elbo=eval_elbo_iterations); ```. @eitanbanks @droa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2984:2178,configurat,configuration,2178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2984,1,['configurat'],['configuration']
Deployability,"t makes a BAM file that contains :; @PG ID:HalpotypeBAMWriter; (note the typo); If then FilterAlignmentArtifacts is run with this file as input and BAM output is asked, it says; java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!; and does not create the file. Command Used:; gatk Mutect2 --input normal.recalibrated.vcf --input tumor.recalibrated.vcf -normal mormal -tumor tumor --reference /data/Homo_sapiens.UCSC.hg38.fa --output tumor.recalibrated.mutect2/vcf --f1r2-tar-gz f1r2.tar.gz --native-pair-hmm-threads 4 --bam-output tumor.recalibrated.realigned.bam --add-output-sam-program-record false -bam-output. The log of the command that generated the error was :. Using GATK jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar. Running:. java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar FilterAlignmentArtifacts --variant tumor.recalibrated.filtered.vcf --input tumor.recalibrated.realigned.bam --reference /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/genome/Homo_sapiens.UCSC.hg38.fa --bwa-mem-index-image /data/genepattern/users/.cache/uploads/cache/data.gp.vib.be/pub/bwa_index_img/Homo_sapiens.UCSC.hg38.img --output tumor.recalibrated.filtered2.vcf --bam-output tumor.recalibrated.realigned2.bam --verbosity ERROR --tmp-dir TMP --QUIET true. 14:38:44.077 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/data/genepattern/patches/gatk-4.1.4.0/gatk-package-4.1.4.0-local.jar!/com/intel/gkl/native/libgkl_utils.so. 14:38:44.103 INFO SmithWatermanAligner - AVX accelerated SmithWaterman implementation is not supported, falling back to the Java implementation. java.lang.IllegalArgumentException: Program record with group id HalpotypeBAMWriter already exists in SAMFileHeader!. at ht",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6287:1127,patch,patches,1127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6287,1,['patch'],['patches']
Deployability,t org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry$CompositeProvider.stop(DefaultServiceRegistry.java:920); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.internal.service.DefaultServiceRegistry.close(DefaultServiceRegistry.java:326); at org.gradle.internal.concurrent.CompositeStoppable$2.stop(CompositeStoppable.java:83); at org.gradle.internal.concurrent.CompositeStoppable.stop(CompositeStoppable.java:98); at org.gradle.initialization.DefaultGradleLauncher.stop(DefaultGradleLauncher.java:199); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:46); at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:28); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:77); at org.gradle.launcher.exec.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:47); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:51); at org.gradle.launcher.exec.DaemonUsageSuggestingBuildActionExecuter.execute(DaemonUsageSuggestingBuildActionExecuter.java:28); at org.gradle.launcher.cli.RunBuildAction.run(RunBuildAction.java:43); at org.gradle.internal.Actions$RunnableActionAdapter.execute(Actions.java:170); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:237); at org.gradle.launcher.cli.CommandLineActionFactory$ParseAndBuildAction.execute(CommandLineActionFactory.java:210); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:35); at org.gradle.launcher.cli.JavaRuntimeValidationAction.execute(JavaRuntimeValidationAction.java:24); at org.gradle.launcher.cli.CommandLineActionFactory$WithLogging.execute(CommandLineActionFactory,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1364:4899,Continuous,ContinuousBuildActionExecuter,4899,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1364,3,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,"t-280806711). I agree with the idea. the devil, as always is in the details. - I think that the model you are interested in is parametrized with; insert-length and position (of bait) in insert, not bait-length and insert; length. Bait length is going to be a constant, or almost so, I suspect; (please check); - I suspect that of these two covariates, position from the (nearest) end; will be the most informative and will probably decay to a constant with a; decay parameter of the order of the bait length.; - the effect of GC bias will effect the overall efficacy of the bait since; it will effect the entire insert when amplified, and so it will introduce; noise that is orthogonal to the question you are after. I would design the; measurement to be as unaffected by this noise as possible. your later model; will find this constant of course. happy to talk about this more!. Yossi. On Fri, Feb 17, 2017 at 6:57 PM, Mehrtash Babadi <notifications@github.com>; wrote:. > The current coverage tool in the GATK CNV pipeline, i.e.; > CalculateTargetCoverage has important caveats that render the ab-initio; > modeling of coverage and subsequent CNV analysis inaccurate. The purpose of; > this issue is to write a new tool for dealing with targetted sequencing; > data. Since a major source of coverage variance in targetted sequencing is; > different capture efficiencies, the most reasonable read-depth calculation; > scheme is to associate *inserts to baits* rather than *single reads to; > targets*.; >; > There is a subtle problem, though: inserts often overlap with more than; > one bait. In such cases, we need to have a model for estimating the; > probability that the insert is captured by either of the overlapping baits.; > The modeling can be done in the following semi-empirical fashion (thanks; > @yfarjoun <https://github.com/yfarjoun>), which needs to be done only; > once each capture technology (Agilent, ICE):; >; > - We locate isolated baits (i.e. those that are separated from one",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2947:3585,pipeline,pipeline,3585,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2947,1,['pipeline'],['pipeline']
Deployability,"t.main(SparkSubmit.scala); 18:49:12.567 INFO PrintReadsSpark - Shutting down engine; [April 27, 2016 6:49:12 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=3858759680; java.io.FileNotFoundException: File file:/Users/louisb/Workspace/gatk-protected/build/libIntelDeflater.so does not exist; at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:609); at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:822); at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:599); at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337); at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289); at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:317); at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:407); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:471); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6$$anonfun$apply$3.apply(Client.scala:470); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:470); at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$6.apply(Client.scala:468); at scala.collection.immutable.List.foreach(List.scala:318); at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:468); at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:727); at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:142); at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnCli",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1780:6912,deploy,deploy,6912,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1780,1,['deploy'],['deploy']
Deployability,t.scala:292); 	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:127); 	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88); 	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); 	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62); 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1482); 	at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:650); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.assemblyRegionEvaluatorSupplierBroadcastFunction(HaplotypeCallerSpark.java:265); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.assemblyRegionEvaluatorSupplierBroadcast(HaplotypeCallerSpark.java:245); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:303); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delega,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6091:2879,pipeline,pipelines,2879,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091,1,['pipeline'],['pipelines']
Deployability,t/resources/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/vcfexample2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/vcfMask.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/ad-bug-input.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/CEUTrio.20.21.missingIndel.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/chr21.bad.pl.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combined_genotype_gvcf_exception.nocall.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combined_genotype_gvcf_exception.original.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.3.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/gvcf.basepairResolution.gvcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/gvcfExample1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/leadingDeletion.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.combined.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.delOnly.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.depr.delOnly.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/testUpdatePGT.gvcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/MarkDuplicatesGATK/example.chr1.1-1K.markedDups.bam.bai; s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:57087,pipeline,pipeline,57087,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['pipeline'],['pipeline']
Deployability,"ta-sources-path /rsrch5/home/tdccct/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s \; --output /rsrch5/home/tdccct/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf \; --output-file-format VCF; ; I get the following error:; ; Using GATK jar /gatk/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.4.0.0-local.jar Funcotator --variant /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_filtered.vcf.gz --reference /home/ppshah/shared/gencode/Homo_sapiens/GATK/GRCh38/Sequence/WholeGenomeFasta/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /home/ppshah/shared/pipelines/mutect/funcotator_dataSources.v1.7.20200521s --output /home/ppshah/shared/CAS_MOSAIC/mutect/mrn_2507919/WES/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC/KShaw-ROPR0004-DNA-229761-WX01-T_HMCKJDSX2-4-ATTGGCTC_funcotated.vcf --output-file-format VCF; 16:36:22.352 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.4.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 16:36:22.392 INFO Funcotator - ------------------------------------------------------------; 16:36:22.396 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.4.0.0; 16:36:22.396 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 16:36:22.396 INFO Funcotator - Executing as ppshah@ldragon1 on Linux v3.10.0-1160.15.2.el7.x86_64 amd64; 16:36:22.396 INFO Funcotator - Java runtime: OpenJDK 64-Bit Server VM v17.0.6+10-Ubuntu-0ubuntu118.04.1; 16:36:22.396 INFO Funcotator - Start Date/Time: January 10, 2024 a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8647:1462,pipeline,pipelines,1462,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8647,1,['pipeline'],['pipelines']
Deployability,"table code path: scanning neighbor chimeric alignment pairs of a contig iteratively and outputs inversion breakpoints as symbolic variant `<INV>`, annotated with `INV55` and `INV33` for signaling if it is the left or right breakpoint of the assumed inversion.; * the experimental code path that separates the alignment pre-processing step from the inference step, and studying the alignments in whole; this code path, in addition to outputting insertion, deletion and small duplication calls as does the stable path, outputs ; * BND records representing assembled breakpoints for which type could not be completely determined using only the contig alignments; this includes supposedly inversion breakpoints; * complex (`<CPX>`) variants from assembly contigs with more than 2 alignments; ; The tool proposed in this PR is based on [manual review](https://github.com/broadinstitute/dsde-methods-sv/tree/sh_inv_filter_init/docs/knowledgeBase/variantReview/inversion/chm) of a callset generated a long time ago (but still useful for studying filtering inversion breakpoints), and is designed to be integrated with the experimental code path. ### proposed algo. #### input:; * the ""INV55/INV33""-annotated `BND` records output by the upstream experimental code path; * BND's have related concepts of `MATE` and `PARTNER` (see figure below, left); * `MATE`: novel adjacency, i.e. contiguity on sample that is absent on reference (e.g. mobile element insertions, deletions); * `PARTNER`: novel disruption, i.e. contiguity on reference disrupted on sample (e.g. insertions, deletions). ![inversion_demo](https://user-images.githubusercontent.com/16310888/40271739-6d999b30-5b6f-11e8-86db-78fa11db4305.png). * complex variants detected by the upstream experimental code path; the reason is that sometimes inversion calls are incorporated as part of a larger, more complex event and the logic implemented in the upstream code, theoretically, allows for arbitrarily complex rearrangement; shown above on the rig",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4789:1434,integrat,integrated,1434,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4789,1,['integrat'],['integrated']
Deployability,"tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215801993). To clarify my position though - I think we should just never need it and simply coordinate between the various tool teams on a common release schedule. The toolkit would then be released because all tool are ready. ---. @droazen commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215816252). @akiezun We should strive for this, but in practice there will be times when Lee needs a release and we're not ready for one, and we need to have a plan in place to deal with that scenario. Since opt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:5405,release,release,5405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,3,['release'],"['release', 'releases']"
Deployability,"take advantage of fixed version of SplitIntervals (#7566); - Document AoU-specific tieout [VS-233] (#7552); - bad param assignment in aou reblocking (#7572); - Small fixes to ImportGenomes (non-write api version) (#7574); - Ah change output of reblocking wdl to external path (#7575); - close BQ Readers (#7583); - Ah spike writeapi (#7530); - bump WDL jar (#7593); - read api bytes logging, upgrade bigquery client versions (#7601); - bump (#7610); - upgrade log4j to 2.17 (#7616); - Add drop_state default of Forty to extract (#7619); - Kc fix type (#7620); - VAT cleanup and documentation (#7531); - fix empty flush (#7627); - presorted avro files, fix performance issue (#7635); - WIP extract for ranges (#7640); - VS-268 import more samples at once (#7629); - clustering vqsr tables by location (#7656); - First Version of a weight-based splitter (#7643); - Update GvsExtractCallset.wdl; - Quoting of table names (#7666); - docs for analysis of shard runtimes for balanced sharding (#7645); - Wire through GvsExtractCohortFromSampleNames with new prepare/extract [VS-283] (#7654); - Update GvsExtractCallset.wdl (#7678); - cherry pick lb_lfs_force change (#7683); - Tweak ingest messaging and failure mode [VS-267] (#7680); - Additional tweaks for GvsExtractCohortFromSampleNames [VS-283] (#7698); - VS-280 Create a VAT intermediary (#7657); - There something about split intervals [VS-306] (#7694); - VS 284 Add prepare step to Quick Start (#7685); - VS-222 dont hard code the dataset name! (#7704); - fixed bug; added tests (#7717); - Clean up optional and inconsistently named inputs [VS-294] [VS-218] (#7715); - VS-263 notes on ingest and beyond (#7618); - Add task to ExtractCallset that verifies filter_set_name exists in GVS dataset [VS-335] (#7734); - Clean up input json files to reflect changes inputs [VS-337] (#7733); - used constants; implemented non-AS transformation (#7718); - Pass dataset name to gatk ExtractFeatures (#7735); - Add withdrawn and is_control columns [VS-70] [VS-2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:21297,Update,Update,21297,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Update'],['Update']
Deployability,"tart Date/Time: August 23, 2021 12:52:14 PM SGT ; ; 12:52:16.268 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.268 INFO GenotypeGVCFs - ------------------------------------------------------------ ; ; 12:52:16.268 INFO GenotypeGVCFs - HTSJDK Version: 2.14.3 ; ; 12:52:16.269 INFO GenotypeGVCFs - Picard Version: 2.17.2 ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION\_LEVEL : 2 ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.USE\_ASYNC\_IO\_READ\_FOR\_SAMTOOLS : false ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_SAMTOOLS : true ; ; 12:52:16.269 INFO GenotypeGVCFs - HTSJDK Defaults.USE\_ASYNC\_IO\_WRITE\_FOR\_TRIBBLE : false ; ; 12:52:16.269 INFO GenotypeGVCFs - Deflater: IntelDeflater ; ; 12:52:16.269 INFO GenotypeGVCFs - Inflater: IntelInflater ; ; 12:52:16.269 INFO GenotypeGVCFs - GCS max retries/reopens: 20 ; ; 12:52:16.269 INFO GenotypeGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from [https://github.com/droazen/google-cloud-java/tree/dr\\\_all\\\_nio\\\_fixes](https://github.com/droazen/google-cloud-java/tree/dr\_all\_nio\_fixes) ; ; 12:52:16.269 INFO GenotypeGVCFs - Initializing engine ; ; terminate called after throwing an instance of 'VariantQueryProcessorException' ; ; what(): VariantQueryProcessorException : Could not open array genomicsdb\_array at workspace: /home/WES-VCFQC/S2\_GenomicsDBImport/temporary/tmp4. Hi, I used GenomicsDBImport to combined 2000 GVCFs. To speed up, I split the bed file and concatenated multiple intervals into a contig. I also met the file locking problem which can be solved by setting  TILEDB\_DISABLE\_FILE\_LOCKING=1 in my Linux system. Currently, I experience some issues with GenotypeGVCFs in GATK version 4.0.3.0. It cannot open ""genomicsdb\_array"" although the directory of genomicsdb\_array does exist. I found someone else has reported this issue here: [https://sites.google.com/a/broadins",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7442:4208,patch,patch,4208,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7442,1,['patch'],['patch']
Deployability,"te.hellbender.engine.filters]; 23:43:52.475 DEBUG ConfigFactory - 	annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:43:52.477 DEBUG ConfigFactory - 	cloudPrefetchBuffer = 40; 23:43:52.477 DEBUG ConfigFactory - 	cloudIndexPrefetchBuffer = -1; 23:43:52.477 DEBUG ConfigFactory - 	createOutputBamIndex = true; 23:43:52.477 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:43:52.477 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:43:52.477 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:43:52.477 INFO GermlineCNVCaller - Requester pays: disabled; 23:43:52.477 INFO GermlineCNVCaller - Initializing engine; 23:43:52.479 DEBUG ScriptExecutor - Executing:; 23:43:52.479 DEBUG ScriptExecutor - python; 23:43:52.479 DEBUG ScriptExecutor - -c; 23:43:52.480 DEBUG ScriptExecutor - import gcnvkernel. INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '11848' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.2009-Core-x86_64-3.6.10-64/lock_dir; INFO (theano.gof.compilelock): Waiting for existing lock by process '18570' (I am process '19216'); INFO (theano.gof.compilelock): To manually release the lock, delete /gpfs/hpc/home/lijc/xiangxud/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-centos-7.9.200",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8938:5058,release,release,5058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8938,1,['release'],['release']
Deployability,"ted on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215219239). I'm still looking for the smoking gun where a query fails using a .crai, but I haven't found one yet; but the BAMIndex metadata is cerrtainly wrong after conversion from .crai. If we do decide to turn off .crai, we should do it in htsjdk. To make a .bai, just use GATK PrintReads to create the .cram. ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220550). still super slow using .bai : 3:51 minutes. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215220983). @akiezun Can you try increasing the -Xmx value to something ridiculous (like 32G) just to eliminate memory usage as a variable here?. ---. @droazen commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221087). (and run on a machine with large memory like gsa6). ---. @akiezun commented on [Wed Apr 27 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215221410). Let's collect problems first, then (tomorrow maybe) go over those discovered and make a list of showstoppers for alpha1. ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469203). This and https://github.com/broadinstitute/gatk/issues/1787 imply that there might have been a CRAM performance regression in htsjdk recently -- we should test with a bunch of GATK revisions from before each successive htsjdk update to see if there was one that killed CRAM performance. I don't recall seeing a big BAM vs. CRAM performance difference when we first hooked up CRAM support to GATK... ---. @droazen commented on [Thu Apr 28 2016](https://github.com/broadinstitute/gatk-protected/issues/467#issuecomment-215469348). @cmnbroad Could you have a look at this when you have time?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2850:3715,update,update,3715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2850,1,['update'],['update']
Deployability,"tensor import blas; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas.py"", line 155, in <module>; from theano.tensor.blas_headers import blas_header_text; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/tensor/blas_headers.py"", line 987, in <module>; if not config.blas.ldflags:; File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configparser.py"", line 332, in __get__; val_str = self.default(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1451, in default_blas_ldflags; check_mkl_openmp(); File ""/lustre04/scratch/helene/Ticket/0196857/ENV_python_3.6.10/lib/python3.6/site-packages/theano/configdefaults.py"", line 1273, in check_mkl_openmp; """"""); RuntimeError: ; Could not import 'mkl'. If you are using conda, update the numpy; packages to the latest build otherwise, set MKL_THREADING_LAYER=GNU in; your environment for MKL 2018. If you have MKL 2017 install and are not in a conda environment you; can set the Theano flag blas.check_openmp to False. Be warned that if; you set this flag and don't set the appropriate environment or make; sure you have the right version you *will* get wrong results. ----. Here is the pip list from my environment:. cached-property 1.5.2+computecanada ; cycler 0.11.0+computecanada ; enum34 1.1.10+computecanada ; gatkpythonpackages 0.1 ; gcnvkernel 0.8 ; h5py 3.1.0+computecanada ; intel-openmp 2021.1.1+computecanada; joblib 0.14.1+computecanada ; kiwisolver 1.3.1+computecanada ; matplotlib 3.3.4+computecanada ; mkl 2021.1.1+computecanada; numpy 1.17.3+computecanada ; pandas 1.0.3+computecanada ; patsy 0.5.3+computecanada ; Pillow 8.1.2+computecanada ; pip 20.0.2 ; pymc3 3.1 ; pyparsing 3.1.0 ; python-dateutil 2.8.2+computecanada ; pytz 2023.3+computecanada ; scipy 1.1.0+computecanada ; setuptools 46.1.3 ; six 1.1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8387:4730,update,update,4730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8387,1,['update'],['update']
Deployability,"ter false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [January 9, 2018 6:30:33 PM CST] Executing as sun@tele-1 on Linux 3.10.0-514.10.2.el7.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_91-b14; Version: 4.beta.5-50-g8d666b6-SNAPSHOT; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Deflater: IntelDeflater; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Inflater: IntelInflater; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - GCS max retries/reopens: 20; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Initializing engine; 18:30:54.424 INFO BwaAndMarkDuplicatesPipelineSpark - Done initializing engine; 18/01/09 18:30:54 INFO spark.SparkContext: Running Spark version 2.2.0.cloudera1; 18/01/09 18:30:54 INFO spark.SparkContext: Submitted application: BwaAndMarkDuplicatesPipelineSpark; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing view acls to: sun; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing modify acls to: sun; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing view acls groups to: ; 18/01/09 18:30:54 INFO spark.SecurityManager: Changing modify acls groups to: ; 18/01/09 18:30:54 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sun); groups with view permissions: Set(); users with modify permissions: Set(sun); groups wit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:4882,patch,patch,4882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['patch'],['patch']
Deployability,"ter for officially supported images.; > 2. However, gCNV relies on the PyMC3 package. PyMC3 3.1 is currently used in GATK master. 3.1 was released in 2017, not long before our release of gCNV in 2018, but it's very old now.; > 3. The latest version of Python that is supported by PyMC3 3.1 in conda is Python 3.6.; > 4. @asmirnov239 has a draft PR (#8094) that updates PyMC3 to 3.5 and Python to 3.7, which clearly still falls short of Python 3.10+. This PR also updated some gCNV code to make it compatible with PyMC3 3.5. (It also removed TensorFlow and added PyTorch.); > 5. @asmirnov239 also merged a PR that added tests for numerical reproducibility of GermlineCNVCaller in cohort mode in #7889.; > 6. The earliest version of PyMC that supports Python 3.10+ is PyMC 4, released in 2022.; > 7. However, PyMC 4 introduces API changes, which will also require additional gCNV code changes and numerical testing.; > 8. These API changes are because the underlying computational backend for PyMC was updated from Theano (think of this as an old alternative to TensorFlow) to Aesara.; > 9. Since then, PyMC 5.9 has been released and the underlying backend has been updated again, from Aesara to PyTensor.; > 10. So if we are going to update the environment to support Python 3.10+, it probably makes sense to go all the way to PyMC 5.9. I've made some strides in this PR; as of [6b08f3a](https://github.com/broadinstitute/gatk/pull/8561/commits/6b08f3af205cb9af1f5c63a0786f9a5a52cd78c1), I've made enough updates to accommodate API changes so that cohort-mode inference for both GermlineCNVCaller and DetermineGermlineContigPloidy runs successfully under Python 3.10 and PyMC 5.9.0---although note that 5.9.1 has been released in the interim!. However, our work has just begun. Results now produced in the numerical tests mentioned above are quite far off from the original expected results. It remains to be seen whether this is due to the randomness of inference, some slight changes to the model pri",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8561:1337,update,updated,1337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8561,1,['update'],['updated']
Deployability,testing nio upgrade,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8420:12,upgrade,upgrade,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8420,1,['upgrade'],['upgrade']
Deployability,"tests will now run as cloud, integration, and unit on travis; this reduces our wallclock time from 30ish -> 20ish minutes. cleaned up some wierdness in the way things were specified as well",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2399:29,integrat,integration,29,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2399,1,['integrat'],['integration']
Deployability,"the dev-oriented material such as coding conventions etc should be moved to a separate wiki page.; The main readme should have examples of how to install it, how to validate the installation and how run it locally, on spark cluster and on the cloud. Candidate for alpha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1049:146,install,install,146,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1049,2,['install'],"['install', 'installation']"
Deployability,"the google genomics API has deprecated all the features we were using,; this includes the reference lookup api, and the google Read data types. removing all google genomics related dependencies; * replacing com.google.cloud.genomics:gatk-tools-java:1.1 with gov.nist.math.jama:gov.nist.math.jama:1.1.1; 	we rely on this transitive dependency, making it a direct dependency instead; * remove com.google.apis:google-api-services-genomics:v1-rev527-1.22.0; * remove com.google.cloud.genomics:google-genomics-utils:v1-0.10. * delete ReferenceAPISource and tests; * delete GoogleGenomicsReadToGATKReadAdapter and tests; * delete CigarConversionUtils and tests. * update other classes to remove references to these types; * improve an error message",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4266:658,update,update,658,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4266,1,['update'],['update']
Deployability,the integration test (testBasic) must check the contents of the created file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/897:4,integrat,integration,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/897,1,['integrat'],['integration']
Deployability,"the main advantage of spark for processing is that we can do stuff in memory and so throughout **of the whole pipeline** should be higher than that of the string of Picard+GATK tools. The thing to do here compare is:; 1) take a bam file, run ReadsPipelineSpark on it, measure time (as function of # nodes, start with 1); 2) take same bam file, run MarkDuplicates, BaseRecalibrator, PrintReads in order, measure time. The bam file should be non-trivial, at least 30GB. The goal for beta should be to beat non-spark on 1 node. It may already be true for alpha.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1196:110,pipeline,pipeline,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1196,1,['pipeline'],['pipeline']
Deployability,"the mean-field decoupling of the two chains yields two independent Markov chains with effective emission, transition, and prior probabilities, all of which must be self-consistency determined. The internal admixing rate would be used to admix the old and new self-consistent fields across the two chains in order to dampen oscillations and improve convergence properties. Once internal convergence is achieved, the converged posteriors must be saved to a workspace in order to be consumed by the continuous sub-model. The new internally converged posteriors will be admixed with the old internally converged posteriors from the previous epoch with the _external_ admixing rate. - Introduced two-stage inference for cohort denoising and calling. In the first (""warm-up"") stage, discrete variables are marginalized out, yielding an effective continuous-only model. The warm-up stage calculates continuous posteriors based on the marginalized model. Once convergence is achieved, continuous and discrete variables are decoupled for the second (""main"") stage. The second stage starts with a discrete calling step (crucial), using continuous posteriors from the warm-up stage as the starting point. The motivation behind the two-stage inference strategy is to avoid getting trapped in spurious local minima that are potentially introduced by mean-field decoupling of discrete and continuous RVs. Note that mean-field decoupling has a tendency to stabilize local minima, most of which will disappear or turn into saddle points once correlations are taken into account. While the marginalized model is free of such spurious local minima, it does not yield discrete posteriors in a tractable way; hence, the necessity of ultimately decoupling in the ""main"" stage. - Capped phred-scaled qualities to maximum values permitted by machine precision in order to avoid NaNs and overflows. - Took a first step toward tracking and logging parameters during inference, starting with the ELBO history. In the future, it",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4720:1412,continuous,continuous,1412,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4720,1,['continuous'],['continuous']
Deployability,the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delega,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5644:1309,pipeline,pipelines,1309,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644,1,['pipeline'],['pipelines']
Deployability,the requirement is to make MD fully work in a tested way (all Picard integration tests must work - perhaps by comparing the sets of reads that got marked as 'duplicate'). Note: we'll migrate this code from genomics-pipeline and adapt it to our needs and style.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/488:69,integrat,integration,69,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/488,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,the requirement is to port DepthOfCoverage or write a new tool that collects coverage information per base (primarily for WGS) and stats as DoC does. Integration tests also need to be ported or created. Current test data is broad-internal but we should move to using public data.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/617:150,Integrat,Integration,150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/617,1,['Integrat'],['Integration']
Deployability,"the requirement is to port the VariantEval walker and all tests. For now, the combinatorial nature of the eval is to be ported. (later on we may split it into multiple tools and a pipeline)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/616:180,pipeline,pipeline,180,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/616,1,['pipeline'],['pipeline']
Deployability,the tests written by David A a while back have not been run or updated and they fail (we now compare more stringently so maybe that's why). The ticket is to figure out why and fix if possible. Depends on code changes in https://github.com/broadinstitute/gatk/pull/1921,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1922:63,update,updated,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1922,1,['update'],['updated']
Deployability,"there are comments like:; "" \* java -Xmx4g -jar GenomeAnalysisTK.jar \; - -T BaseRecalibrator \; - -I my_reads.bam \; - -R resources/Homo_sapiens_assembly18.fasta \; - -knownSites bundle/hg18/dbsnp_132.hg18.vcf \; - -knownSites another/optional/setOfSitesToMask.vcf \; - -o recal_data.table"". which are wrong. There needs to be a pass over all usage examples to update them once we settle on the usage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/182:362,update,update,362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/182,1,['update'],['update']
Deployability,there was a code path that didn't get exercised in integration tests or quickstart data (writeMissingIntervals) that wasn't made aware of the storeCompressedReferences flag. Updated to operate correctly in its presence,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8556:51,integrat,integration,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8556,2,"['Update', 'integrat']","['Updated', 'integration']"
Deployability,this PR puts together BWA and MarkDuplicates. This is a prelude to BWA+MD+BQSR (but simpler because BQSR requires a 2bit reference and bwa wants a fasta reference). It extracts the core BWA/Spark code into a BwaSparkEngine and call that from both BwaSpark and the new pipeline. . It also improves the handling of sorting order for spark writing - adds a way to sort by queryname (relevant for mark duplicates). @tomwhite can you review?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1927:268,pipeline,pipeline,268,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1927,1,['pipeline'],['pipeline']
Deployability,this allows us to also remove the cloudera artifactory repo which will fix #610. removing some traces of gradle 2.2.1 from our build script and rerunning gradle wrapper to generate an updated wrapper,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1023:184,update,updated,184,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1023,1,['update'],['updated']
Deployability,this commandline . ```; java -jar ~/bin/GenomeAnalysisTK-3.4-46/GenomeAnalysisTK.jar -T BaseRecalibrator -R src/test/resources/large/human_g1k_v37.20.21.fasta -I CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam --out gatk3.4-46.recal.txt --knownSites src/test/resources/large/dbsnp_138.b37.20.21.vcf; ```. makes a different table than. ```; build/install/gatk/bin/gatk BaseRecalibrator -R src/test/resources/large/human_g1k_v37.20.21.fasta -I CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.bam --out gatk4.recal.txt --knownSites src/test/resources/large/dbsnp_138.b37.20.21.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1030:345,install,install,345,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1030,1,['install'],['install']
Deployability,this happens on the branch for https://github.com/broadinstitute/gatk/pull/1630 (which uses async IO for tests to mimic non-test usage). This bug is either due to or exposed by asynchronous tribble reading. more logs https://travis-ci.org/broadinstitute/gatk/jobs/118507152. test results; https://storage.googleapis.com/hellbender/test/build_reports/5109.2/tests/classes/org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.html#testClusteredSnps. ```; java.lang.RuntimeException: htsjdk.tribble.TribbleException: Exception encountered in worker thread.; at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:153); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:108); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltrationIntegrationTest.testClusteredSnps(VariantFiltrationIntegrationTest.java:36); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunner.runTest(SuiteRunner.java:357); at org.testng.SuiteRunner.runSequentiall,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638:631,Integrat,IntegrationTestSpec,631,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638,6,['Integrat'],['IntegrationTestSpec']
Deployability,"this is a script which can be used after running gradle installDist to run spark jobs; it can be used identically to ths build/install/bin/gatk script, but has extra features for dealing with spark. running a spark tool and supplying the option --sparkTarget with LOCAL, CLUSTER, or GCS has special behavior; LOCAL will run the tool in the in memory spark runner; CLUSTER along with an appropriate --sparkMaster will run on an accessible spark cluster using spark-submit; arguments to spark-submit may be specified before the arguments to GATK by separating them with a --; GCS will submit jobs to google dataproc using gcloud; common arguments for spark submit will be adapted to match the gcloud formating; this will fail if gcloud isn't installed. if GATK_GCS_STAGING is specified, the jar will be uploaded and cached in the specified bucket for rapid re-use. input files will not be autouploaded to the cloud. --dry-run may be specified before the --, this will only print the commands that will be run instead of actually running them. Adding DataProcArgumentReplace simple tool to convert spark-submit args into gcloud args.; This conversion is not guarenteed to translate all spark command line options to matching gcloud ones.; If you find options that are not translated or are miss-translated please file an issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1211:56,install,installDist,56,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1211,3,['install'],"['install', 'installDist', 'installed']"
Deployability,"this is necessary for sonatype to accept our releases, I forgot to commit it last time i did a release",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1790:45,release,releases,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1790,2,['release'],"['release', 'releases']"
Deployability,"this is the initial port of the Allele Specific annotation for HaplotypeCaller. It mostly focuses on the GVCF mode (ie outputs the 'raw' data). I have a branch in protected https://github.com/broadinstitute/gatk-protected/tree/ak_haplotypecaller_allele_specific_annotations that uses those and I verified that the annotations are correctly output and their values are much closer that before to those from GATK3.5. I did not port any code related to combining the annotations in GenotypeGVCFs or CombinedGVCFs etc. Also, no code for VariantAnnotator or UnifiedGenotyper was ported - gatk4 does not have those tools right now. @droazen can you review? Sorry, this is a whole bunch of code and it's not the final version yet (in particular, little effort was put into redesigning the framework - that will wait until we have integration tests so we can keep the results stable while improving design and code). We also need to add tickets to:; - turn dithering off/on in RankSum tests (it's always off for now to simplify testing); - use AlleleSpecific annotations in the VCF mode; - (later) port code for combining annotations",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1825:823,integrat,integration,823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1825,1,['integrat'],['integration']
Deployability,this should save a non-trivial amount of time in the docker builds; updating from 1.0 -> 1.1 which includes the necessary RScripts; moving scripts/install_R_packages.R -> scripts/docker/gatkbase/. don't install Rscripts during docker builds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3043:203,install,install,203,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3043,1,['install'],['install']
Deployability,this solves a nasty precision issue in the HaplotypeCaller integration tests where tests would pass or fail depending on the order in which they ran!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1764:59,integrat,integration,59,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1764,1,['integrat'],['integration']
Deployability,"this to be specified via the json, so I reverted this change.); - [x] Make simple improvements to ReCapSeg caller (#3825).; - [x] Review and merge modeling/WDL PR. (#3913 awaiting review. Note that this PR also deletes the old germline WDL.); - ~~Write MultidimensionalKernelSegmenterUnitTest.~~ (SL, punting, filed #3916); - ~~Write ModelSegmentsIntegrationTest.~~ (SL, punting, filed #3916); - [x] Preliminary PCAWG or HCC1143 purity evaluation. (@LeeTL1220) (LL, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (PR #4010 awaiting review.); - [x] Add SM tag and sequence dictionary headers to all appropriate files and sort accordingly. (SL, #3914 awaiting review); - [x] Update tutorial data. (@MartonKN); - [ ] (Reach) Add VCF output.; - [ ] (Reach) Add PG tags to all files.; - [ ] (Reach) Replace ReCapSeg caller with improved version. (@MartonKN). gCNV pipeline:; - [x] Review and merge Python code (#3838). (MB and SL, PR #3925 awaiting review.); - [x] CLI for ploidy determination (cohort). (@samuelklee); - [x] CLI for ploidy determination (case). (@samuelklee); - [x] CLI for calling (cohort). (@samuelklee); - [x] CLI for calling (case). (@samuelklee); - [ ] CLI for post-processing calls. (@asmirnov239) (AS, PR issued by 12/4); - [x] Python environment. (Update: I've verified that gCNV works on the gsa server with a manual setup of conda (python=3.6) + @mbabadi's pip install---although I do get an ""install mkl"" warning from theano. We can discuss autoloading of this environment after release, but should at least have some clear documentation.); - [x] WDL and Cromwell tests. (SL, PR issued by 12/1); - [x] Preliminary evaluation. (MB, should be done in time for @vdauwera to present at Broad retreat); - [x] Update docs/arguments (w/ Comms, see #3853). This will follow deletion of prototype tools. (all, PE #3925 awaiting review.). Miscellaneous:; - [x] Update Pr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3826:1802,pipeline,pipeline,1802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3826,1,['pipeline'],['pipeline']
Deployability,"tions ""-Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx8G -Djava.io.tmpdir=/data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/shell/temp -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; 00:09:41.541 INFO  NativeLibraryLoader - Loading libgkl\_compression.so from jar:file:/data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar!/com/intel/gkl/native/libgkl\_compression.so ; ; 00:09:41.554 WARN  NativeLibraryLoader - Unable to load libgkl\_compression.so from native/libgkl\_compression.so (No such file or directory) ; ; 00:09:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:1691,pipeline,pipeline,1691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"titute.hellbender.Main.mainEntry(Main.java:206); 	at org.broadinstitute.hellbender.Main.main(Main.java:292); Caused by: org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python -c import gcnvkernel. Stdout: ; Stderr: Traceback (most recent call last):; File ""<string>"", line 1, in <module>; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/gcnvkernel/__init__.py"", line 1, in <module>; from pymc3 import __version__ as pymc3_version; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/__init__.py"", line 5, in <module>; from .distributions import *; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/__init__.py"", line 1, in <module>; from . import timeseries; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/timeseries.py"", line 5, in <module>; from .continuous import get_tau_sd, Normal, Flat; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 12, in <module>; from scipy import stats; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/scipy/stats/__init__.py"", line 345, in <module>; from .morestats import *; File ""/home/gamer456148/anaconda3/envs/gatk/lib/python3.6/site-packages/scipy/stats/morestats.py"", line 12, in <module>; from numpy.testing.decorators import setastest; ModuleNotFoundError: No module named 'numpy.testing.decorators'. 	at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75); 	at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170); 	at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeCommand(PythonScriptExecutor.java:79); 	at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6467:5059,continuous,continuous,5059,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6467,1,['continuous'],['continuous']
Deployability,"titute.hellbender.engine.FeatureManager.<init>(FeatureManager.java:155) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.initializeFeatures(ReadWalker.java:72) ; ;     at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:726) ; ;     at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). However, the bug wasn't reported when I didn't assign the temp directory:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.test.table ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /dat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:14196,pipeline,pipeline,14196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"titute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:168) ; ; at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:139) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.executeSegmentGermlineCNVCallsPythonScript(PostprocessGermlineCNVCalls.java:739) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.generateSegmentsVCFFileFromAllShards(PostprocessGermlineCNVCalls.java:485) ; ; at org.broadinstitute.hellbender.tools.copynumber.PostprocessGermlineCNVCalls.onTraversalSuccess(PostprocessGermlineCNVCalls.java:456) ; ; at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1089) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ; at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ; at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ; at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ; at org.broadinstitute.hellbender.Main.main(Main.java:289) ; ; Using GATK jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar ; ; Running: ; ; java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -jar /home/yangyxt/software/gatk-4.2.2.0/gatk-package-4.2.2.0-local.jar Po. If not an error, choose a category for your question(REQUIRED): ; ; a)How do I (......)? ; ; b) What does (......) mean? ; ; c) Why do I see (......)? ; ; d) Where do I find (......)? ; ; e) Will (......) be in future releases?<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/181533'>Zendesk ticket #181533</a>)<br>gz#181533</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7444:8078,release,releases,8078,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7444,1,['release'],['releases']
Deployability,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Edit: This was posted accidentally while sw",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7313:1306,release,release,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7313,1,['release'],['release']
Deployability,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Please let us know when log4j Vulnerability",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7603:1306,release,release,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603,1,['release'],['release']
Deployability,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----; I am getting error while implementing docke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5906:1306,release,release,1306,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906,1,['release'],['release']
Deployability,"to the “genomicsdb” folder. ; ; Parent Command : python /gatk/gatk --java-options -Xmx4g -Xms4g GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; Child Process : java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx4g -Xms4g -jar /gatk/gatk-package-4.1.8.1-local.jar GenomicsDBImport --genomicsdb-workspace-path genomicsdb --batch-size 50 -L chrX:51630606-68003941 --sample-name-map inputs.list --reader-threads 5 -ip 500 --gcs-project-for-requester-pays broad-dsde-methods; ; The above command took approx. 3.5 hrs to run while writing to local mount of ec2 i.e. EBS volume.; The same command took 8+ hrs (still running as of this email) to run while writing to FSx for luster mount. And surprisingly through AWS Batch – EC2 as part of complete batch/pipeline, took 40+ hrs.; ; The files being read by this process are already cached into FSx as we have been using this same FSx for 5+ days now and these jobs already succeeded with 30-40 hrs of runtime.; ; While we were testing the below manual execution, nothing was running from batch or FSx perspective. Only the 2 manual jobs - one for writing it to local (EBS) and other for FSx. The FSx we are using is the scratch system type with 16.8 TB of space, which gives us a total throughput of 3.3 GB/s.; ; Below is the snapshot of batch 1 executions.; ; EBS Mount Run : Took a total of 1 hr in batch 1; ![EBS Mount Run Batch 1](https://user-images.githubusercontent.com/64221390/151032847-b0bfc418-c2c4-4d8f-a95a-ab0fc0b8eeee.png). FSX Mount Run : Took 2 hrs 11 mins in batch 1; ![FSX Run Batch 1](https://user-images.githubusercontent.com/64221390/151032872-2cae5890-ee5f-4122-b077-037ed4c38414.png). But when the “dd” command to test the write speeds for both the file system",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7646:1264,pipeline,pipeline,1264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7646,1,['pipeline'],['pipeline']
Deployability,"to validate inputs. (#7845); - Compute filter scatter [VS-392] (#7852); - remove withdrawn req (#7844); - Improve import error message [VS-437] (#7855); - Fix Input Validation python notebook (#7853); - Add VAT Validation check that aa_change and exon_number are consistently set. (#7850); - Ingest 10K [VS-344] (#7860); - X/Y chromosome reweighting for better extract shard runtime balance [VS-389] (#7868); - VET Ingest Validation / Allow Ingest of non-VQSR'ed data (#7870); - Fix AoU workflow bugs (#7874); - Curate input arrays to skip already ingested sample data [VS-246] (#7862); - KM upload GVS product sheet (#7883); - Default extract scatter width [VS-415] (#7878); - Volatile tasks review [VS-447] (#7880); - Update Quickstart Integration for X/Y scaling changes [VS-464] (#7881); - clean up dockstore; - Rc vs 63 vat sop documentation (#7879); - Fix up FQ and race condition issues with volatile tasks work [VS-478] (#7888); - Use gvs-internal project in integration test (#7901); - Add cost observability BQ table [VS-441] (#7891); - Add preliminary labels to queries [VS-381] (#7902); - Workflow compute costs [VS-472] (#7905); - Fix bug and update images (#7912); - VS 483 Beta user wdl (#7894); - Core storage model cost [VS-473] (#7913); - Update Quickstart & Integration to use re-blocked v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:25172,Update,Update,25172,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,12,"['Integrat', 'Update', 'integrat', 'update']","['Integration', 'Update', 'integration', 'update']"
Deployability,"tps://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252258477). I don't have special privileges on the cloud...requests like this need to; go through pipeline-help...sorry. Y. On Fri, Oct 7, 2016 at 9:08 AM, ldgauthier notifications@github.com wrote:. > I don't know what intermediates we save on the cloud but maybe @yfarjoun; > https://github.com/yfarjoun is willing to help.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-252247496,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0lAsJd9NECpPP0JYVp2ziDhga0B9ks5qxkRUgaJpZM4KQT_3; > . ---. @vdauwera commented on [Wed Oct 26 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-256499771). Writing pipeline-help now and cc'ing everyone involved in this thread. Will try to get some kind of protocol set up for debugging things that happen in the cloud pipeline, because I expect this will happen again. But if it gets too complicated we could also mock up some fake records that would reproduce this. It seems to me that shouldn't be too hard. . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-260498705). I need to ping Daniel on getting access to the files. ---. @ronlevine commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275576931). @vdauwera Can you get the data? I can take a look a this issue. ---. @vdauwera commented on [Thu Jan 26 2017](https://github.com/broadinstitute/gsa-unstable/issues/1489#issuecomment-275578721). Oh, they gave me access to the files but I never took the next step of figuring out which files are relevant. There are twenty thousand samples... I'm not sure what is the best way to approach this. ---. @ldgauthier commented on [Wed Mar 01 2017](https://github.com/broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2959:2625,pipeline,pipeline,2625,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2959,1,['pipeline'],['pipeline']
Deployability,"traJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - spark.executor.extraJavaOptions =; 16:16:36.297 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 16:16:36.297 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 16:16:36.297 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 16:16:36.297 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 16:16:36.297 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 16:16:36.297 DEBUG ConfigFactory - createOutputBamIndex = true; 16:16:36.298 INFO GenomicsDBImport - Deflater: IntelDeflater; 16:16:36.298 INFO GenomicsDBImport - Inflater: IntelInflater; 16:16:36.298 INFO GenomicsDBImport - GCS max retries/reopens: 20; 16:16:36.298 INFO GenomicsDBImport - Requester pays: disabled; 16:16:36.298 INFO GenomicsDBImport - Initializing engine; 16:16:36.523 WARN GenomicsDBImport - genomicsdb-update-workspace-path was set, so ignoring specified intervals.The tool will use the intervals specified by the initial import; 16:16:37.372 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 16:16:37.372 DEBUG GenomeLocParser - chr1 (248956422 bp); 16:16:37.373 DEBUG GenomeLocParser - chr2 (242193529 bp); 16:16:37.373 DEBUG GenomeLocParser - chr3 (198295559 bp); 16:16:37.373 DEBUG GenomeLocParser - chr4 (190214555 bp); 16:16:37.373 DEBUG GenomeLocParser - chr5 (181538259 bp); 16:16:37.373 DEBUG GenomeLocParser - chr6 (170805979 bp); 16:16:37.373 DEBUG GenomeLocParser - chr7 (159345973 bp); ...many lines here...; 16:16:37.524 DEBUG GenomeLocParser - HLA-DRB1*15:01:01:01 (11080 bp); 16:16:37.524 DEBUG GenomeLocParser - HLA-DRB1*15:01:01:02 (11571 bp); 16:16:37.524 DEBUG GenomeLocParser - HLA-DRB1*15:01:01:03 (11056 bp); 16:16:37.524 DEBUG GenomeLocParser - HLA-DRB1*15:01:01:04 (11056 bp); 16:16:37.524 DEBUG GenomeLocParser - HLA-DRB1*15:02:01 (10313 bp); 16:16:37.524 DEBUG Genom",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6793:6237,update,update-workspace-path,6237,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6793,1,['update'],['update-workspace-path']
Deployability,tractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBuffe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638:8196,Integrat,IntegrationTestSpec,8196,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638,1,['Integrat'],['IntegrationTestSpec']
Deployability,tractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:94); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:517); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:102); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:151); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:170); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:67); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:74); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:65); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:69); at org.broadinstitute.hellbender.utils.test.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:148); ... 49 more; Caused by: java.nio.channels.ClosedChannelException; at sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:109); at sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:252); at htsjdk.samtools.seekablestream.SeekableFileStream.position(SeekableFileStream.java:64); at htsjdk.tribble.TribbleIndexedFeatureReader$BlockStreamWrapper.read(TribbleIndexedFeatureReader.java:534); at java.io.InputStream.read(InputStream.java:101); at htsjdk.tribble.readers.PositionalBufferedStream.fill(PositionalBufferedStream.java:127); at htsjdk.tribble.readers.PositionalBufferedStream.read(PositionalBufferedStream.java:79); at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284); at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326); at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178); at java.io.InputStreamReader.read(InputStreamReader.java:184); at htsjdk.tribble.readers.LongLineBufferedReader.fill(LongLineBufferedReader.java:140); at ht,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1638:8228,Integrat,IntegrationTestSpec,8228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1638,1,['Integrat'],['IntegrationTestSpec']
Deployability,travis got their act together and enabled at installation of the base R packages in their container build; this removes all uses of sudo so we can make use of the container builds which have faster dispatch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/544:45,install,installation,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/544,1,['install'],['installation']
Deployability,"ts.BuildExceptionReporter] ; 22:05:55.954 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.; 22:05:55.955 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Where:; 22:05:55.956 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Build file '/scratch/var/tmp/portage/sci-biology/gatk-9999/work/gatk-9999/build.gradle' line: 102; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.964 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] A problem occurred evaluating root project 'gatk'.; 22:05:55.966 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 2. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK.; 22:05:55.967 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] ; 22:05:55.968 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Exception is:; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] org.gradle.api.GradleScriptException: A problem occurred evaluating root project 'gatk'.; 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.groovy.scripts.internal.DefaultScriptRunnerFactory$ScriptRunnerImpl.run(DefaultScriptRunnerFactory.java:92); 22:05:55.969 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.DefaultScriptPluginFactory$ScriptPluginImpl$2.run(DefaultScriptPluginFactory.java:176); 22:05:55.970 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.configuration.ProjectScriptTarget.addConfigur",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:1533,install,installed,1533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['install'],['installed']
Deployability,"ttps://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286449442). People care about being able to filter out artifacts that are due to tumor DNA present in the normal sample. How that is to be done is entirely up to you. . ---. @davidbenjamin commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286450985). @LeeTL1220 @vdauwera Is it our responsibility? Do you see it as being part of the Mutect wdl? And what artifacts does tumor-in-normal create? Naively I would expect it to do the opposite i.e. decrease sensitivity by flagging true somatic variants as germline events. If this is important I would consider putting in our next quarterly goals. The time investment would be similar to the new contamination tool, about 3 weeks. ---. @vdauwera commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286453446). I would like us to consider this part of the somatic short variants discovery pipeline (which is probably what you mean by Mutect wdl). I'm not sure how urgently we need it, probably ""not very urgent in the scheme of things, but should be done eventually"". I assume @LeeTL1220 has a better idea of what we need when on the somatic side of things. . And you're right that this creates a loss of sensitivity, not false positives. I'm being distracted by a tiny human. . ---. @LeeTL1220 commented on [Tue Mar 14 2017](https://github.com/broadinstitute/gatk-protected/issues/800#issuecomment-286457270). We should do it. Generally, I agree with Geraldine's statement about; urgency, though. We can meet to discuss prioritization relative to other; auxiliary tools. On Tue, Mar 14, 2017 at 11:15 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > I would like us to consider this part of the somatic short variants; > discovery pipeline (which is probably what you mean by Mutect wdl). I'm not; > sure how urgently we need it, probably ""not very urgent in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2919:1579,pipeline,pipeline,1579,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2919,1,['pipeline'],['pipeline']
Deployability,"turn false if the float is missing, otherwise value <= maxMaf. Don't ever call allFrequenciesFiltered. This request was created from a contribution made by Azza Ahmed on October 14, 2021 10:53 UTC. Link: [https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error](https://gatk.broadinstitute.org/hc/en-us/community/posts/4408348163227-FilterFuncotations-Duplicate-key-error). \--. Hello,. I'm using the `FilterFuncotations` to process the output from the `Functotator` as per this WARP \[pipeline\]( [warp/AnnotationFiltration.wdl at cec97750e3819fd88ba382534aaede8e05ec52df · broadinstitute/warp (github.com)](https://github.com/broadinstitute/warp/blob/cec97750e3819fd88ba382534aaede8e05ec52df/pipelines/broad/annotation_filtration/AnnotationFiltration.wdl)). . ; ; ; ; /home/azzaea/software/gatk/gatk-4.2.2.0/gatk --java-options ""-Xmx3072m"" \ ; FilterFuncotations \ ; --variant /scratch/FPTVM/src/warp/pipelines/broad/annotation\_filtration/cromwell-executions/AnnotationFiltration/4e3bd06b-3018-4c94-ac98-feb78b924d1f/call-FilterFuncotations/shard-0/inputs/1333115969/104566-001-001.filtered.vcf.funcotated.vcf.gz \ ; --output 104566-001-001.filtered.vcf.filtered.vcf.gz \ ; --ref-version hg38 \ ; --allele-frequency-data-source gnomad --lenient true; ; ; ; ; . However, the command fails with the error message below:. ; ; ; ; [October 14, 2021 at 12:20:24 PM CEST] org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations done. Elapsed time: 16.57 minutes. ; Runtime.totalMemory()=1134559232 ; java.lang.IllegalStateException: Duplicate key Gencode\_34\_annotationTranscript (attempted merging values ENST00000450305.2 and ENST00000456328.2) ; at java.base/java.util.stream.Collectors.duplicateKeyException(Collectors.java:133) ; at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:180) ; at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; at java.base/java.util.Ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7504:1356,pipeline,pipelines,1356,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7504,1,['pipeline'],['pipelines']
Deployability,tute.hellbender.engine.spark.GATKSparkTool.initializeReads(GATKSparkTool.java:284); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:264); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:255); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:36); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:146); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:165); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:66); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:73); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:68); at org.broadinstitute.hellbender.tools.spark.pipelines.metrics.MeanQualityByCycleSparkIntegrationTest.test_ADAM(MeanQualityByCycleSparkIntegrationTest.java:96); at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-1); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:85); at org.testng.internal.Invoker.invokeMethod(Invoker.java:639); at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:821); at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1131); at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:124); at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:108); at org.testng.TestRunner.privateRun(TestRunner.java:773); at org.testng.TestRunner.run(TestRunner.java:623); at org.testng.SuiteRunn,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1280:2684,pipeline,pipelines,2684,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1280,1,['pipeline'],['pipelines']
Deployability,"tute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:119); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:176); > 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:195); > 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:137); > 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:158); > 	at org.broadinstitute.hellbender.Main.main(Main.java:239); > 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); > 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); > 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); > 	at java.lang.reflect.Method.invoke(Method.java:498); > 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:733); > 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:177); > 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:202); > 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:116); > 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); > Caused by: java.nio.file.ProviderNotFoundException: Provider ""maprfs"" not found; > 	at java.nio.file.FileSystems.newFileSystem(FileSystems.java:341); > 	at org.seqdoop.hadoop_bam.util.NIOFileUtil.asPath(NIOFileUtil.java:40); > 	at org.seqdoop.hadoop_bam.BAMRecordReader.initialize(BAMRecordReader.java:143); > 	at org.seqdoop.hadoop_bam.BAMInputFormat.createRecordReader(BAMInputFormat.java:226); > 	at org.seqdoop.hadoop_bam.AnySAMInputFormat.createRecordReader(AnySAMInputFormat.java:190); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.liftedTree1$1(NewHadoopRDD.scala:178); > 	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:177); > 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:3610,deploy,deploy,3610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['deploy'],['deploy']
Deployability,"ty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay under Google quotas for new accounts into beta workflow (#8200); - Update docs for Nirvana reference disk [VS-531] [VS-796] (#8170); - VS-694 - Extract Callset for VQSR Lite (#8182); - Updating docker image (#8210); - Document VCF generation [VS-795] (#8202); - Variants GATK Docker image building docs + script [VS-827] (#8207); - Update GATK jar used in GvsJointVariantCalling WDL (#8216); - Hello Azure SQL Database from Cromwell on Azure [VS-812] (#8220); - Remove what appear to be accidentally added files [VS-834] (#8225); - VS-815: Add Support for YNG to VQSR Lite (#8206); - Disentangle non-GVS code from GVS code [VS-834] (#8229); - VS-695. Updates to run Precision and Sensitivity on VQSR Lite (#8230); - Track avro export costs [VS-769] (#8236); - Add note that we deleted a VDS! (#8214); - Vs 822 Add documentation for the work that we did on the latest iteration of Delta (#8205); - Rc vs 822 gq0 documentation (#8240). [VS-16]: https://broadworkbench.atlassian.net/browse/VS-16?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:33076,Update,Updates,33076,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,1,['Update'],['Updates']
Deployability,"ty to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column and MLEAC/F format field) cannot be updated based on them... I think that best way to move forward here is:; 1. Lift up that maximum number of Genotypes to output PLs based on the ploidy parameter (I think the limit was quite modest perhaps as low as 20).; 2. Implement the alt. allele `culling` or `collapsing` that I mention above in HaplotypeCaller already. ; 3. Implement the alt. allele `re-culling` or `re-collapsing` in GVCF (VCF as well?) merging tools such as CombineGVCFs/GenotypeGVCFs.; 4. Regenotyping and QUAL recalculating tools would need to make sure that PLs less input are handled appropriately, not sure what would happen now if some of the inputs lack PLs... (an Exception?) ; - For example QUAL could be approximated as the max of the input Quals, and QD as the average? ; - Or simple lift them blank?. So it would a bit of work I would say... 3 of the old PTs worth. ---. @vdauwera commented on [Thu May 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:2405,update,updated,2405,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['update'],['updated']
Deployability,uality_tail_5.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.mismatches_context_size_4.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.quantizing_levels_6.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.CEUTrio.HiSeq.WGS.b37.ch20.1m-1m20k.NA12878.recal.txt; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.HiSeq.1mb.1RG.2k_lines.alternate.recalibrated.DIQ.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.HiSeq.1mb.1RG.2k_lines.bqsr.qq-1.alternate.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.HiSeq.1mb.1RG.2k_lines.bqsr.qq6.alternate.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.alternate_allaligned.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.alternate.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.alternate.recalibrated.DIQ.sharded.bam/part-r-00001.bam; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/HiSeq.1mb.1RG.2k_lines.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/human_b36_both.chr1_1k.dict; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/human_b36_both.chr1_1k.fasta.fai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.chr17_69k_70k.dictFix.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/NA12878.oq.read_consumes_zero_ref_bases.chr20.bam.bai; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/na.bam; src/test/resources/org/broadinstitute/hellbender/tools/BQSR/na.bam.bai; sr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:21264,pipeline,pipeline,21264,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['pipeline'],['pipeline']
Deployability,ubuntu 18 -> 22; conda upgraded,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8068:23,upgrade,upgraded,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8068,1,['upgrade'],['upgraded']
Deployability,"udIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 5, 2017 2:52:10 PM EDT] Executing as shlee@gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 14:52:10.875 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 14:52:10.875 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:52:10.875 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:52:10.875 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:52:10.875 INFO PrintReads - Deflater: IntelDeflater; 14:52:10.875 INFO PrintReads - Inflater: IntelInflater; 14:52:10.876 INFO PrintReads - GCS max retries/reopens: 20; 14:52:10.876 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:52:10.876 INFO PrintReads - Initializing engine; 14:52:21.901 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:52:21.917 INFO PrintReads - Done initializing engine; 14:52:22.027 INFO ProgressMeter - Starting traversal; 14:52:22.027 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 14:52:32.033 INFO ProgressMeter - chr17:6779805 0.2 494000 2962814.9; 14:52:42.035 INFO ProgressMeter - chr17:18100301 0.3 1275000 3823661.7; 14:52:52.089 INFO ProgressMeter - chr17:32183301 0.5 2017000 4025814.2; 14:53:02.141 INFO ProgressMeter - chr17:38342966 0.7 2500000 3739436.1; 14:53:12.267 INFO ProgressMeter - chr17:46549838 0.8 3360000 4012818.7; 14:53:22.273 INFO ProgressMeter - chr17:63099258 1.0 4210000 4192879.1; 14:53:30.687 INFO PrintReads - No reads filtered by: WellformedReadFilter; 14:53:30.687 INFO ProgressMeter - chr17:831",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:3321,patch,patch,3321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['patch'],['patch']
Deployability,"udIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 5, 2017 2:59:15 PM EDT] Executing as shlee@gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 14:59:15.872 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:59:15.873 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:59:15.873 INFO PrintReads - Deflater: IntelDeflater; 14:59:15.873 INFO PrintReads - Inflater: IntelInflater; 14:59:15.873 INFO PrintReads - GCS max retries/reopens: 20; 14:59:15.873 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:59:15.873 INFO PrintReads - Initializing engine; 14:59:21.404 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 14:59:21.421 INFO PrintReads - Shutting down engine; [October 5, 2017 2:59:22 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.11 minutes.; Runtime.totalMemory()=2129133568; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```. Still fails with `-readIndex`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:11645,patch,patch,11645,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['patch'],['patch']
Deployability,"udIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 5, 2017 2:59:57 PM EDT] Executing as shlee@gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 14:59:57.393 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 14:59:57.393 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:59:57.393 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:59:57.393 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:59:57.394 INFO PrintReads - Deflater: IntelDeflater; 14:59:57.394 INFO PrintReads - Inflater: IntelInflater; 14:59:57.394 INFO PrintReads - GCS max retries/reopens: 20; 14:59:57.394 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 14:59:57.394 INFO PrintReads - Initializing engine; ^C-bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram \; > -R /humgen/gsa-hpprojects/dev/shlee/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa \; > -readIndex gs://shlee-dev/1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram.crai \; > -O HG00190_cram.bam \; > -L chr17; Using GATK jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=1 -Dsnappy.disable=true -jar /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-package-4.beta.5-local.jar PrintReads -I gs://shlee-dev/1kg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:15848,patch,patch,15848,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['patch'],['patch']
Deployability,"udIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 5, 2017 3:00:08 PM EDT] Executing as shlee@gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:08.250 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:08.250 INFO PrintReads - Deflater: IntelDeflater; 15:00:08.250 INFO PrintReads - Inflater: IntelInflater; 15:00:08.250 INFO PrintReads - GCS max retries/reopens: 20; 15:00:08.250 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:08.250 INFO PrintReads - Initializing engine; 15:00:13.258 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:13.275 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:14 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2233466880; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; ```; ```; -bash-4.1$ /humgen/gsa-h",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:19209,patch,patch,19209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['patch'],['patch']
Deployability,"udIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 5, 2017 3:00:28 PM EDT] Executing as shlee@gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:00:28.284 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:00:28.284 INFO PrintReads - Deflater: IntelDeflater; 15:00:28.284 INFO PrintReads - Inflater: IntelInflater; 15:00:28.285 INFO PrintReads - GCS max retries/reopens: 20; 15:00:28.285 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:00:28.285 INFO PrintReads - Initializing engine; 15:00:33.117 INFO IntervalArgumentCollection - Processing 83257441 bp from intervals; 15:00:33.134 INFO PrintReads - Shutting down engine; [October 5, 2017 3:00:34 PM EDT] org.broadinstitute.hellbender.tools.PrintReads done. Elapsed time: 0.10 minutes.; Runtime.totalMemory()=2255486976; ***********************************************************************. A USER ERROR has occurred: Traversal by intervals was requested but some input files are not indexed.; Please index all input files:. samtools index /1kg/exome_GRCh38DH/cram/HG00190.alt_bwamem_GRCh38DH.20150826.FIN.exome.cram. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.; -bash-4.1$ ; ```. ## Confirm all f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:23368,patch,patch,23368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['patch'],['patch']
Deployability,"udIndexPrefetchBuffer -1 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [October 5, 2017 3:05:33 PM EDT] Executing as shlee@gsa5.broadinstitute.org on Linux 2.6.32-642.15.1.el6.x86_64 amd64; Java HotSpot(TM) 64-Bit Server VM 1.8.0_121-b13; Version: 4.beta.5; 15:05:33.887 INFO PrintReads - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 15:05:33.887 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 15:05:33.887 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 15:05:33.887 INFO PrintReads - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 15:05:33.887 INFO PrintReads - Deflater: IntelDeflater; 15:05:33.887 INFO PrintReads - Inflater: IntelInflater; 15:05:33.887 INFO PrintReads - GCS max retries/reopens: 20; 15:05:33.887 INFO PrintReads - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 15:05:33.887 INFO PrintReads - Initializing engine; 15:05:39.011 INFO PrintReads - Done initializing engine; 15:05:39.298 INFO ProgressMeter - Starting traversal; 15:05:39.299 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 15:05:49.302 INFO ProgressMeter - chr1:12666181 0.2 578000 3467306.5; 15:05:59.302 INFO ProgressMeter - chr1:21255922 0.3 1287000 3860613.9; 15:06:09.320 INFO ProgressMeter - chr1:36027022 0.5 2121000 4239032.7; 15:06:19.323 INFO ProgressMeter - chr1:52397728 0.7 3017000 4522786.3; 15:06:29.323 INFO ProgressMeter - chr1:86811190 0.8 4064000 4874460.3; 15:06:39.479 INFO ProgressMeter - chr1:111761145 1.0 5079000 5063808.6; ...; ```. Adding in `-L` causes the command to error despite the presence of the index within the same folder.; ```; -bash-4.1$ /humgen/gsa-hpprojects/GATK/gatk4/gatk-4.beta.5/gatk-launch PrintReads \; > -I g",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3669:7865,patch,patch,7865,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3669,1,['patch'],['patch']
Deployability,"uest**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); Funcotator; gatk Funcotator --variant test.somatic.vcf --reference ucsc.hg19.fasta --ref-version hg19 --data-sources-path funcotator_dataSources.v1.7.20200521s --output test.maf --output-file-format MAF; ### Affected version(s); gatk4.1.8.1 (installed using conda). ### Description ; I want to use Funcotator to annotate the VCF file given by Illumina TruSight Oncology 500 pipeline. But when I run the command above, it throws out an error, seems something related with malformat. I check my VCF file and think it should be OK. So I wonder if you can kindly tell me how to fix this bug?; The ERROR is:; `Using GATK jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/shiyang/softwares/gatk-4.1.8.1/gatk-package-4.1.8.1-local.jar Funcotator --variant /home/shiyang/Project/BGB900_101/TSO_result/TSO_somatic_vcf/112-0005-0031-B1_L1.UP12.tmb.tsv.tso.somatic.vcf --reference /storage01/ref_genome/hg19/bwa/ucsc.hg19.fasta --ref-version hg19 --data-sources-path /home/shiyang/softwares/funcotator_dataSources/funcotator_dataSources.v1.7.20200521s --output /home/shiyang/Project/BGB900_101/TSO_result/test.maf --output-file-for",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6758:1592,pipeline,pipeline,1592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6758,1,['pipeline'],['pipeline']
Deployability,"ug/' \; HaplotypeCallerSpark \; --reference /projects/rdocking_prj/software/bcbio-nextgen/data/genomes/Hsapiens/hg19/ucsc/hg19.2bit \; --annotation MappingQualityRankSumTest --annotation MappingQualityZero \; --annotation QualByDepth --annotation ReadPosRankSumTest \; --annotation RMSMappingQuality --annotation BaseQualityRankSumTest \; --annotation FisherStrand --annotation MappingQuality \; --annotation DepthPerAlleleBySample --annotation Coverage \; -I /projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/work/align/MOLM13_rep1/MOLM13_rep1-dedup.splitN.bam \; -L /projects/karsanlab/rdocking/KARSANBIO-1254_pipeline/KARSANBIO-1390_rna_seq_runs/data/gatk_debug/chr1_70k.bed \; --interval-set-rule INTERSECTION \; --spark-master local[12] \; --conf spark.local.dir=/projects/karsanscratch/rdocking/KARSANBIO-1390_rna_seq_runs/molm13_replicate_one_small/debug \; --conf spark.driver.host=localhost \; --conf spark.network.timeout=800 \; --conf spark.executor.heartbeatInterval=100 \; --annotation ClippingRankSumTest --annotation DepthPerSampleHC \; --emit-ref-confidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80 \; --output MOLM13_rep1-chr1-70k-gatk-haplotype.vcf; ```. When I run this command on a single chromosome with `-Xmx94349m`, the command completes successfully, but the resulting VCF header does not contain this expected header line:. ```; ##INFO=<ID=END,Number=1,Type=Integer,Description=""Stop position of the interval"">; ```. (along with most of the other header lines associated with gVCF output). When I up the memory request to 110g for the same input files, the proper VCF header is present. I discovered this in the context of running GATK within the bcbio pipeline, the original descriptions are at: https://github.com/bcbio/bcbio-nextgen/issues/2375. On the linked issue, I have examples of GATK output from runs that produced correct and incorrect output - please let me know if there's any other information you need. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4821:2170,pipeline,pipeline,2170,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4821,1,['pipeline'],['pipeline']
Deployability,uildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:75); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:49); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ServicesSetupBuildActionExecuter.execute(ServicesSetupBuildActionExecuter.java:31); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:67); 11:54:40.435 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 11:54:40.435 [ERROR] [org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2802:8140,Continuous,ContinuousBuildActionExecuter,8140,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2802,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,uildExceptionReporter] 	at org.gradle.launcher.exec.GradleBuildController.run(GradleBuildController.java:66); 22:05:55.977 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ExecuteBuildActionRunner.run(ExecuteBuildActionRunner.java:28); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.ChainingBuildActionRunner.run(ChainingBuildActionRunner.java:35); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:41); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.exec.InProcessBuildActionExecuter.execute(InProcessBuildActionExecuter.java:26); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:79); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.tooling.internal.provider.ContinuousBuildActionExecuter.execute(ContinuousBuildActionExecuter.java:51); 22:05:55.978 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.ExecuteBuild.doBuild(ExecuteBuild.java:59); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.BuildCommandOnly.execute(BuildCommandOnly.java:36); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.api.DaemonCommandExecution.proceed(DaemonCommandExecution.java:120); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 	at org.gradle.launcher.daemon.server.exec.WatchForDisconnection.execute(WatchForDisconnection.java:47); 22:05:55.979 [ERROR] [org.gradle.internal.buildevents.BuildException,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4687:7288,Continuous,ContinuousBuildActionExecuter,7288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4687,1,['Continuous'],['ContinuousBuildActionExecuter']
Deployability,uler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:16213,pipeline,pipelines,16213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['pipeline'],['pipelines']
Deployability,"unTool(BaseRecalibratorSpark.java:86); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:348); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:109); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:167); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:186); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:76); at org.broadinstitute.hellbender.Main.main(Main.java:92); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer; Serialization stack:; **\- object not serializable (class: java.nio.HeapByteBuffer, value: java.nio.HeapByteBuffer[pos=0 lim=775456500 cap=775456500])**; - field (class: org.bdgenomics.adam.util.TwoBitFile, name: bytes, type: class java.nio.ByteBuffer); - object (class org.bdgenomics.adam.util.TwoBitFile, org.bdgenomics.adam.util.TwoBitFile@863c31e); - field (class: org.broadinstitute.hellbender.engine.spark.datasources.ReferenceTwoBitSource, name: twoBitFile, type: class org.bdgenomics.adam.util.TwoBitFile); - object (class org.broadinstitute.hellbender.engi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2216:2574,deploy,deploy,2574,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2216,1,['deploy'],['deploy']
Deployability,unnerWorker.java:86); at org.testng.TestNG.runSuitesSequentially(TestNG.java:1185); at org.testng.TestNG.runSuitesLocally(TestNG.java:1110); at org.testng.TestNG.run(TestNG.java:1018); at org.testng.remote.RemoteTestNG.run(RemoteTestNG.java:111); at org.testng.remote.RemoteTestNG.initAndRun(RemoteTestNG.java:204); at org.testng.remote.RemoteTestNG.main(RemoteTestNG.java:175); at org.testng.RemoteTestNGStarter.main(RemoteTestNGStarter.java:125); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:483); at com.intellij.rt.execution.application.AppMain.main(AppMain.java:140); Caused by: java.lang.RuntimeException: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.dataflow.sdk.Pipeline.run(Pipeline.java:166); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.runPipeline(DataflowCommandLineProgram.java:145); at org.broadinstitute.hellbender.engine.dataflow.DataflowCommandLineProgram.doWork(DataflowCommandLineProgram.java:107); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:98); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:151); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:71); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:78); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:75); at org.broadinstitute.hellbender.tools.IntegrationTestSpec.executeTest(IntegrationTestSpec.java:126); ... 33 more; Caused by: java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.createStarted()Lcom/google/common/base/Stopwatch;; at com.google.cloud.genomics.dataflow.readers.bam.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/866:3031,Pipeline,Pipeline,3031,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/866,1,['Pipeline'],['Pipeline']
Deployability,"up on it. Would it perhaps be more viable to add an option to toggle the level of stringency, ie choose in the command line whether to blow up or skip on these invalid intervals? . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260496001). @yfarjoun will want to opine on this, I think. . ---. @yfarjoun commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260513266). I hope that when we move exomes to hg38 we will correct this silly thing; and a few decades later we will no need this code (hehe). Y. On Mon, Nov 14, 2016 at 6:19 PM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > From what I understand of the referenced thread, the ""incorrect"" interval; > list may always be around, so we may never be able to just blow up on it.; > Would it perhaps be more viable to add an option to toggle the level of; > stringency, ie choose in the command line whether to blow up or skip on; > these invalid intervals?; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260495927,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACnk0uvegvUmCq7_G7U2PSuTpvIYl0wQks5q-Ox0gaJpZM4JNjE-; > . ---. @vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260519118). So, would adding a toggle be acceptable? And more importantly, can we make stringent validation default, with the option to not blow up on silly exome files? Will production accept that?. ---. @yfarjoun commented on [Tue Nov 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1438#issuecomment-260617185). let me talk with production to see if we can post-facto change the exome; file... On Mon, Nov 14, 2016 at 8:27 PM, Geraldine Van der Auwera <; notificati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2520:1873,toggle,toggle,1873,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2520,1,['toggle'],['toggle']
Deployability,update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5516:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5516,1,['update'],['update']
Deployability,update AUTHORS file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3394:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3394,1,['update'],['update']
Deployability,update AUTHORS file with new authors from protected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3048:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3048,1,['update'],['update']
Deployability,update AoU docs,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7540:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7540,1,['update'],['update']
Deployability,update CreateSomaticPanelOfNormals javadoc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6584:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6584,1,['update'],['update']
Deployability,update Dockerfile to reflect correct haplocheckCLI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6867:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6867,1,['update'],['update']
Deployability,update GATKSparkTool.getRecommendedNumReducers for GCS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1717:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1717,1,['update'],['update']
Deployability,update HaplotypeCallerSpark to use the new style iterators,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4278:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4278,1,['update'],['update']
Deployability,update README with correct path to install_R_packages.R,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3602:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3602,1,['update'],['update']
Deployability,update Readme with new script name and location,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/990:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/990,1,['update'],['update']
Deployability,update SV Spark pipeline example shell scripts saving results to GCS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6114:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6114,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,update SmithWatermanAligner in preparation for native optimized aligner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3600:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3600,1,['update'],['update']
Deployability,update `VariantFiltration` code when a version with https://github.com/samtools/htsjdk/pull/273 is released. Argument types can then be changed from `ArrayList` to `List`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/672:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/672,2,"['release', 'update']","['released', 'update']"
Deployability,update all usage examples,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/182:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/182,1,['update'],['update']
Deployability,update apache commons-io 2.4->2.5,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4053:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4053,1,['update'],['update']
Deployability,update branch conditions in FragmentUtils.adjustQualsOfOverlappingPairedFragments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7151:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7151,1,['update'],['update']
Deployability,update com.google.guava version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3102:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3102,1,['update'],['update']
Deployability,update deploy key,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7524:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7524,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,update dist commands to include gatk-launch and sparkJar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1779:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1779,1,['update'],['update']
Deployability,update docs and arg names for ParallelCopyGCSDirectoryIntoHDFSSparkIn…,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3911:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3911,1,['update'],['update']
Deployability,update error message when sample name in VCF cannot be looked up in sampleMap.tsv,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7074:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7074,1,['update'],['update']
Deployability,update for assign ids and changes in import,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7439:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7439,1,['update'],['update']
Deployability,update for genomes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6918:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6918,1,['update'],['update']
Deployability,update for gradle 6.9.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7604:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7604,1,['update'],['update']
Deployability,update htjsdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6083:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6083,1,['update'],['update']
Deployability,update htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/329:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/329,1,['update'],['update']
Deployability,update htsjdk 2.13.2 and picard 2.16.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3962:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3962,1,['update'],['update']
Deployability,update htsjdk 2.15.1 -> 2.16.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4914:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4914,1,['update'],['update']
Deployability,update htsjdk downstream tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3235:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3235,1,['update'],['update']
Deployability,update htsjdk to 2.19.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5812:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5812,1,['update'],['update']
Deployability,update htsjdk to 2.21.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6250:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6250,1,['update'],['update']
Deployability,update htsjdk to a current snapshot and fix the test failures,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3417:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3417,1,['update'],['update']
Deployability,update http-nio to 1.1.0 which implements Path.resolve() methods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8626:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8626,1,['update'],['update']
Deployability,update hyperlink to new GATK forum page in Readme,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6381:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6381,1,['update'],['update']
Deployability,update import for is_loaded,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7416:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7416,1,['update'],['update']
Deployability,update nirvana version and nirvana tar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8117:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8117,1,['update'],['update']
Deployability,update picard and htsjdk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6306:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6306,1,['update'],['update']
Deployability,update protected to use Intel GKL,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2872:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2872,1,['update'],['update']
Deployability,update public key for installing R in docker (key just expired),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6116:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6116,2,"['install', 'update']","['installing', 'update']"
Deployability,update publishing for failed tests,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5108:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5108,1,['update'],['update']
Deployability,update readme,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/361:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/361,1,['update'],['update']
Deployability,update scripts and help with new gcloud auth application-defaults login,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2660:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2660,1,['update'],['update']
Deployability,"update sv scripts to only copy a single bam file and index, and respect project parameter",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4646:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4646,1,['update'],['update']
Deployability,update the artifactory url to point to the new artifactory; update the travis build with another environment variable called UPLOAD which determines if that build should upload a snapshot or not; fixes #3068,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3075:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3075,2,['update'],['update']
Deployability,update the gradle default assembleDist task,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3059:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3059,1,['update'],['update']
Deployability,update the readme of gatk so that it contains all necessary information from protected,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2775:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2775,1,['update'],['update']
Deployability,update to 4.1 - apache collections 4.0 had a serialization exploit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1746:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1746,1,['update'],['update']
Deployability,update to GKL_0.8.3 with compression improvments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4311:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4311,1,['update'],['update']
Deployability,update to a current htsjdk snapshot,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3417:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3417,1,['update'],['update']
Deployability,update to latest GKL with compression related optimizations,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4379:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4379,1,['update'],['update']
Deployability,update usage example for CreateHadoopBamSplittingIndex,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5898:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5898,1,['update'],['update']
Deployability,update warp version,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7906:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7906,1,['update'],['update']
Deployability,update warp!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7906:0,update,update,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7906,1,['update'],['update']
Deployability,updated FilterMutectCalls usage examples,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5890:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5890,1,['update'],['updated']
Deployability,updated GvsJointVariantCalling.wdl with latest override_gatk,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8376:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8376,1,['update'],['updated']
Deployability,updated extract example script,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7195:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7195,1,['update'],['updated']
Deployability,updated m2 docs filter names and comments on FilterAlignmentArtifacts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6967:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6967,1,['update'],['updated']
Deployability,"updated shadowJar to 1.2.3 since version 1.2.2 of the shadowJar plugin had some issues with gradle ; 2.11 which just released. some `build.gradle` cleanup; - removed dependency on `lib/tools.java` since it doesn't seem to be used and should be provided by the system anyway; - removed individual excludes of `guava-jdk5` since we exclude them globally; - changed our plugin application to use the newer style; - updated jacoco, coverals, and versions plugin versions; - added group and description to sparkJar task so it shows up in `gradle tasks`; - updated gradle wrapper version to 2.11; - readme now states 2.11 as minimum version",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1478:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1478,4,"['release', 'update']","['released', 'updated']"
Deployability,updated the documentation to no longer contain a misleading usage example,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5938:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5938,1,['update'],['updated']
Deployability,updated the query to calculate num hets and homvars; add in excess het and check threshold,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7175:0,update,updated,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7175,1,['update'],['updated']
Deployability,updates sv manager and cluster creation scripts to utilize dataproc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3579:0,update,updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3579,1,['update'],['updates']
Deployability,updates to ImportGenomes and LoadBigQueryData,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7112:0,update,updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7112,1,['update'],['updates']
Deployability,updates to ReadSparkSink,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1461:0,update,updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1461,1,['update'],['updates']
Deployability,updates to mutect2_opt wdl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3653:0,update,updates,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3653,1,['update'],['updates']
Deployability,updating GenomicsDB integration to match the changes in the importer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2626:20,integrat,integration,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2626,1,['integrat'],['integration']
Deployability,"updating bams, sams, and cram to sam spec version 1.5 (some invalid bams were not updated); updated interval list headers for bed tests from v 1.4 - 1.5; updating several tests to give a better error message if an index IS present when it's expected to not be",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/763:82,update,updated,82,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/763,2,['update'],['updated']
Deployability,updating dataflow and htsjdk to newest versions; adding gradle versions plugin to help with identifying dependencies that need updates. This broke one of our spark related tests so I've excluded it for now. See #581. It should be reeneabled when https://github.com/cloudera/spark-dataflow/issues/49 is complete.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/582:127,update,updates,127,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/582,1,['update'],['updates']
Deployability,updating travis installed r-version to 3.2.5 which matches what's in …,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6073:16,install,installed,16,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6073,1,['install'],['installed']
Deployability,updating wrapper from 2.13 -> 3.0. disable daemon on travis since it's now enabled by default and gradle recommends disabling it on CI servers; remove jacoco version specification since 3.0 specifies a reasonable version by default; update the test result html path on travis since it changed in 3.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2097:233,update,update,233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2097,1,['update'],['update']
Deployability,upgrade Picard dependency from 2.18.13 -> 2.18.15,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5344:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5344,1,['upgrade'],['upgrade']
Deployability,upgrade Picard to 2.18.15,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5344:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5344,1,['upgrade'],['upgrade']
Deployability,upgrade bq libraries,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7264:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7264,1,['upgrade'],['upgrade']
Deployability,upgrade htsjdk to 2.13.1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3854:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3854,1,['upgrade'],['upgrade']
Deployability,upgrade log4j to 2.17,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7616:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7616,1,['upgrade'],['upgrade']
Deployability,upgrade spark to 1.6.1 and a few more upgrades,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1834:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1834,2,['upgrade'],"['upgrade', 'upgrades']"
Deployability,upgrade to google-cloud-dataflow-java-sdk-all:0.4.150710,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/754:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/754,1,['upgrade'],['upgrade']
Deployability,upgrade to gradle 2.12,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1578:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1578,1,['upgrade'],['upgrade']
Deployability,"upgrade to latest BQ libraries, V1 of APIs",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7262:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7262,1,['upgrade'],['upgrade']
Deployability,upgrade to newest shiniest gradle 3.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2097:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2097,1,['upgrade'],['upgrade']
Deployability,upgrade to spark 1.5.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1431:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1431,1,['upgrade'],['upgrade']
Deployability,upgrade to spark 2.2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2555:0,upgrade,upgrade,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2555,1,['upgrade'],['upgrade']
Deployability,"upgraded from 1.127 to 1.128; removed the local repo entirely, we now have no non-maven dependencies!. there was a small API change so I updated all those files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/172:0,upgrade,upgraded,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/172,2,"['update', 'upgrade']","['updated', 'upgraded']"
Deployability,upgrading picard dependency from 2.18.1 -> 2.18.2. this way we'll be on the latest release when we start doing MarkDuplicates tieout,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4676:83,release,release,83,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4676,1,['release'],['release']
Deployability,upgrading to htsjdk 2.13.1; 4 commits; 1. is the bare upgrade to 2.13.0 with deprecations fixed; 2. replacing our custom NON_REF_SYMBOLIC_ALLELE with the newly introduced one in HTSJDK Allele; 3. replacing our ClassFinder with the newly introduced one in htsjdk; 4. update to 2.13.1 which has an important bug fix,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3854:54,upgrade,upgrade,54,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3854,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,"uptools-36.4.0 | 563 KB | ########## | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a string.; Multidimensional indexing with anything but a tuple is deprecated. This means that the index list in ind = [slice(None), 0]; arr[ind] should be changed to a tuple, e.g., ind = [slice(None), 0]; arr[tuple(ind)] or arr[(slice(None), 0)]. That change is necessary to avoid ambiguity in expressions such as arr[[[0, 1], [0, 1]]], currently interpreted as arr[array([0, 1]), array([0, 1])], that will be interpreted as arr[array([[0, 1], [0, 1]])] in the future.; Imports from the following sub-modules are deprecated, they will be removed at some future date.; numpy.testing.utils; numpy.testing.decorators; numpy.testing.nosetester; numpy.testing.noseclasses; numpy.core.um",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:1677,release,release,1677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,1,['release'],['release']
Deployability,urces/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/variantFiltrationInfoField.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/vcfexample2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/filters/VariantFiltration/vcfMask.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/ad-bug-input.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/CEUTrio.20.21.missingIndel.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/chr21.bad.pl.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combined_genotype_gvcf_exception.nocall.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combined_genotype_gvcf_exception.original.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.2.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/combine.single.sample.pipeline.3.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/gvcf.basepairResolution.gvcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/gvcfExample1.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/leadingDeletion.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.combined.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.delOnly.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/spanningDel.depr.delOnly.g.vcf.idx; src/test/resources/org/broadinstitute/hellbender/tools/walkers/GenotypeGVCFs/testUpdatePGT.gvcf.idx; sr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3905:56968,pipeline,pipeline,56968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3905,1,['pipeline'],['pipeline']
Deployability,"uring handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5766:2615,INSTALL,INSTALLDIRGATK,2615,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766,1,['INSTALL'],['INSTALLDIRGATK']
Deployability,"urs:. `java.lang.IllegalArgumentException: cannot add a genotype with GQ=-1 because it's not within bounds [0,20); `. #### Steps to reproduce. Using a gVCF created with 4.2.0.0 HaplotypeCaller... `gatk ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz`. #### Expected behavior; Should run to completion and create reblocked GVCF. #### Actual behavior; ```; Reblocking gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz to gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; Using GATK jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar ReblockGVCF -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa -V gvcf.gather/GARDWGSN00001.autosome.g.vcf.gz -drop-low-quals -rgq-threshold 20 -do-qual-approx -O gvcf.reblock_gq20/GARDWGSN00001.autosome.g.vcf.gz; 11:25:55.531 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/pkg.7/gatk/4.2.0.0/install/bin/gatk-package-4.2.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; Jun 30, 2021 11:25:55 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 11:25:55.708 INFO ReblockGVCF - ------------------------------------------------------------; 11:25:55.709 INFO ReblockGVCF - The Genome Analysis Toolkit (GATK) v4.2.0.0; 11:25:55.709 INFO ReblockGVCF - For support and documentation go to https://software.broadinstitute.org/gatk/; 11:25:55.709 INFO ReblockGVCF - Executing as farrell@scc-hadoop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7334:1162,install,install,1162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7334,1,['install'],['install']
Deployability,use updated jars---even in split intervals!. One less place to track the jars,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7788:4,update,updated,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7788,1,['update'],['updated']
Deployability,"used with certain modifications. Let us define the ""target coverage noise"" for sample s as:. u_{st} = \sum_{\mu} W_{t \mu} z_{s \mu} + m_t. (1) [implementation] First of all, the target coverage noise must be regularized on each contig separately. It doesn't make sense to stack up all targets and take once giant FFT of u_{st}. This can be fixed in the current implementation with little effort. (2) [formal development + implementation] Within each contig, u_{st} must be mapped from target space to genomic position space e.g. via kernel density estimation. It is crucial to take into account the uncertainty in density estimation in the penalty function. For example, if the pre-image of a genomic position $x$ lies at the middle of a certain target $t$, the estimated value is much more reliable than the case where it lies between two largely separated targets. The penalty must be weighted according to the certainty of estimation. (3) [formal development + implementation] once step 1 and 2 are done, the iterative solver code must be updated accordingly. ---. @mbabadi commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246015485). @samuelklee @davidbenjamin @asmirnov239 Let's have a joint meeting at some point to discuss the problem. It is (probably) not too hard to figure out, and it will make our model really shine!. ---. @mbabadi commented on [Tue Sep 13 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-246822768). I have some notes written on this which I can discuss tomorrow in the CNV meeting. ---. @mbabadi commented on [Tue Sep 27 2016](https://github.com/broadinstitute/gatk-protected/issues/701#issuecomment-249735634). Here's a nice demonstration @asmirnov239 @samuelklee @davidbenjamin. The first sample is denoised with D=2 principal components, the second with D=8 principal components. The obvious deletion events are gone. All common events will be absorbed in the PoN once one spends """,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2892:1544,update,updated,1544,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2892,1,['update'],['updated']
Deployability,"using --conf passes args into the spark configuration. these args will take precendence over spark args specified in any other way. moved --sparkMaster and --conf to their own SparkCommandLineArgumentCollection. passing spark options through to gatk with DIRECT, this will only work with --sparkMaster and --conf. fixes #1339",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1356:40,configurat,configuration,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1356,1,['configurat'],['configuration']
Deployability,using travis' git lfs instead of installing it,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3226:33,install,installing,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3226,1,['install'],['installing']
Deployability,"ut allow NONE as input (#7206); - SA support and consistent naming for all GVS WDLs (#7205); - fix GvsExtractCallset inputs file (#7210); - add clustering to tables (#7207); - add vqsr cutoffs to GvsExtractCallset wdl; clean up dockstore yml (#7209); - Avro test (#7192); - Enable call caching of TSV generation in GvsImportGenomes (#7226); - 266 Clean up ExtractCohort -- remove query mode param (#7227); - 288 Add an excess alleles param (#7221); - take sample name as a param (#7236); - How to run GIAB comparisons (#7237); - Update GvsCreateFilterSet.wdl (#7239); - Use GatherVcfsCloud in GvsCreateFilterSet.wdl (#7241); - parameterize TTL with defaults, reduce memory allocation (#7244); - Addressing OOM in CohortExtract (#7245); - make outputs optional, change case in output (#7252); - Support for FORMAT/FT VQSLod Filtering and cohort-wide LowQual filter (#7248); - removed arrays code, renamed packages (#7260); - 279 labels (#7233); - add conda commands to GIAB readme (#7268); - remove gvs branch (#7263); - remove gvs branch (#7263); - upgrade bq libraries (#7264); - #299 - Sample list ease of use for cohort extracts (#7272); - check for duplicate ids (#7273); - Rc 274 passing sites only (#7275); - added default value to drop_state; broadinstitute/dsp-spec-ops#310 (#7278); - version bump for reliability (#7284); - add timestamp check to ExtractTask call https://github.com/broadinstitute/dsp-spec-ops/issues/320; - serial inserts for scaling prepare, factored out sample name (#7288); - Remove training sites only param from ExtractFeatures broadinstitute/dsp-spec-ops#261; - add param for mem for indels (#7282); - Ah prepare localize option (#7299); - Export sites only vcf STEP 1-- 317 add AC, AN, AF to the final VCF (#7279); - AoU GVS Cohort Extract wdl (#7242); - reliability (#7310); - bump to include FT tag filtering (#7316); - First pass at a Terra QuickStart (#7267); - Ah fix timestamp query (#7319); - 313 Cleanup Extract Cohort params (#7293); - bump bq storage versi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:14515,upgrade,upgrade,14515,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['upgrade'],['upgrade']
Deployability,"ut then for Haplotypecaller, and you have opened a bugreport to add a feature to ValidateVariants: https://github.com/broadinstitute/gatk/issues/6553. However, it would be nice if you could actually investigate the formatting error. Unfortunately my formatting error isn't the same as reported in the other post. I have 105 error in which the 1st alternative allele is a spanning deletion and the 2nd (and 3rd) is either an indel or snp. It's true that the 2nd and 3rd allele is actually not found in my samples. I even have 7 occurances in which the 1st allele (spanning deletion) has allele frequency 1.00. my code is the following for GenotypeGVCFs:. java -Xms32G -Xmx32G -jar ${gatk4} GenotypeGVCFs -R ${ref} -V ${pipeline}/${name}\_v4.1.6.0.g.vcf.gz -O ${vcf}/${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list 2> ${log}/${name}\_v4.1.6.0\_genotype.log. for ValidateVariants:. java -Xms10G -Xmx10G -jar ${gatk4} ValidateVariants -R ${ref} -V ${name}\_v4.1.6.0.vcf.gz -L ${pipeline}/${name}\_intervals.list --warn-on-errors 2> ${log}/${name}\_v4.1.6.0\_genotype\_valivar.log. the warning in ValidateVariants and the site look like this:. 14:12:15.126 WARN ValidateVariants - \*\*\*\*\* Input 1st\_v4.1.6.0.vcf.gz fails strict validation of type ALL: one or more of the ALT allele(s) for the record at position chr\_1:1088200 are not observed at all in the sample genotypes \*\*\*\*\* ; ; chr\_1 1088200 . T \*,TAAAAAAAAAAAA 64.39 . AC=8,0;AF=0.667,0.00;AN=12;DP=118;ExcessHet=3.0103;FS=0.000;InbreedingCoeff=0.4286;MLEAC=7,7;MLEAF=0.583,0.583;MQ=58.73;QD=32.19;SOR=2.303 GT:AD:DP:GQ:PL ./.:9,0,0:9:.:0,0,0,0,0,0 0/0:9,0,0:9:0:0,0,113,0,113,113 ./.:10,0,0:10:.:0,0,0,0,0,0 ./.:5,0,0:5:.:0,0,0,0,0,0 1/1:0,0,1:1:0:225,15,0,15,0,0 ./.:0,0,0:0:.:0,0,0,0,0,0 ./.:12,0,0:12:.:0,0,0,0,0,0 ./.:8,0,0:8:.:0,0,0,0,0,0 0/0:3,0,0:3:0:0,0,43,0,43,43 ./.:7,0,0:7:.:0,0,0,0,0,0 ./.:1,0,0:1:.:0,0,0,0,0,0 ./.:0,0,0:0:.:0,0,0,0,0,0 ./.:3,0,0:3:.:0,0,0,0,0,0 ./.:7,0,0:7:.:0,0,0,0,0,0 1/1:0,0,0:0:0:45,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6630:1722,pipeline,pipeline,1722,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6630,1,['pipeline'],['pipeline']
Deployability,ute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.bro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:53651,deploy,deploy,53651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['deploy'],['deploy']
Deployability,"ute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:4612,deploy,deploy,4612,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy']
Deployability,"util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Ut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:12584,pipeline,pipelines,12584,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['pipeline'],['pipelines']
Deployability,"v2 gVCFs [VS-491] (#7924); - KM GVS documentation (#7903); - Track BigQuery costs of GVS python VS-480 (#7915); - Read cost observability table [VS-475] (#7923); - Fix Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail im",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:26729,Update,Update,26729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Update'],['Update']
Deployability,"va.io.tmpdir=/cromwell_root/tmp.H9t5pC; [December 14, 2017 7:41:30 PM UTC] GenomicsDBImport --genomicsDBWorkspace genomicsdb --batchSize 50 --sampleNameMap /cromwell_root/broad-jg-dev-storage/freimer_dutch_fin_wgs_v1/v1/sample_map --readerThreads 5 --intervals chr1:1-391754 --interval_padding 500 --genomicsDBSegmentSize 1048576 --genomicsDBVCFBufferSize 16384 --overwriteExistingGenomicsDBWorkspace false --consolidate false --validateSampleNameMap false --interval_set_rule UNION --interval_exclusion_padding 0 --interval_merging_rule ALL --readValidationStringency SILENT --secondsBetweenProgressUpdates 10.0 --disableSequenceDictionaryValidation false --createOutputBamIndex true --createOutputBamMD5 false --createOutputVariantIndex true --createOutputVariantMD5 false --lenient false --addOutputSAMProgramRecord true --addOutputVCFCommandLine true --cloudPrefetchBuffer 0 --cloudIndexPrefetchBuffer 0 --disableBamIndexCaching false --help false --version false --showHidden false --verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [December 14, 2017 7:41:30 PM UTC] Executing as root@7ca892f01ff3 on Linux 4.9.0-0.bpo.3-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_111-8u111-b14-2~bpo8+1-b14; Version: 4.beta.6; [December 14, 2017 7:41:30 PM UTC] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 0.01 minutes.; Runtime.totalMemory()=4116185088; ***********************************************************************. A USER ERROR has occurred: Bad input: Expected a file of format; Sample	File; but found line: I-PAL_FR02_000639 001	gs://broad-gotc-prod-storage/pipeline/G87944/gvcfs/I-PAL_FR02_000639_001.023ca2f7-4fba-4617-9f65-cb989818c858.g.vcf.gz. ***********************************************************************; Set the system property GATK_STACKTRACE_ON_USER_EXCEPTION (--javaOptions '-DGATK_STACKTRACE_ON_USER_EXCEPTION=true') to print the stack trace.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3979:2815,pipeline,pipeline,2815,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3979,1,['pipeline'],['pipeline']
Deployability,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5997:2261,deploy,deploy,2261,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997,7,['deploy'],['deploy']
Deployability,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5942:5787,deploy,deploy-mode,5787,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942,1,['deploy'],['deploy-mode']
Deployability,variantcontext.VariantContextBuilder.make(VariantContextBuilder.java:638); at picard.util.LiftoverUtils.liftVariant(LiftoverUtils.java:92); at picard.vcf.LiftoverVcf.doWork(LiftoverVcf.java:426); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:305); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:206); at org.broadinstitute.hellbender.Main.main(Main.java:292). ```. #### Steps to reproduce. Download vcf from here:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_SVs_Integration_v0.6/HG002_SVs_Tier1_v0.6.vcf.gz. gatk LiftoverVcf \; -I b37/HG002_SVs_Tier1_v0.6.vcf.gz \; -O b38/HG002_SVs_Tier1_v0.6.hg38.vcf.gz \; -CHAIN grch37_to_grch38.over.chain.gz \; --REJECT b38/HG002_SVs_Tier1_v0.6.rejected.vcf.gz \; -R /restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa. #### Expected behavior; The original b37 vcf has a deletion here:. 1 532077 ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT A; SVLEN=-100;;SVTYPE=DEL;END=532177;sizecat=100to299;. The liftover to hg38 should look like this:; chr1 596697 REF=ACATTCATGCTCACTCATACACACCCAGATCATATATACACTCGTGCACACATTCACACTCATACACACCCAAATCATACTCACATTCATGCACACATGTT; ALT=A; INFO Fields; SVLEN=-100; SVTYPE=DEL;END=596797;sizecat=100to299;. The error message suggests LiftoverVcf is not updating the INFO/END field from 532177 to 596797 and an error is being triggered since the END is before the start. An incorrect INFO/END will cause problems with tabix and other programs. #### Actual behavior; It generates an error when the INFO/END is before the start and aborts.. ----. ## Feature request; Liftover INFO/END . ### Description; ; The INFO/END position also needs to be updated-not just the site position.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6725:5341,update,updated-not,5341,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6725,1,['update'],['updated-not']
Deployability,"verbosity INFO --QUIET false --use_jdk_deflater false --use_jdk_inflater false --gcs_max_retries 20 --disableToolDefaultReadFilters false; [November 15, 2017 7:43:09 PM UTC] Executing as root@gatk-test-8875b999-b609-4a3f-86ea-973b929fe662-m on Linux 3.16.0-4-amd64 amd64; OpenJDK 64-Bit Server VM 1.8.0_131-8u131-b11-1~bpo8+1-b11; Version: 4.beta.6-37-g0a135f8-SNAPSHOT; 19:43:09.992 INFO PrintVariantsSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 1; 19:43:09.992 INFO PrintVariantsSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:43:09.993 INFO PrintVariantsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 19:43:09.993 INFO PrintVariantsSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:43:09.993 INFO PrintVariantsSpark - Deflater: IntelDeflater; 19:43:09.993 INFO PrintVariantsSpark - Inflater: IntelInflater; 19:43:09.993 INFO PrintVariantsSpark - GCS max retries/reopens: 20; 19:43:09.993 INFO PrintVariantsSpark - Using google-cloud-java patch c035098b5e62cb4fe9155eff07ce88449a361f5d from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 19:43:09.993 INFO PrintVariantsSpark - Initializing engine; 19:43:09.993 INFO PrintVariantsSpark - Done initializing engine; 17/11/15 19:43:11 INFO org.spark_project.jetty.util.log: Logging initialized @4976ms; 17/11/15 19:43:11 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 17/11/15 19:43:11 INFO org.spark_project.jetty.server.Server: Started @5092ms; 17/11/15 19:43:11 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@5917b44d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/15 19:43:12 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 17/11/15 19:43:13 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at gatk-test-8875b999-b609-4a3f-86ea-973b929fe662-m/10.240.0.18:8032; 17/11/15 19:43:17 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted applicatio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:3369,patch,patch,3369,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['patch'],['patch']
Deployability,"versed.selfRef.shifted.homoplasmies.vcf.bgz \\ ; ; \--annotation StrandBiasBySample \\ ; ; \--mitochondria-mode \\ ; ; \--max-reads-per-alignment-start 75 \\ ; ; \--max-mnp-distance 0 \\ ; ; \-L chrM:8023-9140 \\ ; ; \--genotype-filtered-alleles \\ ; ; \--debug-assembly-variants-out /rej.vcf \\ ; ; \--bam-output bamout.bam. In this instance the variant in question is listed in the rej.vcf file obtained via `--debug-assembly-variants-out`. I have examined `bamout.bam` as well as the input bam and there appears to be ample coverage at the site of interest (the T at position 8316 is the position of interest, highlighted):. ![](https://gatk.broadinstitute.org/hc/user_images/aGbHKebG7Tb8Lgu33gGzXw.png). I have tried running this with some of the additional parameters in \[[https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant\](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)](https://gatk.broadinstitute.org/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant](/hc/en-us/articles/360043491652-When-HaplotypeCaller-and-Mutect2-do-not-call-an-expected-variant)) (namely `--linked-de-bruijn-graph` and `--recover-all-dangling-branches`) to no avail. Coverage is very deep at this position (>2000x). Notably if I edit the input to `--alleles` and change the allele of interest (8316:T>A) to anything else (8316:T>C or T>G) it appropriately shows up in the output VCF. What am I missing here? Let me know if you have any solutions or if you need any additional files. UPDATE: Adding `--disable-adaptive-pruning` now produces the variant of interest specified in --alleles, but also adds several other new calls, in case that is helpful in isolating where this force-call variant is being lost.<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/270138'>Zendesk ticket #270138</a>)<br> gz#270138</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7672:2832,UPDATE,UPDATE,2832,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7672,1,['UPDATE'],['UPDATE']
Deployability,version: gatk 4.0.2.1; I use the pipeline :BwaAndMarkDuplicatesPipelineSpark-》BQSRPipelineSpark-》HaplotypeCallerSpark，and I get the bad result。by testing，HaplotypeCallerSpark lose a lot of variable sites and HaplotypeCallerSpark 'result jitter to the same input bam。,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4488:33,pipeline,pipeline,33,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4488,1,['pipeline'],['pipeline']
Deployability,"via htsjdk's new wrapper feature.; Also provide a command-line switch to tune or disable it if necessary. A test with CountReads on a ~900MB input shows a 40MB buffer; gives over 5x speedup. DO NOT SUBMIT until htsjsk's new version is released; that incorporates the [wrapper feature](https://github.com/samtools/htsjdk/pull/775).; Then, update the build file before submitting. Sample run:. $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=0; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 2.82 minutes.; $ ./gatk-launch CountReads -I ""gs://${INPUTFOLDER}/CEUTrio.HiSeq.WGS.b37.ch20.4m-12m.NA12878.bam"" --cloudPrefetchBuffer=40; (...); org.broadinstitute.hellbender.tools.CountReads done. Elapsed time: 0.49 minutes. cc: @lbergelson @droazen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2331:235,release,released,235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2331,2,"['release', 'update']","['released', 'update']"
Deployability,"wa alignment step. Here's the header of one of our bam files.  . **@RG ID:A00721 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-5777C63B PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-3114896D PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-6C451827 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-1FFBF48 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-1556FE90 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-372502F PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-36D79EE7 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-475DB4E0 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-11A8D08 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-EA51124 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-7CB06E78 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-2EFFCB0F PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-2393C5A8 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-D5FCA3D PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-126BE531 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-4AA85F15 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-5F830B17 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-4D22D207 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-5BACA407 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-51BD9ACF PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-6BF8B69A PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-24C33FB1 PL:Illumina SM:17039\_N LB:17039\_N** ; **@RG ID:A00721-49B17692 PL:Illumina SM:17039\_N LB:17039\_N**.  . For clarification the version of JAVA which runs GATK is written below. openjdk version ""1.8.0\_152-release"" ; OpenJDK Runtime Environment (build 1.8.0\_152-release-1056-b12) ; OpenJDK 64-Bit Server VM (build 25.152-b12, mixed mode).  . Thank you. . Min-Hwan<br><br><i>(created from <a href='https://broadinstitute.zendesk.com/agent/tickets/4866'>Zendesk ticket #4866</a>)<br>gz#4866</i>",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:54802,release,release,54802,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,2,['release'],"['release', 'release-']"
Deployability,"we need a canonical set of tests that we run when we upgrade the cluster. We've been running terasort but it's not enough: 1) it does not run our code and 2) it does not even run java8 (recent config error when 2 nodes were running java7 was undetected). The task here is to write, in readme or in scripts directory, a script or set of scripts that must be run after every change to the cluster.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1392:53,upgrade,upgrade,53,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1392,1,['upgrade'],['upgrade']
Deployability,we need a way to bind to a BWA-MEM library to align reads programmatically from within the GATK. . SVs need it and it would simplify the reads pipeline to have it start with a fasta. Commandline behavior we need: `bwa mem -K 100000000 -p` (we may not care about multithreading); - `-K` is undocumented - reading code shows it refers to `fixed_chunk_size` which then sets `actual_chunk_size` @lh3 can you help us understand what it `-K` does?; - `-p` is for interleaved input,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1517:143,pipeline,pipeline,143,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1517,1,['pipeline'],['pipeline']
Deployability,"we should select and migrate code from googlegenomics/genomics-pipeline and move development to the hellbender repository. For now, move everything and we'll clean it up once it's in.; Assigning to @wbrockman to decide when it can be done (it's a private repo).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/427:63,pipeline,pipeline,63,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/427,1,['pipeline'],['pipeline']
Deployability,"we've been recommending a very old version, might as well upgrade",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/941:58,upgrade,upgrade,58,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/941,1,['upgrade'],['upgrade']
Deployability,"will still provide the background default (or the built-in ploidy of 2 for humans), but the user input value will supersede these in overlapping regions. Note that the overlap is checked against the active region, meaning variants near the boundary of the `--ploidy-regions` file may end up with GT fields having ploidy slightly differently than expected, for example if your custom region overlaps a given active region but the variant ends up being written to a location outside that interval. In this case the ploidy from the user input would be used rather than any other default. # Implementation Details. The key idea is to allow `HaplotypeCallerEngine` to initialize multiple genotyping engines based on the `--ploidy-regions` input. The intervals are first parsed to check for positive integer ploidy values, and then used to create hashmaps of ploidy -> genotyper. The engine uses two types of genotypers: one for active region determination and one for doing the actual genotyping. Both admit a ploidy paramter passed via `hcArgs`. This PR modifies the `HaplotypeCallerArgumentCollection` class to include a method for creating copies of this object with differing ploidy amounts. These then get fed to the constructors of the appropriate genotyper classes, which are organized into two hashmaps. In every situation where one of these genotypers is used, we instead begin the scope by calling a ""get local genotyper"" method that performs the logic of checking whether the region of interest overlaps any of the user-provided regions, and then selects the appropriate `localEngine` genotyper for the task, ensuring the user-provided ploidy supersedes any other defaults. # A Note on Dependency. The flexibility of using either .bed or .interval_list files to specify this information depends on [this](https://github.com/samtools/htsjdk/pull/1680) PR in htsjdk being made into a full release, and then bumping the dependency of GATK. The code in this PR would not compile until this happens.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8464:2424,release,release,2424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8464,1,['release'],['release']
Deployability,"working to setup a singularity container for gatk-4.1.4.0. while preparing the gatk conda environment numpy-1.13.3 ins installed but biopython==1.70 requirement from the pip section of the gatkcondaenv.yml. removes it and install numpy-1.18.1. see relevant part of conda env create -n gatk -f gatk-4.1.4.0/gatkcondaenv.yml 2>&1 | tee log; NB full log is attached : [log.txt](https://github.com/broadinstitute/gatk/files/4091802/log.txt). ```; Collecting package metadata (repodata.json): ...working... done; Solving environment: ...working... done. Downloading and Extracting Packages. keras-preprocessing- | 36 KB | ########## | 100%; astor-0.8.0 | 46 KB | ########## | 100%; setuptools-36.4.0 | 563 KB | ########## | 100%; termcolor-1.1.0 | 8 KB | ########## | 100%; protobuf-3.11.2 | 635 KB | ########## | 100%; keras-applications-1 | 33 KB | ########## | 100%; readline-6.2 | 606 KB | ########## | 100%; libgfortran-ng-7.3.0 | 1006 KB | ########## | 100%; numpy-1.13.3 | 3.1 MB | ########## | 100%; ```. numpy-1.13.3 is corectly installed . but then . ```; Collecting numpy (from biopython==1.70->-r /root/gatk-4.1.4.0/condaenv.g1uyq0ce.requirements.txt (line 1)); Downloading https://files.pythonhosted.org/packages/62/20/4d43e141b5bc426ba38274933ef8e76e85c7adea2c321ecf9ebf7421cedf/numpy-1.18.1-cp36-cp36m-manylinux1_x86_64.whl (20.1MB); ```. that does . ```; Found existing installation: numpy 1.13.3; Uninstalling numpy-1.13.3:; Successfully uninstalled numpy-1.13.3; ```. this causes ```gatk DetermineGermlineContigPloidy ```; to exit with an error related to numpy.testing.decorators which is deprecated since numpy 1.15.0 see https://docs.scipy.org/doc/numpy-1.15.0/release.html. ```; Deprecations. Aliases of builtin pickle functions are deprecated, in favor of their unaliased pickle.<func> names:; numpy.loads; numpy.core.numeric.load; numpy.core.numeric.loads; numpy.ma.loads, numpy.ma.dumps; numpy.ma.load, numpy.ma.dump - these functions already failed on python 3 when called with a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6396:119,install,installed,119,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6396,2,['install'],"['install', 'installed']"
Deployability,"x Race Condition, Add Support for Extract by Array of Sample Names (ie from a Sample Set) (#7917); - Rightsize import batches [VS-486] (#7925); - [AoU DRC] Support uppercase site_ids for reblocking (#7929); - Populate cost metadata for GATK tasks. (#7919); - remove accidentally added input (#7931); - VS_492 - Beta User Jar release (#7934); - Cost WDL should throw on FISS API errors [VS-518] (#7942); - Fix bad check for missing workflow name [VS-520] (#7943); - Remove usage of service account from GvsValidateVAT.wdl (#7937); - refactoring for testablity (#7946); - More import retries [VS-532] (#7953); - A few last doc changes (#7927); - WDL to extract a single callset cost (BQ only, not Terra) (#7940); - Temporarily swap in Corretto for Temurin as we can't download Temurin. (#7969); - GL-548 - Update CreateVat code to handle samples that do not contain all population groups. (#7965); - Restore Temurin 11 [VS-570] (#7972); - Add table size check to quickstart integration test [VS-501] (#7970); - Consolidate various docs for AoU callset generation into one to rule them all [VS-553] (#7971); - VS-567. Removing usage of ServiceAccount from CreateVat related WDLs (#7974); - WDL to extract Avro files for Hail import [VS-579] (#7981); - Removed usage of service account from WDLs (#7985); - Document steps for GVS cleanup for base use case [VS-586] (#7989); - Change backticks to single quotes in several error messages - causing shell to attempt to execute. (#7995); - VS-598 - Minor update to AoU Documentation. (#7994); - Allow for incremental addition of data to alt_allele [VS-52] (#7993); - Minor AoU Documentation Update (#7999); - Batch population of alt_allele table from vet_ tables [VS-265] (#7998); - Change drop_state to NONE for Ingest/Extract [VS-607] (#8000); - python -> python3 (#8001); - Generate Hail import/export script [VS-605] (#8002); - clearer error when values are missing (#7939); - Ah [VS-565] output intervals and sample list (#8010); - make CreateAltAlleleT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:26897,integrat,integration,26897,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['integrat'],['integration']
Deployability,"xtension of the 'coherent' evidence concept previously used in determining evidence thresholds for assembly. The code in this PR contains the following changes:. - Evidence intervals and distal targets now are treated as stranded, and evidence-target link clustering depends on overlaps between both intervals and strands.; - Evidence target interval and distal target interval calculations have been modified to make sure that evidence supporting the same event clusters together (has overlapping intervals). This includes several changes such as extending the 'rest-of-fragment-size' calculation to try to capture almost all non-outlier fragment sizes in the library; increasing the split read location uncertainty a little; and being more precise about the boundaries of distal target intervals by taking advantage of information in the MD and MC tags if available.; - Evidence target links are gathered for every piece of evidence supporting a high-quality distal target. ; - Evidence target links are clustered together and store the amount of split-read and read-pair evidence that went into each cluster.; - All evidence target link clusters that are composed of at least 1 split read or at least 2 read pairs are collected in the driver and emitted in a BEDPE formatted file specified in the command line parameters.; - A `PairedStrandedIntervalTree` data structure is introduced to allow `SVIntervalTree`-style lookups for paired intervals. To finish this work, future PRs will 1) use the collected evidence target links to annotate our assembly called-variants with the number of split reads and read pairs observed in the original mappings and 2) create IMPRECISE VCF records for events that have enough evidence-target-link support, first for deletions and then possibly for other variant types. Initial testing shows that these changes slightly increase the number of variants called by the current pipeline, on both the CHM mix and NA12878 data sets, without greatly affecting run time.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3469:2132,pipeline,pipeline,2132,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3469,1,['pipeline'],['pipeline']
Deployability,"y exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----. Please let us know when log4j Vulnerability issue can be released to Conda? we are using this package and waiting log4j issue fixed.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7603:2336,release,released,2336,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7603,1,['release'],['released']
Deployability,"y increasing TILEDB_UPLOAD_BUFFER_SIZE to at least 5MB ; ```. #### Steps to reproduce. Can't produce a small reproducible examples because it only happens with the full dataset. However, below is the command that I ran. . ```sh; gatk --java-options -Xms16g GenomicsDBImport \; --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 \; --batch-size 50 -L 0000-scattered.interval_list \; --sample-name-map sample_map.csv \; --reader-threads 16 \; --merge-input-intervals \; --consolidate; ```. * `sample_map.csv` contains GCS paths to the GVCFs.; * `0000-scattered.interval_list` is one interval generated by calling SplitIntervals to make 50 intervals. #### Expected behavior. Finish without an error, write DB to the specified bucket. #### Actual behavior. Throws a TileDB error. . Does it have to do with the `--consolidate` flag? I couldn't find what `TILEDB_UPLOAD_BUFFER_SIZE` means, but the [TileDB docs](https://docs.tiledb.com/main/how-to/configuration) reference ""sm.consolidation.buffer_size"" with the default size of 50000000 (50MB?). I'll try rerunning without consolidation. Full log:. ```sh; Using GATK jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xms16g -jar /root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar GenomicsDBImport --genomicsdb-workspace-path gs://cpg-seqr-main-analysis/seqr_loader/v0/genomicsdbs/interval_0_outof_50 --batch-size 50 -L /io/batch/8900b8/inputs/kownK/0000-scattered.interval_list --sample-name-map /io/batch/8900b8/inputs/ZHdri/sample_map.csv --reader-threads 16 --merge-input-intervals --consolidate; 14:26:51.130 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/root/micromamba/share/gatk4-4.2.3.0-1/gatk-package-4.2.3.0-local.jar!/com/intel/gkl/native/libgkl_compre",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7653:1623,configurat,configuration,1623,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7653,1,['configurat'],['configuration']
Deployability,"y, at least) ready for release. This solves the original problem of release of some tools being blocked by others, but creates some other problems: last-minute merge conflicts across dev teams, large amounts of code being held back for months while it undergoes testing, harder to share code across groups, more complex git workflows for everyone.; 4. Everyone is free to merge development versions of tools to master (as is currently the case), and most of the time we try to release everything in the GATK together. On rare occasions when, eg., CNV needs a release now and HC is not ready, we create a branch off of the last tagged release, cherry-pick the CNV tools (or whatever) into it, and release that. Then when the HC stabilizes and master is once again releasable, we do the next release from master. I've renamed this issue to make the problem we're trying to solve clearer. @akiezun @lbergelson @LeeTL1220 @vdauwera would you vote for any of the above options? Do you have alternate proposals that solve the same problem and you think are better? Should we seek professional (release engineering) help?. ---. @akiezun commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215761749). only 4 seems remotely sane to me. ---. @vdauwera commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215779225). 3 and 4 both produce an acceptable result for me but I could see 3 being too hard on the dev team. So I'll go with 4. I think the inconvenience of cutting a special cherry picked release is enough to dissuade casual/unnecessary releases, but low enough to not be a blocker if we really do need to release a hot fix. ---. @LeeTL1220 commented on [Fri Apr 29 2016](https://github.com/broadinstitute/gatk-protected/issues/473#issuecomment-215793338). Cherry-picking sounds awful to me, but not as awful as the others... I could do number three. ---. @akiezun commented on [Fri Apr 29 2016](ht",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2851:4903,release,release,4903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2851,1,['release'],['release']
Deployability,"y.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:111); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceSequenceDictionary(ReferenceHadoopSparkSource.java:41); at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceSequenceDictionary(ReferenceMultiSparkSource.java:93); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeReference(GATKSparkTool.java:604); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.initializeToolInputs(GATKSparkTool.java:553); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:544); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$4.run(ApplicationMaster.scala:721) ; ```; The above makes sense, that's why I added the hostname and port for the namenode. It seems like after verifying that the file `hdfs://cromwellhadooptest/user/hadoop/gatk/common/human_g1k_v37.20.21.fasta` exists, some code transforms this path into `file:///user/hadoop/gatk/common/human_g1k_v37.20.21.fasta`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6730:8033,deploy,deploy,8033,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6730,1,['deploy'],['deploy']
Deployability,"y; 18/04/23 20:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:36833 with 4.0 GB RAM, BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/23 20:41:43 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 329.7 KB, free 4.0 GB); 18/04/23 20:41:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180423204143-0003/0 is now RUNNING; 00:09 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 4.0 GB); 18/04/23 20:41:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:36833 (size: 27.5 KB, free: 4.0 GB); 18/04/23 20:41:47 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/23 20:41:47 INFO FileInputFormat: Total input files to process : 1; 18/04/23 20:41:51 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/23 20:41:51 INFO DAGScheduler: Got job 0 (first at ReadsSparkSource.java:221) with 1 output partitions; 18/04/23 20:41:51 INFO DAGScheduler: Final stage: ResultStage 0 (first at ReadsSparkSource.java:221); 18/04/23 20:41:51 INFO DAGScheduler: Parents of final stage: List(); 18/04/23 20:41:51 INFO DAGScheduler: Missing parents: List(); 18/04/23 20:41:51 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at filter at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:10393,update,updated,10393,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['update'],['updated']
Deployability,"~Draft PR for mobbing discussion~ Ready for review, integration tested [here](https://app.terra.bio/#workspaces/broad-firecloud-dsde/VS-415%20GVS%20Quickstart%20Default%20Extract%20Scatter/job_history/7ef604ff-46e8-45d9-be39-e88276db993b).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7880:52,integrat,integration,52,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7880,1,['integrat'],['integration']
Deployability,"    at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:51) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192) ; ;     at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211) ; ;     at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160) ; ;     at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203) ; ;     at org.broadinstitute.hellbender.Main.main(Main.java:289). And I will get the same error when I assign the temp directory in another way:. /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk --java-options ""-Xmx30G"" BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_resource/dbsnp\_146.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/1000G\_phase1.snps.high\_confidence.hg38.vcf.gz --known-sites /data/reference/gatk\_resource/Mills\_and\_1000G\_gold\_standard.indels.hg38.vcf.gz  -O /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.recal\_data.table --tmp-dir /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam ; ; Using GATK jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar ; ; Running: ; ;     java -Dsamjdk.use\_async\_io\_read\_samtools=false -Dsamjdk.use\_async\_io\_write\_samtools=true -Dsamjdk.use\_async\_io\_write\_tribble=false -Dsamjdk.compression\_level=2 -Xmx30G -jar /data/xieduo/WES\_pipe/pipeline/bin/gatk-4.2.6.1/gatk-package-4.2.6.1-local.jar BaseRecalibrator -R /data/reference/gatk\_resource/Homo\_sapiens\_assembly38.fasta -I /data/xieduo/Immun\_genomics/data/Łuksza\_2022\_Nature/bam/PAAD11N.rmdup.bam --known-sites /data/xieduo/WES\_pipe/pipeline/gatk\_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8005:7680,pipeline,pipeline,7680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8005,1,['pipeline'],['pipeline']
Deployability,"… as well as excluding log4j 1.x. GKL 0.5.6 now uses the log4j 1.x API for logging, and we use the log4j-1.2-api bridge JAR to redirect to log4j2 implementation. See [here](https://logging.apache.org/log4j/2.0/faq.html#which_jars) for details. This change was made because GATK 3.x uses log4j 1.x, and users were reporting errors in the output. This release fixes those errors. GATK 4 uses log4j2 and, in order to make the API compatible with the GKL, we need to add a dependency on the log4j-1.2-api bridge. Unfortunately, the log4j 1.X JAR is also brought in due to some transitive dependency from another package, which causes conflicts with the log4j-1.2-api bridge package. To solve that, we need to exclude log4j 1.X from the dependencies, and let log4j-1.2-api take care of any calls to the log4j 1.X API, redirecting them to the log4j2 implementation. See [here](https://logging.apache.org/log4j/2.0/faq.html#exclusions) for details.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3416:350,release,release,350,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3416,1,['release'],['release']
Deployability,"…ation (AVX vs AVX-OMP), max num threads, and toggle using double precision. The changes are based off of [the lb_connect_pairhmmargs branch](https://github.com/broadinstitute/gatk/tree/lb_connect_pairhmmargs), which, apparently, never got merged. The main difference between that branch and this fork is that the PairHMMNativeArguments object is passed all the way through to VectorLoglessPairHMM, which was not the case in the lb_connect_pairhmmargs branch. There is a corresponding [fork of gatk-protected](https://github.com/erniebrau/gatk-protected-1) which defines the new CLI arguments that talk to the changes in this PR. There should be a PR from that fork to gatk-protected, if and when this one is merged.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2574:46,toggle,toggle,46,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2574,1,['toggle'],['toggle']
Deployability,"…bly to be activated if a mininum number of pieces of evidence agree on the distal target. Also:. - Some refactoring of the SATagAlignment and builder classes to support better treatment of SA tags.; - Increased the spark network timeout values for the SV pipeline to prevent nodes from losing heartbeats and being orphaned with running tasks. Since I made this change I have not had the issue. On the performance of this change on our calls:. I compared this branch with master. Master's results on the CHM1/13 mix:. ```; 16:57:37.270 INFO StructuralVariationDiscoveryPipelineSpark - Metadata retrieved.; 16:58:20.436 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 25977 intervals.; 16:58:20.517 INFO StructuralVariationDiscoveryPipelineSpark - Killed 377 intervals that were near reference gaps.; 16:58:49.939 INFO StructuralVariationDiscoveryPipelineSpark - Killed 175 intervals that had >1000x coverage.; 16:59:33.036 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 8773016 mapped template names.; 17:00:07.058 INFO StructuralVariationDiscoveryPipelineSpark - Ignoring 19200460 genomically common kmers.; 17:05:25.896 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 34752266 kmers.; 17:10:46.253 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 31945322 unique template names for assembly.; 17:45:06.748 INFO StructuralVariationDiscoveryPipelineSpark - Wrote SAM file of aligned contigs.; 17:45:26.199 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 5716 variants.; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INV: 231; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DEL: 3262; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - DUP: 1065; 17:45:26.210 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1158; 17:45:26.397 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 8, 2017 5:45:26 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDis",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2684:256,pipeline,pipeline,256,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2684,1,['pipeline'],['pipeline']
Deployability,"…en running on Spark. These are tests that rely on user exceptions being returned to the driver, which Spark does not yet support. (If there's a better way of excluding tests, then please let me know.). The upgrade includes some changes to the runner that fix some of the failing tests too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/574:207,upgrade,upgrade,207,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/574,1,['upgrade'],['upgrade']
Deployability,…gning singletons to BwaEngine classes. Updates bwamem-jni depedency to 1.0.2 and adds the possibility of aligning singletons to BwaEngine classes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3474:40,Update,Updates,40,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3474,1,['Update'],['Updates']
Deployability,"…joint-genotyping from the resulting GVCFs. @ldgauthier & @davidbenjamin this PR is a follow up from our short conversation in #4650 a couple of months ago, where I was wanting to generate GVCFs with MNP support. My goal here is that I really want to be able to generate a single VCF that a) gives me reference confidence and b) gives me MNPs for close by variants. This is for a clinical pipeline where all calling is done one sample at a time, so the problem of joint-genotyping from different MNP representations doesn't come up. I did briefly look at using `--emit-ref-confidence BP_RESOLUTION` but that has two issues that make me prefer this route:. 1) The generated files are really very large because they have a row for every single BP; 2) More problematic, is that when there is a MNP of say `ACG/GCT` two things happen that are less than ideal from my perspective. The first is that rows are emitted into the VCF for all three positions (the variant at A's position, and two `<NON_REF>` lines at the positions for the C and T respectively). Secondly, when one or more bases is the same in both MNP alleles (the C in this case) that base is output with a very high hom-ref GQ, which feels wrong!. I'm more than happy to modify this PR to address any concerns you have (e.g. adding a `--force-mnps-with-gvcfs` parameter that has to be specified, or requiring `--unsafe` to enable this). I'm also open to other solutions, but this seemed expedient and reasonable for folks running single-sample pipelines like you see in clinical settings.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5182:389,pipeline,pipeline,389,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5182,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,…ration tests. The fix for the original bug (CompareSAMs not obeying stringency) is a one line fix in CompareSAMs. The two BQSR integration tests referenced in the issue use a different code path and required a different fix (assuming that relaxing the stringency is the right thing to do in those cases). I also added a new CompareSAMs integration test and changed the CompareSAMs tool to return result of the comparison.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/604:128,integrat,integration,128,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/604,2,['integrat'],['integration']
Deployability,"…tion plots). Should prevent VariantRecalibrator from failing in a docker without R. I tested by building a new docker from the image with the NIO fix, adding a jar from this branch, then running the SNPSVariantRecalibratorCreateModel task from the joint calling pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3383:263,pipeline,pipeline,263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3383,1,['pipeline'],['pipeline']
Energy Efficiency," 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. [Stage 21:> (0 + 60) / 3539]18/12/21 16:08:30 ERROR org.apache.spark.scheduler.TaskSetManager: Task 26 in stage 21.0 failed 4 times; aborting job; 18/12/21 16:08:30 ERROR org.apache.spark.internal.io.SparkHadoopMapReduceWriter: Aborting job job_20181221160412_0054.; org.apache.spark.SparkException: Job aborted due to stage failure: Task 26 in stage 21.0 failed 4 times, most recent failure: Lost task 26.3 in stage 21.0 (TID 2498, readpipeline-w-4.c.broad-gatk-test.internal, executor 21): java.io.IOException: com.esotericsoftware.kryo.KryoException: java.lang.UnsupportedOperationException; Serialization trace:; requestOptions (com.google.cloud.storage.BlobReadChannel); channel (com.google.cloud.storage.contrib.nio.CloudStorageReadChannel); channel (htsjdk.samtools.reference.IndexedFastaSequenceFile); rsFile (htsjdk.samtools.cram.ref.ReferenceSource); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:4680,schedul,scheduler,4680,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['schedul'],['scheduler']
Energy Efficiency," 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [Januar",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:4988,schedul,scheduler,4988,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['schedul'],['scheduler']
Energy Efficiency, 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:8364,schedul,scheduler,8364,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['schedul'],['scheduler']
Energy Efficiency, ( hdfs compatible ).; GATK spark tools does not seems to recognize it.; When running the following command:; > /home/axverdier/Tools/GATK4/gatk-4.beta.6/gatk-launch CountReadsSpark --programName gatk4-testing --input maprfs://spark-ics/user/axverdier/data/710-PE-G1.bam --output maprfs://spark-ics/user/axverdier/testOutGATK_CountReadsSpark --sparkRunner SPARK --sparkMaster yarn --javaOptions -Dmapr.library.flatclass; I got the following error!. > Driver stacktrace:; > 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1436); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); > 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); > 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at scala.Option.foreach(Option.scala:257); > 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595); > 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); > 	at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:1044,schedul,scheduler,1044,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['schedul'],['scheduler']
Energy Efficiency," -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexome/include/python3.6m -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/theano/gof -L/home/tintest/miniconda2/envs/aurexome/lib -fvisibility=hidden -o /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/m421cdb2b133a2578e9a2670dfbb5d33e.so /home/tintest/.theano/compiledir_Linux-4.9--amd64-x86_64-with-debian-9.4--3.6.2-64/tmpueark7lw/mod.cpp -lpython3.6m; ERROR (theano.gof.cmodule): [Errno 12] Cannot allocate memory; Traceback (most recent call last):; File ""/tmp/tintest/cohort_denoising_calling.4390748645603329412.py"", line 143, in <module>; shared_workspace, initial_params_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; denoising_model = DenoisingModel(denoising_config, shared_workspace, initial_param_supplier); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; instance.__init__(*args, **kwargs); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 851, in __init__; observed=shared_workspace.n_st); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; return model.Var(name, dist, data, total_size); File ""/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/pymc3/model.py"", line 545,",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:63195,allocate,allocate,63195,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['allocate'],['allocate']
Energy Efficiency," 17/11/15 19:43:35 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@5917b44d{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 17/11/15 19:43:35 WARN org.apache.spark.ExecutorAllocationManager: No stages are running, but numRunningTasks != 0; 19:43:35.858 INFO PrintVariantsSpark - Shutting down engine; [November 15, 2017 7:43:35 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintVariantsSpark done. Elapsed time: 0.43 minutes.; Runtime.totalMemory()=823132160; org.apache.spark.SparkException: Job aborted due to stage failure: Exception while getting task result: com.esotericsoftware.kryo.KryoException: Error during Java deserialization.; Serialization trace:; genotypes (org.seqdoop.hadoop_bam.VariantContextWithHeader); interval (org.broadinstitute.hellbender.engine.spark.SparkSharder$PartitionLocatable); 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:8660,schedul,scheduler,8660,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['schedul'],['scheduler']
Energy Efficiency," 18/01/09 18:31:26 INFO server.AbstractConnector: Stopped Spark@283ab206{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/09 18:31:26 INFO ui.SparkUI: Stopped Spark web UI at http://192.168.1.4:4040; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 18/01/09 18:31:26 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 18/01/09 18:31:26 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 18/01/09 18:31:26 INFO cluster.YarnClientSchedulerBackend: Stopped; 18/01/09 18:31:26 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 18/01/09 18:31:26 INFO memory.MemoryStore: MemoryStore cleared; 18/01/09 18:31:26 INFO storage.BlockManager: BlockManager stopped; 18/01/09 18:31:26 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 18/01/09 18:31:26 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/01/09 18:31:26 INFO spark.SparkContext: Successfully stopped SparkContext; 18:31:26.896 INFO BwaAndMarkDuplicatesPipelineSpark - Shutting down engine; [January 9, 2018 6:31:26 PM CST] org.broadinstitute.hellbender.tools.spark.pipelines.BwaAndMarkDuplicatesPipelineSpark done. Elapsed time: 0.89 minutes.; Runtime.totalMemory()=881328128; ***********************************************************************. A USER ERROR has occurred: Input files reference and reads have incompatible contigs: No overlapping contigs found.; reference contigs = [chrM, chr1, chr2, chr3, chr4, chr5, chr6, chr7, chr8, chr9, chr10, chr11, chr12, chr13, chr14, chr15, chr16, chr17, chr18, chr19, chr20, chr21, chr22, chrX, chrY, chr1_gl000191_random, chr1_gl000192_random, chr4_ctg9_hap1, chr4_gl000193_random, chr4_gl000194_random, chr6_apd_hap1, chr6_cox_hap2, chr6_dbb_hap3, chr6_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:30835,schedul,scheduler,30835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['schedul'],['scheduler']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 12; cpu cores	: 14; apicid		: 24; initial apicid	: 24; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 12; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 13; cpu cores	: 14; apicid		: 26; initial apicid	: 26; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:56878,power,power,56878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 13; cpu cores	: 14; apicid		: 26; initial apicid	: 26; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 13; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 14; cpu cores	: 14; apicid		: 28; initial apicid	: 28; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:58053,power,power,58053,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 14; cpu cores	: 14; apicid		: 28; initial apicid	: 28; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 14; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 0; cpu cores	: 14; apicid		: 32; initial apicid	: 32; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:59228,power,power,59228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 13; cpu cores	: 14; apicid		: 58; initial apicid	: 58; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 27; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.687; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 14; cpu cores	: 14; apicid		: 60; initial apicid	: 60; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:74494,power,power,74494,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 10; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 22; initial apicid	: 22; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:54528,power,power,54528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 22; initial apicid	: 22; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 11; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 12; cpu cores	: 14; apicid		: 24; initial apicid	: 24; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:55703,power,power,55703,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 52; initial apicid	: 52; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 24; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 54; initial apicid	: 54; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:70969,power,power,70969,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 54; initial apicid	: 54; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 25; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 12; cpu cores	: 14; apicid		: 56; initial apicid	: 56; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:72144,power,power,72144,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 12; cpu cores	: 14; apicid		: 56; initial apicid	: 56; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 26; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 13; cpu cores	: 14; apicid		: 58; initial apicid	: 58; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:73319,power,power,73319,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency," 2021. ### Description ; The Mutect2 WDL's Funcotate task has an unintuitive setup with regard to setting memory for the Funcotate task. Funcotate task memory is defined [here](https://github.com/broadinstitute/gatk/blob/4.1.8.1/scripts/mutect2_wdl/mutect2.wdl#L1108); ![image](https://user-images.githubusercontent.com/45641912/139333822-aa0b3adc-b92e-4317-a75e-da322f96822f.png). This is using the dictionary defined earlier called **standard_runtime**. ![image](https://user-images.githubusercontent.com/45641912/139333917-0d97ef00-88e6-4340-8cee-e3295127eab8.png). This dictionary uses a variable called **machine_mem** which is calculated using the workflow's **small_task_mem** input, which is configurable. ![image](https://user-images.githubusercontent.com/45641912/139333959-4465b06d-b2ce-4ab2-bae9-285e25168c1d.png); ![image](https://user-images.githubusercontent.com/45641912/139333973-c8e2c1f6-0efd-4f45-9d1e-10f6c4a2baac.png). To allocate more memory for the Funcotate task, one has to define this **small_task_mem** variable at the workflow level. This effectively changes the amount of memory for all tasks that make use of this dictionary, rather than just the Funcotate task. Funcotate has two input variables **default_ram_mb** and **default_disk_space_gb** which have no bearing on the memory and disk space configuration for the task.; ![image](https://user-images.githubusercontent.com/45641912/139334343-8e614e17-27ef-4fef-815d-fe6e8c39ffef.png). This leads to user confusion when they see these variables in the method configuration page, put values in, and don't see their Funcotate task use the specified values.; ![image](https://user-images.githubusercontent.com/45641912/139334535-4b9a0353-910e-4764-a6d2-a454f4d344aa.png). #### Steps to reproduce; Define the input variables **default_ram_mb** and **default_disk_space_gb** for a run of the Mutect2 workflow to be different from the amounts defined by [*small_task_mem*](https://github.com/broadinstitute/gatk/blob/4.1.8.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7532:1131,allocate,allocate,1131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7532,1,['allocate'],['allocate']
Energy Efficiency," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 5; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 10; initial apicid	: 10; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:49317,monitor,monitor,49317,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:50490,monitor,monitor,50490,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:51663,monitor,monitor,51663,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency," 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:52836,monitor,monitor,52836,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency, ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)** ; **at org.apac,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45991,schedul,scheduler,45991,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['schedul'],['scheduler']
Energy Efficiency, ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotationMap.createAsAllTableFuncotationsFromVcf(FuncotationMap.java:224) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.lambda$createAlleleToFuncotationMapFromFuncotationVcfAttribute$5(FuncotatorUtils.java:2256) ; ;     at java.base/java.util.stream.Collectors.lambda$uniqKeysMapAccumulator$1(Collectors.java:178) ; ;     at java.base/java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ;     at java.base/java.util.stream.IntPipeline$1$1.accept(IntPipeline.java:180) ; ;     at java.base/java.util.stream.Streams$RangeIntSpliterator.forEachRemaining(Streams.java:104) ; ;     at java.base/java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:699) ; ;     at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ; ;     at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ; ;     at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ; ;     at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorUtils.createAlleleToFuncotationMapFromFuncotationVcfAttribute(FuncotatorUtils.java:2255) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.buildArHetByGene(ArHetvarFilter.java:77) ; ;     at org.broadinstitute.hellbender.tools.funcotator.filtrationRules.ArHetvarFilter.firstPassApply(ArHetvarFilter.java:50) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FilterFuncotations.firstPassApply(FilterFuncotations.java:161) ; ;     at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.nthPassApply(TwoPassVariantWalker.java:17) ; ;     at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7865:8016,Reduce,ReduceOps,8016,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7865,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency," BwaAndMarkDuplicatesPipelineSpark --bam-partition-size 64000000 or 4000000 \; --input hdfs://namenode:8020/$dir_prepro$ubam \; --reference hdfs://namenode:8020/hg19-ucsc/ucsc.hg19.2bit \; --bwa-mem-index-image /reference_image/ucsc.hg19.fasta.img \; --disable-sequence-dictionary-validation true \; --output hdfs://namenode:8020/$dir_prepro$output -- \; --spark-runner SPARK --spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:7213,Reduce,ReduceOps,7213,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['Reduce'],['ReduceOps']
Energy Efficiency," INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.131.101.159:34044 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1004; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at SparkUtils.java:157) (first 15 tasks are for partitions Vector(0)); 17/10/11 14:19:18 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks; 17/10/11 14:19:19 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1); 17/10/11 14:19:23 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (com2:35572) with ID 1; 17/10/11 14:19:23 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1); 17/10/11 14:19:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, com2, executor 1, partition 0, NODE_LOCAL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on com2:38568 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:26 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on com2:38568 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4180 ms on com2 (executor 1) (1/1); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at SparkUtils.java:157) finished in 8.951 s; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: looking for newly run",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:12940,schedul,scheduler,12940,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency, KB). The maximum recommended task size is 100 KB.; 17:20:00.012 INFO StructuralVariationDiscoveryPipelineSpark - 324 contigs indicating IntraChrStrandSwitch; 18/01/25 17:20:00 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:11.779 INFO StructuralVariationDiscoveryPipelineSpark - 3946 contigs indicating MappedInsertionBkpt; 18/01/25 17:20:11 WARN org.apache.spark.scheduler.TaskSetManager: Stage 37 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:23.416 INFO StructuralVariationDiscoveryPipelineSpark - 853 contigs indicating Cpx; 18/01/25 17:20:23 WARN org.apache.spark.scheduler.TaskSetManager: Stage 41 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:34.830 INFO StructuralVariationDiscoveryPipelineSpark - 1521 contigs indicating Incomplete; 18/01/25 17:20:34 WARN org.apache.spark.scheduler.TaskSetManager: Stage 45 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:44.949 INFO StructuralVariationDiscoveryPipelineSpark - 5277 contigs indicating Ambiguous; 18/01/25 17:20:45 WARN org.apache.spark.scheduler.TaskSetManager: Stage 49 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:55.516 INFO StructuralVariationDiscoveryPipelineSpark - 15 contigs indicating MisAssemblySuspect; 18/01/25 17:20:55 WARN org.apache.spark.scheduler.TaskSetManager: Stage 53 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:21:06.632 INFO StructuralVariationDiscoveryPipelineSpark - Discovered 14088 variants.; 17:21:06.647 INFO StructuralVariationDiscoveryPipelineSpark - BND_NOSS: 6100; 17:21:06.647 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV33: 240; 17:21:06.647 INFO StructuralVariationDiscoveryPipelineSpark - BND_INV55: 230; 17:21:06.648 INFO S,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4260:3173,schedul,scheduler,3173,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4260,1,['schedul'],['scheduler']
Energy Efficiency," N/A; 	 ApplicationMaster host: 10.131.101.159; 	 ApplicationMaster RPC port: 0; 	 queue: root.users.hdfs; 	 start time: 1507702753100; 	 final status: UNDEFINED; 	 tracking URL: http://mg:8088/proxy/application_1507683879816_0006/; 	 user: hdfs; 17/10/11 14:19:17 INFO cluster.YarnClientSchedulerBackend: Application application_1507683879816_0006 has started running.; 17/10/11 14:19:17 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34044.; 17/10/11 14:19:17 INFO netty.NettyBlockTransferService: Server created on 34044; 17/10/11 14:19:17 INFO storage.BlockManager: external shuffle service port = 7337; 17/10/11 14:19:17 INFO storage.BlockManagerMaster: Trying to register BlockManager; 17/10/11 14:19:17 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.131.101.159:34044 with 530.0 MB RAM, BlockManagerId(driver, 10.131.101.159, 34044); 17/10/11 14:19:17 INFO storage.BlockManagerMaster: Registered BlockManager; 17/10/11 14:19:17 INFO scheduler.EventLoggingListener: Logging events to hdfs://mg:8020/user/spark/applicationHistory/application_1507683879816_0006; 17/10/11 14:19:17 INFO spark.SparkContext: Registered listener com.cloudera.spark.lineage.ClouderaNavigatorListener; 17/10/11 14:19:17 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 17/10/11 14:19:17 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 285.6 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 26.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.131.101.159:34044 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:112; 17/10/11 14:19:18 INFO storage.MemoryStore: Bl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:9009,schedul,scheduler,9009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," Note that the annotation for the 3.1 call says MQ=60. I'm very suspicious of the large deletion that gets introduced after the SNP in question in the 3.1 bamout. I suspect that might be related to the GQ0 ref call in 3.5. Can I get some more info on that? (e.g. end position, zoomed out screenshot, etc.). ---. @chandrans commented on [Tue May 31 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-222817988). Okay. I met the user at last week's MPG and I told him I would put in a good word for him :) ; He came back today confirming that the SNP is real by Sanger sequencing. As for your questions Laura, the deletion present in the artificial haplotypes and some of the reads does not get called in 3.1 or in 3.5. Here is a zoomed out screenshot of the bamout file from 3.1. <img width=""1440"" alt=""screen shot 2016-05-31 at 4 56 20 pm"" src=""https://cloud.githubusercontent.com/assets/6998669/15690232/9dfc6992-2750-11e6-94c4-0c055b3ad1bc.png"">; The first green SNP on the left is the one in question. ---. @chandrans commented on [Tue Jun 14 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-225999879). Figured out at Support meeting that the variant SNP is called when you include -allowNonUniqueKmersInRef in the command. . It seems the kmer including the SNP is quite common the region. I am going to tell the user about using the flag. However, I think David will take a look into the code to see what exactly is going on and whether it is a good idea to recommend using the flag in repeat regions. ---. @ldgauthier commented on [Wed Jun 15 2016](https://github.com/broadinstitute/gsa-unstable/issues/1360#issuecomment-226191676). Valentin has found that that arg is able to recover a lot of our missed; indels in the pseudo-diploid truth data, so it's worth investigating.; However, I believe when I tried it for MuTect2 against the LUAD data I; introduced a not insignificant number of additional variants, likely false; positives. On",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2916:10959,green,green,10959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2916,1,['green'],['green']
Energy Efficiency, at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4466,schedul,scheduler,4466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['schedul'],['scheduler']
Energy Efficiency, by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:10048,schedul,scheduler,10048,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['schedul'],['scheduler']
Energy Efficiency," containing any reads, with a empty collection error. It would be great if we could catch this cleanly and generate a VCF without any calls. Here is a small self contained test case which demonstrates the issue:. https://s3.amazonaws.com/chapmanb/testcases/gatk/gatk4_hcspark_noreads.tar.gz. and the full error message:; ```; java.lang.UnsupportedOperationException: empty collection; at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1004); at org.apache.spark.rdd.RDD$$anonfun$reduce$1$$anonfun$apply$35.apply(RDD.scala:1004); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1004); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.api.java.JavaRDDLike$class.reduce(JavaRDDLike.scala:384); at org.apache.spark.api.java.AbstractJavaRDDLike.reduce(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCaller(HaplotypeCallerSpark.java:229); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:182); at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.runTool(HaplotypeCallerSpark.java:143); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4234:1120,reduce,reduce,1120,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4234,1,['reduce'],['reduce']
Energy Efficiency," don't need to specify the rest. Both \<chr\> \<start\> \<stop\> and \<chr\> can be present in the same file. You can also specify intervals in this format directly at the command line instead of writing them in a file. As a relative GATK noob, maybe I'm making a dumb interpretation mistake, but I don't see how \<chr\> \<start\> \<stop\> matches the format that seems to be in actual sample files provided by GATK, which is: \<chr\>:\<start\>-\<stop\>, . My point is though, if I'm making such a dumb mistake then probably tons and tons of other people are too when they first read it. I know a colleague of mine did as well the first time he tried using intervals. . Can we get a format description for arguments that is actually useful? E.g. like what most bash tools have. . This is not a request for personal help/explanations. I know how to make those requests elsewhere, e.g. the forums. I am requesting a general change for more readable and useful documentation. Documentation should help reduce such requests for help. I give the following just as another example:; I was reading about CombineGVCFs, and I was very surprised that someone would specify as many as ten thousand or more variants with --variant sample1.g.vcf ... --variant sample10000.g.vcf as in the example, this may even involve using xargs on some systems, so I looked in the documentation. I do see an option for giving the sample arguments in some sort of file form:; https://gatk.broadinstitute.org/hc/en-us/articles/360041416212-CombineGVCFs#--arguments_file; But again, what is: List[File] [] ?; Does this have a page describing what it actually needs like intervals does? It doesn't link to one, a quick Googling doesn't find one. The tool indices are ultimately much less useful if we don't know 1) The actual format and 2) what it does. These things are sometimes present, but often not. ; The docs also don't cover some less technical things. E.g. I see some people in the forums say they do hierarchical merging o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6639:2272,reduce,reduce,2272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6639,1,['reduce'],['reduce']
Energy Efficiency," exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. Driver stacktrace:; 17/10/11 14:19:38 INFO spark.ExecutorAllocationManager: Existing executor 2 has been removed (new total is 0); 17/10/11 14:19:38 INFO scheduler.DAGScheduler: Job 0 failed: saveAsNewAPIHadoopFile at ReadsSparkSink.java:203, took 19.909238 s; 17/10/11 14:19:38 INFO ui.SparkUI: Stopped Spark web UI at http://10.131.101.159:4040; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asking each executor to shut down; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Stopped; 17/10/11 14:19:38 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 17/10/11 14:19:38 INFO storage.MemoryStore: MemoryStore cleared; 17/10/11 14:19:38 INFO storage.BlockManager: BlockManager stopped; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: BlockManagerMaster stopped; 17/10/11 14:19:38 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:30647,schedul,scheduler,30647,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - wip; - initial cohort extract; - minor changes; - wip; - get genotypes working; - clarify sample -> sample_id; - add mode; - mode is mandatory, uses location instead of position; - add query mode; - fix contig name; - forgot this file; - fix location bug; - Ingest wip to be added to other var db code (#6582); - ingest arrays refactored; - add filter, change sample to sample_id; - fix bugs; - wip; - major refactor splitting ingest for arrays from exomes/genomes; - create output files for actual raw array tables; - change site_name to rsid; - change GT encoding, change output file names and remove dir structure, get probe metadata; - fix prefix; - update GT encoding; - remove filter, rename columns, allow sample id as input; - array cohort extract (#6666); - new bit-compression (#6691);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:5765,Reduce,Reduce,5765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Reduce'],['Reduce']
Energy Efficiency," first pass through the variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:273); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:281); 	at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:738); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyReadPositionFilter(Mutect2FilteringEngine.java:223); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:529); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5684:3734,Reduce,ReduceOps,3734,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency," id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:28 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, com2, executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000002 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000002; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:20098,schedul,scheduler,20098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 10.702 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:28437,schedul,scheduler,28437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 WARN scheduler.TaskSetManager: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:27111,schedul,scheduler,27111,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," in memory (estimated size 2.1 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.131.101.159:34044 (size: 2.1 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 1 from broadcast at ReadsSparkSink.java:195; 17/10/11 14:19:18 INFO Configuration.deprecation: mapred.output.dir is deprecated. Instead, use mapreduce.output.fileoutputformat.outputdir; 17/10/11 14:19:18 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1; 17/10/11 14:19:18 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false; 17/10/11 14:19:18 INFO spark.SparkContext: Starting job: saveAsNewAPIHadoopFile at ReadsSparkSink.java:203; 17/10/11 14:19:18 INFO input.FileInputFormat: Total input paths to process : 1; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Registering RDD 5 (mapToPair at SparkUtils.java:157); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Got job 0 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) with 1 output partitions; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at SparkUtils.java:157), which has no missing parents; 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.131.101.159:34044 (size: 6.9 KB, free: 530.0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:11160,schedul,scheduler,11160,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," may be processed with ""/usr/lib/systemd/systemd-coredump %P %u %g %s %t %c %h %e"" (or dumping to /bigdata/ramadugulab/luy/SNPcallingBreeding/core.1058615); #; # If you would like to submit a bug report, please visit:; # https://bugreport.java.com/bugreport/crash.jsp; # The crash happened outside the Java Virtual Machine in native code.; # See problematic frame for where to report the bug.; #. --------------- S U M M A R Y ------------. Command Line: -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 /bigdata/operations/pkgadmin/opt/linux/centos/8.x/x86_64/pkgs/gatk/4.6.0.0/gatk-package-4.6.0.0-local.jar HaplotypeCaller -R /rhome/luy/bigdata/genomes/Cclementina_182_v1_2.fa -I AlignedCalToCcl_Scaffolds_MarkDupOut.bam -O AlignedCalToCcl_Scaffolds.vcf.gz -ERC GVCF. Host: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz, 64 cores, 20G, Rocky Linux release 8.8 (Green Obsidian); Time: Sat Sep 28 04:11:19 2024 PDT elapsed time: 58592.788414 seconds (0d 16h 16m 32s). --------------- T H R E A D ---------------. Current thread (0x00007f06e4025b70): JavaThread ""main"" [_thread_in_native, id=1058616, stack(0x00007f06edc7a000,0x00007f06edd7b000)]. Stack: [0x00007f06edc7a000,0x00007f06edd7b000], sp=0x00007f06edbe6458, free space=18014398509481393k; Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code); C [libc.so.6+0xcf291] __memset_avx2_erms+0x11; C [libgkl_pairhmm_omp5311772482084658743.so+0x1500f] Java_com_intel_gkl_pairhmm_IntelPairHmm_computeLikelihoodsNative._omp_fn.0+0xcf. Java frames: (J=compiled Java code, j=interpreted, Vv=VM code); J 8942 com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoodsNative([Ljava/lang/Object;[Ljava/lang/Object;[D)V (0 bytes) @ 0x00007f06d563401c [0x00007f06d5633fa0+0x000000000000007c]; J 10003 c2 com.intel.gkl.pairhmm.IntelPairHmm.computeLikelihoods([Lorg/broadinstitute/gatk/nativebindings/pairhmm/ReadDataHolder;[Lorg/broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8988:1797,Green,Green,1797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8988,1,['Green'],['Green']
Energy Efficiency, org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:18635,schedul,scheduler,18635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['schedul'],['scheduler']
Energy Efficiency, org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3909:4209,schedul,scheduler,4209,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3909,3,['schedul'],['scheduler']
Energy Efficiency, org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6509,schedul,scheduler,6509,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency, org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19308,schedul,scheduler,19308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency, org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6098:6165,Reduce,ReduceOps,6165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency," ploidy and numbers of alleles in the high teens and above produces too many possible genotypes for GenotypeGVCFs to handle under its current architecture. . For example, in the case reported here, the ploidy is 19 and the number of alternate alleles is 21, so GenotypeGVCFs cannot handle the large number of possible genotypes that result from all the possible combinations. A reasonable way to deal with this would be to cull the possible combinations dynamically at runtime to eliminate the most unlikely combinations up front. ; #### Test data. Has been provided by the user ; #### [Original forum post](http://gatkforums.broadinstitute.org/discussion/4954/combination-of-ploidy-and-number-of-alleles-error-when-running-genotypegvcfs/p1). ---. @vruano commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-77993425). The error message explain the reason well ... a possibility to actually address this issue is to dynamically reduce the number of alt alleles loosing the less likely ones base on a maximum number of possible genotypes. So the user does not indicate the maximum number of alternative but the maximum number of genotypes. Which alt. alleles make it could be decided by taking a look in the corresponding hom. alt genotype likelihood dropping those alternatives with the worst hom. PLs. ---. @vdauwera commented on [Tue Mar 10 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-78122186). @vruano What you propose sounds great. How much work would it take to implement this? . ---. @vruano commented on [Mon Mar 23 2015](https://github.com/broadinstitute/gsa-unstable/issues/855#issuecomment-85066881). Looking into that particular use case... the problem seem to be in position:. 45SrDNA_Jacobsen 9283. That seems to be very polymorphic or noisy even within individual samples, to the point that many lack PLs so perhaps merging would not work or at least the exact model depending annotations (QUAL column a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2955:1347,reduce,reduce,1347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2955,1,['reduce'],['reduce']
Energy Efficiency, scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/08/29 10:20:49 ERROR Executor: Exception in task 12.0 in stage 12.0 (TID 3228); ```. I am running version 4.0.8.1 of GATK using openjdk version 1.8.0_212.; The command I am using is:. ```; gatk StructuralVariationDiscoveryPipelineSpark \; --aligner-index-image refrance.fasta.img \; --contig-sam-file contigs-aligned.sam \; --spark-master local[30] \; --kmers-to-ignore kmers_to_ignore.txt \; -R $fasta \; -I $sample.bam \; -O $sample.vcf; ```. Thanks for taking a look!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5145:2333,schedul,scheduler,2333,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5145,2,['schedul'],['scheduler']
Energy Efficiency," scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAnd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:4874,schedul,scheduler,4874,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['schedul'],['scheduler']
Energy Efficiency," sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:7205,schedul,scheduler,7205,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency," spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanned.zip](https://github.com/broadinstitute/gatk/files/3374898/spanned_allele_not_spanned.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6031:1510,reduce,reduced,1510,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031,1,['reduce'],['reduced']
Energy Efficiency," table creation and data loading in LoadData (#7056); - WIP; - tieout scripts; - notes files; - updated diff scripts; - fixed bug...; - add wdl and inputs file for warp pipeline; - reverting logging; - included top level WDL; - use gnarly with BQ extract cohort; - remove unused file; - cleaning up; - tidy; - tidy up before PR; - tidy up before PR; - PR comments; - merge conflict misfires; - added example SQL to create alt allele table from VET; - option to remove PLs; - fixed and enhanced unit test; - removing unused config, causing travis to fail; - add CreateVariantIngestFiles integration test (#7071); - add sampleName (instead of NULL) to error message (#7074); - Update To handle if no data error (#7084); - Memory improvement when writing missing positions to pet (#7098); - added support for loading QUALapprox into VET (#7101); - Add -m flag to gsutil step; add dockstore branch filters to facilitate development (#7104); - updates to ImportGenomes and LoadBigQueryData (#7112); - Add ngs to cohort extract Dockerfile; remove exception catching in extract python script (#7113); - remove problematic storage_location imports (#7119); - Reduce memory and CPU for CreateImportTsvs task, check for files before attempting load (#7121); - add -m flag to gsutil mv step (#7129); - ah_var_store : Add sample file argument to cohort extract (#7117); - Perform full WGS cohort extract scientific tieout for 35 ACMG59 samples (#7106); - Enable Read/Execution Project for BQ Queries (#7136); - ah - optional service account (#7140); - Add load lock file to prevent accidental re-loading of data to BQ (#7138); - #251 Address gvcf no-calls missing QUALapprox and other features (#7146); - Job Add labels to BQ operations from GATK (Issues-199) (#7115); - parse map to list to avoid brackets and spaces in vcf output (#7168); - #259 Inline schema for importgenomes.wdl (#7171); - Created AvroFileReader and unittest, Update ExtractCohort and ExtractCohortEngine (#7174); - #224 Import WDL: handle ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:11737,Reduce,Reduce,11737,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['Reduce'],['Reduce']
Energy Efficiency," task 0.0 in stage 0.0 (TID 0, com2, executor 1, partition 0, NODE_LOCAL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on com2:38568 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:26 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on com2:38568 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4180 ms on com2 (executor 1) (1/1); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at SparkUtils.java:157) finished in 8.951 s; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: looking for newly runnable stages; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: running: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: failed: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244), which has no missing parents; 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 86.1 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.3 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.131.101.159:34044 (size: 32.3 KB, free: 529.9 MB); 17/10/11 14:19:27 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1004; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244) (first 15 tasks are for partiti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:13996,schedul,scheduler,13996,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency," them. This meant it didn't report somatic mutations that involved loss of heterozygosity or new alleles at variant germline sites. Mutect 2 should report these sites as variant. . @davidangb Not sure if this already happens, but it seems like a good thing to fix if it doesn't. ---. @vdauwera commented on [Mon Jan 23 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-274585906). I support this feature request. ---. @davidbenjamin commented on [Wed Jan 25 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-275116409). @lbergelson LoH is a great idea that we don't do already; we *do* handle new alleles at germline variant sites now. ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-294329953). Actually, I'm having second thoughts. LoH is a copy-number event that occurs in chunks, so we could end up emitting (and spending CPU time on) a huge number of additional sites. Also, @samuelklee is there any reason not to leave the LoH-finding to aCNV?. ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295324137). Yeah, LoH is aCNV's job. Mutect would do the same thing at much greater expense and with less power. ---. @lbergelson commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295335240). @davidbenjamin, aCNV won't detect point mutations leading to LOH at specific sites. It's an admittedly rare case, and maybe not clinically relevant since reversion to the reference is probably not disease causing, but it means missing real somatic variants. . ---. @davidbenjamin commented on [Wed Apr 19 2017](https://github.com/broadinstitute/gatk-protected/issues/864#issuecomment-295336177). @lbergelson Ah, I see your point. Then it becomes an interesting trade-off of time vs sensitivity. I suppose we could make it optional. I'll re-open.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2934:1505,power,power,1505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2934,1,['power'],['power']
Energy Efficiency, to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:2179,schedul,scheduler,2179,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['schedul'],['scheduler']
Energy Efficiency," yes, I know is still in beta but I’ve found these problems when I compared the outputs from Haplotypecaller in spark and in not Spark versions. For comparing these results I've used this tool [https://drive.google.com/file/d/1r2WHyiz5WqOIyY_EZ1VZt92wGlL19SE4/view?usp=sharing](url) and I've obtained these plots for sensitivity and specificity( The sensitivity is defined as the number of sites inwhich both sequencing and microarrays detected a deviation from the reference sequencedivided by the number of sites where a variant was detected by using the microarrays). **Spark**; Sensitivity; ![spark_sensitivity_hg19](https://user-images.githubusercontent.com/10074137/47148261-86b77280-d2d0-11e8-8b5a-9ecfef16d889.png); Specificity; ![sparkspecificityhg19](https://user-images.githubusercontent.com/10074137/47148277-933bcb00-d2d0-11e8-97eb-1adceb4e5ee2.png). **Local non Spark tool with GATK 2.7**; ![hg19local](https://user-images.githubusercontent.com/10074137/47148427-fcbbd980-d2d0-11e8-87d8-04ec20c1005d.png); furthermore I've executed the pipeline until BQSR in Spark version and after, I am focused just on Haplotypecaller because I've used this ""backwards"" approach and I've discovered that the pipeline is deterministic from the phase Variant Discovery, but don't in the phase of Preprocessing because when I've executed this phase more times, I've obtained results completely, this is the test with one single sample:; ![comparisons_pfc32](https://user-images.githubusercontent.com/10074137/47148552-49071980-d2d1-11e8-8b1c-aec468285699.png); furthermore when I've used the output from BQSR (executed in Spark) for execute of Haplotypecaller in local(not in Spark) and adapting this output for Haplotypecaller, I had to use the tool Samtools for sort the outputs and after this step the outputs are passed from average of 19 gigabytes to 13 gigabytes average for the all samples. I've opened this Issue because I would to help you with my experiments to improvement your tool.; thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5323:1923,adapt,adapting,1923,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5323,1,['adapt'],['adapting']
Energy Efficiency,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetMan",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:7010,schedul,scheduling,7010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduling']
Energy Efficiency,"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: PrintReadsSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:6879,schedul,scheduler,6879,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,"!!!!!. 21:02:08.892 INFO PrintReadsSpark - Initializing engine; 21:02:08.892 INFO PrintReadsSpark - Done initializing engine; 18/07/24 21:02:08 WARN org.apache.spark.SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 18/07/24 21:02:09 INFO org.spark_project.jetty.util.log: Logging initialized @6492ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: jetty-9.3.z-SNAPSHOT; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.Server: Started @6584ms; 18/07/24 21:02:09 INFO org.spark_project.jetty.server.AbstractConnector: Started ServerConnector@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/07/24 21:02:09 WARN org.apache.spark.scheduler.FairSchedulableBuilder: Fair Scheduler configuration file not found so jobs will be scheduled in FIFO order. To use fair scheduling, configure pools in fairscheduler.xml or set spark.scheduler.allocation.file to a file that contains the configuration.; 18/07/24 21:02:09 INFO com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase: GHFS version: 1.9.0-hadoop2; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.RMProxy: Connecting to ResourceManager at shuang-small-m/10.128.5.217:8032; 18/07/24 21:02:10 INFO org.apache.hadoop.yarn.client.AHSProxy: Connecting to Application History server at shuang-small-m/10.128.5.217:10200; 18/07/24 21:02:12 INFO org.apache.hadoop.yarn.client.api.impl.YarnClientImpl: Submitted application application_1532457503538_0038; 21:02:16.702 INFO FeatureManager - Using codec BEDCodec to read file hdfs://shuang-small-m:8020/data/intervals.bed; 21:02:16.863 INFO IntervalArgumentCollection - Processing 1219 bp from intervals; 18/07/24 21:02:17 INFO org.apache.hadoop.mapreduce.lib.input.FileInputFormat: Total input files to process : 1; 18/07/24 21:02:25 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, shuang-sma",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:7072,schedul,scheduler,7072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,"## Bug Report. ### Affected tool(s) or class(es): Mutect2. ### Affected version(s); gatk 4.2.5. ### Description ; Like most use cases, I acquired a high-confidence, ""consensus"" VCF from a large batch of samples, and I run force-calling on each individual sample again to:; (1) rescue rare variants.; (2) for variants that are not called in a sample, get the REF/ALT counts for them for downstream analysis. However, compared to the first pass (where Mutect2 is in simple germline calling mode), the second pass (force-calling) is extremely slow. Sorry I have not done any precise measurement, but the difference is quite significant. Given my use case, do you still recommend using force-calling? Or is there any alternative, more efficient method? I tried using bcftools call, but that tool has several issues as well such as omitting indels, not supporting multiallelic force-calling etc. #### Steps to reproduce. My command for force-calling is:; ```; ""gatk Mutect2 ""; ""-alleles {input.q_vcf} ""; ""-L {input.q_vcf} ""; ""--genotype-filtered-alleles ""; ""--max-reads-per-alignment-start {params.mrpas} ""; ""-R {params.REF} ""; ""-I {input.sc_bam} ""; ""-O {output.sc_vcf}; ""; ```. I can upload some BAMs for testing if needed. Thanks in advance!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7825:731,efficient,efficient,731,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7825,1,['efficient'],['efficient']
Energy Efficiency,"## Bug Report. ### Affected tool(s) or class(es); GermlineCNVCaller. ### Affected version(s); - GATK4 4.0.5.1. ### Description ; I'm trying to do a germline CNV calling with 387 exomes samples (I know it's a lot). The CollectReadCounts and DetermineGermlineContigPloidy were successfull. But for the GermlineCNVCaller I got what I think is a Python ""cannot allocate memory"" error. I tried to specify to the JVM a max memory to allocate ``` --java-options ""-Xmx192G"" ``` , but no improvements. The machine I'm working on got 32 threads and 192 Gb RAM. #### Steps to reproduce; I guess try to do a CNV calling with a large cohort. #### Output; ```10:56:25.124 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/tintest/miniconda2/share/gatk4-4.0.5.1-0/gatk-package-4.0.5.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:56:25.342 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.343 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.0.5.1; 10:56:25.344 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:56:25.345 INFO GermlineCNVCaller - Executing as tintest@dahu39 on Linux v4.9.0-6-amd64 amd64; 10:56:25.346 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_121-b15; 10:56:25.347 INFO GermlineCNVCaller - Start Date/Time: July 25, 2018 10:56:24 AM CEST; 10:56:25.348 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.349 INFO GermlineCNVCaller - ------------------------------------------------------------; 10:56:25.350 INFO GermlineCNVCaller - HTSJDK Version: 2.15.1; 10:56:25.351 INFO GermlineCNVCaller - Picard Version: 2.18.2; 10:56:25.352 INFO GermlineCNVCaller - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:56:25.353 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:56:25.354 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:357,allocate,allocate,357,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,2,['allocate'],['allocate']
Energy Efficiency,"## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [ ] Latest public release version [version?]; 4.0.8.1; - [ ] Latest master branch as of [date of test?]; Sep 10, 2018. ### Description ; 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 515, localhost, executor 1, partition 0, NODE_LOCAL, 5270 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 516, localhost, executor 2, partition 1, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 6.0 (TID 517, localhost, executor 1, partition 2, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 6.0 (TID 518, localhost, executor 2, partition 3, NODE_LOCAL, 5594 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 6.0 (TID 519, localhost, executor 1, partition 4, NODE_LOCAL, 5317 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 6.0 (TID 520, localhost, executor 2, partition 5, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 6.0 (TID 521, localhost, executor 1, partition 6, NODE_LOCAL, 5315 bytes); 18/09/08 10:32:49 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 6.0 (TID 522, localhost, executor 2, partition 7, NODE_LOCAL, 5316 bytes); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:37617 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:49 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:45786 (size: 59.2 KB, free: 2004.5 MB); 18/09/08 10:32:50 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:37617 (size: 9.0 B, free: 2004.5 MB); 18/09/08 10:32:50 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 6.0 (TID 523, localhost, executor 1, partition 8, NODE_LOCAL, 5604 bytes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5169:258,schedul,scheduler,258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169,6,['schedul'],['scheduler']
Energy Efficiency,"## Bug Report. ### Affected tool(s) or class(es); VariantRecalibrator. ### Affected version(s); - [X] Latest public release version [4.5.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; As of v1.3.0 the `scales` R package turns the use of deprecated values for the `space` parameter into a hard error, resulting in the VariantRecalibrator R-script terminating with the following message:. > The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as of scales 0.3.0. This parameter is used repeatedly in the generated R-script via. ```R; scale_fill_gradient(high=""green"", low=""red"", space=""rgb""); ```. #### Steps to reproduce. ```shell; $ R --version; R version 4.1.2 (2021-11-01) -- ""Bird Hippie""; $ rm -rf ~/R; $ R; > install.packages(""ggplot2"", repos=""https://cloud.r-project.org/""); > packageVersion(""scales""); [1] ‘1.3.0’; > quit(); $ gatk --version; The Genome Analysis Toolkit (GATK) v4.5.0.0; HTSJDK Version: 4.1.0; Picard Version: 3.1.1; $ gatk VariantRecalibrator [arguments omitted for brevity]; org.broadinstitute.hellbender.utils.R.RScriptExecutorException: ; Rscript exited with 1; Command Line: Rscript -e tempLibDir = '/tmp/Rlib.9339186078473502558';source('/path/to/rscript.r');; Stdout: ; Stderr: Error:; ! The `space` argument of `pal_gradient_n()` only supports be ""Lab"" as; of scales 0.3.0.; Backtrace:; ▆; 1. ├─base::source(""/path/to/rscript.r""); 2. │ ├─base::withVisible(eval(ei, envir)); 3. │ └─base::eval(ei, envir); 4. │ └─base::eval(ei, envir); 5. └─ggplot2::scale_fill_gradient(high = ""green"", low = ""red"", space = ""rgb""); 6. ├─ggplot2::continuous_scale(...); 7. │ └─ggplot2::ggproto(...); 8. │ └─rlang::list2(...); 9. └─scales::seq_gradient_pal(low, high, space); 10. └─scales::pal_gradient_n(c(low, high), space = space); 11. └─lifecycle::deprecate_stop(""0.3.0"", ""pal_gradient_n(space = 'only supports be \""Lab\""')""); 12. └─lifecycle:::deprecate_stop0(msg); 13. └─rlang::cnd_signal(...); Execution halted; $ R; > install.packages(""remot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8664:599,green,green,599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8664,1,['green'],['green']
Energy Efficiency,## Bug Report. ### Affected tool(s) or class(es); _GenomicsDBImport_. ### Affected version(s); - 4.2.0.0; - after git revision 2b949f0dda49f495ce8fc7e3b8528cbc534e4819. ### Description ; _Progress meter does not accurately describe genomic locus being processed._. #### Steps to reproduce; _Run GenomicsDBImport (e.g. using -L to subset to a specific genomic locus)_. #### Expected behavior; _Progress meter should display the locus that is currently being processed during the import_. #### Actual behavior; _Progress meter displays unmaped locus. See [this forum post](https://gatk.broadinstitute.org/hc/en-us/community/posts/360077628671-GenomicsDBImport-output-unmapped-in-ProgressMeter-and-running-very-slow-with-WGS-data-of-large-sample-size) for an example_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7222:197,meter,meter,197,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7222,3,['meter'],['meter']
Energy Efficiency,"## Bug Report. ### Affected tool(s) or class(es); gatk SplitNCigarReads. ### Affected version(s); - gatk 4.2.6.1. ### Description ; I produced the bam files using STAR, and adjusted the MQ value to 60. I then used sambamba markdup to mark duplicate, then I proceeded to use SplitNCigarReads. The CPU load for SplitNCigarReads was very high and at certain times can spike up to 2400%. I tried limiting the cpu usage with commands like `-XX:ParallelGCThreads=1` and `-XX:ConcGCThreads=1`, but it doesn't seem to have an effect. (The cpu usage sometimes do stay at 100%) I also adjusted the MQ value in STAR to lessen the load in SplitNCigarReads. I also tried to increase the read size to reduce I/O time.; ![image](https://user-images.githubusercontent.com/106958825/175206165-08b28567-d671-45fa-b033-f20c4792edb7.png). #### Steps to reproduce; STAR; ```; STAR \; --genomeDir ${star_reference_path} \; --runThreadN 16 \; --readFilesIn ${file_1} ${file_2} \; --readFilesCommand ""gunzip -c"" \; --sjdbOverhang 149 \; --outSAMtype BAM SortedByCoordinate \; --outBAMsortingThreadN 16 \; --outSAMmultNmax 1 \; --outSAMmapqUnique 60 \; --outSAMattrRGline ID:${id} LB:RNASEQ SM:${sample_name} PL:ILLUMINA PU:${platform_unit} PM:${instrument_id} \; --limitBAMsortRAM 50000000000 \; --twopassMode Basic \; --outFileNamePrefix /rawdata/rnaseq/clean/bam/1.; ```. Mark Duplicate; ```; sambamba markdup \; -t 4 \; --tmpdir=/tmp \; --hash-table-size=262144 \; --overflow-list-size=67108864 \; /rawdata/rnaseq/clean/bam/1.Aligned.sortedByCoord.out.bam \; /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; ```. SplitNCigarReads; ```; gatk --java-options ""-Djava.io.tmpdir=/tmp -Xmx20G -XX:ParallelGCThreads=1 -XX:ConcGCThreads=1"" SplitNCigarReads \; -R ${reference_path} \; --tmp-dir /tmp \; -I /rawdata/rnaseq/clean/bam/1.aligned.duplicate_marked.sorted.bam \; -O /rawdata/rnaseq/clean/bam_gatk/1.aligned.duplicate_marked.sorted.bam \; --create-output-bam-md5 TRUE \; --max-reads-in-memory 1000000 \; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7914:687,reduce,reduce,687,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7914,1,['reduce'],['reduce']
Energy Efficiency,"## Documentation request. ### Tool(s) or class(es) involved; GermlineCNVCaller. ### Description ; I'm trying to get a pipeline running to call germline CNVs on small cohorts (20-40) PCR free whole genome samples sequenced to ~45X depth. I'm running into problems figuring out how wide to scatter the analysis, and how to allocate resources. It would be incredibly helpful to have some very clear guidelines about how number of samples and the number of intervals within each scatter affect both runtime and memory usage. Here's what I've been able to infer from the WDL pipelines, tool docs and experimentation (though I suspect some of it is wrong):. 1. Memory usage is approximately proportional to number of samples, number of intervals, number of bias covariates and max copy number. What the docs don't say is what the default is for the number of bias covariates _and_ how to take these numbers and project an approximate memory usage. 2. It would appear that GermlineCNVCaller will, by default, attempt to use all CPU cores available on the machine. From the WDL I see that setting environment variables `MKL_NUM_THREADS` and `OMP_NUM_THREADS` seems to control the parallelism? It would be nice if `GermlineCNVCaller` took a `--threads` and then set these before spawning the python process. 3. Runtime? This would be really nice to have some guidelines around as I get wildly varying results depending on how I'm running. My experimentation is with a) 20 45X WGS samples, b) bin size = 500bp, c) running on a 96-core general purpose machine at AWS with 384GB of memory. My first attempt a) scattered the genome into 48 shards of approximately 115k bins each, representing ~50mb of genome and b) ran 24 jobs concurrently but failed to set the environment variables to control parallelism. In that attempt the first wave of jobs were still running after 24 hours and getting close to finishing up the initial de-noising epoch, with 3/24 having failed due to memory allocation failures. My second",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6166:321,allocate,allocate,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6166,1,['allocate'],['allocate']
Energy Efficiency,"## Documentation request. ### Tool(s) or class(es) involved; _online documentation_. ### Description ; _First of all, thank you for a great tool with amazing documentation. The issue that I noticed recently is that all that great documentation on the website became almost impossible to read. The text is light grey on white and parameters are shown in pale blue on white. Surely I cannot be the only one who gets a headache after looking at it for 5 minutes._; ![image](https://user-images.githubusercontent.com/22867431/204846210-c6d03c9c-f91b-4b3f-88b3-6927768b4946.png). Ideally a dark theme would be amazing, but anything with a little more contrast would be a big help. ----. P.S.: Before you say so, I did try to make a post about this on the forum. Any sign in request bounces back to the main page without doing anything.; P.P.S: I do think that the issue here is appropriate considering the fact that the influx of new issues might reduce significantly if people can actually read the documentation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8115:942,reduce,reduce,942,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8115,1,['reduce'],['reduce']
Energy Efficiency,"## Documentation request; Filing issue at the request of @samuelklee stemming from the discussion in https://github.com/broadinstitute/gatk/pull/5829. ### Tool(s) or class(es) involved; GermlineCNVCaller and related tools. . ### Description ; In particular, I am currently needing information for the gCNV tutorial writeup for the following parameters. What direction increases sensitivity?. - `--depth-correction-tau` has a default of 10000.0 (10K) and defines the precision of read-depth concordance with the global depth value.; - `--p-active` has a default of 1e-2 (0.01) and defines the expected probability of CNV events.; - `p-alt` has a default of 1e-6 (0.000001) and defines the prior probability of CNV states. . Here are some other parameters of particular interest and descriptions I worked on with help from @mwalker174 and @samuelklee:; - Decreasing `--class-coherence-length` from its default of 10,000bp to 1000bp decreases the expected length of contiguous segments. Factor for bin size when tuning. ; - Decreasing `--cnv-coherence-length` from its default 10,000bp to 1000bp decreases the expected length of CNV events. Factor for bin size when tuning. ; - Turning off `--enable-bias-factors` from the default `true` state to `false` turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data. ; - Decreasing `--interval-psi-scale` from its default of 0.001 to 1.0E-6 reduces the scale the tool considers normal in per-interval noise.; - Decreasing `--log-mean-bias-standard-deviation` from its default of 0.1 to 0.01 reduces what is considered normal noise in bias factors.; - Decreasing `--sample-psi-scale` from its default of 0.0001 to 1.0E-6 reduces the scale that is considered normal in sample-to-sample variance. . In general, all of the parameter descriptions could be friendlier. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5850:1476,reduce,reduces,1476,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5850,3,['reduce'],['reduces']
Energy Efficiency,"## Feature request. ### Tool(s) or class(es) involved. Engine level argument. ### Description. This is a new capability. Presently tools like Mutect2 and HaplotypeCaller ignore; soft-clipping by default. In some sequencing products that use long reads relative; to the insert size, the reads often contain some amount of adapter. These reads; are typically soft-clipped by upstream tools like MergeBamAlignments. The result is an increase in false positive rates in somatic samples that have long read lengths compared to insert size. These false positives can be eliminated using the `-no-soft-clips` option, but this ignores all soft clips regardless of why the read was soft-clipped. The proposal here is to add a new engine level argument that will allow GATK tools to ignore soft-clips that occur at the start position of the reads mate. This will allow tools to utilize soft-clips that may contain evidence of indels without providing support for artifactual variants due to adapter sequence.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346:321,adapt,adapter,321,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346,2,['adapt'],['adapter']
Energy Efficiency,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5629:110,meter,meter,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629,2,['meter'],['meter']
Energy Efficiency,"## Feature request. ### Tool(s) or class(es) involved; _org.broadinstitute.hellbender.engine.ProgressMeter_; _org.broadinstitute.hellbender.utils.nio.NioFileCopierWithProgressMeter_. ### Description; One `ProgressBar` to rule them all. One `ProgressBar` class hierarchy to bind them. Progress bars/meters should be consolidated into a single class hierarchy, with threaded updates that are triggered by both a `time interval` and a `percentage/# of records completed count` (whichever occurs first). This class hierarchy should have an abstract base `ProgressMeter` class, which has at least 2 concrete child classes - `GenomicProgressMeter` (equivalent to `ProgressMeter`), and `NumericProgressMeter` (which encapsulates the progress meter functionality inside `NioFileCopierWithProgressMeter`). . The progress meter functionality inside `NioFileCopierWithProgressMeter` should be replaced with the resulting progress meter class. The `ProgressMeter` class should be similarly updated / replaced within `GATKTool` to leverage the new class hierarchy.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5178:298,meter,meters,298,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5178,4,['meter'],"['meter', 'meters']"
Energy Efficiency,"### Instructions. ## Bug Report. ### Affected tool(s) or class(es); MarkDuplicatesSpark. ### Affected version(s); - [v ] Latest public release version [4.4.0.0]. ### Description ; Since switching to 4.4.0.0 we are experiencing an increased memory consumption of MarkDuplicatesSpark. We see it on large BAM/CRAM files, not tested on small files. #### Steps to reproduce; This command: ; ```; java -Xmx190g -jar /usr/gitc/GATK_ultima.jar MarkDuplicatesSpark \; --spark-master local[24] \; --input 019242_old.ua.aln.bam \; --output 019242_old.aligned.sorted.duplicates_marked.bam \; --create-output-bam-index true \; --spark-verbosity WARN \; --verbosity WARNING \; --flowbased; ```; required 90GB memory on 4.3.0.0. The input BAM is large: 270GB; However on the 4.4.0.0 it requires >160GB RAM. #### Expected behavior; The memory requirement is not expected to change ; #### Actual behavior; Significantly increased memory requirement",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8307:247,consumption,consumption,247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8307,1,['consumption'],['consumption']
Energy Efficiency,$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237); at scala.Option.getOrElse(Option.scala:120); at org.apache.spark.rdd.RDD.partitions(RDD.scala:237); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:331); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:330); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:381); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:380); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1187); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1187); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1186); at org.apache.spark.api.java.JavaRDDLike$class.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2113:5882,reduce,reduceByKey,5882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2113,1,['reduce'],['reduceByKey']
Energy Efficiency,$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:248); at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:246); at scala.Option.getOrElse(Option.scala:121); at org.apache.spark.rdd.RDD.partitions(RDD.scala:246); at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:65); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:328); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:327); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.PairRDDFunctions.countByKey(PairRDDFunctions.scala:371); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1175); at org.apache.spark.rdd.RDD$$anonfun$countByValue$1.apply(RDD.scala:1175); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.countByValue(RDD.scala:1174); at org.apache.spark.api.java.JavaRDDLike$class.co,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3360:10571,reduce,reduceByKey,10571,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3360,1,['reduce'],['reduceByKey']
Energy Efficiency,"$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). 18/07/24 21:02:27 ERROR org.apache.spark.scheduler.TaskSetManager: Task 1 in stage 0.0 failed 4 times; aborting job; 18/07/24 21:02:27 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@42ecc554{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 21:02:27.703 INFO PrintReadsSpark - Shutting down engine; [July 24, 2018 9:02:27 PM UTC] org.broadinstitute.hellbender.tools.spark.pipelines.PrintReadsSpark done. Elapsed time: 0.32 minutes.; Runtime.totalMemory()=2463629312; org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:10692,schedul,scheduler,10692,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:14347,schedul,scheduler,14347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! Thi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:22365,schedul,scheduler,22365,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:14519,schedul,scheduler,14519,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:10922,schedul,scheduler,10922,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,2,['schedul'],['scheduler']
Energy Efficiency,(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:32047,schedul,scheduler,32047,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,(DiscoverVariantsFromContigAlignmentsSAMSpark.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAd,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:2750,schedul,scheduler,2750,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,3,['schedul'],['scheduler']
Energy Efficiency,"(GATK) v4.1.4.1_. b) Exact GATK commands used. _/usr/bin/time -v gatk --java-options ""-Xmx10G"" Mutect2 -R ../reference/indices\_010920/GRCh38.d1.vd1.fa -L chr4.bed -I chr4.bam --max-mnp-distance 0 --interval-padding 100 -O chr4.vcf.gz_. c) The entire error log if applicable. _java.lang.IllegalArgumentException: Need one or two reads to construct a fragment_ ; _at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:43)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.createAndAvoidFailure(Fragment.java:58)_ ; _at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)_ ; _at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1376)_ ; _at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)_ ; _at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)_ ; _at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)_ ; _at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)_ ; _at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)_ ; _at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikelihoods.java:589)_ ; _at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:93)_ ; _at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251)_ ; _at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320)_ ; _at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308)_ ; _at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281)_ ; _at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048)_ ; _at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineP",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6419:1727,Reduce,ReduceOps,1727,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6419,1,['Reduce'],['ReduceOps']
Energy Efficiency,(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantcontext.writer.VCFWriter.add(VCFWriter.java:242); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:93); at org.disq_bio.disq.impl.formats.vcf.HeaderlessVcfOutputFormat$VcfRecordWriter.write(HeaderlessVcfOutputFormat.java:56); at org.apache.spark.internal.io.HadoopMapReduceWriteConfigUtil.write(SparkHadoopWriter.scala:358); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:132); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); ... 10 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:16528,schedul,scheduler,16528,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['schedul'],['scheduler']
Energy Efficiency,"(Iterator.scala:408); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5239,schedul,scheduler,5239,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['schedul'],['scheduler']
Energy Efficiency,(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6258,schedul,scheduler,6258,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency,(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19057,schedul,scheduler,19057,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency,"(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; java.lang.OutOfMemoryError: Required array length 2147483639 + 798 is too large; at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); at java.base/jdk.internal.util.ArraysSupport.n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:7968,schedul,scheduler,7968,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:12292,schedul,scheduler,12292,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:31011,schedul,scheduler,31011,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 11:00:54.334 INFO ShutdownHookManager - Shutdown hook called; 11:00:54.335 INFO ShutdownHookManager - Deleting directory /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/spark-f9c7c336-4e98-4fcc-855b-ba8a5a29e074; ```. The first lines of the log file:; ```; vm.max_map_count = 2147483642; Using GATK jar /Public/Everythings/misc/gatk-4.4.0.0/gatk-package-4.4.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -XX:+UnlockDiagnosticVMOptions -XX:GCLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:36882,schedul,scheduler,36882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,(UnivariateOptimizer.java:148); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:225); 	at org.apache.commons.math3.optim.univariate.BrentOptimizer.doOptimize(BrentOptimizer.java:43); 	at org.apache.commons.math3.optim.BaseOptimizer.optimize(BaseOptimizer.java:153); 	at org.apache.commons.math3.optim.univariate.UnivariateOptimizer.optimize(UnivariateOptimizer.java:70); 	at org.broadinstitute.hellbender.utils.OptimizationUtils.max(OptimizationUtils.java:40); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.lambda$calculateContamination$13(ContaminationModel.java:214); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.calculateContamination(ContaminationModel.java:215); 	at org.broadinstitute.hellbender.tools.walkers.contamination.ContaminationModel.<init>(ContaminationModel.java:67); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:127); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:163); 	at org.broadinstitute.hel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6282:1569,Reduce,ReduceOps,1569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6282,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,"(added here for easier tracking). In bwajni.c; - [ ] lift `(*env)->GetArrayLength(env,baseArray)` out of loops; - [ ] pass pointers directly not wrapped in classes, eg. BwaIndex could be passed as a pointer; - [ ] cache all fieldIDs, methodIDs and classes; - [ ] pass in data directly without the ShortRead; - [ ] batch multiple calls to align (1 read) ; - [ ] pass in a struct for the native code to fill rather then allocate a new AlnRgn everytime",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1857:418,allocate,allocate,418,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1857,1,['allocate'],['allocate']
Energy Efficiency,); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4368,schedul,scheduler,4368,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['schedul'],['scheduler']
Energy Efficiency,); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:49759,schedul,scheduler,49759,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['schedul'],['scheduler']
Energy Efficiency,"* Adding a new GATKTool level argument `--variant-output-interval-filtering-mode` which allows filtering output variants according to the input interval list. This replaces `--only-output-calls-starting-in-intervals` which was available in GenotypeGvcfs and GnarlyGenotyper. It works by adding a filtering decorator to the vcf writers created through `GATKTool.createVCFWriter`. ; There are several different filtering modes:; `STARTS_IN`, `ENDS_IN`, `OVERLAPS`, `CONTAINED`, and `ANYWHERE`. The default for tools is not to apply the decorator, but they may optionally change that behavior by overriding the new `getDefaultVariantOutputFilterMode`. `--variant-output-interval-filtering-mode STARTS_IN` is equivalent to the previous behavior of `--only-output-calls-starting-in-intervals true`. MockVcfWriter is now a testUtils class. The naming is a bit awkward so improvements would be helpful. This doesn't fix the weird behavior in HaplotypeCaller but does allow subsetting unique shards with SelectVariants and other variant outputting tools. We could adapt this to apply to bam outputs as well if that seems useful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6388:1056,adapt,adapt,1056,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6388,1,['adapt'],['adapt']
Energy Efficiency,"* It turns out Rdd.reduce crashes when it encounters empty data, use fold instead.; * Fix https://github.com/broadinstitute/gatk/issues/6319",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6767:19,reduce,reduce,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6767,1,['reduce'],['reduce']
Energy Efficiency,* When running recursive deletion file hooks we now catch all exceptions and log them at DEBUG level instead of letting them propagate.; * This should reduce confusion when test have deletion failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6125:151,reduce,reduce,151,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6125,1,['reduce'],['reduce']
Energy Efficiency,* fix partitioning bug by moving edge fixing from coordinateSortReads -> querynameSortReads; * refactor methods to reduce code duplication; * renaming and moving some methods; * disallow duplicate sort order on spark because it doesn't work with headerless reads,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4765:115,reduce,reduce,115,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4765,1,['reduce'],['reduce']
Energy Efficiency,"* some classes were missing registration in kryo which causes less efficient serialization; * adding registrations for a number of classes that MarkDuplicatesSpark needs that weren't registered yet. * notably, BAMRecord wasn't registered to use the correct serializer which could cause major inefficiencies; * it's not clear what circumstances we're serializing BAMRecord instead of SAMRecordToGATKReadAdapter so how much this will help is not obvious",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4451:67,efficient,efficient,67,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4451,1,['efficient'],['efficient']
Energy Efficiency,"**After** we've ported reduce support for allele-specific annotations in https://github.com/broadinstitute/gatk/issues/1893 (and not as we're porting!), we should refactor the relevant interfaces to clean them up a bit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3293:23,reduce,reduce,23,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3293,1,['reduce'],['reduce']
Energy Efficiency,- ApplyBQSR adapted to fit into the Skeleton pipeline; - command-line version still works and passes tests (including cloud); - BaseRecalibrator's testPlottingWorkflow now passes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/815:12,adapt,adapted,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/815,1,['adapt'],['adapted']
Energy Efficiency,- CPX: 0; 18/01/25 17:19:14 WARN org.apache.spark.scheduler.TaskSetManager: Stage 19 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 18/01/25 17:19:24 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (4378 KB). The maximum recommended task size is 100 KB.; 17:19:33.313 INFO StructuralVariationDiscoveryPipelineSpark - Processing 821484 raw alignments from 708052 contigs.; 18/01/25 17:19:33 WARN org.apache.spark.scheduler.TaskSetManager: Stage 22 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:19:46.133 INFO StructuralVariationDiscoveryPipelineSpark - Filtering on MQ left 573670 contigs.; 17:19:46.995 INFO StructuralVariationDiscoveryPipelineSpark - 23730 contigs with chimeric alignments potentially giving SV signals.; 17:19:47.546 INFO StructuralVariationDiscoveryPipelineSpark - 8559 contigs indicating InsDel; 18/01/25 17:19:47 WARN org.apache.spark.scheduler.TaskSetManager: Stage 29 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:00.012 INFO StructuralVariationDiscoveryPipelineSpark - 324 contigs indicating IntraChrStrandSwitch; 18/01/25 17:20:00 WARN org.apache.spark.scheduler.TaskSetManager: Stage 33 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:11.779 INFO StructuralVariationDiscoveryPipelineSpark - 3946 contigs indicating MappedInsertionBkpt; 18/01/25 17:20:11 WARN org.apache.spark.scheduler.TaskSetManager: Stage 37 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:23.416 INFO StructuralVariationDiscoveryPipelineSpark - 853 contigs indicating Cpx; 18/01/25 17:20:23 WARN org.apache.spark.scheduler.TaskSetManager: Stage 41 contains a task of very large size (4041 KB). The maximum recommended task size is 100 KB.; 17:20:34.830 INFO StructuralVariationDiscoveryPipelineSpark - 1521 contig,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4260:2103,schedul,scheduler,2103,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4260,1,['schedul'],['scheduler']
Energy Efficiency,- Refactors both the base and root `dockerfile` to reduce the total # of layers. Addresses: https://github.com/broadinstitute/gatk/issues/8684,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8686:51,reduce,reduce,51,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8686,1,['reduce'],['reduce']
Energy Efficiency,"- Requester pays: disabled; 14:50:59.205 INFO FilterMutectCalls - Initializing engine; 14:51:00.692 INFO FeatureManager - Using codec VCFCodec to read file file:///workdir/mparment/data/process/A2683/PTC2_unfiltered.vcf.gz; 14:51:01.406 INFO FilterMutectCalls - Done initializing engine; 14:51:02.360 INFO FilterMutectCalls - Shutting down engine; [December 12, 2020 2:51:02 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2385510400; java.lang.IllegalStateException: Duplicate key 7.395307178412063E-4; at java.util.stream.Collectors.lambda$throwingMerger$138(Collectors.java:133); at java.util.stream.Collectors$$Lambda$67/403388441.apply(Unknown Source); at java.util.HashMap.merge(HashMap.java:1245); at java.util.stream.Collectors.lambda$toMap$196(Collectors.java:1320); at java.util.stream.Collectors$$Lambda$69/854719230.accept(Unknown Source); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.<init>(ContaminationFilter.java:26); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.buildFiltersList(Mutect2FilteringEngine.java:290); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.<init>(Mutect2FilteringEngine.java:60); at org.broadinstitute.hellbender.tools.walkers.mute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:3906,Reduce,ReduceOps,3906,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,1,['Reduce'],['ReduceOps']
Energy Efficiency,"- reduced retries for task calling write API because if it fails more than once, chances are it will continue to fail because the import process was stopped before completion; - hopefully made the error message less scary, also included table number for easier cleanup. Closes https://broadworkbench.atlassian.net/browse/VS-267",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7680:2,reduce,reduced,2,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7680,1,['reduce'],['reduced']
Energy Efficiency,"--spark-master spark://$SPARK_MASTER_HOST:7077 \; --driver-memory 20g --executor-cores 4 --executor-memory 8g; ```. Furthermore I have this problem with this version v4.0.4.0-23-g6e1cc8c-SNAPSHOT. > mark duplicate records objects corresponding to read with name, this could be the result of readnames spanning more than one partition; 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$null$0(MarkDuplicatesSpark.java:109); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$62928560$1(MarkDuplicatesSpark.java:109); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:7592,Reduce,ReduceOps,7592,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,"-23590f0ab31f/call-PathSeqAlign/MMRF_2072_2_BM.microbe_aligned.paired.bam:33554432+33554432 20/07/17 09:38:46 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 5) java.util.NoSuchElementException: next on empty iterator at scala.collection.Iterator$$anon$2.next(Iterator.scala:39) at scala.collection.Iterator$$anon$2.next(Iterator.scala:37) at scala.collection.Iterator$$anon$13.next(Iterator.scala:469) at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31) at org.broadinstitute.hellbender.relocated.com.google.common.collect.Iterators$PeekingImpl.next(Iterators.java:1155) at org.broadinstitute.hellbender.utils.spark.SparkUtils.lambda$putReadsWithTheSameNameInTheSamePartition$7bd206b0$1(SparkUtils.java:190) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:153) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346) at org.apache.spark.rdd.RDD.iterator(RDD.scala:310) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90) at org.apache.spark.scheduler.Task.run(Task.scala:123) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408) at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748); `. Looking at the aligned bams that go into the scoring task, they don't appear to be empty or different to the rest of the cohort. Any thoughts?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6709:1677,schedul,scheduler,1677,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6709,2,['schedul'],['scheduler']
Energy Efficiency,-GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:3337,schedul,scheduler,3337,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['schedul'],['scheduler']
Energy Efficiency,"-Prints the current locus, the elapsed time, number of records processed,; and the rate at which records are being processed. -Hooked up for ReadWalkers, VariantWalkers, and IntervalWalkers. -A new command-line arg in GATKTool allows control over the frequency of; progress meter updates. -Tweaked the log4j output format to create more screen space for logger output. Resolves #974 (for alpha purposes)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1037:274,meter,meter,274,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1037,1,['meter'],['meter']
Energy Efficiency,"-Reduce memory usage of AssemblyRegion traversal by an order of magnitude; by loading the reads for each shard more lazily. -Add a sharding mode that creates one shard per user interval (or per contig,; if there are no explicit intervals), and make it the default for both HaplotypeCaller; and Mutect2. -When determining active regions, only consider loci within the user's intervals (but; still include surrounding reads in the final region). This mimics GATK3.x behavior. -Serve up empty pileup objects for uncovered loci (this also mimics GATK3.x behavior).; The fact that we weren't doing this before was responsible for much of the remaining; difference vs. the GATK 3.x HaplotypeCaller. -Ported GATK 3 PR 1389 (use median rather than the second-best likelihood for the; NON_REF allele). -Ported a change to the ReferenceConfidenceModel from GATK3. -Fixed a bug in ReadLikelihoods that was causing ArrayIndexOutOfBoundsException. -Added special handling of RawMQ to HaplotypeCaller (mirrors the handling of RawMQ; from GenotypeGVCFs). -Added updated concordance test data generated with HaplotypeCaller 3.8-4-g7b0250253f. Resolves #1950; Resolves #3516; Resolves #3517; Resolves #3518; Resolves #3233; Resolves #2848",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3519:1,Reduce,Reduce,1,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3519,1,['Reduce'],['Reduce']
Energy Efficiency,"-Tools can now customize the progress meter to use a different word than; ""records"" in its output (eg., ""reads"", ""regions"", etc.). -Updated standard walker classes to specify appropriate labels. -Hooked up GenomicsDBImport to the progress meter (it was always reporting; ""Processed 0 records"" at traversal end). Resolves #1943; Resolves #2683",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2690:38,meter,meter,38,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2690,2,['meter'],['meter']
Energy Efficiency,".4 2000 4890.4; 09:11:01.889 INFO ProgressMeter - 1:809005 0.6 3000 5117.3; 09:11:13.838 INFO ProgressMeter - 1:818424 0.8 5000 6366.2; 09:11:16.811 INFO CombineGVCFs - Shutting down engine; [September 3, 2020 at 9:11:16 AM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.20 minutes.; Runtime.totalMemory()=107374182400; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1624); 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.makeRawAnnotationString(StrandBiasUtils.java:46); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_StrandBiasTest.combineRawData(AS_StrandBiasTest.java:115); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPrevious",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6790:2423,Reduce,ReduceOps,2423,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6790,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,".889 INFO ProgressMeter - 1:809005 0.6 3000 5117.3; 09:11:13.838 INFO ProgressMeter - 1:818424 0.8 5000 6366.2; 09:11:16.811 INFO CombineGVCFs - Shutting down engine; [September 3, 2020 at 9:11:16 AM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 1.20 minutes.; Runtime.totalMemory()=107374182400; java.lang.NullPointerException; 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195); 	at java.base/java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1624); 	at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484); 	at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474); 	at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913); 	at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.base/java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.makeRawAnnotationString(StrandBiasUtils.java:46); 	at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_StrandBiasTest.combineRawData(AS_StrandBiasTest.java:115); 	at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:210); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); 	at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); 	at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6790:2461,Reduce,ReduceOps,2461,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6790,1,['Reduce'],['ReduceOps']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:934); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:8846,schedul,scheduler,8846,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:15385,schedul,scheduler,15385,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:7027,schedul,scheduler,7027,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:14666,schedul,scheduler,14666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,3,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:5332,schedul,scheduler,5332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOpera,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:4247,schedul,scheduler,4247,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:41546,schedul,scheduler,41546,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19826,schedul,scheduler,19826,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:938); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:50763,schedul,scheduler,50763,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOpe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:17492,schedul,scheduler,17492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['schedul'],['scheduler']
Energy Efficiency,".ExecutorAllocationManager: New executor 1 has registered (new total is 1); 17/10/11 14:19:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, com2, executor 1, partition 0, NODE_LOCAL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on com2:38568 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:26 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on com2:38568 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4180 ms on com2 (executor 1) (1/1); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at SparkUtils.java:157) finished in 8.951 s; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: looking for newly runnable stages; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: running: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: failed: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244), which has no missing parents; 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 86.1 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.3 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.131.101.159:34044 (size: 32.3 KB, free: 529.9 MB); 17/10/11 14:19:27 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1004; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:13851,schedul,scheduler,13851,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more**. 00:59 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:55:54 INFO TaskSetManager: Starting task 1.1 in stage 2.0 (TID 5, xx.xx.xx.24, executor 1, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:55:54 INFO TaskSetManager: Lost task 0.0 in stage 2.0 (TID 3) on xx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:26193,schedul,scheduler,26193,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:12 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:07 INFO TaskSetManager: Starting task 1.2 in stage 2.0 (TID 9, xx.xx.xx.27, executor 0, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:37 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on xx",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:30691,schedul,scheduler,30691,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. 01:44 DEBUG: [kryo] Write: WrappedArray(null); 18/04/24 17:56:39 INFO TaskSetManager: Starting task 1.3 in stage 2.0 (TID 10, xx.xx.xx.16, executor 3, partition 1, PROCESS_LOCAL, 5371 bytes); 18/04/24 17:56:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on x",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:33288,schedul,scheduler,33288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,".PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; 18/04/24 17:56:39 INFO DAGScheduler: Job 2 failed: count at PathSeqPipelineSpark.java:245, took 45.308012 s; 18/04/24 17:56:39 INFO SparkUI: Stopped Spark web UI at http://xx.xx.xx.16:4040; 18/04/24 17:56:39 INFO StandaloneSchedulerBackend: Shutting do",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:36438,schedul,scheduler,36438,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:39769,schedul,scheduler,39769,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ... 16 more; ```. Thank you. Full log:; ````; 17:54:54.447 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 17:54:54.891 INFO NativeLibraryLoader - ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:2178,schedul,scheduler,2178,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:3919,schedul,scheduler,3919,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['schedul'],['scheduler']
Energy Efficiency,.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:4894,schedul,scheduler,4894,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['schedul'],['scheduler']
Energy Efficiency,.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:7555,schedul,scheduler,7555,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,1,['schedul'],['scheduler']
Energy Efficiency,.TorrentBroadcast$$anonfun$8.apply(TorrentBroadcast.scala:293); 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337); 	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:294); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:226); 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303); 	... 21 more; Caused by: java.lang.UnsupportedOperationException; 	at shaded.cloud_nio.com.google.common.collect.ImmutableMap.put(ImmutableMap.java:407); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:162); 	at com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:39); 	at com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:708); 	at com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125); 	... 38 more. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:9597,schedul,scheduler,9597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['schedul'],['scheduler']
Energy Efficiency,.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-29T18:18:04.001846904Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-29T18:18:04.002024760Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002140012Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-29T18:18:04.002232542Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-29T18:18:04.002242727Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-29T18:18:04.002292461Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-10-29T18:18:04.002301667Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-10-29T18:18:04.002307019Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-29T18:18:04.002311722Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 2019-10-29T18:18:04.002316449Z 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 2019-10-29T18:18:04.002321526Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002358113Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 2019-10-29T18:18:04.002377342Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 2019-10-29T18:18:04.002383406Z 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 2019-10-29T18:18:04.002431769Z 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traver,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:1896,Reduce,ReduceOps,1896,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,.UserException$CouldNotReadInputFile: Couldn't read file. Error was: hg19mini.hss with exception: hg19mini.hss (No such file or directory); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:112); at org.broadinstitute.hellbender.tools.spark.pathseq.PSKmerUtils.readKmerFilter(PSKmerUtils.java:131); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilter.<init>(ContainsKmerReadFilter.java:27); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:35); at org.broadinstitute.hellbender.tools.spark.pathseq.ContainsKmerReadFilterSpark.call(ContainsKmerReadFilterSpark.java:15); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at org.apache.spark.api.java.JavaRDD$$anonfun$filter$1.apply(JavaRDD.scala:78); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.io.FileNotFoundException: hg19mini.hss (No such file or directory); at java.io.FileInputStream.open0(Native Method); at java.io.FileInputStream.open(FileInputStream.java:195); at java.io.FileInputStream.<init>(FileInputStream.java:138); at java.io.FileInputStream.<init>(FileInputStream.java:93); at org.broadinstitute.hellbender.utils.gcs.BucketUtils.openFile(BucketUtils.java:103); ```; `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:45012,schedul,scheduler,45012,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,3,['schedul'],['scheduler']
Energy Efficiency,.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)** ; **at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)** ; **at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)** ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:46342,schedul,scheduler,46342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['schedul'],['scheduler']
Energy Efficiency,.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.computeNewRefSpanAndCigar(ContigAlignmentsModifier.java:159); at org.broadinstitute.hellbender.tools.spark.sv.discovery.alignment.ContigAlignmentsModifier.clipAlignmentInterval(ContigAlignmentsModifier.java:42); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.removeOverlap(CpxVariantInterpreter.java:179); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.deOverlapAlignments(CpxVariantInterpreter.java:122); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.furtherPreprocess(CpxVariantInterpreter.java:79); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.CpxVariantInterpreter.lambda$inferCpxVariant$bdd686a3$1(CpxVariantInterpreter.java:51); at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4648:2029,schedul,scheduler,2029,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4648,3,['schedul'],['scheduler']
Energy Efficiency,.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:18987,schedul,scheduler,18987,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['schedul'],['scheduler']
Energy Efficiency,.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 05:09:10.813 ERROR Executor:91 - Exception in task 16.0 in stage 1.0 (TID 353); org.apache.spark.SparkException: Error communicating with MapOutputTracker; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:104); at org.apache.spark.MapOutputTracker.getStatuses(MapOutputTracker.scala:202); at org.apache.spark.MapOutputTracker.getMapSizesByExecutorId(MapOutputTracker.scala:142); at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49); at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:3678,schedul,scheduler,3678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['schedul'],['scheduler']
Energy Efficiency,".apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:109); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); Caused by: java.lang.NullPointerException; at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:100); ... 24 more; 05:12:04.045 INFO HaplotypeCallerSpark - Shutting down engine; [May 18, 2017 5:12:04 AM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 131.63 minutes.; Runtime.totalMemory()=16201547776; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 1.0 failed 1 times, most recent failure: Lost task 8.0 in stage 1.0 (TID 345, localhost): java.lang.ArrayI; ndexOutOfBoundsException: 16777215; at com.esotericsoftware.kryo.util",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3019:5562,schedul,scheduler,5562,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3019,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:19772,schedul,scheduler,19772,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:264); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:126); 	at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62); 	at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:4122,schedul,scheduler,4122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1444); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1862); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1875); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:34639,schedul,scheduler,34639,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.computePartitionReadExtents(SparkSharder.java:274); 	at org.broadinstitute.hellbender.engine.spark.SparkSharder.joi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:9797,schedul,scheduler,9797,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.writeSAM(SvDiscoverFromLocalAssemblyContigAlignmentsSpark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:9640,schedul,scheduler,9640,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:134); 	at org.broadinstitute.hellbender.cmdline.Command,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:3536,schedul,scheduler,3536,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); 	at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:439); 	at org.apache.spark.api,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:2538,schedul,scheduler,2538,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(P,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:10833,schedul,scheduler,10833,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:938); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:168); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:148); 	at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62); 	at org.apache.spark.rdd.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:16060,schedul,scheduler,16060,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.makeInterpretation(SimpleNovelAdjacencyInterpreter.java:48); 	at org.broad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:4958,schedul,scheduler,4958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['schedul'],['scheduler']
Energy Efficiency,.broadinstitute.hellbender.utils.NaturalLogUtils.logSumExp(NaturalLogUtils.java:84); 	at org.broadinstitute.hellbender.utils.NaturalLogUtils.normalizeLog(NaturalLogUtils.java:51); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6255:10751,Reduce,ReduceOps,10751,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6255,1,['Reduce'],['ReduceOps']
Energy Efficiency,.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$0(ErrorProbabilities.java:27) ; ; at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321) ; ; at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169) ; ; at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:25) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:138) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:154) ; ; at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWal,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298:7726,Reduce,ReduceOps,7726,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298,1,['Reduce'],['ReduceOps']
Energy Efficiency,".compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.scheduler.Task.run(Task.scala:108); [2018-04-15T03:55Z] ip-10-0-0-57: 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [2018-04-15T03:55Z] ip-10-0-0-57: 	at java.lang.Thread.run(Thread.java:745); ```; and in case I am missing anything in how I'm calling HaplotypeCallerSpark, here is the full command line we're using:; ```; gatk-launch --java-options '-Xms1000m -Xmx46965m -Djava.io.tmpdir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2018-04-14-195952.723/root/variantcall/2/variantcall_batch_region/3/bcbiotx/tmpno7wyh' HaplotypeCallerSpark --reference /mnt/work/cwl/bcbio_validation_workflows/giab-joint/biodata/collections/hg38/ucsc/hg38.2bit --annotation MappingQualityRankSumTest --annotation Mapping",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4661:6147,schedul,scheduler,6147,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4661,1,['schedul'],['scheduler']
Energy Efficiency,.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:13752,schedul,scheduler,13752,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1104); at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:438); at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:39); at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:159); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:5009,reduce,reduce,5009,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['reduce'],['reduce']
Energy Efficiency,.funcotator.DataSourceFuncotationFactory.determineFuncotations(DataSourceFuncotationFactory.java:239) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:211) ; ;     at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:182) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForSegment$2(FuncotatorEngine.java:218) ; ;     at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ;     at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175) ; ;     at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382) ; ;     at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ;     at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ;     at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ;     at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ;     at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForSegment(FuncotatorEngine.java:221) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:191) ; ;     at org.broadinstitute.hellbender.tools.funcotator.FuncotateSegments.apply(FuncotateSegments.java:59) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.lambda$traverse$0(FeatureWalker.java:99) ; ;     at java.util.Iterator.forEachRemaining(Iterator.java:116) ; ;     at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801) ; ;     at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580) ; ;     at org.broadinstitute.hellbender.engine.FeatureWalker.traverse(FeatureWalker.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7676:4468,Reduce,ReduceOps,4468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7676,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,".hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. 17/10/11 14:19:38 ERROR scheduler.TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool ; 17/10/11 14:19:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 2 from BlockManagerMaster.; 17/10/11 14:19:38 INFO storage.BlockManagerMaster: Removal of executor 2 requested; 17/10/11 14:19:38 INFO cluster.YarnClientSchedulerBackend: Asked to remove non-existent executor 2; 17/10/11 14:19:38 INFO cluster.YarnScheduler: Cancelling stage 1; 17/10/11 14:19:38 INFO scheduler.DAGScheduler: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203) failed in 10.702 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, com2, executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container marked as failed: container_1507683879816_0006_01_000003 on host: com2. Exit status: 50. Diagnostics: Exception from container-launch.; Container id: container_1507683879816_0006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:29010,schedul,scheduler,29010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,".hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add(IndexingVariantContextWriter.java:203); at htsjdk.variant.variantconte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:21566,schedul,scheduler,21566,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['schedul'],['scheduler']
Energy Efficiency,.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:353); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:38); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:116); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:179); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:198); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:121); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:142); at org.broadinstitute.hellbender.Main.main(Main.java:220); Caused by: java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculation,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:16958,Reduce,ReduceOps,16958,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['Reduce'],['ReduceOps']
Energy Efficiency,.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at java.base/java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:600); at java.base/java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:678); at java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:737); at java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919); at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); at java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558); at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$14(CalibrateDragstrModel.java:568); at java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448); at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290); at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020); at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656); at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594); at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183); Caused by: java.lang.IllegalArgumentException: Requested start 8613 is beyond the sequence length HLA-DRB1*04:03:01; at htsjdk.samtools.cram.ref.ReferenceSource.getReferenceBasesByRegion(ReferenceSource.java:207); at htsjdk.samtools.cram.build.CRAMReferenceRegion.fetchReferenceBasesByRegion(CRAMReferenceRegion.java:169); at htsjdk.samtools.cram.structure.Slice.normalizeCRAMRecords(Slice.java:502); at htsjdk.samtools.cram.structure.Container.getSAMRecords(Container.java:322); at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8139:8394,Adapt,AdaptedCallable,8394,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8139,1,['Adapt'],['AdaptedCallable']
Energy Efficiency,".java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_202408111100502620487673658411251_0021.; org.apache.spark.SparkException: Job aborted due t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:7635,schedul,scheduler,7635,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608) ~[gatk-package-4.4.0,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:11959,schedul,scheduler,11959,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStag,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:30678,schedul,scheduler,30678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 11:00:54.334 INFO ShutdownHookManager - Shutdown hook called; 11:00:54.335 INFO ShutdownHookManager - Deleting directory /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp/spark-f9c7c336-4e98-4fcc-855b-ba8a5a29e074; ```. The first lines of the log file,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:36549,schedul,scheduler,36549,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,.java:452); 	at org.broadinstitute.hellbender.utils.tsv.DataLine.get(DataLine.java:581); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:193); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary$PileupSummaryTableReader.createRecord(PileupSummary.java:188); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.fetchNextRecord(TableReader.java:364); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.access$200(TableReader.java:99); 	at org.broadinstitute.hellbender.utils.tsv.TableReader$1.hasNext(TableReader.java:472); 	at java.util.Iterator.forEachRemaining(Iterator.java:115); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.utils.tsv.TableReader.toList(TableReader.java:532); 	at org.broadinstitute.hellbender.tools.walkers.contamination.PileupSummary.readFromFile(PileupSummary.java:139); 	at org.broadinstitute.hellbender.tools.walkers.contamination.CalculateContamination.doWork(CalculateContamination.java:116); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Ma,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7707:4481,Reduce,ReduceOps,4481,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7707,1,['Reduce'],['ReduceOps']
Energy Efficiency,.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:8077,schedul,scheduler,8077,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['schedul'],['scheduler']
Energy Efficiency,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6064:3459,schedul,scheduler,3459,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064,1,['schedul'],['scheduler']
Energy Efficiency,.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:49994,schedul,scheduler,49994,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['schedul'],['scheduler']
Energy Efficiency,".reportAllBlocks(BlockManager.scala:217); at org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:236); at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:522); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:547); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:547); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180); at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 18/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.sc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:2233,Schedul,ScheduledThreadPoolExecutor,2233,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)** ; **at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)** ; **at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)** ; **at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)** ; **at org.apache.spark.rdd.RDD.count(RDD.scala:1168)** ; **at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455)** ; **at org.apache.spark.api.java.AbstractJavaRDDLike.count(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:46610,schedul,scheduler,46610,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:934); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:8941,schedul,scheduler,8941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:470); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:15480,schedul,scheduler,15480,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:748); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:7122,schedul,scheduler,7122,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:360); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:14761,schedul,scheduler,14761,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,3,['schedul'],['scheduler']
Energy Efficiency,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.scala:911); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunctions.scala:372); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$countByKey$1.apply(PairRDDFunction,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:5427,schedul,scheduler,5427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:4342,schedul,scheduler,4342,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); at org.apache.spark.rdd.RDD.count(RDD.scala:1158); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4699:41641,schedul,scheduler,41641,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4699,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1368); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19921,schedul,scheduler,19921,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:938); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:50858,schedul,scheduler,50858,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1891); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1083); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1081); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:17587,schedul,scheduler,17587,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['schedul'],['scheduler']
Energy Efficiency,.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6589,schedul,scheduler,6589,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency,.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19388,schedul,scheduler,19388,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency,.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1424); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); > 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); > 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1423); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); > 	at scala.Option.foreach(Option.scala:257); > 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1651); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1606); > 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1595); > 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); > 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); > 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); > 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); > 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); > 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); > 	at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:38); > 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:362); > 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3936:1678,schedul,scheduler,1678,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3936,1,['schedul'],['scheduler']
Energy Efficiency,.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:1941,schedul,scheduler,1941,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['schedul'],['scheduler']
Energy Efficiency,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5647:3250,schedul,scheduler,3250,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647,2,['schedul'],['scheduler']
Energy Efficiency,.utils.IntHistogram.addObservation(IntHistogram.java:50); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$LibraryRawStatistics.addRead(ReadMetadata.java:367); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata$PartitionStatistics.<init>(ReadMetadata.java:431); at org.broadinstitute.hellbender.tools.spark.sv.evidence.ReadMetadata.lambda$new$1dcab782$1(ReadMetadata.java:57); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:152); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:785); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:1511,schedul,scheduler,1511,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['schedul'],['scheduler']
Energy Efficiency,"/05/01 14:23:29 INFO BlockManagerMaster: BlockManagerMaster stopped; 18/05/01 14:24:38 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2399,schedul,scheduler,2399,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['schedul'],['scheduler']
Energy Efficiency,0 2:51:02 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2385510400; java.lang.IllegalStateException: Duplicate key 7.395307178412063E-4; at java.util.stream.Collectors.lambda$throwingMerger$138(Collectors.java:133); at java.util.stream.Collectors$$Lambda$67/403388441.apply(Unknown Source); at java.util.HashMap.merge(HashMap.java:1245); at java.util.stream.Collectors.lambda$toMap$196(Collectors.java:1320); at java.util.stream.Collectors$$Lambda$69/854719230.accept(Unknown Source); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.<init>(ContaminationFilter.java:26); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.buildFiltersList(Mutect2FilteringEngine.java:290); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.<init>(Mutect2FilteringEngine.java:60); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.onTraversalStart(FilterMutectCalls.java:138); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1047); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:140); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6996:4263,Reduce,ReduceOps,4263,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6996,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,0); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.getFeatureReader(FeatureDataSource.java:300); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:256); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:230); 	at org.broadinstitute.hellbender.engine.FeatureDataSource.<init>(FeatureDataSource.java:214); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.openFeatureSource(JoinReadsWithVariants.java:63); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$null$0(JoinReadsWithVariants.java:44); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.utils.spark.JoinReadsWithVariants.lambda$join$60e5b476$1(JoinReadsWithVariants.java:44); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:186); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5316:2781,Reduce,ReduceOps,2781,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5316,2,['Reduce'],['ReduceOps']
Energy Efficiency,0); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2114); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78); ... 34 more; Caused by:; java.util.ConcurrentModificationException; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6513:7740,schedul,scheduler,7740,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6513,6,['schedul'],['scheduler']
Energy Efficiency,"000); eden space 946688K, 10% used [0x000000066ab00000,0x0000000670bff978,0x00000006a4780000); from space 56832K, 99% used [0x00000006a5900000,0x00000006a9076d70,0x00000006a9080000); to space 85504K, 0% used [0x00000006a9b80000,0x00000006a9b80000,0x00000006aef00000); ParOldGen total 1497088K, used 20019K [0x00000003c0000000, 0x000000041b600000, 0x000000066ab00000); object space 1497088K, 1% used [0x00000003c0000000,0x00000003c138ceb0,0x000000041b600000); Metaspace used 36791K, capacity 37258K, committed 37504K, reserved 1081344K; class space used 5023K, capacity 5176K, committed 5248K, reserved 1048576K. Card table byte_map: [0x00002b5f67df9000,0x00002b5f69dfa000] byte_map_base: 0x00002b5f65ff9000. Marking Bits: (ParMarkBitMap*) 0x00002b5f57e71fa0; Begin Bits: [0x00002b5f6b656000, 0x00002b5f7b656000); End Bits: [0x00002b5f7b656000, 0x00002b5f8b656000). Polling page: 0x00002b5f56e61000. CodeCache: size=245760Kb used=5233Kb max_used=5233Kb free=240526Kb; bounds [0x00002b5f58a39000, 0x00002b5f58f59000, 0x00002b5f67a39000]; total_blobs=2060 nmethods=1583 adapters=391; compilation: enabled. Compilation events (10 events):; Event: 4.330 Thread 0x000056487672d800 1579 1 java.lang.ThreadLocal::getMap (5 bytes); Event: 4.330 Thread 0x000056487672d800 nmethod 1579 0x00002b5f58f55ed0 code [0x00002b5f58f56020, 0x00002b5f58f56130]; Event: 4.333 Thread 0x000056487672d800 1580 3 java.io.FileOutputStream::write (12 bytes); Event: 4.333 Thread 0x000056487672d800 nmethod 1580 0x00002b5f58f56550 code [0x00002b5f58f566c0, 0x00002b5f58f56848]; Event: 4.333 Thread 0x000056487672d800 1582 3 java.io.FilterInputStream::read (9 bytes); Event: 4.333 Thread 0x000056487672d800 nmethod 1582 0x00002b5f58f56910 code [0x00002b5f58f56a80, 0x00002b5f58f56ca8]; Event: 4.344 Thread 0x000056487672d800 1583 3 java.util.Formatter$Flags::<init> (10 bytes); Event: 4.344 Thread 0x000056487672d800 nmethod 1583 0x00002b5f58f56d50 code [0x00002b5f58f56ec0, 0x00002b5f58f57070]; Event: 4.344 Thread 0x000056487672",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:15516,adapt,adapters,15516,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['adapt'],['adapters']
Energy Efficiency,006_01_000003; Exit code: 50; Stack trace: ExitCodeException exitCode=50: ; 	at org.apache.hadoop.util.Shell.runCommand(Shell.java:601); 	at org.apache.hadoop.util.Shell.run(Shell.java:504); 	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:786); 	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:213); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302); 	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82); 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Container exited with a non-zero exit code 50. Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1457); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:33363,schedul,scheduler,33363,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,"020/hadoop-datanode:2.0.0-hadoop2.7.4-java8; networks:; - workbench; volumes:; - datanode:/hadoop/dfs/data; environment:; SERVICE_PRECONDITION: ""namenode:50070""; # depends_on:; # - namenode; env_file:; - ./hadoop.env; deploy:; mode: global; restart_policy:; condition: on-failure; labels:; traefik.docker.network: workbench; traefik.port: 50075. volumes:; datanode:; namenode:. networks:; workbench:; external: true; ```; the datanodes and namenode and spark master and workers are all working.; My hardware resources are:; 16 core and 1Tb memory ssd and 56Gb ram for 3 machines. I have this problem when I launch the version(GATK) v4.0.4.0 but not with this version v4.0.2.0-4-gb59d863-SNAPSHOT:. >java.lang.IllegalStateException: Duplicate key -1; 	at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); 	at java.util.HashMap.merge(HashMap.java:1253); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSpark.lambda$mark$2142e97f$1(MarkDuplicatesSpark.java:82); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$10$1.apply(JavaRDDLike.scala:319); 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89); 	at org.apache.spark.rdd.RDD.compute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4820:3848,Reduce,ReduceOps,3848,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4820,1,['Reduce'],['ReduceOps']
Energy Efficiency,04.001367357Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.clusterProbabilities(SomaticClusteringModel.java:203); 2019-10-29T18:18:04.001518160Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.clustering.SomaticClusteringModel.probabilityOfSequencingError(SomaticClusteringModel.java:96); 2019-10-29T18:18:04.001673083Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.TumorEvidenceFilter.calculateErrorProbability(TumorEvidenceFilter.java:27); 2019-10-29T18:18:04.001846904Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 2019-10-29T18:18:04.002024760Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002140012Z 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 2019-10-29T18:18:04.002232542Z 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 2019-10-29T18:18:04.002242727Z 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 2019-10-29T18:18:04.002292461Z 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 2019-10-29T18:18:04.002301667Z 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 2019-10-29T18:18:04.002307019Z 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 2019-10-29T18:18:04.002311722Z 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 2019-10-29T18:18:04.002316449Z 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 2019-10-29T18:18:04.002321526Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 2019-10-29T18:18:04.002358113Z 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6237:1458,Reduce,ReduceOps,1458,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6237,1,['Reduce'],['ReduceOps']
Energy Efficiency,"0423204143-0003/0 on hostPort xx.xx.xx.xx:59994 with 16 cores, 1024.0 MB RAM; 18/04/23 20:41:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36833.; 18/04/23 20:41:43 INFO NettyBlockTransferService: Server created on xx.xx.xx.xx:36833; 18/04/23 20:41:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 18/04/23 20:41:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMasterEndpoint: Registering block manager xx.xx.xx.xx:36833 with 4.0 GB RAM, BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, xx.xx.xx.xx, 36833, None); 18/04/23 20:41:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0; 18/04/23 20:41:43 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 329.7 KB, free 4.0 GB); 18/04/23 20:41:46 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20180423204143-0003/0 is now RUNNING; 00:09 DEBUG: [kryo] Write: SerializableConfiguration; 18/04/23 20:41:46 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.5 KB, free 4.0 GB); 18/04/23 20:41:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on xx.xx.xx.xx:36833 (size: 27.5 KB, free: 4.0 GB); 18/04/23 20:41:47 INFO SparkContext: Created broadcast 0 from newAPIHadoopFile at ReadsSparkSource.java:113; 18/04/23 20:41:47 INFO FileInputFormat: Total input files to process : 1; 18/04/23 20:41:51 INFO SparkContext: Starting job: first at ReadsSparkSource.java:221; 18/04/23 20:41:51",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:10025,Schedul,SchedulerBackend,10025,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,2,"['Schedul', 'schedul']","['SchedulerBackend', 'scheduling']"
Energy Efficiency,0950787185363106.zip -> hdfs://tele-1:8020/user/sun/.sparkStaging/application_1515493209401_0001/__spark_conf__.zip; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing view acls to: sun; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing modify acls to: sun; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing view acls groups to: ; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing modify acls groups to: ; 18/01/09 18:31:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sun); groups with view permissions: Set(); users with modify permissions: Set(sun); groups with modify permissions: Set(); 18/01/09 18:31:00 INFO yarn.Client: Submitting application application_1515493209401_0001 to ResourceManager; 18/01/09 18:31:00 INFO impl.YarnClientImpl: Submitted application application_1515493209401_0001; 18/01/09 18:31:00 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1515493209401_0001 and attemptId None; 18/01/09 18:31:01 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:01 INFO yarn.Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: root.users.sun; 	 start time: 1515493860237; 	 final status: UNDEFINED; 	 tracking URL: http://tele-1:8088/proxy/application_1515493209401_0001/; 	 user: sun; 18/01/09 18:31:02 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:03 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:04 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:05 INFO yarn.Client: Application report for application_1515493209401_0001 (state: ACCEPTED); 18/01/09 18:31:05 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster regi,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:12577,Schedul,SchedulerExtensionServices,12577,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['Schedul'],['SchedulerExtensionServices']
Energy Efficiency,1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:6823,schedul,scheduler,6823,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['schedul'],['scheduler']
Energy Efficiency,1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); 	at org.apache.spark.scheduler.Task.run(Task.scala:89); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:18496,schedul,scheduler,18496,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['schedul'],['scheduler']
Energy Efficiency,1); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); 	at org.broadinstitute.hellbender.Main.main(Main.java:289); Caused by: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at java.util.concurrent.ForkJoinTask.getThrowableException(ForkJoinTask.java:593); 	at java.util.concurrent.ForkJoinTask.reportException(ForkJoinTask.java:677); 	at java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:735); 	at java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:714); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233); 	at java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:546); 	at org.broadinstitute.hellbender.tools.dragstr.CalibrateDragstrModel.lambda$collectCaseStatsParallel$13(CalibrateDragstrModel.java:489); 	at java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1424); 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289); 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056); 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692); 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:157); Caused by: java.lang.IllegalArgumentException: A reference must be supplied that includes the reference sequence for chr12).; 	at htsjdk.samtools.cram.ref.CRAMLazyReferenceSource.getReferenceBases(CRAMLazyReferenceSource.java:41); 	at htsjdk.samtools.cram.build.CRAMReferenceRegion.getReferenceBases,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7060:2795,Reduce,ReduceOps,2795,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7060,1,['Reduce'],['ReduceOps']
Energy Efficiency,1); 	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:303); 	at org.apache.spark.RangePartitioner$$anonfun$13.apply(Partitioner.scala:301); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$26.apply(RDD.scala:847); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ERROR: (gcloud.dataproc.jobs.submit.spark) Job [5838bd7dec2d4533ad090ce03ecc7c0c] entered state [ERROR] while waiting for [DONE].; ```. #### Steps to reproduce. See command given in stack trace above.; WGS bam is available at ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam` ; and ; `gs://broad-dsde-methods/shuang/tmp/HG00512.cram.samtools1_9.bam.bai`. Interval list BED file content given below. ```; chrX	67113957	67114130; chrX	71903370	71903687; chrX	74330484	74330552; chrX	75379902	75379965; chrX	78441355	78441953; ```. #### Expected behavior; Pass. #### Actual behavior; Error! This could be related to ticket #2722,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:22437,schedul,scheduler,22437,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,"1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala:83, took 3.799268 s; 11:00:53.979 ERROR SparkHadoopWriter - Aborting job job_2024081111",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:7554,schedul,scheduler,7554,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:11878,schedul,scheduler,11878,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGSch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:30597,schedul,scheduler,30597,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 11:00:54.334 INFO ShutdownHookManager - Shutdown hook called; 11:00:54.335 INFO ShutdownHookManager - Deleting directory /raid/tmp/d6/c66ba827e22dbc38625af1cbc85adc/tmp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:36468,schedul,scheduler,36468,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,"19:27 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.131.101.159:34044 (size: 32.3 KB, free: 529.9 MB); 17/10/11 14:19:27 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1004; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244) (first 15 tasks are for partitions Vector(0)); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks; 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, com2, executor 1, partition 0, NODE_LOCAL, 1990 bytes); 17/10/11 14:19:27 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on com2:38568 (size: 32.3 KB, free: 529.9 MB); 17/10/11 14:19:27 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to com2:35572; 17/10/11 14:19:27 INFO spark.MapOutputTrackerMaster: Size of output statuses for shuffle 0 is 134 bytes; 17/10/11 14:19:28 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, com2, executor 1): java.lang.AbstractMethodError: org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink$$Lambda$26/353370312.call(Ljava/lang/Object;)Ljava/lang/Iterable;; 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:30",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:15599,schedul,scheduler,15599,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,2); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:13662,schedul,scheduler,13662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['schedul'],['scheduler']
Energy Efficiency,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:13:58 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 9999, span 21707, expected MD5 059b07ed1e0589040ada9b236b88b509; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apach",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:6038,schedul,scheduler,6038,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 16.0 in stage 0.0 (TID 2, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:7618,schedul,scheduler,7618,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:00 WARN scheduler.TaskSetManager: Lost task 5.0 in stage 0.0 (TID 3, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 0, start 124511724, span 7265, expected MD5 cf58e0adc447a66b188474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:9204,schedul,scheduler,9204,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:10788,schedul,scheduler,10788,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,"2); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:09 ERROR scheduler.TaskSetManager: Task 16 in stage 0.0 failed 4 times; aborting job; 13:14:09.675 INFO CountReadsSpark - Shutting down engine; [December 21, 2018 1:14:09 PM EST] org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark done. Elapsed time: 0.97 minutes.; Runtime.totalMemory()=937426944; org.apache.spark.SparkException: Job aborted due to stage failure: Task 16 in stage 0.0 failed 4 times, most recent failure: Lost task 16.3 in stage 0.0 (TID 11, scc-q16.scc.bu.edu, executor 2): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 1, start 146479558, span 42247, expected MD5 8e364a33b9a9350f9ebfac1db38af647; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:12372,schedul,scheduler,12372,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,2/align/NA24631/NA24631-sort-recal.bam -L /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/gatk-haplotype/chr15/NA24631-chr15_68578892_84670250-block-regions.bed --interval_set_rule INTERSECTION --sparkMaster local[16] --conf spark.local.dir=/mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH --annotation ClippingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:2482,schedul,scheduler,2482,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['schedul'],['scheduler']
Energy Efficiency,"20:18 INFO BlockManagerInfo: Added rdd_53_0 in memory on d01.capitalbiotech.local:41352 (size: 0.0 B, free: 399.8 GB); 23/05/23 13:20:18 INFO Executor: Finished task 0.0 in stage 15.0 (TID 1964). 1226 bytes result sent to driver; 23/05/23 13:20:18 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 1964) in 147 ms on localhost (executor driver) (1/2); 23/05/23 13:20:18 INFO Executor: Finished task 1.0 in stage 15.0 (TID 1965). 1183 bytes result sent to driver; 23/05/23 13:20:18 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 1965) in 146 ms on localhost (executor driver) (2/2); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool ; 23/05/23 13:20:18 INFO DAGScheduler: ShuffleMapStage 15 (mapPartitionsToPair at PSScorer.java:68) finished in 0.185 s; 23/05/23 13:20:18 INFO DAGScheduler: looking for newly runnable stages; 23/05/23 13:20:18 INFO DAGScheduler: running: Set(); 23/05/23 13:20:18 INFO DAGScheduler: waiting: Set(ResultStage 16); 23/05/23 13:20:18 INFO DAGScheduler: failed: Set(); 23/05/23 13:20:18 INFO DAGScheduler: Submitting ResultStage 16 (ShuffledRDD[61] at reduceByKey at PSScorer.java:71), which has no missing parents; 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 4.7 KB, free 399.8 GB); 23/05/23 13:20:18 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 2.6 KB, free 399.8 GB); 23/05/23 13:20:18 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on d01.capitalbiotech.local:41352 (size: 2.6 KB, free: 399.8 GB); 23/05/23 13:20:18 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163; 23/05/23 13:20:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (ShuffledRDD[61] at reduceByKey at PSScorer.java:71) (first 15 tasks are for partitions Vector(0, 1)); 23/05/23 13:20:18 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks; 23/05/23 13:20:18 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8339:38995,reduce,reduceByKey,38995,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8339,1,['reduce'],['reduceByKey']
Energy Efficiency,"23.apply(RDD.scala:801); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to parse header with error: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf: Too many open files, for input source: /local/scratch/rieder/spark-bb59423b-0368-4de5-85e0-e6641fb25380/userFiles-a91d5958-33f5-4685-bf9d-c8fc0924f7c6/Homo_sapiens_assembly38.known_indels.vcf; at htsjdk.tribble.TribbleIndexedFeatureReader.readHeader(TribbleIndexedFeatureRe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6578:5617,schedul,scheduler,5617,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6578,1,['schedul'],['scheduler']
Energy Efficiency,"3 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (com2:35572) with ID 1; 17/10/11 14:19:23 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1); 17/10/11 14:19:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, com2, executor 1, partition 0, NODE_LOCAL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on com2:38568 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:26 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on com2:38568 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4180 ms on com2 (executor 1) (1/1); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at SparkUtils.java:157) finished in 8.951 s; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: looking for newly runnable stages; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: running: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: failed: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244), which has no missing parents; 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 86.1 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.3 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.131.101.159:34044 (size: 32.3 KB, free: 529.9 MB); 17/10/11 14:19:27",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:13730,schedul,scheduler,13730,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,319); at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3462:2427,schedul,scheduler,2427,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3462,1,['schedul'],['scheduler']
Energy Efficiency,"362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1084); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply$mcV$sp(PairRDDFunctions.scala:1003); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopFile$2.apply(PairRDDFunctions.scala:994); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:994); 	at org.apache.spark.api.java.JavaPairRDD.saveAsNewAPIHadoopFile(JavaPairRDD.scala:823); 	at org.disq_bio.disq.impl.formats.sam.AnySamSinkMultiple.save(AnySamSinkMultiple.java:96); 	at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:206); 	at org.broadinstitute.hellbender.engine.sp...; ```. here’s the command line; bamIn=gs://broad-gatk-test-jenkins-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam; refIn=gs://broad-gatk-test-jenkins-robust/human_g1k_v37.fasta; bamOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.readnamesort.bam.md.bqsr; knownIn=gs://broad-gatk-test-jenkins-robust/dbsnp_138.b37.excluding_sites_after_129.vcf; vcfOut=gs://broad-gatk-test-jenkins-write-robust/CEUTrio.HiSeq.WEx.b37.NA12892.vcf; ./gatk ReadsPipelineSpark \; -I $bamIn \; -R $refIn \; --output-bam $bamOut \; -O $vcfOut \; --known-sites $knownIn \; --sharded-output true \; --emit-original-quals \; --duplicate-scoring-strategy SUM_OF_BASE_QUALITIES \; --num-reducers 0 \; -- \; --spark-runner GCS \; --cluster $CLUSTERNAME \; --driver-memory 8G \; --conf 'spark.yarn.executor.memoryOverhead=2000' \; --executor-memory 18g \; --executor-cores 6 \; --conf spark.yarn.executor.memoryOverhead=2000""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:13527,reduce,reducers,13527,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['reduce'],['reducers']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 14; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 0; cpu cores	: 14; apicid		: 32; initial apicid	: 32; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:59884,monitor,monitor,59884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 10; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 11; cpu cores	: 14; apicid		: 22; initial apicid	: 22; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat ps",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:54010,monitor,monitor,54010,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:61058,monitor,monitor,61058,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:62232,monitor,monitor,62232,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:63406,monitor,monitor,63406,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:64580,monitor,monitor,64580,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:65754,monitor,monitor,65754,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:66928,monitor,monitor,66928,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:68102,monitor,monitor,68102,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 23; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 52; initial apicid	: 52; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:69276,monitor,monitor,69276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['monitor'],['monitor']
Energy Efficiency,"4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 0; cpu cores	: 14; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 1; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 2; initial apicid	: 2; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:43978,power,power,43978,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 2; initial apicid	: 2; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 2; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 4; initial apicid	: 4; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:45149,power,power,45149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 4; initial apicid	: 4; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 3; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 6; initial apicid	: 6; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:46320,power,power,46320,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 6; initial apicid	: 6; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 4; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 8; initial apicid	: 8; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rd",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:47491,power,power,47491,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 8; initial apicid	: 8; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 5; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 10; initial apicid	: 10; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:48662,power,power,48662,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,4951576950052743.tsv /tmp/tintest/sample-3745007638909244994571.tsv /tmp/tintest/sample-3758817480300622528681.tsv /tmp/tintest/sample-3765561422653477541111.tsv /tmp/tintest/sample-377681127346074691924.tsv /tmp/tintest/sample-3788006936711929575536.tsv /tmp/tintest/sample-3794598448303416401276.tsv /tmp/tintest/sample-380910670101098136635.tsv /tmp/tintest/sample-3815864583095389374312.tsv /tmp/tintest/sample-3821063008346821202582.tsv /tmp/tintest/sample-3836550848258521825191.tsv /tmp/tintest/sample-3842488752532231097400.tsv /tmp/tintest/sample-3855124216409092357090.tsv /tmp/tintest/sample-3866989755460133829309.tsv ; Stdout: 10:58:52.820 INFO cohort_denoising_calling - Loading 387 read counts file(s)...; 11:01:01.618 INFO gcnvkernel.io.io_metadata - Loading germline contig ploidy and global read depth metadata...; 11:02:11.422 INFO gcnvkernel.tasks.task_cohort_denoising_calling - Instantiating the denoising model (warm-up)...; 11:04:53.672 ERROR theano.gof.cmodule - [Errno 12] Cannot allocate memory. Stderr: Problem occurred during compilation with the command line below:; /usr/bin/g++ -shared -g -O3 -fno-math-errno -Wno-unused-label -Wno-unused-variable -Wno-write-strings -fopenmp -march=knl -mmmx -mno-3dnow -msse -msse2 -msse3 -mssse3 -mno-sse4a -mcx16 -msahf -mmovbe -maes -mno-sha -mpclmul -mpopcnt -mabm -mno-lwp -mfma -mno-fma4 -mno-xop -mbmi -mbmi2 -mno-tbm -mavx -mavx2 -msse4.2 -msse4.1 -mlzcnt -mrtm -mhle -mrdrnd -mf16c -mfsgsbase -mrdseed -mprfchw -madx -mfxsr -mxsave -mxsaveopt -mavx512f -mno-avx512er -mavx512cd -mno-avx512pf -mno-prefetchwt1 -mclflushopt -mxsavec -mxsaves -mavx512dq -mavx512bw -mno-avx512vl -mno-avx512ifma -mno-avx512vbmi -mclwb -mno-mwaitx -mno-clzero -mpku --param l1-cache-size=32 --param l1-cache-line-size=64 --param l2-cache-size=22528 -mtune=generic -DNPY_NO_DEPRECATED_API=NPY_1_7_API_VERSION -m64 -fPIC -I/home/tintest/miniconda2/envs/aurexome/lib/python3.6/site-packages/numpy/core/include -I/home/tintest/miniconda2/envs/aurexo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5053:61736,allocate,allocate,61736,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5053,1,['allocate'],['allocate']
Energy Efficiency,5); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$1$1.apply(JavaRDDLike.scala:125); at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:435); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:441); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:130); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$4.apply(SparkHadoopWriter.scala:129); at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394); at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:141); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: htsjdk.samtools.SAMException: Fasta index file could not be opened: /private/var/folders/5s/v5t08tmd42z_2m2c30vqf6kc0000gn/T/spark-556aa7a2-4d88-4bae-ad16-36d5af920fa9/userFiles-aeb68992-3215-4897-8f8a-040396296185/Homo_sapiens_assembly18.fasta.fai; at htsjdk.samtools.reference.FastaSequenceIndex.<init>(FastaSequenceIndex.java:74); at htsjdk.samtools.reference.IndexedFastaSequenceFile.<init>(IndexedFastaSequenceFile.java:98); at htsjdk.samto,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6642:3257,schedul,scheduler,3257,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6642,1,['schedul'],['scheduler']
Energy Efficiency,"524, localhost, executor 1, partition 9, NODE_LOCAL, 5598 bytes); 18/09/08 10:32:50 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 6.0 (TID 515, localhost, executor 1): java.lang.NullPointerException; 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.EmptyFragment.<init>(EmptyFragment.java:35); 	at org.broadinstitute.hellbender.utils.read.markduplicates.sparkrecords.MarkDuplicatesSparkRecord.newEmptyFragment(MarkDuplicatesSparkRecord.java:37); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$null$0(MarkDuplicatesSparkUtils.java:114); 	at java.util.stream.ReferencePipeline$11$1.accept(ReferencePipeline.java:372); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.transforms.markduplicates.MarkDuplicatesSparkUtils.lambda$transformToDuplicateNames$17d832cf$1(MarkDuplicatesSparkUtils.java:123); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$3$1.apply(JavaRDDLike.scala:143); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:199); 	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5169:3093,Reduce,ReduceOps,3093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5169,1,['Reduce'],['ReduceOps']
Energy Efficiency,615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1925); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1938); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1951); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1965); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6930,schedul,scheduler,6930,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958); at org.apache.spark.rdd.RDD.count(RDD.scala:1157); at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.pipelines.CountReadsSpark.runTool(CountReadsSpark.java:80); at org.broadinstitut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:15288,schedul,scheduler,15288,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3679:5235,schedul,scheduler,5235,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3679,1,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:50666,schedul,scheduler,50666,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:14569,schedul,scheduler,14569,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,3,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1354); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.take(RDD.scala:1327); at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:19729,schedul,scheduler,19729,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency,7-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH --annotation ClippingRankSumTest --annotation DepthPerSampleHC --output /mnt/work/cwl/bcbio_validation_workflows/giab-joint/bunny_work/main-giab-joint-2017-10-03-104521.457/root/variantcall/8/variantcall_batch_region/16/bcbiotx/tmp7exzBH/NA24631-chr15_68578892_84670250-block.vcf.gz --emitRefConfidence GVCF -GQB 10 -GQB 20 -GQB 30 -GQB 40 -GQB 60 -GQB 80; ```; and the full traceback is:; ```; 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:2886,schedul,scheduler,2886,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['schedul'],['scheduler']
Energy Efficiency,"8); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:912); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); at org.apache.spark.scheduler.Task.run(Task.scala:86); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748); 00:11:09.632 WARN TaskSetManager:66 - Lost task 15.0 in stage 1.0 (TID 519, localhost): java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$Redu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:4945,schedul,scheduler,4945,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['schedul'],['scheduler']
Energy Efficiency,"8/03/09 09:22:08 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/03/09 09:22:08 INFO SparkContext: Successfully stopped SparkContext; 09:22:08.389 INFO BaseRecalibratorSpark - Shutting down engine; [March 9, 2018 9:22:08 AM UTC] org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark done. Elapsed time: 61.53 minutes.; Runtime.totalMemory()=16815489024; org.apache.spark.SparkException: Job aborted due to stage failure: Task 8 in stage 0.0 failed 1 times, most recent failure: Lost task 8.0 in stage 0.0 (TID 8, localhost): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:3478,schedul,scheduler,3478,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['schedul'],['scheduler']
Energy Efficiency,81); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). 11:00:53.977 INFO DAGScheduler - Job 1 failed: runJob at SparkHadoopWriter.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:7466,schedul,scheduler,7466,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,81); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGSchedu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:11790,schedul,scheduler,11790,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,2,['schedul'],['scheduler']
Energy Efficiency,81); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1572); at java.base/java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1529); at java.base/java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1438); at java.base/java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181); at java.base/java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:350); at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:46); at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:115); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1501); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 11:00:54.334 INFO ShutdownHookManager - Shutdown hook called; 11:00:54.335 INFO ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:36380,schedul,scheduler,36380,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,"8474efc3c84a43; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 18/12/21 13:14:03 WARN scheduler.TaskSetManager: Lost task 23.0 in stage 0.0 (TID 6, scc-q15.scc.bu.edu, executor 1): htsjdk.samtools.cram.CRAMException: Reference sequence MD5 mismatch for slice: sequence id 2, start 93470412, span 157, expected MD5 56b7844faa4e0c4f61fd6774df454b09; at htsjdk.samtools.CRAMIterator.nextContainer(CRAMIterator.java:184); at htsjdk.samtools.CRAMIterator.hasNext(CRAMIterator.java:258); at org.disq_bio.disq.impl.formats.AutocloseIteratorWrapper.hasNext(AutocloseIteratorWrapper.java:52); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); at scala.collect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:10446,schedul,scheduler,10446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,"86); - Remove AI/AN from VDS docs [VS-726] (#8096); - Add flag for cost_observability table writing to support sub-cohort use case [VS-521] (#8093); - Document STS delivery process for VDS [VS-727] (#8101); - delete obsolete callset_QC directory and its contents [VS-318] (#8108); - doc link typo and add check for control samples in AVRO export (#8110); - Add defaults for scatter_count in GvsExtractCohortFromSampleNames [VS-496] (#8109); - Escape table names properly in ValidateVat WDL (#8116); - Vs 741 fix indefinite freeze in split intervals task when using exome data (#8113); - VAT Readme updates (#8090); - WDL and python scripts to use the VDS in the VAT (#8077); - VS-757 - Use JASIX to make sub-jsons of annotated output of Nirvana (#8133); - add note about permissions for P&S workflow to work (#8135); - VS-759 (and VS-760) (#8137); - VS-765. Scatter the RemoveDuplicates task. (#8144); - update delivery docs based on latest VDS delivery run [VS-770] (#8150); - Add monitoring to index vcf (#8151); - Make some noise when VDS validation succeeds (#8155); - Handle empty genes annotation file. (#8153); - Add escapes for otherwise problematic dataset / table names. (#8162); - New WDL to create VAT tsvs from previously generated BigQuery table. (#8165); - Treat withdrawn samples in sub-cohort prepare correctly [VS-772] (#8156); - Remove unused VAT Creation WDL (#8172); - Gg consistently use dataset name as input parameter (#8173); - AoU cleanup docs, round 1 [VS-671] (#8104); - VDS docs remove samples and correct GT [VS-807] (#8178); - [VS-693] Add support for VQSR Lite to GvsCreateFilterSet (#8157); - VAT Documentation Update Round 1 [VS-531]; - VS-530 VDS creation documentation for AoU (#8169); - Update beta docs to tell people not to use free credits (#8184); - VS-816 Keeping ingestion under quota (#8193); - CromwellOnAzure + Azure SQL DB + AAD first steps doc [VS-805] (#8191); - Edit and re-format VDS -> VAT doc [VS-821] (#8187); - VS-820 Incorporate code to stay un",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8248:31409,monitor,monitoring,31409,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8248,2,['monitor'],['monitoring']
Energy Efficiency,"9 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:41.089 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:10:45.460 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 00:11:09.609 WARN TaskMemoryManager:381 - leak 166.6 MB memory from org.apache.spark.util.collection.ExternalAppendOnlyMap@60a3c432; 00:11:09.611 ERROR Executor:91 - Exception in task 15.0 in stage 1.0 (TID 519); java.lang.IllegalStateException: Duplicate key [B@4e233a3c; at java.util.stream.Collectors.lambda$throwingMerger$0(Collectors.java:133); at java.util.HashMap.merge(HashMap.java:1253); at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1320); at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1374); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.buildGapContinuationPenalties(PairHMMLikelihoodCalculat; ionEngine.java:304); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculationEngi; ne.java:253); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.PairHMMLikelihoodCalculationEngine.computeReadLikelihoods(PairHMMLikelihoodCalculation",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3018:1564,Reduce,ReduceOps,1564,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3018,1,['Reduce'],['ReduceOps']
Energy Efficiency,"92 WARN ReferenceConfidenceVariantContextMerger - Detected invalid annotations: When trying to merge variant contexts at location chrM:63 the annotation MLEAC=[2, 0] was not a numerical value and was ignored; 12:01:49.505 INFO CombineGVCFs - Shutting down engine; [August 24, 2020 12:01:49 PM HKT] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.21 minutes.; Runtime.totalMemory()=6277824512; java.lang.NullPointerException; at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.encode(StrandBiasUtils.java:52); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.StrandBiasUtils.makeRawAnnotationString(StrandBiasUtils.java:46); at org.broadinstitute.hellbender.tools.walkers.annotator.allelespecific.AS_StrandBiasTest.combineRawData(AS_StrandBiasTest.java:115); at org.broadinstitute.hellbender.tools.walkers.annotator.VariantAnnotatorEngine.combineAnnotations(VariantAnnotatorEngine.java:210); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.mergeAttributes(ReferenceConfidenceVariantContextMerger.java:318); at org.broadinstitute.hellbender.tools.walkers.ReferenceConfidenceVariantContextMerger.merge(ReferenceConfidenceVariantContextMerger.java:142); at org.broadinstitute.hellbender.tools.walkers.CombineGVCFs.endPreviousStates(CombineGVCFs.java:403",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6766:6093,Reduce,ReduceOps,6093,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6766,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,": 1170; 20:38:14.506 INFO StructuralVariationDiscoveryPipelineSpark - INS: 1394; 18/01/12 20:38:16 WARN org.apache.spark.scheduler.TaskSetManager: Stage 17 contains a task of very large size (2518 KB). The maximum recommended task size is 100 KB.; 18/01/12 20:38:22 WARN org.apache.spark.scheduler.TaskSetManager: Stage 18 contains a task of very large size (2307 KB). The maximum recommended task size is 100 KB.; 20:38:27.207 INFO StructuralVariationDiscoveryPipelineSpark - Processing 501267 raw alignments from 426041 contigs.; 18/01/12 20:38:27 WARN org.apache.spark.scheduler.TaskSetManager: Stage 20 contains a task of very large size (2518 KB). The maximum recommended task size is 100 KB.; 20:38:35.835 INFO StructuralVariationDiscoveryPipelineSpark - Primitive filtering based purely on MQ left 339065 contigs.; 20:38:37.378 INFO StructuralVariationDiscoveryPipelineSpark - 17574 contigs with chimeric alignments potentially giving SV signals.; 18/01/12 20:38:37 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 284.0 in stage 25.0 (TID 43189, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:26962677-26962806_-_482S130M491S_60_-1_281_S; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:681); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.ContigAlignmentsModifier.removeOverlap(ContigAlignmentsModifier.java:36); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.prototype.AssemblyContigAlignmentSignatureClassifier.lambda$processContigsWithTwoAlignments$e28aa838$1(AssemblyContigAlignmentSignatureClassifier.java:114); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(JavaPairRDD.scala:1040); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$13.hasNext(Ite",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:2903,schedul,scheduler,2903,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['schedul'],['scheduler']
Energy Efficiency,: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1127); at org.apache.spark.rdd.RDDOperationScope$.with,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:4150,schedul,scheduler,4150,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['schedul'],['scheduler']
Energy Efficiency,: ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 126542 ms; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1936); at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1002); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); at org.apache.spark.rdd.RDD.reduce(RDD.scala:984); at org.apache.spark.rdd.RDD$$anonfu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4515:4062,schedul,scheduler,4062,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4515,1,['schedul'],['scheduler']
Energy Efficiency,:1760); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1157); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1944); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5547:14616,schedul,scheduler,14616,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5547,1,['schedul'],['scheduler']
Energy Efficiency,:304); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.getFeaturesFromFeatureContext(DataSourceFuncotationFactory.java:219); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:197); 	at org.broadinstitute.hellbender.tools.funcotator.DataSourceFuncotationFactory.createFuncotations(DataSourceFuncotationFactory.java:172); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.lambda$createFuncotationMapForVariant$0(FuncotatorEngine.java:147); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566); 	at org.broadinstitute.hellbender.tools.funcotator.FuncotatorEngine.createFuncotationMapForVariant(FuncotatorEngine.java:157); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.enqueueAndHandleVariant(Funcotator.java:903); 	at org.broadinstitute.hellbender.tools.funcotator.Funcotator.apply(Funcotator.java:857); 	at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:104); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7090:12347,Reduce,ReduceOps,12347,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7090,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,":33:08.183 INFO MemoryStore - Block broadcast_0 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:08.581 INFO MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 1076.2 GiB); 10:33:08.585 INFO BlockManagerInfo - Added broadcast_0_piece0 in memory on 172.20.19.130:43279 (size: 41.8 KiB, free: 1076.2 GiB); 10:33:08.591 INFO SparkContext - Created broadcast 0 from newAPIHadoopFile at PathSplitSource.java:96; 10:33:09.126 INFO MemoryStore - Block broadcast_1 stored as values in memory (estimated size 268.7 KiB, free 1076.2 GiB); 10:33:09.142 INFO MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 41.8 KiB, free 1076.2 GiB); 10:33:09.144 INFO BlockManagerInfo - Added broadcast_1_piece0 in memory on 172.20.19.130:43279 (size: 41.8 KiB, free: 1076.2 GiB); 10:33:09.145 INFO SparkContext - Created broadcast 1 from newAPIHadoopFile at PathSplitSource.java:96; 10:33:09.336 INFO SortSamSpark - Using 44262 reducers; 10:33:09.615 INFO FileInputFormat - Total input files to process : 4791; 10:33:09.793 INFO SparkContext - Starting job: sortByKey at SparkUtils.java:165; 10:33:09.849 INFO DAGScheduler - Got job 0 (sortByKey at SparkUtils.java:165) with 15769 output partitions; 10:33:09.850 INFO DAGScheduler - Final stage: ResultStage 0 (sortByKey at SparkUtils.java:165); 10:33:09.850 INFO DAGScheduler - Parents of final stage: List(); 10:33:09.862 INFO DAGScheduler - Missing parents: List(); 10:33:09.869 INFO DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[16] at sortByKey at SparkUtils.java:165), which has no missing parents; 10:33:10.193 INFO MemoryStore - Block broadcast_2 stored as values in memory (estimated size 571.5 KiB, free 1076.2 GiB); 10:33:10.207 INFO MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 214.8 KiB, free 1076.2 GiB); 10:33:10.208 INFO BlockManagerInfo - Added broadcast_2_piece0 in memory on 172.20.19.130:43279 (size: 2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:49076,reduce,reducers,49076,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['reduce'],['reducers']
Energy Efficiency,:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)** ; **at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45889,schedul,scheduler,45889,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['schedul'],['scheduler']
Energy Efficiency,":53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.TaskSetManager: Task 284 in stage 25.0 failed 4 times; aborting job; 18/01/12 20:38:37 INFO org.spark_project.jetty.server.AbstractConnector: Stopped Spark@23007ed{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}; 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(50,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(52,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(34,WrappedArray()); 18/01/12 20:38:37 ERROR org.apache.spark.scheduler.LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(60,WrappedArray()); 20:38:37.897 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [January 12, 2018 8:38:37 PM UTC] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 42.74 minutes.; Runtime.totalMemory()=16692805632; org.apache.spark.SparkException: Job aborted due to stage failure: Task 284 in stage 25.0 failed 4 times, most recent failure: Lost task 284.3 in stage 25.0 (TID 43224, cw-test-w-6.c.broad-dsde-methods.internal, executor 7): java.lang.IllegalArgumentException: two input alignments' overlap on read consumes completely one of them.	1_1097_chrUn_JTFH01000492v1_decoy:501-1597_+_1097M6H_60_1_1092_O	483_612_chr17:2696267",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4141:5591,schedul,scheduler,5591,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4141,1,['schedul'],['scheduler']
Energy Efficiency,:78)** ; **at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:464)** ; **at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)** ; **at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:187)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)** ; **at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)** ; **at org.apache.spark.scheduler.Task.run(Task.scala:121)** ; **at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)** ; **at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)** ; **at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)** ; **at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)** ; **at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)** ; **at java.lang.Thread.run(Thread.java:745)**. **Driver stacktrace:** ; **at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)** ; **at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)** ; **at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)** ; **at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)** ; **at scala.Option.foreach(Option.scala:257)** ; **at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)** ; **at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnRecei,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6493:45746,schedul,scheduler,45746,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6493,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1922); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2268:19607,schedul,scheduler,19607,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2268,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:264); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:126); 	at org.apache.spark.rdd.Ordered,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3659:3957,schedul,scheduler,3957,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3659,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1445); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1444); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1444); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799); 	at scala.Option.foreach(Option.scala:236); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1668); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1627); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1616); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1862); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1875); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1144); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1074); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316); 	at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:34474,schedul,scheduler,34474,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.br,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3840:9632,schedul,scheduler,9632,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3840,2,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2043); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2062); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2087); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at org.apache.spark.api.java.JavaRDDLike$class.count(JavaRDDLike.scala:455); 	at org.apache.spark.api.java.AbstractJavaRDDLike.count(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark.runTool(PathSeqPipelineSpark.java:245); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:387); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:3371,schedul,scheduler,3371,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:2373,schedul,scheduler,2373,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); 	at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.write(SparkHadoopMapReduceWriter.scala:88); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:1085); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5545:10668,schedul,scheduler,10668,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5545,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:938); 	at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:306); 	at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:168); 	at org.apache.spark.RangePartit,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5051:15895,schedul,scheduler,15895,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5051,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.br,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6070:4793,schedul,scheduler,4793,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070,1,['schedul'],['scheduler']
Energy Efficiency,"; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (saveAsNewAPIHadoopFile at ReadsSparkSink.java:203); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0); 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at SparkUtils.java:157), which has no missing parents; 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.2 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 6.9 KB, free 529.7 MB); 17/10/11 14:19:18 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.131.101.159:34044 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:18 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1004; 17/10/11 14:19:18 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[5] at mapToPair at SparkUtils.java:157) (first 15 tasks are for partitions Vector(0)); 17/10/11 14:19:18 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks; 17/10/11 14:19:19 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1); 17/10/11 14:19:23 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (com2:35572) with ID 1; 17/10/11 14:19:23 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1); 17/10/11 14:19:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, com2, executor 1, partition 0, NODE_LOCAL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcas",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:12283,schedul,scheduler,12283,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,"; GenomicsDBImport. ### Affected version(s); - [ ] Public release version 4.1.4.1 . ### Description ; Running GenomicsDBImport on an HPC cluster using SLURM, admin mentioned that the jobs are writing inefficiently to shared storage (@spikebike will follow up with HPC specifics and logs). . #### Steps to reproduce; ```; Using GATK jar /share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx60g -Xms60g -jar /share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar GenomicsDBImport --genomicsdb-workspace-path data/interim/combined_database_bpres/0004 --batch-size 50 --reader-threads 6 --sample-name-map data/processed/sample_map --intervals data/processed/scattered_intervals/0004-scattered.intervals --tmp-dir /scratch/sdturner/genomicsdbimport/0004; ```. #### Expected behavior; My understanding is that it may be more efficient to use a small buffer and write the final database in full. . #### Actual behavior; Again my (limited) understanding is that the tool is writing output multiple times and throwing out all but the last write. Here is an example of a log for a 2.6 Mb region and 295 samples: ; ; ```; 07:24:39.198 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/apps/gatk-4.1.4.1/gatk-package-4.1.4.1-local.jar!/com/intel/gkl/native/libgkl_compression.so; Feb 28, 2020 7:24:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 07:24:39.616 INFO GenomicsDBImport - ------------------------------------------------------------; 07:24:39.617 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.4.1; 07:24:39.617 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 07:24:39.617 INFO GenomicsDBImport - Execu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6487:1046,efficient,efficient,1046,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6487,1,['efficient'],['efficient']
Energy Efficiency,"; at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:122); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:33); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:159); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306); at org.apache.spark.rdd.RDD.iterator(RDD.scala:270); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:89); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Then I check HCC1954.sam, and found . @RG ID:HCC1954 LB:HCC1954 SM:HCC1954. Since it complains no Platform information, I manually add PL:illumina to this line. then step3 works. . My question is do you also manually add PL, platform information to the generated sam file from bwa mem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1949:2526,schedul,scheduler,2526,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1949,3,['schedul'],['scheduler']
Energy Efficiency,"=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --deploy-mode cluster /home/jacky/Exec/gatk/build/libs/gatk-spark.jar MarkDuplicatesSpark -I hdfs://192.168.0.104:9000/user/jacky/NA12878.mapped.illumina.mosaik.CEU.exome.20110411.bam -O hdfs://192.168.0.104:9000/user/jacky/marked_dup.bam -M hdfs://192.168.0.104:9000/user/jacky/marked_dup_metrics.txt --spark-master yarn; 20/10/22 12:02:26 INFO client.RMProxy: Connecting to ResourceManager at /192.168.0.104:8032; 20/10/22 12:02:26 INFO yarn.Client: Requesting a new application from cluster with 2 NodeManagers; 20/10/22 12:02:26 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (1536 MB per container); 20/10/22 12:02:26 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 20/10/22 12:02:26 INFO yarn.Client: Setting up container launch context for our AM; 20/10/22 12:02:26 INFO yarn.Client: Setting up the launch environment for our AM container; 20/10/22 12:02:26 INFO yarn.Client: Preparing resources for our AM container; 20/10/22 12:02:26 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 20/10/22 12:02:29 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f-5056e9b51e43/__spark_libs__7655440475844189559.zip -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/__spark_libs__7655440475844189559.zip; 20/10/22 12:02:31 INFO yarn.Client: Uploading resource file:/home/jacky/Exec/gatk/build/libs/gatk-spark.jar -> hdfs://192.168.0.104:9000/user/jacky/.sparkStaging/application_1603353714322_0004/gatk-spark.jar; 20/10/22 12:02:33 INFO yarn.Client: Uploading resource file:/tmp/spark-28ab5ef4-82d1-425e-879f",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6906:2021,allocate,allocate,2021,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6906,1,['allocate'],['allocate']
Energy Efficiency,"> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074876 . G *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074877 . T *,<NON_REF> . . DP=2 GT:AD:DP:GQ:MIN_DP:PGT:PID:PL:SB ./.:.:0:0:0:.:.:0,0,0,0,0,0 ./.:0,2,0:2:6:.:0|1:65074846_T_C:90,6,0,90,6,90:0,0,2,0 ./.:.:1:0:0:.:.:0,0,0,0,0,0; chr7 65074878 . G <NON_REF> . . END=65074923 GT:DP:GQ:MIN_DP:PL ./.:0:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:1:0:0:0,0,0 ./.:2:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:0:0:0:0,0,0 ./.:1:0:0:0,0,; ```; Where the second genotype column with the `65074846_T_C` tag is for NA12891, which is the sample that has the deletion. I suspect that the GATK engine logic is smart enough to look upstream since those genotyped get annotations. This is admittedly sort of an ambiguous case, but GDB certainly should be able to power through. The stack trace looks like this:; ```; Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code); C [libtiledbgenomicsdb4636568691140868757.dylib+0x1a2cf] BroadCombinedGVCFOperator::handle_deletions(Variant&, VariantQueryConfig const&)+0xa7f; C [libtiledbgenomicsdb4636568691140868757.dylib+0x18f12] BroadCombinedGVCFOperator::operate(Variant&, VariantQueryConfig const&)+0x22; C [libtiledbgenomicsdb4636568691140868757.dylib+0x59d98] VariantQueryProcessor::handle_gvcf_ranges(std::__1::priority_queue<VariantCall*, std::__1::vector<VariantCall*, std::__1::allocator<VariantCall*> >, EndCmpVariantCallStruct>&, VariantQueryConfig const&, Variant&, SingleVariantOperatorBase&, long long&, long long, bool, unsigned long long&, GTProfileStats*) const+0xa8; C [libtiledbgenomicsdb4636568691140868757.dylib+0x5a8e2] VariantQueryProcessor::scan_handle_cell(VariantQueryConfig const&, unsigned int, Variant&, SingleVariantOperatorBase&",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5449:2049,power,power,2049,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5449,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 10; initial apicid	: 10; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 6; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:49835,power,power,49835,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 12; initial apicid	: 12; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 7; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:51008,power,power,51008,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 0; cpu cores	: 14; apicid		: 32; initial apicid	: 32; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 15; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:60402,power,power,60402,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 1; cpu cores	: 14; apicid		: 34; initial apicid	: 34; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 16; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:61576,power,power,61576,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 2; cpu cores	: 14; apicid		: 36; initial apicid	: 36; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 17; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:62750,power,power,62750,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 3; cpu cores	: 14; apicid		: 38; initial apicid	: 38; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 18; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:63924,power,power,63924,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 4; cpu cores	: 14; apicid		: 40; initial apicid	: 40; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 19; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:65098,power,power,65098,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2799.937; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 5; cpu cores	: 14; apicid		: 42; initial apicid	: 42; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 20; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:66272,power,power,66272,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.875; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 6; cpu cores	: 14; apicid		: 44; initial apicid	: 44; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 21; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:67446,power,power,67446,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 9; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 20; initial apicid	: 20; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:53354,power,power,53354,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 48; initial apicid	: 48; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 22; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:68620,power,power,68620,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 50; initial apicid	: 50; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4803.25; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 23; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 1; siblings	: 14; core id		: 10; cpu cores	: 14; apicid		: 52; initial apicid	: 52; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:69794,power,power,69794,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2900.062; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 8; cpu cores	: 14; apicid		: 16; initial apicid	: 16; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts; bogomips	: 4788.74; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. processor	: 8; vendor_id	: GenuineIntel; cpu family	: 6; model		: 79; model name	: Intel(R) Xeon(R) CPU E5-2680 v4 @ 2.40GHz; stepping	: 1; microcode	: 0xb000021; cpu MHz		: 2899.968; cache size	: 17920 KB; physical id	: 0; siblings	: 14; core id		: 9; cpu cores	: 14; apicid		: 18; initial apicid	: 18; fpu		: yes; fpu_exception	: yes; cpuid level	: 20; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 intel_pt tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4158:52181,power,power,52181,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4158,1,['power'],['power']
Energy Efficiency,"@chandrans commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881). User reports BaseRecalibratorSpark in gatk-4.beta.6-17 took 3.79 minutes vs 40 minutes in official release. ----; User Report; ----. Dear GATK_team, I'd like to run Spark-enabled GATK tools on a Spark cluster. Precisely I am launching a Spark cluster in the standalone mode submitting the `BaseRecalibratorSpark` application via Slurm. Before the official release, I was running the `gatk-4.beta.6-17` version, with the following allocated resources, and the following command line for the Spark arguments: `./gatk-launch BaseRecalibratorSpark \ --sparkRunner SPARK --sparkMaster spark://${MASTER} --driver-memory 80g --num-executors 16 --executor-memory 8g`. The speed-up achieved was 3.79 min. However, with the official release GATK-4.0.0.0, with the same datafiles and the same Spark arguments I don't see the same nice speed-up anymore (~ 40 min). Am I missing something with the new version? Or with the invoking command line? Thanks in advance for your time and kind answer. Best, Giuseppe. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/11260/gatk-4-0-0-0-baserecalibratorspark-low-performance/p1. ---. @chandrans commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881#issuecomment-361324925). @droazen @lbergelson Hi David and Louis. Do you have any comments? I was supposed to put this in gatk but put it in dsde-docs. Thanks. ---. @droazen commented on [Mon Jan 29 2018](https://github.com/broadinstitute/dsde-docs/issues/2881#issuecomment-361342262). @chandrans Could you move this ticket to the gatk repo so that we can remember to have a look at the tool? Someone will have to re-profile.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4300:530,allocate,allocated,530,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4300,1,['allocate'],['allocated']
Energy Efficiency,"@cmnbroad This is related to discussion on issue 5439. This is not a final product yet. I'm opening the PR to see how it works on travis and to push discussion here. This PR is not trying to fix all issues with VariantEval. It's trying to address these:. 1) Switch to MultiVariantWalkerGroupedOnStart, primarily to avoid the constant re-querying of variants per site that took place in VariantEvalUtils.bindVariantContexts(). I believe this will substantially reduce the number of instance in which featureContext.getValues() is called. 2) I tried to move, but not full fix, some of the tight linkage between the VariantEval Walker class and the plugin classes. I also made a VariantEvalArgumentCollection to start separating these. Toward this objective, this PR does: a) makes a VariantEvalContext class, which is what gets passed to the VariantStratifier classes, and b) I try to reduce exposing the walker class directly to VariantStratifier and VariantEvaluator. The latter is not completely done, but I think this is moving it in that direction. At several points I stopped for the sake of keeping changes in one PR manageable.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6973:460,reduce,reduce,460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6973,2,['reduce'],['reduce']
Energy Efficiency,"@cwhelan @tedsharpe please review. There are 4 new classes here:. 1. LongHopscotchSet - based on HopscotchCollection/Set but adapted to store primitive longs instead of objects. The most significant bit is used to tell if a bucket is null or not, so the longs being stored must be non-negative. This works for use with k-mers, which we are assume are odd-length up to 31 and thus consume up to 62 bits.; 2. LargeLongHopscotchSet - for sets of longs greater than ~2 billion (the max Java array size) using a List of LongHopscotchSets.; 3. LongBloomFilter - Bloom filter for long's; 4. LargeLongBloomFilter - Bloom filter when the filter index size exceeds 2GB using a List of LongBloomFilters. - LongIterator and QueryableLongSet interfaces for convenience.; - Minor change to HopscotchSet max legal size, which was higher than the actual allowed Java array size. PS I just had a thought that the Bloom filters could use long instead of byte buckets to expand the max index size 8-fold. Could maybe be done for the Hopscotch sets as well, but with considerably more difficulty. Thoughts? On the other hand, the performance is already adequate so perhaps I'll save this idea for later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2729:125,adapt,adapted,125,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2729,1,['adapt'],['adapted']
Energy Efficiency,@cwhelan mentioned at @jamesemery's presentation last week that there are more efficient options available for getting data into HDFS. It's worth exploring these.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2014:79,efficient,efficient,79,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2014,1,['efficient'],['efficient']
Energy Efficiency,"@davidbenjamin commented on [Mon Jan 30 2017](https://github.com/broadinstitute/gatk-protected/issues/886). My original infinite HMM joint segmentation was nifty on paper but is 1) horribly slow and 2) falls into spurious local minima. One way to improve upon both of these is to learn an initial set of hidden states via Chinese Restaurant Process clustering of the raw allelic count and coverage data, without regard to segmentation and the HMM structure. Because this doesn't constrain neighboring sites to (usually) have the same state, it will yield a liberal set of initial states, which is fine because they can always be pruned. Probably, we can run a single pass of the HMM to prune most states. This will reduce the number of iterations enormously and obviate expensive max-likelihoods learning learning of the hidden state values.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2936:715,reduce,reduce,715,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2936,1,['reduce'],['reduce']
Energy Efficiency,"@droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977). The docker tests take about 40 minutes in travis, while the next-slowest travis task takes about 30 minutes. We should try to reduce the runtime of the docker tests to <= 30 minutes. ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382). For @LeeTL1220 . ---. @LeeTL1220 commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032644). I thought we were going to address this with a GATK base image... what is; target time? (Within reason). On Apr 5, 2017 20:08, ""droazen"" <notifications@github.com> wrote:. For @LeeTL1220 <https://github.com/LeeTL1220>. —; You are receiving this because you were mentioned. Reply to this email directly, view it on GitHub; <https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292032382>,; or mute the thread; <https://github.com/notifications/unsubscribe-auth/ACDXk_R_mkaEcEJlOt3lJntscqeum3-lks5rtC0SgaJpZM4M09tE>; . ---. @droazen commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/977#issuecomment-292033277). That is tracked at https://github.com/broadinstitute/gatk/issues/2457 and slated for beta (mid-May). This ticket in protected can be considered as blocked until https://github.com/broadinstitute/gatk/issues/2457 is done.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2965:228,reduce,reduce,228,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2965,1,['reduce'],['reduce']
Energy Efficiency,"@ilyasoifer I noticed this method wasn't doing anything, it looks like it should have a return here. . Also, I made the logger static because it's probably expensive to allocate one for every read and you don't need to.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8576:169,allocate,allocate,169,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8576,1,['allocate'],['allocate']
Energy Efficiency,"@jamesemery This fixes the case you found, hopefully bringing us closer to turning on linked de Bruijn graphs. I will start testing M2. If you test in HC, continue to keep in mind that adaptive pruning is not default. This change will be most important for rare complex graphs and in combination with junction trees but I did see modest improvements to indel sensitivity even with the old assembly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6520:185,adapt,adaptive,185,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6520,1,['adapt'],['adaptive']
Energy Efficiency,@lbergelson commented on [Fri Sep 09 2016](https://github.com/broadinstitute/gatk-protected/issues/698). `CountSet` is a small class that is only used in 1 place. Investigate if it can be replaced with a set from one of our many collection libraries. It needs to implement an efficient max and min operation.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2890:276,efficient,efficient,276,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2890,1,['efficient'],['efficient']
Energy Efficiency,"@mbabadi commented on [Fri May 19 2017](https://github.com/broadinstitute/gatk-protected/issues/1069). This is a long shot, but the idea is to be able to learn biases from mixed N/T cohorts. In a way, this is similar to semisupervised learning where the _stiff_ integer-state HMM on normal samples lead the way of learning biases (as a matter of imposing a strong copy-neutrality prior), and tumor samples along with a _loose_ infinite HMM provide additional (though generalically less) statistical power. Weak tumor-in-normal contamination can be handled using an adaptive integer-state HMM where the quantizied copy ratio states are chosen uniformly, though, adaptively. In the future, we must move toward a generic CLI tool called something like FancySchmancyCNVCaller that can perform the following tasks in its idealized form:. - create PoN and make calls from normals; - create PoN and make calls from tumors (possible with iHMM); - create PoN and make calls from mixed normals and tumors (possible with iHMM); - make calls from a given model on normals; - make calls from a given model on tumors; - make calls from a given model on mixed normals and tumors. The tool would then additionally take a sample annotation table (normal, tumor) and perform its job. For the first release, all samples have be annotated as normal; otherwise, an UnsupportedFeatureException is thrown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3004:499,power,power,499,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3004,3,"['adapt', 'power']","['adaptive', 'adaptively', 'power']"
Energy Efficiency,@mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/1059). We have to learn the upcoming Nd4j _workspaces_ and use it to reduce the memory footprint of gCNV. It is already merged but the latest Nd4j release (0.8.0) doesn't have it yet. API:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/conf/WorkspaceConfiguration.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspaceManager.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-api-parent/nd4j-api/src/main/java/org/nd4j/linalg/memory/abstracts/Nd4jWorkspace.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-buffer/src/main/java/org/nd4j/linalg/api/memory/MemoryWorkspace.java. Tests:; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/BasicWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/EndlessWorkspaceTests.java; https://github.com/deeplearning4j/nd4j/blob/master/nd4j-backends/nd4j-tests/src/test/java/org/nd4j/linalg/workspace/SpecialWorkspaceTests.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2996:165,reduce,reduce,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2996,1,['reduce'],['reduce']
Energy Efficiency,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/746). Here are some possible culprits:. 1) Low base call quality; 2) Low mapping quality; 3) Significantly different noise covariates (i.e. sample coming from a different platform, or a bizarre library preparation/sequencing issue resulting in a different bias modality). (1) and (2) can be investigated by taking a few good and bad BAMs and studying the distribution of their base/mapping qualities. (3) can be studied in various ways. One approach is as follows:. Make PoNs with different numbers of principal components D and plot the total denoising power (TDP), defined as ||W z_s||^2, for each sample as a function D. For ""good"" samples, we expect a steady increase of TDP vs. D. For ""bad"" samples, we expect a lag; why? for small D, most benefit comes from choosing principal components that describe the bias covariates of ""good"" samples (majority). Eventually, they will be depleted and further data likelihood gains will require dedicating the next principal components to ""bad"" samples (minority). Another (complementary) approach is as follows:. Fix D to a reasonable value such that a differentiation between good and bad samples is still noticeable (remember: if D = #samples, no signal is left after denoising, so D should be still). Next, obtain the empirical distribution of z_s^\mu, i.e. the projection of each sample s over each principal component \mu. If bad samples do not regress, it shows the incompatibility of their bias covariates with the rest of the samples. ---. @mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/746#issuecomment-254800421). @davidbenjamin @samuelklee anything to add?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2906:650,power,power,650,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2906,1,['power'],['power']
Energy Efficiency,"@mbabadi commented on [Wed Oct 19 2016](https://github.com/broadinstitute/gatk-protected/issues/747). We use Genome STRiP TCGA/GPC2 call sets as ground truth. It is desirable to evaluate:. (1) XHMM and CODEX vs. GATK (almost done @asmirnov239); (2) GATK ROC curves as a function of bias latent space dimension D; (3) GATK ROC curves for a fixed D, and for different unexplained variance models: (isotropic, target-resolved, adaptive), w/ and wo/ sample-specific unexplained variance calling during PoN creations, and calling. That is, 3 x 2 x 2 = 12 cases. ---. @mbabadi commented on [Thu May 18 2017](https://github.com/broadinstitute/gatk-protected/issues/747#issuecomment-302465336). This was done a while ago. Keeping open for upcoming evaluations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2907:424,adapt,adaptive,424,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2907,1,['adapt'],['adaptive']
Energy Efficiency,"@samuelklee commented on [Wed Apr 05 2017](https://github.com/broadinstitute/gatk-protected/issues/975). Should be an equivalent of PadTargets for WGS that outputs a file specifying the bins. Alternatively, the WES coverage collection CLI should calculate padded targets on the fly. This will simplify the WDL and reduce the number of tasks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2964:314,reduce,reduce,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2964,1,['reduce'],['reduce']
Energy Efficiency,"@sooheelee has some serious concerns about `ReadClipper.hardClipAdaptorSequence()`, which is called in `Mutect2` and `HaplotypeCaller` via `AssemblyBasedCallerUtils.finalizeRegion()`. She thinks that the method being used to find the adaptor boundary for clipping purposes is completely bogus!. This is some pretty old code that was also in the GATK3 versions of these tools, so if it's crazy, then we can at least take ""comfort"" in the fact that it's been like this for a very long time...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3184:234,adapt,adaptor,234,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3184,1,['adapt'],['adaptor']
Energy Efficiency,"@takutosato @LeeTL1220 As mentioned, this change scraps all the p values and replaces it with a simple and cheap probabilistic model. All our validations either improve or stay the same and speed is much better. * Spurious active regions are reduced by almost 50%.; * DREAM 4 goes from 40 hours total CPU time to 20 hours. All DREAM genomes now take less than a day.; * Hapmap sensitivity is the same.; * DREAM sensitivities for SNVs and indels all go up a bit.; * Upon manual review we no longer make any obviously bad inactive calls, except for very long deletions, which remain an issue. @takutosato This is a higher priority review than either of the documentation PRs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3304:242,reduce,reduced,242,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3304,1,['reduce'],['reduced']
Energy Efficiency,"@takutosato Here's another little one. This didn't affect sensitivity in Hapmap or DREAM and in DREAM it reduced indel false positives by about half. Not a bad short-term improvement for MC3, although deep learning will handle it better.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4845:105,reduce,reduced,105,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4845,1,['reduce'],['reduced']
Energy Efficiency,"@takutosato Not a huge difference, but makes output a bit more meaningful in some cases and reduces false positives by a bit.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6288:92,reduce,reduces,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6288,1,['reduce'],['reduces']
Energy Efficiency,"@takutosato The extra strength of normal reads informing the ref allele's annotations improves results (very) slightly in all of our validations. The deeper reason for this change is in anticipation of multi-sample mode, where filtering based on a single INFO field will be simpler and probably statistically more powerful than filtering on a bunch of separate genotype fields.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5518:314,power,powerful,314,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5518,1,['power'],['powerful']
Energy Efficiency,"@tedsharpe please review. - SVKmerizer takes in an integer specifying the spacing between between kmers. This is is an effective way to reduce the kmer set size without affecting sensitivity much.; - SVKmerShort - added masking function that returns a copy of the current kmer after deleting bases at the specified positions; - Reworded some error messages about kmer length; - Moved and added some hashing functions to SVUtils, which will be used in another PR for the long-typed set classes and de-duplication filter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2662:136,reduce,reduce,136,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2662,1,['reduce'],['reduce']
Energy Efficiency,"@vdauwera commented on [Mon Nov 14 2016](https://github.com/broadinstitute/gatk-protected/issues/772). @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064). The PCR error model applied pre-pairHMM does not seem to always do the right thing. . This is explained in class TandemRepeatFinder JavaDoc (soon to be merged in). Moreover the code responsible to detect STR repeats seems rather inefficient doing multiple passes on the read bases for each position on the read when it seems that it must be possible to accomplish the same just doing at most one pass per possible STR length. This task is to fix the PCR artifact modeling issues evaluating whether there is at least no a drop in calling accuracy all. Also try to make the code more efficient. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431158). @yfarjoun and I just added a Palantir issue for this this morning -- should the analysis wait until you're done updating the code?. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123431971). Just waiting for test to pass...; So you knew about this issue already?. ---. @ldgauthier commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123432816). We were talking about it because the PCR-free option doesn't get used in production (on PCR-free data) and we didn't know how much difference it actually makes. ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481297). Merged. ; I think that you can go ahead with the analysis and I would borrow your set up to see if the eventual code update improves things for PCR-plus. . ---. @vruano commented on [Tue Jul 21 2015](https://github.com/broadinstitute/gsa-unstable/issues/1064#issuecomment-123481614). Sorry for the confusion, that merge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2915:785,efficient,efficient,785,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2915,1,['efficient'],['efficient']
Energy Efficiency,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/946). @vruano commented on [Sat Dec 24 2016](https://github.com/broadinstitute/gsa-unstable/issues/1537). ### Tool(s) involved; GenotypeGVCFs CombineGVCFs. ### Description. With large ploidy an exception may be thrown in GenotypeGVCFs/CombineGVCFs when the number of alleles is rather large (after combining several VCFs). . We already have a safe-guard mechanism that works well with diploids. When there is 50+ alt. alleles we stop emitting PLs. . There is already a branch that contains a solution for high ploidy. . https://github.com/broadinstitute/gsa-unstable/tree/vrr_max_alt_alleles_generate_pls. This consist in exposing this maximum constant as a user argument so that users can lower this maximum threshold as they need to avoid an exception. This task is about make this standard in master. Notice however that this is still just a hack; a better solution would reduce the list of alt. alleles as needed to handle it as needed. . ---. @vdauwera commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288229143). @vruano Do you plan to pursue this in gsa-unstable or can this be migrated to GATK4?. ---. @vruano commented on [Tue Mar 21 2017](https://github.com/broadinstitute/gsa-unstable/issues/1537#issuecomment-288281209). No plans, just move it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2956:972,reduce,reduce,972,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2956,1,['reduce'],['reduce']
Energy Efficiency,"@vdauwera commented on [Wed Mar 22 2017](https://github.com/broadinstitute/gatk-protected/issues/950). @vruano commented on [Wed May 18 2016](https://github.com/broadinstitute/gsa-unstable/issues/1376). ## Bug Report; ### Affected tool(s). All tools that use AFCalculators (HC, GenotypeGVCFs etc).; ### Affected version(s); - master.; ### Description. When a alternative allele number reduction is needed (e.g. when the number of alt.alleles is larger than --maxAltAlleles). ExactAFCalculator descendants reduce the number of alt alleles using the reduceScope method. Different implementation of the exact-af-calculator may differ slightly on this but for the most part all of them apply the following algorithm:. For each sample, get its PLs, get the best genotype based on those. Then for the alleles included in that genotype increase their ""best allele score"" by the GQ of the genotype. Then we chose those alleles that have the highest scores. I guess often this is ok when we are dealing with many samples and the ""good"" alleles are present in the top genotypes with high confidence in a few samples and the ""bad"" alleles are not. However one can see how this criterion fails when either we are working with just a few samples (e.g. 1 sample in HC GVCF mode) or with low coverage data. . For example with a single sample only the alleles in the best genotype may have a score different than 0. All the rest have the same probability of been picked up if maxAltAlleles gives us room for more. Despite that the likelihoods of other genotypes may indicate which ones are a better choice amongst the ""loosers"" we throw that info away.; ### Proposed solution. Simply do a quick and dirty AF estimation and choose the alleles with the larger frequencies. This estimate should use all the genotype likelihoods rather than just the top genotype giving a nominal score for all the alleles that would allow us to sort them all and make a better and less arbitrary selection. ---. @vdauwera commented on [M",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2958:505,reduce,reduce,505,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2958,2,['reduce'],"['reduce', 'reduceScope']"
Energy Efficiency,A basic progress meter for walker-based tools,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1037:17,meter,meter,17,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1037,1,['meter'],['meter']
Energy Efficiency,"A collection of changes in non-GVS packages required to build a working version of GVS against master:. 1. Support for an optional monitoring script for VQSR Lite `JointVcfFiltering.wdl`.; 2. `VQS_SENS_FAILURE_PREFIX ` VCF header value updated for correctness.; 3. Moved all BigQuery classes under a `gvs` package to make clear these are currently considered to be GVS specific.; 4. Added method to BigQueryUtils.; 5. ~ExcessHet calculation fixes for the case of no PLs.~ Removed, no longer required with Annotation changes in `ExtractTool`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8362:131,monitor,monitoring,131,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8362,1,['monitor'],['monitoring']
Energy Efficiency,"A matrix table is a dense matrix. GVS's refs and vets tables are sparse matrices, specifically using the [coordinate-list; representation](https://en.wikipedia.org/wiki/Sparse_matrix#Coordinate_list_(COO)). For example, the following coordinate-list representation:. ```; row, col, value; 0, 2, 42.0; 0, 3, 48.0; 0, 5, 43.0; 1, 0, 41.0; 1, 1, 45.0; 1, 3, 43.0; ```. corresponds to this dense representation; ```; NA NA 42.0 48.0 NA 43.0; 41.0 45.0 NA 43.0 NA NA; ```. I must read the entire coordinate-list representation to conclude this matrix has five columns. Moreover, in vets and refs, instead of column indices, we have unique column identifiers [1], so we even lack the index of each column. In a single-threaded, single-machine world, we could assign identifiers to indices as we scanned the refs/vets from top to bottom. However, when processing the refs and vets in parallel, we must know the identifier-to-index mapping before execution. The current implementation reads the entire refs Avro to discover the present sample identifiers. The new implementation uses a new `sample_info` Avro file which contains one record per sample. This reduces the complexity of assigning samples to indices from $O(N_{SAMPLES} N_{REFS})$ to $O(N_{SAMPLES})$. In practice, this is a substantial reduction because the genomic axis is much larger than 4000, the typical group size. [1] In practice, currently, the column identifiers are a dense interval of the global column; indices. Concretely: sample group one contains only the samples 0 through 3999. Sample group two; contains only the samples 4000 through 7999. We do not use this fact in import_gvs.py.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8676:1149,reduce,reduces,1149,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8676,1,['reduce'],['reduces']
Energy Efficiency,"A user building GATK on a POWER system tried to set `GATK_SKIP_NATIVE_BUILD=true`, but it didn't prevent the build from failing. ```; FAILURE: Build failed with an exception. * What went wrong:; A problem occurred configuring root project 'gatk'.; > Exception thrown while executing model rule: NativeComponentModelPlugin.Rules#createBinaries; > Invalid NativePlatform: linux_ppc64. BUILD FAILED; ```. the temporary workaround was the following change:. ```; $ diff build.gradle.org build.gradle; 406c406,408; < VectorLoglessPairHMM(NativeLibrarySpec) {. ---; > if(System.properties[""os.arch""] != ""ppc64""); > {; > VectorLoglessPairHMM(NativeLibrarySpec) {; 458a461; > }; ```. This is a bug and is likely to cause failures on other systems as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1711:26,POWER,POWER,26,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711,1,['POWER'],['POWER']
Energy Efficiency,"AAGAACACATAGATGCATTTGGAAGCCAGTGTGGACGCCATGTGATCTGTGCCCACATATCACATGGCCGCTTTGGGATAGGGCCTGTCTGCCCATACTGGCTTCCAAACGCCTCTGTGTGTTCCTGTATGTGGGTGTGCACGTACCTGTCACATGTGTATGCACAGACCACAGGATGTCCACACTGGCTTCCAAATGCGTCTCTGTGTTCCTGTCTGTGAGTTCCAAATGTGTGCACACCTACAGACAGGAACATGGAAACACATTTGGAAGCCAGTGTGGACACCCTGTGATCTGTGCGTACACATGTGACACGTGCATGCACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGACGCCCTGTGATCTGTGCCCACACACATCACACGTGCATACACACCCACAGACAGGAACACAGAGACACATTTGGAAGCCAGTGTGGATGCCCTGTGATCTGTGTGTACACGTGACACGTGCGTACACACCCACATACAGGAACACAGCCACATTTGGAAGCCAGTGCAGACGCCCTGTGATCTGTGTGTACACATGTGACACGTGCGTGCACACTCACAGACAGGAACACAGAGACGCATTTGGAAGCCAGTGTGGACATCCTGTGGTCTGCGCGTACACATGTGACAGGTACGTGCACGCCCACATACAGGAACACACAGAGGCCTTTGGAAGCCAGCATGGGCAGACAGGCCCTATCCCAAAGCGGCC;SVLEN=1454;SVTYPE=CPX;TOTAL_MAPPINGS=1; ```. So the strategy taken in this branch is; * for the first two cases, re-interpretation is easy and done in this ""post-processing"" tool, and bare-bone annotated simple variants are given , annotated with `EVENT` that links the simple variants back to the complex variant; * for the last case, ; * re-collect the contigs that induced the CPX call, preprocess its alignment, then send the contig to the current pair-iteration algorithm for re-interpretation, the returned simple variants will be checked for consistency with the CPX variant that was induced by the same contig, and dropped if it is inconsistent (the two types of variants `<DEL>` and `<INV>`, are main concerns as they could easily stem from mis-interpretations of small dispersed duplications); then, ; * the CPX variants who have rejected re-interpreted simple variants will be analyzed one last time, to extract `<DEL>` and `<INV>`; ; * these variants will also be annotated with `EVENT` to link back to the CPX variants. Based on manual review, this salvages ~600 variants that would be dropped by evaluation scripts that would simply ignore the CPX variants. ---; Tests will be added if this strategy is given the green light (so no merging yet).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4602:4855,green,green,4855,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4602,1,['green'],['green']
Energy Efficiency,AGScheduler.abortStage(DAGScheduler.scala:2607) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2281) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.internal.io.SparkHadoopWriter$.write(Spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:14506,schedul,scheduler,14506,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,"AL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on com2:38568 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:26 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on com2:38568 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4180 ms on com2 (executor 1) (1/1); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at SparkUtils.java:157) finished in 8.951 s; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: looking for newly runnable stages; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: running: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: failed: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244), which has no missing parents; 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 86.1 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 32.3 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.131.101.159:34044 (size: 32.3 KB, free: 529.9 MB); 17/10/11 14:19:27 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1004; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244) (first 15 tasks are for partitions Vector(0)); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Adding ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:14072,schedul,scheduler,14072,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,"Adapt PGEN extract to work with Cromwell's ""Retry with more memory"" feature to address issues with a small percentage of ""problem"" shards OOMing. Successful run [here](https://app.terra.bio/#workspaces/gvs-dev/GVS%20Integration/job_history/dd29b0d9-73e5-4e1b-af83-e4fba53c0c65).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8754:0,Adapt,Adapt,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8754,1,['Adapt'],['Adapt']
Energy Efficiency,Adapt for Avro 1.11 behavior of throwing on get()s of non-existent fields [VS-860],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8266:0,Adapt,Adapt,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8266,1,['Adapt'],['Adapt']
Energy Efficiency,Adaptive assembly graph pruning,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4867:0,Adapt,Adaptive,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4867,1,['Adapt'],['Adaptive']
Energy Efficiency,Adaptive pruning option for local assembly,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5473:0,Adapt,Adaptive,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5473,1,['Adapt'],['Adaptive']
Energy Efficiency,Add dependency submission workflow so we can monitor vulnerabilities,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/9002:45,monitor,monitor,45,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/9002,1,['monitor'],['monitor']
Energy Efficiency,Add engine level argument to selectively ignore soft-clipped bases due to adapter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6346:74,adapt,adapter,74,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6346,1,['adapt'],['adapter']
Energy Efficiency,Add monitoring to index vcf,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8151:4,monitor,monitoring,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8151,1,['monitor'],['monitoring']
Energy Efficiency,Add progress meter to CreateSomaticPanelOfNormals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5629:13,meter,meter,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629,1,['meter'],['meter']
Energy Efficiency,"Adding a new method `getVariantCacheLookAheadBases` to `VariantWalkerBase` which allows subclasses to set how far to look ahead when caching variants. This may help reduce memory use in GenotypeGVCFs. This also changes the side inputs to use FeatureDataSource.DEFAULT_QUERY_LOOKAHEAD_BASES which is `1000`, this is the value used by the other tools. I'm not sure if that's the right thing to do, but it makes variant walkers more consistent with other tools. Alternatively we could add a separate configuration method that lets tools change the side input value. We could also expose an optional parameter in the feature input that lets you set that on a per input basis if we need it. . This doesn't seem to have any negative effect on performance for genotypegvcfs, but it's hard to tell from short runs. It's also hard to tell if it's improving memory usage. It doesn't seem to make an appreciable difference at random places in the genome, but I'm hoping it will make a difference in very bad locations that have a lot of variation. Ideally our caches would be based on size rather than number of variants, but that's a more complicated change. fixes #3471",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3480:165,reduce,reduce,165,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3480,1,['reduce'],['reduce']
Energy Efficiency,"Addresses #4397 and #5054. Restructured gCNV WDLs to pass data more efficiently to postprocessing tasks, and added a wrapper workflow for gCNV case WDL that scatters samples in multiple blocks. Also cleaned up some of the unused cromwell travis tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5176:68,efficient,efficiently,68,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5176,1,['efficient'],['efficiently']
Energy Efficiency,"Addresses [219](https://github.com/broadinstitute/dsp-spec-ops/issues/219). Major changes. - calculate site level metrics in `feature_extract.sql`; - extract metrics, apply thresholds, and set filter field in ExtractFeature; - CreateSiteFilteringFiles to translate from input VCF with filter fields into format for BQ loading, especially `location` fields; - update WDL to call CreateSiteFilteringFiles and upload results to BQ. Minor changes; - added call_GQ to alt_allele creation; - reduced memory requirements in WDL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7197:486,reduce,reduced,486,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7197,1,['reduce'],['reduced']
Energy Efficiency,Adds additional filtering steps to the PathSeq filter to 1) trim adapter sequences and 2) mimic a simple filter used in RepeatMasker that masks windows with excessive A/T or G/C content.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3354:65,adapt,adapter,65,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3354,1,['adapt'],['adapter']
Energy Efficiency,"All implementations of `GATKRead` should ideally agree on String representation -- the adapter classes should all implement `toString()`, and do so consistently.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/624:87,adapt,adapter,87,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/624,1,['adapt'],['adapter']
Energy Efficiency,Also reduced the overall number of tests to execute by eliminating non-docker unit and integration tests from the travis.yml. I would suggest checking that the docker tests in this travis execution actually uploaded to where it says they do on gcloud.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3353:5,reduce,reduced,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3353,1,['reduce'],['reduced']
Energy Efficiency,"Apologies for re-opening, this is becoming an increasing issue for those looking to run GATK via Docker or singularity in a multi-tenant environment. Currently:; Docker creation and images provided run with a default user root within the container. Dropping privileges within the instance to a gatk user, would reduce the risk of inadvertent data access or harm when run in a multi-user environment. A possible solution:; Add something like the following within the Dockerfile:; RUN useradd -ms /bin/bash dev; WORKDIR /home/dev; USER dev. Providing:; Making changes like the above would bring the GATK docker container into line with best practice and greatly assist sites which are also looking to apply minimum standards enforcable through 3rd party applications, i.e. Aqua etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5959:311,reduce,reduce,311,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5959,1,['reduce'],['reduce']
Energy Efficiency,ArrayListSpliterator.tryAdvance(ArrayList.java:1351); at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:149); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); at org.apache.spark.scheduler.Task.run(Task.scala:99); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoint,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:2908,schedul,scheduler,2908,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,3,['schedul'],['scheduler']
Energy Efficiency,"As described in #1464, I ported a `LocusWalker` class as the GATK3 one using `LocusIteratorByState` (LIBS). The default implementation uses no downsampling (probably it should be change once #64 is addressed), includes reads with deletions and does not track the previous reads in the LIBS. One important think is that the `intervalsForTraversal` is not used at all, so it is up to the author discard regions out of this ones. I was thinking to check every position for overlap in any of the interval in the list, but I'm not sure if the `intervalsForTraversal` is sorted or not; and an exhaustive checking could reduce performance. I don't know if it could be possible to implement some query in LIBS, to return only the positions that overlaps some intervals, but that will be easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1526:613,reduce,reduce,613,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1526,1,['reduce'],['reduce']
Energy Efficiency,"As part of #8083 we are drastically rewriting the entire Pileup-Caller infrastructure for DRAGEN-GATK. In doing so we have largely neglected its original functionality in Mutect2 and some of the changes (namely the re-factoring of that code to now happen after trimming like in with the GGA code) are going to impact the overall results for pileupcalling. It seems that we never added a real test of this functionality and its unclear to me currently what the meterics are that we want to assure ourselves that its working as intended. In #8083 I have checked that the code is hooked up manually, but its not clear to me what a proper test looks like for mutect without re-hashing the test samples that were being used in the bacterial project. I'm a little skeptical about adding a test that just asserts ""these results were different somehow"" and yet thats essentially the sort of test i would like and that would have saved me here. I would really like to have something better in place, especially if we are going to keep sharing the pileup-calling code between HC and M2 going forward.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8242:460,meter,meterics,460,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8242,1,['meter'],['meterics']
Energy Efficiency,"As part of my work in the Pipeline Dev team, I created 2 GATK images to address issue discussed [here](https://github.com/broadinstitute/gatk/issues/8684) (ie. having too many docker layers, we hit ACR limits very quickly). The images are in terrapublic, a premium-tier ACR and is publicly accessible. I made two images, one is squashed to just 1 layer, the other is reduced to just 12 layers (from the original 45). With these changes and the fact that terrapublic is on [premium](https://learn.microsoft.com/en-us/azure/container-registry/container-registry-skus#registry-throughput-and-throttling) tier, the maximum docker pulls per minute becomes 833 (ie. 10k readOps / 12 layers) for the reduced-layers image and 10,000 for the squashed one. We have yet to test these in our pipelines but I anticipate the squashed version to be slower since it won’t be able to take advantage of any parallel pulls or caching, hence the two versions to allow pipeline devs to decide which one is better for their use-case.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:367,reduce,reduced,367,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,2,['reduce'],"['reduced', 'reduced-layers']"
Energy Efficiency,Automatically schedule temporary resource files for delete on exit.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4616:14,schedul,schedule,14,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4616,1,['schedul'],['schedule']
Energy Efficiency,BQSR optimization - reduce object churn by not creating as many Cigar objects and less bases/quals,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/1477:20,reduce,reduce,20,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/1477,1,['reduce'],['reduce']
Energy Efficiency,"Base qualities of two (`#`) are handled specially by BWA and our tools and are typically used to indicate adapter sequence. See reply to jhess in <https://gatkforums.broadinstitute.org/gatk/discussion/comment/35120#Comment_35120>:. > That's correct, Q2 bases are considered to be special and left untouched by BQSR. Currently, there is no easy way to convert base qualities to two. The only instances I am aware of is (i) for SamToFastq, which then unaligns the reads and (ii) MergeBamAlignment, which isn't necessarily a part of everyone's workflow. Also, MergeBamAlignment's `CLIP_ADAPTERS` softclips XT tagged sequence, which then becomes fair game for our assembly-based callers. MarkIlluminaAdapters uses aligned reads to mark those with 3' adapter sequence with the XT tag. The XT tag values note the start of the 3' adapter sequence in the read. During MergeBamAlignment, one must especially request that this XT tag is retained in the merged output. Because our assembly-based callers throw out CIGAR strings from the aligner when reassembling reads, so as to use soft-clipped sequence that may contain true variants we wish to resolve, adapter sequence can be incorporated into the graph. This is not an issue for libraries with low levels of adapter read through and for germline calling as we prune nodes in the graph that have less than two reads supporting it. . However, for somatic cases and for libraries where there is considerable adapter read through, the current solution is to hard-clip adapter sequences out of reads or to toss these reads altogether so as not to increase the extent of spurious calls. The issue with hard-clipping is that our reads become malformed due to a mismatch in CIGAR string and sequence length. These the GATK engine filters. So the solution is to either correct the CIGAR strings or to go back and re-align the clipped reads or again to toss the reads. It would be great not to have to throw out reads that include some adapter sequence in somatic wor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3540:106,adapt,adapter,106,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3540,3,['adapt'],['adapter']
Energy Efficiency,"Based on my HaplotypeCaller GVCF performance evaluation, we are spending a significant amount of time in `ReferenceConfidenceModel.calcNIndelInformativeReads()', including upwards of 40% of the overall runtime on a bam under some conditions. To this end we have already done some performance work optimizing its constituent methods (#5469, #5470). Even with those changes it appears that the method can take upwards of 25% of the total runtime, which appears to be a consequence of the nature of the algorithm. It appears that the core of the problem appears to be associated with the calls we make to `isReadInformativeAboutIndelsOfSize()` which has a complexity overall of approximately `O(pileupsPerRegion * readsPerPileup * basesPerRead * maxIndelSize)` which ends up being a large number. One approach to fixing this problem be to rethink the repetitive operations we do for every pileup and instead do it on a per-read basis, and furthermore we could exploit the nature of the existing algorithm to avoid checking mismatches at the front of the read when we know that down the line we will fail out because of mismatches at the end of the read. Furthermore there is the broader philisophical question of whether there are changes that could be made to the algorithm that might carry a bigger risk of changing the results, like applying some heuristic based on the complexity of the reference sequence at a given site to reduce the amount of work we have to do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5488:1426,reduce,reduce,1426,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5488,1,['reduce'],['reduce']
Energy Efficiency,"Based on timing results (see below), we should do two things:; 	1) Switch to the HTSJDK `ParsingUtils::split` method for all; 	 cases where we split a string using a single character.; 	2) Switch to the `Utils::split` method for all cases where; 	 we split a string by another string of length > 1. A quick stopgap that will reduce splitting time by ~1/2 is to just; replace all calls to Java's `String::split` with `Utils::split` (since; they both take two Strings as arguments, it should be very easy). Also, I have disabled the test so that it doesn't slow down testing cycles. Fixes #3759. | Method | Benchmark | Total Time (ns) | Total Time (ms) | Time Per Split Operation (ns) | Time Per Split Operation (ms) | ; | --- | --- | --- | --- | --- | --- |; | Java String::split | Split on Words | 131867865203 | 131867.865203 | 6048.98464233945 | 0.00604898464233945 |; | Java String::split | Split on Chars | 12917243085 | 12917.243085 | 3004.010019767442 | 0.003004010019767442 |; | HTSJDK ParsingUtils::split | Split on Words | N/A | N/A | N/A | N/A | ; | HTSJDK ParsingUtils::split | Split on Chars | 5882790859 | 5882.790859 | 1368.0908974418605 | 0.0013680908974418606 | ; | GATK Utils::split | Split on Words | 38734463275 | 38734.463275 | 1776.8102419724771 | 0.0017768102419724772 |; | GATK Utils::split | Split on Chars | 7120052467 | 7120.052467 | 1655.826155116279 | 0.0016558261551162792 |",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3776:325,reduce,reduce,325,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3776,1,['reduce'],['reduce']
Energy Efficiency,BlockDataMode(ObjectInputStream.java:2722); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1565); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2227); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2151); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2009); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1533); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:420); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:298); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3050:6023,schedul,scheduler,6023,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3050,1,['schedul'],['scheduler']
Energy Efficiency,BlockDataMode(ObjectInputStream.java:2740); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1567); at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2245); at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2169); at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2027); at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1535); at java.io.ObjectInputStream.readObject(ObjectInputStream.java:422); at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75); at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:114); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:309); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4694:18822,schedul,scheduler,18822,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4694,1,['schedul'],['scheduler']
Energy Efficiency,Breakpoints(NovelAdjacencyReferenceLocations.java:78); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:293); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:42); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.DiscoverVariantsFromContigAlignmentsSAMSpark.lambda$discoverNovelAdjacencyFromChimericAlignments$7(DiscoverVariantsFromContigAlignmentsSAMSpark.java:409); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.ArrayList$ArrayListSpliterator.tryAdvance(ArrayList.java:1351); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.lambda$initPartialTraversalState$0(StreamSpliterators.java:294); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.fillBuffer(StreamSpliterators.java:206); 	at java.util.stream.StreamSpliterators$AbstractWrappingSpliterator.doAdvance(StreamSpliterators.java:161); 	at java.util.stream.StreamSpliterators$WrappingSpliterator.tryAdvance(StreamSpliterators.java:300); 	at java.util.Spliterators$1Adapter.hasNext(Spliterators.java:681); 	at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:147); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3874:2099,schedul,scheduler,2099,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3874,3,['schedul'],['scheduler']
Energy Efficiency,Bug report recieved via email: . I am testing GATK4 walker version. I tried the following command. ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list -jdk_inflater=true**. It will fail with the following error message `htsjdk.samtools.SAMFormatException: Invalid GZIP header`. If I run ; ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-L Broad.human.exome.b37.interval_list**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf **-jdk_inflater=true**. or . ./gatk-launch BaseRecalibrator -R human_g1k_v37_decoy.fasta -I NA12878.md.bam -O NA12878.br.table --knownSites 1000G_phase1.indels.b37.vcf. all three will run with no problem. However when combined **-L and -jdk_inflater=true** it will fail with error. This problem is on both Intel system and Power system. It feels like an easy fix. Would you please look into that? Thanks!,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2722:1067,Power,Power,1067,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2722,1,['Power'],['Power']
Energy Efficiency,Caused by: java.lang.IllegalArgumentException: Invalid interval. Contig:chrUn_JTFH01000312v1_decoy start:0 end:0; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:687); at org.broadinstitute.hellbender.utils.SimpleInterval.validatePositions(SimpleInterval.java:61); at org.broadinstitute.hellbender.utils.SimpleInterval.<init>(SimpleInterval.java:37); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations$BreakpointsInference.getLeftJustifiedBreakpoints(NovelAdjacencyReferenceLocations.java:86); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.leftJustifyBreakpoints(NovelAdjacencyReferenceLocations.java:301); at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyReferenceLocations.<init>(NovelAdjacencyReferenceLocations.java:46); ... 18 more. Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4458:7882,schedul,scheduler,7882,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4458,1,['schedul'],['scheduler']
Energy Efficiency,Changing defaults for mitochondria mode now that we have adaptive pruning,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5544:57,adapt,adaptive,57,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5544,1,['adapt'],['adaptive']
Energy Efficiency,Changing the cron trigger to hopefully reduce the change of jobs being dropped.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7784:39,reduce,reduce,39,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7784,1,['reduce'],['reduce']
Energy Efficiency,Check UUID in read adapter equals() and hashCode() methods,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/653:19,adapt,adapter,19,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/653,1,['adapt'],['adapter']
Energy Efficiency,"Closes #1493. @droazen @cmnbroad Is this what was intended by #1493 -- just replace `LinkedList` with `ArrayList` and `clear` the reservoir, keeping its capacity allocated, when possible?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5074:162,allocate,allocated,162,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5074,1,['allocate'],['allocated']
Energy Efficiency,"Closes #3607 ; In cleaning rare or ubiquitous kmers from the list for local assemblies, we now make our min and max counts depend on the measured coverage.; We also count the number of partitions in which a kmer appears, and discard those that appear in too many dispersed locations. This is a big win for keeping the assemblies small and local, and allows us to do more of them (there are fewer ""too bigs"").; Finally, we reorder the assemblies so that we do the big ones first. This keeps us from being hung up on stragglers.; This reduces by a percent or so the number of events called, but preliminary results suggest that these were mostly false positives caused by assembling homologous regions together.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3819:533,reduce,reduces,533,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3819,1,['reduce'],['reduces']
Energy Efficiency,"Closes #4868. @takutosato This reduces false positives and improves speed at no cost to sensitivity. I'm not quite ready to turn it on by default but I want it in the code to begin experiments, such as combining with linked de Bruijn graphs and FFPE error correction.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6470:31,reduce,reduces,31,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6470,1,['reduce'],['reduces']
Energy Efficiency,"Closes #5775. @takutosato This doesn't affect M2 results (well, actually it improves sensitivity by 0.01%) but it reduces runtime by about 5% and makes the docs and code cleaner. @jamesemery could you verify that in abstracting out `Log10Cache` as `IntToDoubleFunctionCache` I didn't spoil its thread safety?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5814:114,reduce,reduces,114,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5814,1,['reduce'],['reduces']
Energy Efficiency,"Commit 558160ea5bfde8be3b6e4bdd5283c529fb905fca, which upgrades gkl to 0.3.1 fails on PowerPC. The reason is in gkl-0.3.1, the following code block in IntelGKLUtils.java:. try {; // try to extract from classpath; String resourcePath = ""native/"" + System.mapLibraryName(libFileName);; URL inputUrl = IntelGKLUtils.class.getResource(resourcePath);; if (inputUrl == null) {; logger.warn(""Unable to find Intel GKL library: "" + resourcePath);; return false;; }. logger.info(String.format(""Trying to load Intel GKL library from:\n\t%s"", inputUrl.toString()));. File temp = File.createTempFile(FilenameUtils.getBaseName(resourcePath),; ""."" + FilenameUtils.getExtension(resourcePath), tempDir);; FileUtils.copyURLToFile(inputUrl, temp);; temp.deleteOnExit();; logger.debug(String.format(""Extracted Intel GKL to %s\n"", temp.getAbsolutePath()));. System.load(temp.getAbsolutePath());; logger.info(""Intel GKL library loaded from classpath."");; } catch (IOException ioe) {; // not supported; logger.warn(""Unable to load Intel GKL library."");; return false;; }. does not check machine architecture, nor catches any exception from `System.load()` function. On PowerPC, the dynamic library (.so file) still exists, but it's in illegal format. Hence the crash.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2302:86,Power,PowerPC,86,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2302,2,['Power'],['PowerPC']
Energy Efficiency,"CommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 18/05/01 14:25:36 INFO SparkContext: Successfully stopped SparkContext; 14:25:37.027 INFO PathSeqPipelineSpark - Shutting down engine; [May 1, 2018 2:25:37 PM EDT] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 37.98 minutes.; Runtime.totalMemory()=23999283200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 20 in stage 1.0 failed 1 times, most recent failure: Lost task 20.0 in stage 1.0 (TID 891, localhost, executor driver): ExecutorLostFailure (executor driver exited caused by one of the running tasks) Reason: Executor heartbeat timed out after 131031 ms; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4725:2497,schedul,scheduler,2497,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4725,1,['schedul'],['scheduler']
Energy Efficiency,Context$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5685:50245,schedul,scheduler,50245,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685,1,['schedul'],['scheduler']
Energy Efficiency,"ContextHandler: Started o.s.j.s.ServletContextHandler@6afbe6a1{/stages/stage/kill,null,AVAILABLE,@Spark}; 18/01/09 18:30:56 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://192.168.1.4:4040; 18/01/09 18:30:56 INFO spark.SparkContext: Added JAR file:/opt/NfsDir/BioDir/GATK4/gatk/build/libs/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar at spark://192.168.1.4:38793/jars/gatk-package-4.beta.5-50-g8d666b6-SNAPSHOT-spark.jar with timestamp 1515493856032; 18/01/09 18:30:56 INFO gcs.GoogleHadoopFileSystemBase: GHFS version: 1.6.1-hadoop2; 18/01/09 18:30:57 INFO client.RMProxy: Connecting to ResourceManager at tele-1/192.168.1.4:8032; 18/01/09 18:30:57 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers; 18/01/09 18:30:58 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (18432 MB per container); 18/01/09 18:30:58 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 18/01/09 18:30:58 INFO yarn.Client: Setting up container launch context for our AM; 18/01/09 18:30:58 INFO yarn.Client: Setting up the launch environment for our AM container; 18/01/09 18:30:58 INFO yarn.Client: Preparing resources for our AM container; 18/01/09 18:30:59 INFO yarn.Client: Uploading resource file:/tmp/sun/spark-5a3e539e-2e2b-4da2-b218-2bda166bd4c0/__spark_conf__7100950787185363106.zip -> hdfs://tele-1:8020/user/sun/.sparkStaging/application_1515493209401_0001/__spark_conf__.zip; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing view acls to: sun; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing modify acls to: sun; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing view acls groups to: ; 18/01/09 18:31:00 INFO spark.SecurityManager: Changing modify acls groups to: ; 18/01/09 18:31:00 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(sun); groups with view per",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4112:11199,allocate,allocate,11199,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4112,1,['allocate'],['allocate']
Energy Efficiency,Create Spark partitioner that can efficiently load records for overlapping genomic regions,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1988:34,efficient,efficiently,34,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1988,1,['efficient'],['efficiently']
Energy Efficiency,"Cromwell still struggles with call caching and metadata bloat in our gCNV workflows. Specific improvements to reduce overhead will modify scattered tasks:. 1. `GermlineCNVCallerCohort(Case)Mode` - Replace input `Array[File] read_count_files` with a list of files, i.e. `File read_count_file_paths`. This should be generated using `WritePathList`, rather than using `write_lines()` which does not function in WDL workflow blocks on some Cromwell servers. Replace output `Array[File] gcnv_call_tars` with a single tarball `File gcnv_call_tar` containing all of the calls. It appears there are a number of redundant outputs - kernel version, denoising configs, output file lists, etc. that were added with the [joint calling pipeline](https://github.com/broadinstitute/gatk/commit/31df35bb9204b5551cc1a3ee7468e2b0e577215d). We should rework that to extract/generate those in joint calling workflow when needed and eliminate these outputs.; 2. Add a transpose task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_call_tar` output, and generates a sample-sharded `Array[File] gcnv_calls_by_sample` output of tarballs.; 2. Add a model bundling task following `GermlineCNVCallerCohort(Case)Mode` that consumes the interval-sharded `Array[File] gcnv_model_tar` output, extracts the files, and tarballs all of them together to produce a single tarball output. Make this the output of the cohort workflow and input to the case mode workflows, rather than an array of model tars (retain the current `Array[File]` input as an optional type `Array[File]?` that will be used as the default if provided to case mode in order to support users still working with the old paradigm).; 3. `PostprocessGermlineCNVCalls` - replace input `Array[File] gcnv_calls_tars` with `File gcnv_sample_calls`, the sample-sharded output from the aforementioned transpose task. Delete inputs `calling_configs`, `denoising_configs`, `gcnvkernel_version`, `sharded_interval_lists`, as the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7721:110,reduce,reduce,110,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7721,1,['reduce'],['reduce']
Energy Efficiency,"Currently GATK4 walker mode won't recognize altivec-based libVectorLoglessPairHMM.so only AVX based library. On POWER it will Falling back to the MUCH slower LOGLESS_CACHING implementation. To avoid performance degradation on POWER for Haplotyecaller, please include support for Altivec based pairhmm library libVectorLoglessPairHMM.so; Two possible ways to do it:; 1. integrate support by using ""grep -i altivec /proc/cpuinfo"" to identify availability of Altivec support and then integrate the library; 2. We can setup Java path or other options that will look for any libVectorLoglessPairHMM.so available and test compatibility. We would do all necessary works to get this done, but would appreciate your direction on which way to peruse. Thanks!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3180:112,POWER,POWER,112,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3180,2,['POWER'],['POWER']
Energy Efficiency,"Currently in the WDLs there is a single boolean that determines whether funcotator will annotate with gnomAD. If it is `true`, funcotator will annotate with both exome and genome annotations which can be filtered out later. Instead, the funcotator WDL should have two separate booleans - one for exome and one for genome. This will reduce the output file size and speed up the annotation process.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6549:332,reduce,reduce,332,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6549,1,['reduce'],['reduce']
Energy Efficiency,"Currently when running a spark tool we spam the console with lots of useless garbage. Let's suppress this at the default logging level, and instead output a link to the spark monitoring console and/or a progress meter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/962:175,monitor,monitoring,175,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/962,2,"['meter', 'monitor']","['meter', 'monitoring']"
Energy Efficiency,"Currently when we publish an artifactory snapshot, we upload both the maven jars (~50 MB) plus the fully-packaged jars (~500 MB). We only need the maven jars! By eliminating the packaged jars, we can reduce our artifact size by an order of magnitude. Once this is done, we should talk to @davidbernick about increasing the artifact expiration time from the current 60 days to something longer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4567:200,reduce,reduce,200,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4567,1,['reduce'],['reduce']
Energy Efficiency,"Currently, FilterByOrientation requires replacing a required input's contents with a sed command: . ```; sed -r ""s/picard\.analysis\.artifacts\.SequencingArtifactMetrics\\\$PreAdapterDetailMetrics/org\.broadinstitute\.; hellbender\.tools\.picard\.analysis\.artifacts\.SequencingArtifactMetrics\$PreAdapterDetailMetrics/g"" \; ""metrics.pre_adapter_detail_metrics"" > ""gatk.pre_adapter_detail_metrics""; ```. The required input is the detailed pre-adapter summary metrics from Picard's CollectSequencingArtifactMetrics. The sed command replaces; ```; ## METRICS CLASS	picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```; with ; ```; ## METRICS CLASS	org.broadinstitute.hellbender.tools.picard.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; ```. If this class is not replaced, then the tool errors with: ; ```; htsjdk.samtools.SAMException: Could not locate class with name gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at htsjdk.samtools.metrics.MetricsFile.read(MetricsFile.java:356); 	at org.broadinstitute.hellbender.tools.exome.FilterByOrientationBias.onTraversalStart(FilterByOrientationBias.java:102); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:777); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:115); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:170); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:189); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:122); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:143); 	at org.broadinstitute.hellbender.Main.main(Main.java:221); Caused by: java.lang.ClassNotFoundException: gatk.analysis.artifacts.SequencingArtifactMetrics$PreAdapterDetailMetrics; 	at java.net.URLClassLoader.findClass(URLClassLoader.java:381); 	at java.lang.ClassLoader.loadCla",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3030:443,adapt,adapter,443,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3030,1,['adapt'],['adapter']
Energy Efficiency,"Currently, we need to copy files on S3 to local storage before using; them. This patch enables gatk local and spark modes to access s3a://; files directly to reduce copy overhead and local disk usages. s3a file accesses require additional configuration of core-site.xml; located in CLASSPATH as well as other hadoop applications. Spark; already has hadoop dependencies but local modes need to add hadoop; jars in the classpath. Example core-site.xml:. ```; <configuration>; <property>; <name>fs.s3a.access.key</name>; <value>{Your AWS_ACCESS_KEY_ID}</value>; </property>; <property>; <name>fs.s3a.secret.key</name>; <value>{Your AWS_SECRET_ACCESS_KEY}</value>; </property>; </configuration>; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/6698:158,reduce,reduce,158,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6698,1,['reduce'],['reduce']
Energy Efficiency,DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); 	at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:439); 	at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:38); 	at org.broadinstitute.hellbender.to,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5854:2765,reduce,reduce,2765,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854,1,['reduce'],['reduce']
Energy Efficiency,"DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; 11:00:54.078 INFO AbstractConnector - Stopped Spark@2f829853{HTTP/1.1, (http/1.1)}{0.0.0.0:4040}; 11:00:54.091 INFO SparkUI - Stopped Spark web UI at http://172.20.19.130:4040; 11:00:54.122 INFO MapOutputTrackerMasterEndpoint - MapOutputTrackerMasterEndpoint stopped!; 11:00:54.175 INFO MemoryStore - MemoryStore cleared; 11:00:54.175 INFO BlockManager - BlockManager stopped; 11:00:54.193 INFO BlockManagerMaster - BlockManagerMaster stopped; 11:00:54.211 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - OutputCommitCoordinator stopped!; 11:00:54.302 INFO SparkContext - Successfully stopped SparkContext; 11:00:54.303 INFO SortSamSpark - Shutting down engine; [August 11, 2024 at 11:00:54 AM CST] org.broadinstitute.hellbender.tools.spark.pipelines.SortSamSpark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:23714,schedul,scheduler,23714,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,DAGScheduler.scala:1329) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.collection.immutable.List.foreach(List.scala:431) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2281) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:83) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsNewAPIHadoopDataset$1(PairRDDFunctions.scala:1078) ~[gatk-package-4.4.0.0-local.jar:4.4.0.0]; at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$m,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:14782,schedul,scheduler,14782,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,Determine whether and to what degree the GATK engine should provide a reduce facility,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/114:70,reduce,reduce,70,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/114,1,['reduce'],['reduce']
Energy Efficiency,"Different traversal options might optimize some pipelines and give flexibility to the API user to have more efficient tools. Some of the different traversal per-pass that I am interested are:; * Allow different filters applied in each pass. Example usage: collect some metric with all reads in the first pass, and use it for the second pass only on the filtered reads.; * Allow different transformers in each pass. Example usage: in the first pass, untransformed reads are used to collect a metric; based on that metric, the second pass transform the reads to be output.; * Allow different intervals applied in each pass. Example usage: iterate over the user intervals to get some information about the reads, and then in the second pass use that information to iterate over a different set of intervals. I came out with this feature in the `TwoPassReadWalker` to implement a tool for pair-end data. The use case is to iterate in the first pass to the user intervals/filters, collecting where the mates are located (and/or read names), and in the second pass iterate over the reads and its mates independently of where they are located. If giving this flexibility to the `TwoPassReadWalker` is not desired, another option is to implement another walker-type `ReadAndMatesWalker` which implement the first pass in a private method and the second as a `ReadWalker.apply` for the reads and its mates. Which one is the preferred option for the GATK engine team, @droazen?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4849:108,efficient,efficient,108,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4849,1,['efficient'],['efficient']
Energy Efficiency,Docs for monitoring quotas and requesting increases [VS-1320],MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8760:9,monitor,monitoring,9,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8760,1,['monitor'],['monitoring']
Energy Efficiency,Ensure all tools (public + protected) use a standard progress meter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2470:62,meter,meter,62,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2470,1,['meter'],['meter']
Energy Efficiency,EventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49). at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608); at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607); at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607); at org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1523); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1329); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5(DAGScheduler.scala:1332); at org.apache.spark.scheduler.DAGScheduler.$anonfun$submitStage$5$adapted(DAGScheduler.scala:1331); at scala.collection.immutable.List.foreach(List.scala:431); at org.apache.spark.scheduler.DAGScheduler.submitStage(DAGScheduler.scala:1331); at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2810); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2228); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2249); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2281); at org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoop,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/8949:32308,schedul,scheduler,32308,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/8949,1,['schedul'],['scheduler']
Energy Efficiency,Explore more efficient ways of getting inputs into HDFS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2014:13,efficient,efficient,13,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2014,1,['efficient'],['efficient']
Energy Efficiency,"External bams sometimes use faulty adapter trimming algorithms that leave a few identical, repeated reads in a bam (i.e. not PCR duplicates but exactly the same read name etc). While these bams are faulty there is no reason not to be able to handle them, as we could as of GATK 3.6. This patch prevents an error without changing how we process good bams.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3122:35,adapt,adapter,35,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3122,1,['adapt'],['adapter']
Energy Efficiency,"FO FilterMutectCalls - Shutting down engine ; ; \[June 4, 2021 11:03:51 AM CST\] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.19 minutes. ; ; Runtime.totalMemory()=625999872 ; ; java.lang.NumberFormatException: **For input string: ""167|35|14""** ; ; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65) ; ; at java.lang.Integer.parseInt(Integer.java:580) ; ; at java.lang.Integer.valueOf(Integer.java:766) ; ; at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:288) ; ; at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193) ; ; at java.util.Collections$2.tryAdvance(Collections.java:4717) ; ; at java.util.Collections$2.forEachRemaining(Collections.java:4725) ; ; at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481) ; ; at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471) ; ; at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708) ; ; at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ; ; at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:274) ; ; at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:282) ; ; at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:827) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.DuplicatedAltReadFilter.areAllelesArtifacts(DuplicatedAltReadFilter.java:26) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.HardAlleleFilter.calculateErrorProbabilityForAlleles(HardAlleleFilter.java:16) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2AlleleFilter.errorProbabilities(Mutect2AlleleFilter.java:86) ; ; at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lamb",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7298:6551,Reduce,ReduceOps,6551,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7298,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,"FO GenotypeGVCFs - Picard Version: 2.21.2; 22:28:44.640 INFO GenotypeGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 22:28:44.641 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 22:28:44.641 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 22:28:44.641 INFO GenotypeGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 22:28:44.641 INFO GenotypeGVCFs - Deflater: IntelDeflater; 22:28:44.641 INFO GenotypeGVCFs - Inflater: IntelInflater; 22:28:44.641 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 22:28:44.641 INFO GenotypeGVCFs - Requester pays: disabled; 22:28:44.641 INFO GenotypeGVCFs - Initializing engine; ```. #### Steps to reproduce; I've followed the recommendation to process my genome in parallel, each chromosome at a time, so I created the commands based on the following pipeline:; ```; # HC; gatk HaplotypeCaller -R GRCh38.fasta -I sample.dedup.rg.csorted.bam -O sample1.HC.gvcf -ERC GVCF; gatk HaplotypeCaller -R GRCh38.fasta -I sample.dedup.rg.csorted.bam -O sample2.HC.gvcf -ERC GVCF; gatk HaplotypeCaller -R GRCh38.fasta -I sample.dedup.rg.csorted.bam -O sample3.HC.gvcf -ERC GVCF. # GenomicsDBImport for Chr1; export TILEDB_DISABLE_FILE_LOCKING=1 ; gatk GenomicsDBImport --java-options ""-Xmx4g -Xms4g"" -V sample1.HC.gvcf -V sample2.HC.gvcf -V sample3.HC.gvcf --genomicsdb-workspace-path GenomicsDB_1 --tmp-dir /tmp -L 1. # GenotypeGVCFs; gatk GenotypeGVCFs --java-options ""-Xmx12g -Xms12g"" -R GRCh38.fasta -V gendb://GenomicsDB_1 --tmp-dir /tmp -O samples.1.vcf; ```; The jobs were send to the HPC scheduler and were allocated 2 CPUs and up to 16GB of RAM each. Everything till the last genotype calling step worked fine (and quite quickly) ; #### Expected behavior; The tool should call variants from the GenomicsDB for the requested interval. #### Actual behavior; Runs indefinitely (or until walltime is reached), but never gets passed the ""Initializing engine"" step. (tried this with 2 different datasets).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7007:3473,schedul,scheduler,3473,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7007,2,"['allocate', 'schedul']","['allocated', 'scheduler']"
Energy Efficiency,"FO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks; 17/10/11 14:19:19 INFO spark.ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1); 17/10/11 14:19:23 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(null) (com2:35572) with ID 1; 17/10/11 14:19:23 INFO spark.ExecutorAllocationManager: New executor 1 has registered (new total is 1); 17/10/11 14:19:23 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, com2, executor 1, partition 0, NODE_LOCAL, 2235 bytes); 17/10/11 14:19:23 INFO storage.BlockManagerMasterEndpoint: Registering block manager com2:38568 with 530.0 MB RAM, BlockManagerId(1, com2, 38568); 17/10/11 14:19:25 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on com2:38568 (size: 6.9 KB, free: 530.0 MB); 17/10/11 14:19:26 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on com2:38568 (size: 26.1 KB, free: 530.0 MB); 17/10/11 14:19:27 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 4180 ms on com2 (executor 1) (1/1); 17/10/11 14:19:27 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool ; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (mapToPair at SparkUtils.java:157) finished in 8.951 s; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: looking for newly runnable stages; 17/10/11 14:19:27 INFO scheduler.DAGScheduler: running: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: failed: Set(); 17/10/11 14:19:27 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at mapToPair at ReadsSparkSink.java:244), which has no missing parents; 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 86.1 KB, free 529.6 MB); 17/10/11 14:19:27 INFO storage.MemoryStore: Block broadcast_3_piece0 stored ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3686:13492,schedul,scheduler,13492,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3686,1,['schedul'],['scheduler']
Energy Efficiency,"FileOutputCommitter: Saved output of task 'attempt_20210413073224_0026_r_000002_0' to file:/dev/shm/chr4_GL000008v2_random.g.vcf.gz.parts; 21/04/13 07:32:25 INFO SparkHadoopMapRedUtil: attempt_20210413073224_0026_r_000002_0: Committed; 21/04/13 07:32:25 INFO Executor: Finished task 2.0 in stage 5.0 (TID 107). 848 bytes result sent to driver; 21/04/13 07:32:25 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 107) in 279 ms on localhost (executor driver) (2/3); 21/04/13 07:32:25 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 105, localhost, executor driver): org.apache.spark.SparkException: Task failed while writing rows; at org.apache.spark.internal.io.SparkHadoopWriter$.org$apache$spark$internal$io$SparkHadoopWriter$$executeTask(SparkHadoopWriter.scala:157); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:83); at org.apache.spark.internal.io.SparkHadoopWriter$$anonfun$3.apply(SparkHadoopWriter.scala:78); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:123); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); Caused by: java.lang.IllegalArgumentException: Sequence [VC HC @ chr4_GL000008v2_random:7168-7691 Q. of type=SYMBOLIC alleles=[T*, <NON_REF>] attr={END=7691} GT=[[NA12878 T*/T* GQ 0 DP 0 PL 0,0,0 {MIN_DP=0}]] filters= added out of order currentReferenceIndex: 25, referenceIndex:37; at htsjdk.tribble.index.tabix.AllRefsTabixIndexCreator.addFeature(AllRefsTabixIndexCreator.java:79); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.add",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7199:8884,schedul,scheduler,8884,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7199,1,['schedul'],['scheduler']
Energy Efficiency,Fix adapter boundary for positive strand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2270:4,adapt,adapter,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2270,1,['adapt'],['adapter']
Energy Efficiency,Fix adapter bounday for positive strand,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/2346:4,adapt,adapter,4,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/2346,1,['adapt'],['adapter']
Energy Efficiency,Fix bug and reduce excessive logging from VariantAnnotatorEngine and JumboGenotypes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7612:12,reduce,reduce,12,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7612,1,['reduce'],['reduce']
Energy Efficiency,Fixed a rare non-determinism in the AdaptiveChainPruner,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7851:36,Adapt,AdaptiveChainPruner,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7851,1,['Adapt'],['AdaptiveChainPruner']
Energy Efficiency,FlagStatDataflow and tests should be easy to port and a useful tool that requires a non-trivial reduce,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/933:96,reduce,reduce,96,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/933,1,['reduce'],['reduce']
Energy Efficiency,For AoU/BQ solution we want to be able to produce VCFs w/o PLs. In a 1k sample WGS VCF this reduces the .vcf.gz total size from 120GB to 73GB (almost 40%),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/7067:92,reduce,reduces,92,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/7067,1,['reduce'],['reduces']
Energy Efficiency,"For large `x`, `NB(x | mu, alpha)` can be replaced with a Gaussian approximation which is much faster to compute. In practice, we get ~3x speedup if we naively replace all `NB` with `Normal`. However, we will lose accuracy on deletions and low-coverage loci. It is desirable to be able to adaptively switch between the two based on a criterion (i.e. if the relative difference of exact/approximate lpdf is below a given threshold). The match can be worked out, however, limitations in `theano` makes the implementation tricky: `theano.tensor.switch(condition, a, b)` is not lazy (undesirable) and evaluates both `a` and `b`, however, it works for tensor `condition` (desirable). On the other hand,`theano.ifelse.ifselse(condition, a, b)` is lazy (desirable) but only works if `condition` is a scalar (undesirable).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4059:289,adapt,adaptively,289,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4059,1,['adapt'],['adaptively']
Energy Efficiency,For use in genotyping and interpretation of events for human consumption.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/3193:61,consumption,consumption,61,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/3193,1,['consumption'],['consumption']
Energy Efficiency,"From @tomwhite:. What we can do though is take another look at why we need such a small split size and see if we can increase it (by better tuning etc). Even if we can't we know that some for some classes of jobs (e.g. metrics) we can use the default split size without an issue, so we could add logic to choose an appropriate split size based on the type of job. Thinking about this a bit more, we're actually quite close to a solution. In the existing Spark Mark Duplicates implementation, for example, you can set the parallelism (with -P). By default it is the same as the number of input partitions, but if the partitions are too large then each reducer has too much data to deal with and the job fails. This is why we made the number of partitions large by reducing the split size to 10MB. However, by increasing the number of reducers (by setting -P to be input data size/10MB), the input split size can be increased to its default of 128MB without causing a problem. So I think the work here is to work out a good set of defaults and the smallest number of knobs to override them. For example, I think we could always use the default split size, have some heuristics to choose a good value for the parallelism, and allow it to be overridden with -P. We should also decouple the parallelism and whether the output is a single file. For ReadsPipelineSpark we have -shardedOutput, but not for MarkDuplicatesSpark (where it's signalled by -P 1), so we should make this consistent.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1403:651,reduce,reducer,651,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1403,2,['reduce'],"['reducer', 'reducers']"
Energy Efficiency,Fun$1.apply(JavaPairRDD.scala:1030); at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1030); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30); at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.lambda$apply$26a6df3e$1(BaseRecalibratorSparkFn.java:28); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$4$1.apply(JavaRDDLike.scala:156); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297); at org.apache.spark.rdd.RDD.iterator(RDD.scala:264); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73); at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1560:2436,schedul,scheduler,2436,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1560,3,['schedul'],['scheduler']
Energy Efficiency,GATK Reduced Docker Layers for ACR,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/8808:5,Reduce,Reduced,5,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/8808,1,['Reduce'],['Reduced']
Energy Efficiency,"GATK3 is very slow when processing references with large numbers of contigs, such as draft genomes. In the past this mostly affected microbial genomes so we didn't do anything about it, but now the Hg38 has a lot more contigs so we have to make sure that's not going to be a problem with GATK4. . To be clear, efficient processing of reference genomes with thousands of contigs is a must-have. . Efficient processing of e.g. microbial draft genomes with tens of thousands of contigs is a nice-to-have. More than that is just crazy talk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1688:310,efficient,efficient,310,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1688,2,"['Efficient', 'efficient']","['Efficient', 'efficient']"
Energy Efficiency,GATK_SKIP_NATIVE_BUILD=true doesn't work on POWER systems,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/1711:44,POWER,POWER,44,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/1711,1,['POWER'],['POWER']
Energy Efficiency,"Genome Analysis Toolkit (GATK) v4.1.4.1_. b) Exact GATK commands used. _/usr/bin/time -v gatk --java-options ""-Xmx10G"" Mutect2 -R ../reference/indices\_010920/GRCh38.d1.vd1.fa -L chr4.bed -I chr4.bam --max-mnp-distance 0 --interval-padding 100 -O chr4.vcf.gz_. c) The entire error log if applicable. _java.lang.IllegalArgumentException: Need one or two reads to construct a fragment_ ; _at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:725)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.create(Fragment.java:43)_ ; _at org.broadinstitute.hellbender.utils.read.Fragment.createAndAvoidFailure(Fragment.java:58)_ ; _at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)_ ; _at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1376)_ ; _at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)_ ; _at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)_ ; _at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)_ ; _at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)_ ; _at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)_ ; _at org.broadinstitute.hellbender.utils.genotyper.AlleleLikelihoods.groupEvidence(AlleleLikelihoods.java:589)_ ; _at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:93)_ ; _at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:251)_ ; _at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:320)_ ; _at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308)_ ; _at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281)_ ; _at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048)_ ; _at org.broadinstitute.hellbender.cmdline.CommandLineProg",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/6419:1689,Reduce,ReduceOps,1689,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6419,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,GenomeLoc(GenomeLocParser.java:185); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:169); at org.broadinstitute.hellbender.utils.GenomeLocParser.createGenomeLoc(GenomeLocParser.java:150); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.setRead(OverhangFixingManager.java:402); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager$SplitRead.<init>(OverhangFixingManager.java:396); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.getSplitRead(OverhangFixingManager.java:467); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Collections$2.tryAdvance(Collections.java:4717); at java.util.Collections$2.forEachRemaining(Collections.java:4725); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); at org.broadinstitute.hellbender.tools.walkers.rnaseq.OverhangFixingManager.addReadGroup(OverhangFixingManager.java:207); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.splitNCigarRead(SplitNCigarReads.java:259); at org.broadinstitute.hellbender.tools.walkers.rnaseq.SplitNCigarReads.firstPassApply(SplitNCigarReads.java:180); at org.broadinstitute.hellbender.engine.TwoPassReadWalker.lambda$traverseReads$0(TwoPassReadWalker.java:62); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRem,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/5293:3362,Reduce,ReduceOps,3362,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5293,2,['Reduce'],"['ReduceOp', 'ReduceOps']"
Energy Efficiency,GenomicsDBImport is not hooked up to its progress meter,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/2683:50,meter,meter,50,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/2683,1,['meter'],['meter']
Energy Efficiency,"GenotypeGVCFs - Deflater: IntelDeflater; 12:31:14.785 INFO GenotypeGVCFs - Inflater: IntelInflater; 12:31:14.785 INFO GenotypeGVCFs - GCS max retries/reopens: 20; 12:31:14.785 INFO GenotypeGVCFs - Requester pays: disabled; 12:31:14.785 INFO GenotypeGVCFs - Initializing engine; 12:31:15.766 INFO GenomicsDBLibLoader - GenomicsDB native library version : 1.4.3-6069e4a; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; 12:31:17.675 info NativeGenomicsDB - pid=3151 tid=3153 No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 12:31:19.634 INFO IntervalArgumentCollection - Processing 3714165 bp from intervals; 12:31:19.665 INFO GenotypeGVCFs - Done initializing engine; 12:31:19.700 INFO FeatureManager - Using codec BEDCodec to read file file:///home/groups/pgpdata/ColonyData/207/@files/sequenceOutputs/mmul10.WGS-WXS.whitelist.v2.3.sort.merge.bed; 12:34:40.705 INFO ProgressMeter - Starting traversal; 12:34:40.705 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute. and then at: 15 Feb 2022 12:37:38,923:; [TileDB::StorageBuffer] Error: (gzip_read_buffer) Cannot read to buffer; Mem allocation error errno=12(Cannot allocate memory); [TileDB::StorageBuffer] Error: (read_buffer) Cannot decompress and/or read bytes path=/home/exacloud/gscratch/prime-seq/cachedData/16b9ede7-6db8-103a-9262-f8f3fc86a851/WGS_Feb22_1852.gdb/1$1$223616942/__9b9a9e96-139c-4105-81ec-ab1455d1f01d140490873108224_1597099702436/__book_keeping.tdb.gz errno=12(Cannot allocate memory); [TileDB::BookKeeping] Error: Cannot load book-keeping; Reading domain size failed.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:4652,allocate,allocate,4652,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,2,['allocate'],['allocate']
Energy Efficiency,GenotypeGVCFs/GenomicsDB and Cannot allocate memory error,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/7674:36,allocate,allocate,36,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/7674,1,['allocate'],['allocate']
Energy Efficiency,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5715:420,reduce,reduce,420,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715,1,['reduce'],['reduce']
Energy Efficiency,"Google Cloud Storage has ""requester pays"" buckets. When reading from; those buckets, the requester is billed for the bandwidth (normally, it's; the bucket owner who is billed). This pull request enables this feature with GATK. By default it is; turned off (so there are no unexpected charges). To turn it on, use the; command line argument ""--project-for-requester-pays"" (or; ""--requester-project"") to indicate which project to bill. Example:. $ ./gatk PrintReads --input $INPUT --output=/tmp/reads.bam; fails with: com.google.cloud.storage.StorageException: Bucket is requester pays bucket but no user project provided. $ ./gatk PrintReads --input $INPUT --output=/tmp/reads.bam --requester-project=$PROJECT; works. This PR also removes the argumentless version of; setGlobalNIODefaultOptions() because it was confusing (it uses the; default values, not those indicated by the user - usually we'd expect; the user's values to be taken into account).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/5140:284,charge,charges,284,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5140,1,['charge'],['charges']
Energy Efficiency,"Google is deprecating and removing their implementation of the old style GA4GH read and reference API's. . > ; > Reads API functionality is now replaced by the htsget protocol ; > ; > This year, the GA4GH team introduced the htsget protocol to allow users to download read data for subsections of the genome in which they are interested. This is a richer and more flexible approach to working with reads data. It allows you to keep your genomics data in a common BAM file format on Google Cloud Storage and work with it efficiently from your computation pipelines, using standard bioinformatics tools. We have already launched our own open source implementation of this protocol, which you can use to access your reads data. Many popular tools such as samtools and htslib have been updated by the community to support htsget. Documentation is provided here. The Reads API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month by those receiving this notice, whichever comes first. ; > ; > Variants API is now replaced by htsget and Variant Transforms ; > ; > The GA4GH team also plans to extend the htsget protocol to cover variant data, and we will extend our implementation of htsget to cover this use case. ; > ; > After analyzing usage of the Variants API, we found that users primarily used it to import variant data and then export it to BigQuery. To save time and effort, we created Variant Transforms, an open source tool for directly importing VCF data into BigQuery. Variant Transforms and its documentation are published here. Variant Transforms is more scalable than the legacy Variants API, and it has a robust roadmap with a dedicated team. We also welcome collaborators on this project as it advances. ; > ; > The Variants API is now deprecated, and will be decommissioned after one year, or after there has been no API activity for one month, whichever comes first. ; > ; > We are excited to move in step with the global ge",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/issues/4166:520,efficient,efficiently,520,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/4166,1,['efficient'],['efficiently']
Energy Efficiency,"Green light from devs to change this. However, perhaps more discussion is warranted. Please see #3163 for details leading to this PR.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/3164:0,Green,Green,0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/3164,1,['Green'],['Green']
Energy Efficiency,"HC priors will let us reduce the WEx GVCF footprint. As a consequence, CalculateGenotypePosteriors now supports indels. I fixed a bug and changed the args for CGP. We didn't have great tests, but CGP results will be fixed/improved in some cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://github.com/broadinstitute/gatk/pull/4793:22,reduce,reduce,22,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/4793,1,['reduce'],['reduce']
