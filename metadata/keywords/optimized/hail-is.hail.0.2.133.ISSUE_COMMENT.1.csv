quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability," speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1356,install,install,1356,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['install'],['install']
Deployability," take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getting this committed, I have not made changes relating to logging and; error messages. The DLL's are still in the jar, and I think it has to stay that way because; all nodes ne",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:1229,release,released,1229,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,1,['release'],['released']
Deployability," the jars and wheel in release mode:; ```bash; HAIL_RELEASE_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1068,deploy,deploy,1068,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability," the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2981,update,update,2981,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,1,['update'],['update']
Deployability," to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/dataproc/0.2.129/*"" --temporary-hold; ```. Note the following:; - mill is not invoked; - dep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1466,deploy,deploy,1466,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,3,['deploy'],['deploy']
Deployability," use:; - ./spark-submit with --driver-class-path to augment the driver classpath; - spark.executor.extraClassPath to augment the executor classpath; ; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.executor.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; 17/08/09 19:16:02 WARN SparkConf: Setting 'spark.driver.extraClassPath' to '/opt/Software/hail/build/libs/hail-all-spark.jar' as a work-around.; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> sc.textFile('/hail/test/BRCA1.raw_indel.vcf'); /hail/test/BRCA1.raw_indel.vcf MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:-2; >>> ; ```. ----------------; When I executed the command in local mode , there seems to hava some result:; ```; [root@tele-1 ~]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; >>> hc = HailContext();; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel).; hail: info: SparkUI: http://192.168.1.4:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.1-0320a61; >>> hc.import_vcf('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel.vcf').write('/opt/NfsDir/UserDir/wanghn/BRCA1.raw_indel_1.vds'); hail: info: No multiallelics detected.; hail: info: Coerced unsorted dataset; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; ```; ------------------------; How can I check if my spark configuration meet the requirement of the hail?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506:2769,configurat,configuration,2769,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321228506,1,['configurat'],['configuration']
Deployability, varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-d,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3317,deploy,deploy-,3317,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability, varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:14047,release,release,14047,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability," | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to bu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1629,install,install,1629,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['install']
Deployability,# EDIT: See end of thread for new approach. The issue is that `use OpenBLAS` puts `libopenblas.so` on `LD_LIBRARY_PATH`. `netlib-java` links against `libblas.so` which is expected to contain the CBLAS symbols. It appears that Debian-style distributions use `update-alternatives` to symlink `libblas.so` to a library of the user's choice. Broad's UGER cluster does not provide such `update-alternatives` functionality. There exists two fixes:; - create a symlink to `libopenblas.so` named `libblas.so` and put it on the LD_LIBRARY_PATH; - use `LD_PRELOAD` to forcibly load `libopenblas.so`. The two solutions look like:. 1.; ```; mkdir ~/lib; ln -s /broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so ~/lib/libblas.so.3; export LD_LIBARRY_PATH=~/lib:$LD_LIBRARY_PATH; ```. 2.; ```; export LD_PRELOAD=/broad/software/free/Linux/redhat_7_x86_64/pkgs/openblas_0.2.20/lib/libopenblas.so; ```. Clearly neither of these are ideal. I recommend users place the lines from option 1 in an rc file.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523:258,update,update-alternatives,258,https://hail.is,https://github.com/hail-is/hail/issues/5559#issuecomment-472953523,2,['update'],['update-alternatives']
Deployability,"# logLkhd given by global.lmmreg.fit.logLkhdVals; df = read.table('delta.m.25.tsv', header=T, sep=""\t""). ##### method to estimate sigma, the standard deviation of normal approximation of confidence interval for h2; ### h2 = sigmoid(-ln(delta)); df$h2 = 1 / (1 + exp(df$logDelta)). ### fit parabola near maximum logLkhd of h2; maxRow = which.max(df$logLkhd). # h2; x1 = df$h2[maxRow - 1]; x2 = df$h2[maxRow]; x3 = df$h2[maxRow + 1]. # logLkhd at h2; y1 = df$logLkhd[maxRow - 1]; y2 = df$logLkhd[maxRow]; y3 = df$logLkhd[maxRow + 1]. # find a in logLkhd = a * x^2 + b * x + c; a = (x3 * (y2 - y1) + x2 * (y1 - y3) + x1 * (y3 - y2)) / ((x2 - x1) * (x1 - x3) * (x3 - x2)). # logLkhd = - (x - mu)^2 / (2 * sigma^2) + const = -1 / (2 * sigma^2) * x^2 + lower order terms; sigma2 = 1 / (-2 * a); sigma = sqrt(sigma2). ##### Method to plot normalized likelihood function of h2 and normal approximation; # shift log lkhd to have max of 0, to prevent numerical issues; maxLogLkhd = max(df$logLkhd); df$logLkhd = df$logLkhd - maxLogLkhd. ### integrate in h2 coordinates; df$width = df$h2 * (1 - df$h2) # d(h2) / d (ln(delta)) = - h2 * (1 - h2); total = sum(exp(df$logLkhd) * df$width) # normalization constant; df$posterior = exp(df$logLkhd) * df$width / total # normalized likelihood of h2 = posterior of h2 with uniform prior. ### normal approximation; meanPost = sum(df$h2 * df$posterior); sdPost = sqrt(sum((df$h2 - meanPost)**2 * df$posterior)); df$normalApproxPost = dnorm(df$h2, meanPost, sdPost). ### plots; qplot(x = logDelta, y = logLkhd, data = df, geom = 'line', xlab='ln(delta)', ylab='logLkhd(delta)'); qplot(x = h2 , y = logLkhd, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='logLkhd(h2)'); qplot(x = h2 , y = posterior, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='posterior(h2)'); qplot(x = h2, y = normalApproxPost, data = df, xlim =c(0,1), geom = 'line', xlab = 'h2', ylab='normalApprox(h2)'). ##### Reality check that sigma and sdPost are close; sigma; sdPost; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538:1222,integrat,integrate,1222,https://hail.is,https://github.com/hail-is/hail/pull/1720#issuecomment-297590538,1,['integrat'],['integrate']
Deployability,"### Create a User. ```; sudo adduser --system teamcity --ingroup www-data; sudo groupadd teamcity; sudo adduser teamcity teamcity; ```. (for some reason it didn't create the `teamcity` group on the first line). ### Move the Binaries. I moved the TeamCity Server and Agent binaries into `/home/teamcity`:. ```; ubuntu@ip-172-31-8-190:/home/teamcity$ ll; total 32; drwxr-xr-x 8 teamcity www-data 4096 Sep 7 18:55 ./; drwxr-xr-x 4 root root 4096 Sep 7 18:16 ../; drwxrwxr-x 6 teamcity teamcity 4096 Sep 7 18:33 .BuildServer/; drwxr-xr-x 5 teamcity www-data 4096 Sep 7 18:55 .gradle/; drwxr-xr-x 13 teamcity teamcity 4096 Aug 22 19:22 TeamCity/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent1/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent2/; drwxrwxr-x 13 teamcity teamcity 4096 Sep 7 18:54 TeamCityAgent3/; ```. ### Update the `init.d` scripts. #### `/etc/init.d/teamcity`. ```; #!/bin/sh; ### BEGIN INIT INFO; # Provides: teamcity ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcity ; # Description: teamcity build server; ### END INIT INFO; # /etc/init.d/teamcity - startup script for teamcity; export TEAMCITY_DATA_PATH=""/home/teamcity/.BuildServer""; export TEAMCITY_SERVER_OPTS=-Djava.awt.headless=true # Configure TeamCity for use on a headless OS. case $1 in; start); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh start; ;;. stop); start-stop-daemon --start -c teamcity --exec /home/teamcity/TeamCity/bin/teamcity-server.sh stop; ;;. esac. exit 0; ```. #### `/etc/init.d/teamcityAgents`. ```; #!/bin/bash; ### BEGIN INIT INFO; # Provides: teamcityAgents ; # Required-Start: $local_fs $network; # Required-Stop: $local_fs; # Default-Start: 2 3 4 5; # Default-Stop: 0 1 6; # Short-Description: teamcityAgents ; # Description: TeamCity build agents ; ### END INIT INFO. USER=""teamcity""; AGENTS=(TeamCityAgent1 TeamCityAgent2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/675#issuecomment-245383790:849,Update,Update,849,https://hail.is,https://github.com/hail-is/hail/issues/675#issuecomment-245383790,1,['Update'],['Update']
Deployability,"#### Summary. It has happened twice. The failing partition is different in each run. 1. 49340 https://batch.hail.is/batches/8069235/jobs/51280; 2. 25997 https://batch.hail.is/batches/8083195/jobs/27937. The pipeline runs two table collects to get sample information, then converts the matrix table to a table of ndarrays of the value `hl.int(hl.is_defined(mt.GT))`. The entries are getting subsetted, so there is skipping going on. In both cases, we are decoding the entry array when the corrupted block is discovered. In the first case, we are skipping an int (must be RGQ based on the etype and type). In the second case, we are decoding a string (must be FT). Since the error happens on a seemingly arbitrary partition, it seems likely this is related to our transient error handling. Both runs use a version of Hail after we fixed the broken transient error handling in GoogleStorageFS (run 1 used fcaafc533e, run 2 used 0.2.126 / ee77707f4f). ---. #### Path forward. If it *is* a transient error, we need to fix how we handle transient errors. Maybe our position handling logic is wrong? If it is *not* a transient error, maybe our skipping logic is wrong? FT appears immediately after RGQ and we know RGQ is getting skipped. Our implementation of `seek` for the compressed block buffers looks sketchy to me, but we're using PartitionNativeReader which does no seeking. Action items:; 1. Log every transient error.; 2. Log the file name and the offset on failure. ---. #### Debugging information. EType:; ```; +EBaseStruct{; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:+EArray[; EBaseStruct{; GT:EInt32,; GQ:EInt32,; RGQ:EInt32,; FT:EBinary,; AD:EArray[EInt32]}]}; ```; (zipped) Type:; ```; Struct{; locus:Locus(GRCh38),; alleles:Array[String],; filters:Set[String],; info:Struct{; AC:Array[Int32]},; `the entries! [877f12a8827e18f61222c6c8c5fb04a8]`:Array[; Struct{; GT:Call,; GQ:Int32,; FT:String,; AD:Array[Int32]}]}; ```; Source buffer spec:; ```; {""name"":""LEB128BufferSpec"",""child"":; {",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623:207,pipeline,pipeline,207,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834181623,1,['pipeline'],['pipeline']
Deployability,#10612 Is the only user-visible change to batch since the last release.; > CHANGELOG: Made failed Python Jobs have non-zero exit codes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10677#issuecomment-882662210:63,release,release,63,https://hail.is,https://github.com/hail-is/hail/pull/10677#issuecomment-882662210,1,['release'],['release']
Deployability,#13439 updated tornado,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13470#issuecomment-1692321452:7,update,updated,7,https://hail.is,https://github.com/hail-is/hail/pull/13470#issuecomment-1692321452,1,['update'],['updated']
Deployability,"#6927 Should fix the issue I was seeing with the warnings. The issue was that to the `make` invocations that we were making in the compilation of C++ pipelines, it looked like they were in jobserver mode. But they we didn't preserve the right state to execute them. . I think the real solution is unsetting `MAKEFLAGS` in the environment in `pgradle`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524203427:150,pipeline,pipelines,150,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524203427,1,['pipeline'],['pipelines']
Deployability,"#8403 was force merged, hand deploying again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606699423:29,deploy,deploying,29,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606699423,1,['deploy'],['deploying']
Deployability,$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VER,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3655,deploy,deploy-,3655,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9131,deploy,deploy-,9131,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9867,deploy,deploy-,9867,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"'t be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (What Jupyter hub does.). > I thought it would take less time to get a subdirectory working than figure out how to add a new domain and a cert and deal with DNS. Fair. I added a wildcard *.staging.hail.is for staging, I'll do the same thing for Hail. Then you don't need to change the DNS to add a domain, and I'll write ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:1317,update,updates-in-kubernetes-,1317,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,1,['update'],['updates-in-kubernetes-']
Deployability,"'t run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2272,update,update,2272,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['update'],['update']
Deployability,"(I mean I don't think we should be in the habit of deploying docs outside the versioned releases, except in special circumstances)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509642724:51,deploy,deploying,51,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642724,2,"['deploy', 'release']","['deploying', 'releases']"
Deployability,"(If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). ignore all the log messages that have to do with `/wang/blog/healthcheck/`---since it's logging to persistent storage, it's also printing all the old logs from a few days ago to the batch logs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287:53,deploy,deployed,53,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548103287,1,['deploy'],['deployed']
Deployability,"(and to be clear, I'm onboard with the idea of a single command which gets you from zero to simple batch pipelines; jury is out on the Artifact Registry. I think making sure that's configured correctly might be better upstreamed to Sam. Normal users might not have permission to enable/disable things like that.).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157:105,pipeline,pipelines,105,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662843157,1,['pipeline'],['pipelines']
Deployability,(updated just now to what I *think* is more correct behavior),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6917#issuecomment-523633712:1,update,updated,1,https://hail.is,https://github.com/hail-is/hail/pull/6917#issuecomment-523633712,1,['update'],['updated']
Deployability,(would like to include that in patch notes),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4871#issuecomment-443783236:31,patch,patch,31,https://hail.is,https://github.com/hail-is/hail/pull/4871#issuecomment-443783236,1,['patch'],['patch']
Deployability,"**EDIT: If you're finding this thread in 2021 or later, do not attempt to use a tiny block size. LD prune works best with the default block size.**. update: using tiny block size:; ```; print(""extract pruned set of variants""); pruned_tbl = hl.ld_prune(mt.GT, r2 = 0.2, bp_window_size = 1000000, memory_per_core = 1000, block_size=75); pruned_tbl.write(""pruned_tbl.ht"", overwrite=True); ```; takes 16.1s, about 40x faster. This is still wayyyyy too slow -- the total input data is 14M!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506#issuecomment-427593611:149,update,update,149,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427593611,1,['update'],['update']
Deployability,"*Update*. If it helps, our configuration includes three VMs. This includes a master and two workers with autoscaling enabled. We have tried using n1-himem-8 and n1-himem-64 machines. Both configurations failed with similar errors. The one above is form the n1-himem-64 configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420:1,Update,Update,1,https://hail.is,https://github.com/hail-is/hail/issues/12083#issuecomment-1213381420,4,"['Update', 'configurat']","['Update', 'configuration', 'configurations']"
Deployability,*sigh*. ```; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. Azure's `StorageOutputStream.close` method is not idempotent in the version that we use. It has been made idempotent in `12.18.0`. I would be surprised if spark let us upgrade to a version that recent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901:1719,upgrade,upgrade,1719,https://hail.is,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901,1,['upgrade'],['upgrade']
Deployability,"- This problem was fixed in Scala 2.11. I ran into elsewhere, like RichVector in Utils. I left this comment:. // FIXME AnyVal in Scala 2.11. We're using Scala 2.10 because the Spark/Intel cluster is running an old version of, well, everything. It should be getting upgraded tomorrow. Then hopefully we can switch to 2.11 permanently. For now, I'd just put a similar comment.; - I'd remove testSingletonVariants. testFilterSamples is still good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/83#issuecomment-160692682:265,upgrade,upgraded,265,https://hail.is,https://github.com/hail-is/hail/pull/83#issuecomment-160692682,1,['upgrade'],['upgraded']
Deployability,"-0106a51b-pgxq pulling image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Pulled 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Successfully pulled image ""gcr.io/hail-vdc/hail-jupyter:e3f9a751f0a837815afeaf6fff8057f04747a35c908fb1ddf7cad6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1564,install,installed,1564,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,1,['install'],['installed']
Deployability,"-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5819,install,install,5819,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set to ""7"" which is different from old value """"; printf ""7"" > env/ELASTIC_MAJOR_VERSION; make -C src/main/c prebuilt; make[1]: Entering directory `/mnt/tm",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3589,deploy,deploy,3589,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQihd; __RESOURCE__1=33qZtfwg.bed; __RESOURCE__2=33qZtfwg.bim; __RESOURCE__3=33qZtfwg.fam; __RESOURCE_GROUP__2=YXS0tQKi; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}. # __TASK__5 shapeit; __RESOU",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:1992,pipeline,pipeline,1992,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,1,['pipeline'],['pipeline']
Deployability,-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-dock,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4391,deploy,deploy-,4391,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,". using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/lib",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8137,deploy,deployMode,8137,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['deploy'],['deployMode']
Deployability,". using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1339,deploy,deployMode,1339,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['deploy'],['deployMode']
Deployability,".. and I hand-deployed the router and it looks good. Most of our existing services can't handle being located at internal.hail.is/ns/svc, so I'm going to make a series of changes to fix that, possibly folded into my auth changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117:14,deploy,deployed,14,https://hail.is,https://github.com/hail-is/hail/pull/6928#issuecomment-524334117,1,['deploy'],['deployed']
Deployability,".........50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16532,deploy,deploy,16532,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,".8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17214,pipeline,pipeline,17214,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqO,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9081,deploy,deploy,9081,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,".zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or so. Thanks for your patience!; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, v",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1419,install,install,1419,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['install'],['install']
Deployability,"//github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 3520, 51 row lock(s), undo log entries 30; MySQL thread id 1941930, OS thread handle 140298159240960, query id 1869168731 10.32.3.8 dgoldste update; INSERT INTO batch_inst_coll_cancellable_resources (batch_id, inst_coll,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:1270,UPDATE,UPDATE,1270,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,1,['UPDATE'],['UPDATE']
Deployability,"/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6957,install,install,6957,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3142,deploy,deploy-config,3142,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['deploy'],['deploy-config']
Deployability,"/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-an",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1191,install,installer,1191,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['installer']
Deployability,"/hail/pull/14170#discussion_r1473442106). Since this is merely propagating existing bad behavior, let's change it separately so as to keep the conceptual overhead of this change as small as possible. Please make an issue and paste the link into each comment so that we can later track how we resolved each comment. . > Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclination is for everything to represent a sum total over the direct jobs and jobs within any descendant groups. From here on out ""sum total"" means exactly that. OK, so:. 1. In the UI (database should do what makes sense and is fast), the number of jobs should be t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021:1109,UPDATE,UPDATE,1109,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021,2,['UPDATE'],['UPDATE']
Deployability,"/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1278,deploy,deploy,1278,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability,"/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' f",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1301,install,installed,1301,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['install'],['installed']
Deployability,/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:15083,release,release,15083,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"/step1__select_samples.py; @@ -38,14 +38,7 @@ def hemi_expr(mt):; ; def main(args):; ; - hl.init(log=""/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); - meta_ht = hl.import_table(args.sample_metadata_tsv, force_bgz=True); - meta_ht = meta_ht.key_by(""s""); - meta_ht = meta_ht.filter(hl.is_defined(meta_ht.cram_path) & hl.is_defined(meta_ht.crai_path), keep=True); - meta_ht = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants with at least one non-ref GT""); mt = mt.filter_rows(hl.agg.any(mt.GT.is_non_ref())); ; - #logger.info(f""Saving checkpoint""); - #mt = mt.checkpoint(os.path.join(args.temp_bucket, ""readviz_select_samples_checkpoint1.vds""),; - # overwrite=True, _read_if_exists=True); + logger.info(f""Saving checkpoint""); + mt = mt.checkpoint(""readviz_select_samples_checkpoint1.vds"",; + overwrite=True, _read_if_exists=True); ; def sample_ordering_expr(mt):; """"""For variants that are present in more than 10 samples (or whatever number args.num_samples is set to),; ```. And tried running the bad step:. ```bash; python3 step1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:1587,release,release,1587,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,1,['release'],['release']
Deployability,"0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 portalocker-2.7.0 protobuf-3.20.2 py4j-0.10.9.5 pyasn1-0.5.0 pyasn1-modules-0.3.0 pycares-4.3.0 pycparser-2.21 pygments-2.16.1 pyjwt-2.8.0 python-dateutil-2.8.2 python-json-logger-2.0.7 pytz-2023.3.post; 1 pyyaml-6.0.1 regex-2023.8.8 requests-2.31.0 requests-oauthlib-1.3.1 rich-12.6.0 rsa-4.9 s3transfer-0.6.2 scipy-1.11.2 six-1.16.0 sortedcontainers-2.4.0 tabulate-0.9.0 tenacity-8.2.3 tornado-6.3.3 typer-0.9.0 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16 uvloop-0.17.0 wrapt-1.15.0 xyzservices; -2023.7.0 yarl-1.9.2. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; python3 -m pip uninstall -y hail; WARNING: Skipping hail as it is not installed.; python3 -m pip install build/deploy/dist/hail-0.2.124-py3-none-any.whl --no-deps; Defaulting to user installation because normal site-packages is not writeable; Processing ./build/deploy/dist/hail-0.2.124-py3-none-any.whl; Installing collected packages: hail; Successfully installed hail-0.2.124. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; hailctl config set query/backend spark; </p>; </details>",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:44410,release,release,44410,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,15,"['Install', 'deploy', 'install', 'release', 'update', 'upgrade']","['Installing', 'deploy', 'install', 'installation', 'installed', 'release', 'update', 'upgrade']"
Deployability,"0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' p",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30754,release,release,30754,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['release'],['release']
Deployability,"0.2.125 was released with a fix, but it also contained a critical correctness bug. We will try again with 0.2.126.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13712#issuecomment-1785301300:12,release,released,12,https://hail.is,https://github.com/hail-is/hail/issues/13712#issuecomment-1785301300,1,['release'],['released']
Deployability,0.2.35 is a bit of a dud release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8469#issuecomment-609464676:25,release,release,25,https://hail.is,https://github.com/hail-is/hail/issues/8469#issuecomment-609464676,1,['release'],['release']
Deployability,"019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: far",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8253,install,install,8253,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1455,install,install,1455,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,0=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3906,deploy,deploy-,3906,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,0=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9382,deploy,deploy-,9382,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,1. Doesn't something need hail-ci-0-1-deploy?. 2. We should also document the names contents of the k8s secrets that package these tokes up. (Imagine someone trying to rebuild the cluster from scratch. Like me.),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430367241:38,deploy,deploy,38,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430367241,1,['deploy'],['deploy']
Deployability,"1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your add-version-endpoint branch?. One more change request: can we remove the try-catch? I don't expect any errors in that call and I tend to avoid revealing anything about errors to our users. Aiohttp will log the error and the stack trace if you let it rise all the way. Sorry for all the complication! Our build system is the second service we built and is clearly showing its age.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:2025,install,installs,2025,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,2,"['install', 'update']","['installs', 'updated']"
Deployability,"10:14:07 Hail: INFO: Loading 76 fields. Counts by type:; 66 fields: user-specified int32; 6 fields: user-specified str; 3 fields: user-specified float64; 1 field: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5462,install,install,5462,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2753,deploy,deploy-,2753,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4559,release,release,4559,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"2-debian11` (see [here](https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/start.py#L147)) which [was released in January 2023](https://cloud.google.com/dataproc/docs/release-notes#January_23_2023). The latest available version of [Dataproc's Debian images](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) is 2.1.25-debian11 which depends on GoogleCloudDataproc hadoop connector version [2.2.15](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/2.2.15) which relies on Google Cloud Storage client library version [2.22.3](https://github.com/GoogleCloudDataproc/hadoop-connectors/commit/8b79f025ef5e8231de827f4c620cd23e230c3489). I have [a PR](https://github.com/hail-is/hail/pull/13732) to upgrade us to 2.27.1 because the library broke retries in versions [2.25.0, 2.27.0). AFAICT, Google's image version page only shows the most recent five. There's no way to go back further in time. Luckily, the way back machine has [a March 2023 capture](https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) which includes our version. 2.1.2-debian11 used Google Cloud Dataproc hadoop connector version [2.2.9](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.9) This version of the hadoop connector was [using some alpha version of a gRPC version of the cloud storage library](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/18f6e9f1c745e1854d76bea9362e2332898d8895/pom.xml#L96C1-L97C1). I'm not sure what's up with that. OK, here's my proposal: let's change that IMAGE_VERSION to the latest one and see if that fixes things. If that works, let's just merge and forget this happened. If that *doesn't* work, we gotta wade into the Lovecraftian horror of JARs. Most likely we're not fully relocating the dependencies pulled in by the Google Cloud Storage client libraries and they conflict with what Dataproc produces.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645:1459,release,release-,1459,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645,2,['release'],"['release-', 'releases']"
Deployability,"2079a85bce (41 minutes):. I could try to make the tests even more fine-grained and split up even more long-running tests. Seems like some of the bottlenecks I'm hitting now are:; 1. Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency.; 2. The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down). I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. <img width=""2032"" alt=""Screen Shot 2023-05-22 at 12 30 47"" src=""https://github.com/hail-is/hail/assets/106194/aaa3fbb7-176d-4487-b65e-586c235e2089"">; <img width=""541"" alt=""Screen Shot 2023-05-22 at 12 31 23"" src=""https://github.com/hail-is/hail/assets/106194/016f1089-d08d-4555-ae86-c01353f39c78"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015:226,install,installed,226,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015,1,['install'],['installed']
Deployability,"2796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5977,install,install,5977,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,28e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:13213,release,release,13213,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16790,deploy,deploy,16790,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['deploy'],['deploy']
Deployability,"4 Hail: INFO: Number of samples in BGEN files: 487409; 2019-01-22 00:19:54 Hail: INFO: Number of variants across all BGEN files: 1261158; ----------------------------------------; Global fields:; None; ----------------------------------------; Column fields:; 's': str; ----------------------------------------; Row fields:; 'locus': locus<GRCh37>; 'alleles': array<str>; 'rsid': str; 'varid': str; ----------------------------------------; Entry fields:; 'GT': call; 'GP': array<float64>; 'dosage': float64; ----------------------------------------; Column key: ['s']; Row key: ['locus', 'alleles']; ----------------------------------------; [Stage 0:> (0 + 16) / 292]Traceback (most recent call last):; File ""/restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/bgen_count.py"", line 13, in <module>; print(""Count:"",mt.count()); File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py"", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:7394,install,install,7394,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['install'],['install']
Deployability,"507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1014,configurat,configuration,1014,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,1,['configurat'],['configuration']
Deployability,"56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""b",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1442,deploy,deployment,1442,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,1,['deploy'],['deployment']
Deployability,7b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE 2>&1 | tee ${__RESOURCE_FILE__749}; State: Waiting; Reason: ContainerCreating; Ready: False; Restart Count: 0; Requests:; cpu: 1; memory: 500M; Environment:; POD_IP: (v1:status.podIP); POD_NAME: batch-2554-job-4-main-vsk7h (v1:metadata.name); Mounts:; /gsa-key from gsa-key (rw); /io from batch-2554-job-4-8vvgl (rw); /var/run/secrets/kubernetes.io/serviceaccount from default-token-8h99c (ro); Conditions:; Type Status; Initialized True ; Ready False ; ContainersR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:17313,pipeline,pipeline,17313,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1336,install,install,1336,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['install']
Deployability,"9fxo7/compare/{base}...{head}"",; ""merges_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/merges"",. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 7030 100 6999 100 31 9306 41 --:--:-- --:--:-- --:--:-- 9319; ""archive_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/{archive_format}{/ref}"",; ""downloads_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/downloads"",; ""issues_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/issues{/number}"",; ""pulls_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/pulls{/number}"",; ""milestones_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/milestones{/number}"",; ""notifications_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/notifications{?since,all,participating}"",; ""labels_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/labels{/name}"",; ""releases_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/releases{/id}"",; ""deployments_url"": ""https://api.github.com/repos/hail-ci-test/ci-test-p4a9fxo7/deployments"",; ""created_at"": ""2018-10-10T00:32:59Z"",; ""updated_at"": ""2018-10-10T00:32:59Z"",; ""pushed_at"": ""2018-10-10T00:33:00Z"",; ""git_url"": ""git://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""ssh_url"": ""git@github.com:hail-ci-test/ci-test-p4a9fxo7.git"",; ""clone_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7.git"",; ""svn_url"": ""https://github.com/hail-ci-test/ci-test-p4a9fxo7"",; ""homepage"": null,; ""size"": 0,; ""stargazers_count"": 0,; ""watchers_count"": 0,; ""language"": null,; ""has_issues"": true,; ""has_projects"": true,; ""has_downloads"": true,; ""has_wiki"": true,; ""has_pages"": false,; ""forks_count"": 0,; ""mirror_url"": null,; ""archived"": false,; ""open_issues_count"": 0,; ""license"": null,; ""forks"": 0,; ""open_issues"": 0,; ""watchers"": 0,; ""default_branch"": ""master"",; ""permissions"": {; ""admin"": true,; ""push"": true,; ""pull"": true; },; ""allow_squash_merge"": true,; ""allow_merge_commit"": true,; ""allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858:5676,release,releases,5676,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-429024858,1,['release'],['releases']
Deployability,": 1.874s; grm: 1m5.9s; ```. The respective numbers using this PR are 9m36s, 39s, and 12s (so a ~5x speedup in the last case). I'd like to try this on a cluster as well with profile225k. Comparing the output for 10k, the doubles look to agree to around 16 digits (we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoft",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1398,update,updates,1398,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['update'],['updates']
Deployability,": i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https://github.com/broadinstitute/hail/files/417544/gradle_check_info1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1692,install,installed,1692,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,3,"['Install', 'install']","['Installed', 'installed']"
Deployability,": sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazon-ebs: Installing build dependencies: started; 905 | amazon-ebs: Installing build dependencies: finished with status 'done'; 906 | amazon-ebs: Getting requirements to build wheel: started; 907 | amazon-ebs: Getting requirements to build wheel: finished with status 'done'; 908 | amazon-ebs: Preparing metadata (pyproject.toml): started; 909 | amazon-ebs: Preparing metadata (pyproject.toml): finished with status 'done'; 910 | amazon-ebs: Collecting azure-identity==1.6.0; 911 | amazon-ebs: Downloading azure_identity-1.6.0-py2.py3-none-any.whl (108 kB); 912 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.5/108.5 kB 28.5 MB/s eta 0:00:00; 913 | amazon-ebs: Collecting azure-storage-blob==12.11.0; 914 | amazon-ebs: Downloading azure_storage_blob-12.11.0-py3-none-any.whl (346 kB); 915 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 41.0 MB/s eta 0:00:00; 916 | amazon-ebs: Collecting bokeh<2.0,>1.3; 917 | amazon-ebs: Downloading bokeh-1.4.0.tar.gz (32.4 MB); 918 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 32.4/32.4 MB 48.4 MB/s eta 0:00:00; 919 | amazon-ebs: Preparing metadata (setup.py): started; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:2407,Install,Installing,2407,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,2,['Install'],['Installing']
Deployability,"; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs: Downloading asyncinit-0.2.4-py3-none-any.whl (2.8 kB); 901 | amazon-ebs: Collecting avro<1.12,>=1.10; 902 | amazon-ebs: Downloading avro-1.11.1.tar.gz (84 kB); 903 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.2/84.2 kB 22.0 MB/s eta 0:00:00; 904 | amazo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1390,install,install,1390,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['install']
Deployability,; adding 'hailtop/batch/docker.py'; adding 'hailtop/batch/exceptions.py'; adding 'hailtop/batch/globals.py'; adding 'hailtop/batch/hail_genetics_images.py'; adding 'hailtop/batch/job.py'; adding 'hailtop/batch/resource.py'; adding 'hailtop/batch/utils.py'; adding 'hailtop/batch_client/__init__.py'; adding 'hailtop/batch_client/aioclient.py'; adding 'hailtop/batch_client/client.py'; adding 'hailtop/batch_client/globals.py'; adding 'hailtop/batch_client/parse.py'; adding 'hailtop/batch_client/types.py'; adding 'hailtop/cleanup_gcr/__init__.py'; adding 'hailtop/cleanup_gcr/__main__.py'; adding 'hailtop/config/__init__.py'; adding 'hailtop/config/deploy_config.py'; adding 'hailtop/config/user_config.py'; adding 'hailtop/config/variables.py'; adding 'hailtop/fs/__init__.py'; adding 'hailtop/fs/fs.py'; adding 'hailtop/fs/fs_utils.py'; adding 'hailtop/fs/router_fs.py'; adding 'hailtop/fs/stat_result.py'; adding 'hailtop/hailctl/__init__.py'; adding 'hailtop/hailctl/__main__.py'; adding 'hailtop/hailctl/deploy.yaml'; adding 'hailtop/hailctl/describe.py'; adding 'hailtop/hailctl/auth/__init__.py'; adding 'hailtop/hailctl/auth/cli.py'; adding 'hailtop/hailctl/auth/create_user.py'; adding 'hailtop/hailctl/auth/delete_user.py'; adding 'hailtop/hailctl/auth/login.py'; adding 'hailtop/hailctl/batch/__init__.py'; adding 'hailtop/hailctl/batch/batch_cli_utils.py'; adding 'hailtop/hailctl/batch/cli.py'; adding 'hailtop/hailctl/batch/initialize.py'; adding 'hailtop/hailctl/batch/list_batches.py'; adding 'hailtop/hailctl/batch/submit.py'; adding 'hailtop/hailctl/batch/utils.py'; adding 'hailtop/hailctl/batch/billing/__init__.py'; adding 'hailtop/hailctl/batch/billing/cli.py'; adding 'hailtop/hailctl/config/__init__.py'; adding 'hailtop/hailctl/config/cli.py'; adding 'hailtop/hailctl/config/config_variables.py'; adding 'hailtop/hailctl/dataproc/__init__.py'; adding 'hailtop/hailctl/dataproc/cli.py'; adding 'hailtop/hailctl/dataproc/cluster_config.py'; adding 'hailtop/hailctl/dataproc/c,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:26757,deploy,deploy,26757,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,; at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRe,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9228,deploy,deploy,9228,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,"; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hailctl/dev/config/cli.py b/hail/python/hailtop/hailctl/dev/config/cli.py; index c032e7731..d293b07cf 100644; --- a/hail/python/hailtop/hailctl/dev/config/cli.py; +++ b/hail/python/hailtop/hailctl/dev/config/cli.py; @@ -1,6 +1,6 @@; import os; import json; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; ; def init_parser(parser):; @@ -35,6 +35,6 @@ def main(args):; }; ; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); with open(config_file, 'w') as f:; f.write(json.dumps(config)); -- ; 2.23.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:4744,deploy,deploy-config,4744,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,2,['deploy'],['deploy-config']
Deployability,"; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRU",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16900,pipeline,pipeline,16900,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"=registry,ref=gcr.io/hail-vdc/foo; ```; Will use as a cache source the `latest` tag in the `gcr.io/hail-vdc/foo` repository. It is *not* sufficient for an image to be present in the repository and untagged or with a different tag from `latest`. In particular, every push to the `cache` tag prevents us from using other images even though they are in the registry! For example, I pushed two images to `cache`:. ```; (base) # gcloud container images list-tags gcr.io/hail-vdc/dktest; DIGEST TAGS TIMESTAMP; fb551d9bdb94 2022-06-10T14:16:39; afb4c5ad2d7b cache,latest 2022-06-10T14:15:55; ```. If I rebuild [1] the most recently pushed image with; ```; --import-cache type=registry,ref=gcr.io/hail-vdc/dktest:cache; ```; it succeeds in getting the cache. If I rebuild the other image with the same import-cache, it does not see that the (untagged) image is already there! . ---. This all suggests that all our attempts at image caching are failing terribly. Options:; 1. Only deploy builds push to a `:cache` tag, everyone uses that tag.; 2. List all the tags in the repository and include them all as --cache-from's (this doesn't actually work: https://github.com/moby/moby/issues/34715#issuecomment-425933774); 3. Push a tag for each git SHA and then include as --cache-from's the last ten git SHAs on this branch, the most recent common commit with main (i.e. `git merge-base origin/main this-branch`), maybe the current main, and maybe the PR number?; 4. Write our own OCI image builder so we can write our own OCI image cache that actually works the way it ought to (everything in the registry is considered fair game for the cache). It seems like 3 is actually a decent solution that should enable lots of caching.; 1. The last ten SHAs on the branch should speed up repeated builds when you're fixing little bugs.; 2. The most recent common commit with main should avoid rebuilds unless the packages changed.; 3. I suspect the current main is actually not helpful (either 2 will work or 3 wouldn't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800:1543,deploy,deploy,1543,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152646800,1,['deploy'],['deploy']
Deployability,"> (If you want to take a look at a live instance, I've deployed to my namespace and you can look at the batch logs here: https://ci.hail.is/batches/634 ). looks great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-548104248:55,deploy,deployed,55,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-548104248,1,['deploy'],['deployed']
Deployability,"> > > Can you clarify the reasoning for replicating this in a sub-folder? It's much harder to review this change when there's a huge diff and I'm supposed to ignore certain things but those things actually subtly differ from the originals (see the Dockerfile).; > > > If there's some issue on Feb 1 and we're not confident for Feb 2, we'll just use git to revert to an old version.; > > ; > > ; > > Sure, clean separation between the two projects. It's 512 lines now, 434 from notebook.py; > ; > There aren't two projects though. We're updating notebook to use the new authentication system. A diff helps leverage my understanding of the previous notebook to understanding the proposed new notebook. They are different enough at this point that tying one to the other doesn't make much sense to me. But the bigger reason is that if needed, it will be easier to restore notebook from a pristine file, then look back through git commit history to the first breaking change. The goal should always be to introduce as few breaking changes as possible before a public demonstration. Once that is done, I don't mind doing something else. I've had the unfortunate pleasure of doing this with large-ish user-facing deployments (thousands of users), and it's not a fun experience. Git works well, but under the pressure of public-facing issues, entropy is not a friend.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065939:1207,deploy,deployments,1207,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065939,1,['deploy'],['deployments']
Deployability,> > @daniel-goldstein should this PR update line 38 of main.tf which uses Ubuntu 20.04 for the batch worker? Do we need to take special action if that happens?; > > If we do this we also need to change `infra/azure/create_bootstrap_vm.sh` which references `focal`.; > ; > @jigold Is this terraform resource needed in the gallery in order for `az-create-worker-image.sh` to run? It's unclear to me whether `az-create-worker-image.sh` references this shared image. @jigold bump,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13440#issuecomment-1688946114:37,update,update,37,https://hail.is,https://github.com/hail-is/hail/pull/13440#issuecomment-1688946114,1,['update'],['update']
Deployability,"> > Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?; > ; > Yes, sorry for the confusion; > ; > > Also another question is how does the schema update enforce certain order of operations.; > > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?; > ; > Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database. That's why I said this:; > ; > > > I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename.; > ; > I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary. @ehigham Thanks for your comments above. I’ve added the triggers and stored procedures referencing the `job_groups_cancelled` table in `rename-job-groups-cancelled-column.sql`. I was initially confused by `estimate-current.sql`; I thought it was a system-generated file to track the latest batch DDLs after a schema update, rather than a file that is manually updated. After reading this [thread](https://hail.zulipchat.com/#narrow/stream/300487-Hail-Batch-Dev/topic/mysqldump), I completely agree with your point. In other organizations I've worked with, we maintained schema changes in a separate folder, identified by release versions (e.g., semver) and the DLLs are ordered by sequence number. This way, we had a clear history of DDLs and the order they were applied, eliminating the need for files like `estimate-current.sql`. I just have one question: Do we need to manually update `estimate-current.sql` with the schema changes from `rename-job-groups-cancelled-column.sql`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612:174,update,update,174,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2340993612,5,"['release', 'update']","['release', 'update', 'updated']"
Deployability,"> > I should fix it!; > ; > That'd be awesome!; > ; > Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet. Thank you for the background. Makes sense",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311:159,deploy,deployed,159,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540035311,1,['deploy'],['deployed']
Deployability,"> > I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.; > ; > Can you elaborate more? I'm not sure which code paths you are referring to. Mainly that the commit procedure branches on whether the start id is 1 and that we sometimes grab the update id from the batch token and sometimes from the update table. Not very different code paths but slightly different, which could lead to some confusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403:147,update,updates,147,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220825403,4,['update'],"['update', 'updates']"
Deployability,"> > I think we should defer folder creation as an improvement to jgscm; > ; > That's fine with me. As a workaround, users can create folders with gsutil for the time being.; > ; > I think we should just fork it into the hail-is organization, fix it there, install from that repo in our build (don't need to go so far as to publish), and offer changes upstream, which they can take or not. Sound good?. Yep. Shooting to get the fix PRd today.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480349251:256,install,install,256,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480349251,1,['install'],['install']
Deployability,"> > New config should be correct.; > > As an example of this slash issue, the following config (deployed right now) doesn't work.; > ; > I'm confused, is it correct, or does it not work? If there's an issue upstream, it needs to get fixed, too. The new config addresses the requested changes on the config. When this pr was created there was a claim that the redirection to service.internal was not found in gateway. That appears to be not the case (or at least the internal router doesn't seem to be receiving those requests) and still needs to be investigated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715:96,deploy,deployed,96,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541102715,1,['deploy'],['deployed']
Deployability,"> > We have a 35K cohort. The VCF format of chr1 is 2.4T.; > ; > Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" ([samtools/hts-specs#434](https://github.com/samtools/hts-specs/pull/434)) and ""reference blocks"" ([samtools/hts-specs#435](https://github.com/samtools/hts-specs/pull/435)). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF.; > ; > What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. Looks like VDS is a better solution than HailMatrix. However, we got the joint call result as vcf alreay. Can VDS Combiner read joint call VCF and then save it as VDS format? I cannot find any example to transfer VCF to VDS. Thanks. > ; > > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11.; > ; > Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement fo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344:589,pipeline,pipeline,589,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1937459344,1,['pipeline'],['pipeline']
Deployability,> @daniel-goldstein should this PR update line 38 of main.tf which uses Ubuntu 20.04 for the batch worker? Do we need to take special action if that happens?; > ; > If we do this we also need to change `infra/azure/create_bootstrap_vm.sh` which references `focal`. @jigold Is this terraform resource needed in the gallery in order for `az-create-worker-image.sh` to run? It's unclear to me whether `az-create-worker-image.sh` references this shared image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13440#issuecomment-1679634688:35,update,update,35,https://hail.is,https://github.com/hail-is/hail/pull/13440#issuecomment-1679634688,1,['update'],['update']
Deployability,> @iris-garden were you able to create a dataproc cluster and run some trivial hail code (like `hl.utils.range_table(10)._force_count()`) after this change?. Nope! This was just to make sure the pipeline succeeds first. Circling back to the dataproc part now; about to ask the team for some help on that on Zulip.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12965#issuecomment-1535141599:195,pipeline,pipeline,195,https://hail.is,https://github.com/hail-is/hail/pull/12965#issuecomment-1535141599,1,['pipeline'],['pipeline']
Deployability,> @tpoterba do we think this is the actual cause of the 2.4.2 incompatibility?. Didn't you see things fail on initialization? That's not related to serialization. > how do we verify this fixes the issues our users are seeing?. The people seeing those errors are using jars not compiled for the version of Spark (and json4s) they have installed. I don't think we need to test for this case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526:334,install,installed,334,https://hail.is,https://github.com/hail-is/hail/pull/5958#issuecomment-486844526,1,['install'],['installed']
Deployability,"> A couple notes. I'm not sure what this should ultimately look like. I think I want `hailctl create user`, which probably creates a batch that does all the creation? That avoids setting up tunnels to the sql server, etc. Sure, if that's the way we want this to work, I will modify to do that. Seems more elegant than having tunnels, which predates our current deployment solution.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206:361,deploy,deployment,361,https://hail.is,https://github.com/hail-is/hail/pull/6619#issuecomment-515501206,1,['deploy'],['deployment']
Deployability,"> A user reported this error concurrent.futures._base.TimeoutError with no stack trace while copying files in a batch job. . This. No stack trace. If you look at this output, the previous stack trace is part of the WARNING message. ```; INFO:deploy_config:deploy config file not found: None; INFO:hailtop.aiocloud.aiogoogle.credentials:using credentials file /gsa-key/key.json: GoogleServiceAccountCredentials for XXXXX@PROJECT.iam.gserviceaccount.com; WARNING:hailtop.utils:Encountered 2 errors (current delay: 0.2). My stack trace is File ""/usr/lib/python3.7/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/usr/lib/python3.7/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/usr/local/lib/python3.7/dist-packages/hailtop/aiotools/copy.py"", line 128, in <module>; asyncio.run(main()); File ""/usr/lib/python3.7/asyncio/runners.py"", line 43, in run; return loop.run_until_complete(main); File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 735, in retry_transient_errors; st = ''.join(traceback.format_stack()); . Most recent error was; Traceback (most recent call last):; File ""/usr/local/lib/python3.7/dist-packages/hailtop/utils/utils.py"", line 729, in retry_transient_errors; return await f(*args, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/hailtop/httpx.py"", line 134, in request_and_raise_for_status; resp = await self.client_session._request(method, url, **kwargs); File ""/usr/local/lib/python3.7/dist-packages/aiohttp/client.py"", line 634, in _request; break; File ""/usr/local/lib/python3.7/dist-packages/aiohttp/helpers.py"", line 721, in __exit__; raise asyncio.TimeoutError from None; concurrent.futures._base.TimeoutError; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330:256,deploy,deploy,256,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117642330,1,['deploy'],['deploy']
Deployability,"> Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path f'{path}/foo, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like '{path}zzzzz/foo or '{path}szzzzz. We need to ignore these as they don't provide any information on whether {path} is a file or directory. This is where isChildOf is needed because we need to make sure the blob is actually a child of the path such as '{path}/file and not {path}zzzzz/file. Ah thanks, this makes more sense to me now. > The other option is to do 2 queries. One to check if it's a file and the next to check if there are any child blobs. This does sound simpler conceptually. I'm not sure off the top of my head what is better performance-wise: the two network requests for a directory or loading a whole page of results in a single request when we only need max 2 results. What do you prefer?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1677489464,1,['update'],['update']
Deployability,"> Ah, I commented on both PRs by accident. My bad, sorry! In the future, keeping each PR to a single commit can help the reviewer identify the right changes to review for stacked PRs. Please clarify. Unless I misunderstand, 1 commit per PR isn’t something that we currently hold ourselves to. For instance the PR you issued for site deploy has something like 10 commits. Maybe we can specify this in a design doc, much like John and you did with the PR message? This would be a useful place to specify related issues, like settling on rebasing vs merging. . My understanding of our use of the stacked label is that the dependent PR isn’t meant to be reviewed until the child is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132:333,deploy,deploy,333,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638964132,1,['deploy'],['deploy']
Deployability,"> Are you running that `gradlew` command directly? You should be using `make`. https://hail.is/docs/0.2/getting_started_developing.html. I followed the steps on the site but failed to deploy to aws ; `; Invalid input: Exception in thread ""main"" java.io.FileNotFoundException: No such file or directory 's3://xxxxx/artifacts/hail-python.zip'`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-908570000:184,deploy,deploy,184,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-908570000,1,['deploy'],['deploy']
Deployability,"> As an aside, we should definitely have a `HAIL_BATCH_BACKEND` and associated config variables. There is no end to my annoyance that `hb.Batch()` gives me a local backend batch.; > ; > It seems to me that, given the precedent of `hailctl dataproc submit`, `hailctl batch submit` conveys the intent to use QoB or Batch-in-Batch, not ""local mode Batch"" or ""local mode Hail"". It seems very reasonable to have a `--local-mode-query` override (I think we should ignore local mode Batch as much as possible since container-in-container is fraught).; > ; > We need a better name for local mode Spark or Query. I'm slowly realizing that lots of people don't realize you can use Hail on a laptop. Are there other tools that have already settled on terminology here?. @danking Wait so shall I set `HAIL_QUERY_BACKEND=batch`? I can follow up with a PR that adds a similar configuration variable for batch so we don't default to the local backend, but it would be nice to keep this behavior consistent from the start",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324259408:862,configurat,configuration,862,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324259408,1,['configurat'],['configuration']
Deployability,"> Banning versions completely is a little tricky because the user can specify a JAR url directly instead of a version. JARs don't currently have a simple way to report pip version to the worker, though we could cook something up. We could also just delete the old JARs. I feel like we should make a (cached) request to ensure that the JAR exists in the front-end upon job submission and return a 400 if it doesn't exist instead of waiting for the worker to error. It would:. - Allow us to remove support by deleting old jars; - Fail fast (I know I have accidentally messed up deploying a dev jar and had to wait until a worker came online to find out); - Avoid alerts from workers that can't find dev jars due to mistakes like I mention above",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664:576,deploy,deploying,576,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1529754664,1,['deploy'],['deploying']
Deployability,"> CVE-2019-11245 is a vulnerability in k8s that causes some (all?); > containers without a runAsUser configuration to run; > as user id 0, i.e. root. Jupyter refuses to start as root.; > This change enables Jupyter to start successfully. Yeah, causes all containers to run as root upon restart of the container.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6678#issuecomment-512828240:101,configurat,configuration,101,https://hail.is,https://github.com/hail-is/hail/pull/6678#issuecomment-512828240,1,['configurat'],['configuration']
Deployability,> Can I also see the changes that follow after this set of PRs with the interaction between the client and how you account for update ids that aren't hardcoded to be 1?. Yes I'll put up a PR and tag you in it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12154#issuecomment-1245762083:127,update,update,127,https://hail.is,https://github.com/hail-is/hail/pull/12154#issuecomment-1245762083,1,['update'],['update']
Deployability,"> Can you lock down this behaviour with a test?. As I said in the description, I tried hard to write a test, but I couldn't manage to find a way to exercise this with a targeted test. And this bug is currently blocking a user, so I want to get it released asap. I can try again, but I suspect it would end up being a day or two of extra work, and would likely still end up brittle and unsatisfactory. I'd rather spend my effort getting back into the line of work that would eventually make it much easier to target specific compiler code paths.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14673#issuecomment-2338662541:247,release,released,247,https://hail.is,https://github.com/hail-is/hail/pull/14673#issuecomment-2338662541,1,['release'],['released']
Deployability,"> Could you explain this further? The navbar will also not be deployed until the next release, right?. No, the navbar is for all of the UI pages that aren't part of the site.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-612086823:62,deploy,deployed,62,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612086823,2,"['deploy', 'release']","['deployed', 'release']"
Deployability,"> Could you explain why you think the boundary is invalid?. Boundary can only be used by consumers who generate an iterator with a new context. Here we were just inserting a clear before each next(), no matter who was consuming the iterator. The particular pipeline that triggered this error was a TableMapPartitions with a ToArray(Ref rows) after an IR with a repartitionedOrderedRDD2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263:257,pipeline,pipeline,257,https://hail.is,https://github.com/hail-is/hail/pull/9425#issuecomment-689597263,1,['pipeline'],['pipeline']
Deployability,"> Could you help me understand what the benefit of gidgethub is over our previous strategy? Since we generally prefer minimal deps. This isn't more deps. I switched gidgethub for PyGithub. gidgethub is async, PyGithub is not. Now that we're using aiohttp over Flask, if we don't use an async library to query Github handling web requests will hang while the synchronous library blocks waiting for requests. Actually, this is fewer deps. We already use gidgethub in CI, so overall this removes the PyGithub dependency. Since both CI and scorecard depend on gidgethub, I moved it from the CI-specific installation to the service base image.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870:599,install,installation,599,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531942870,1,['install'],['installation']
Deployability,"> Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?. Yes, sorry for the confusion. > Also another question is how does the schema update enforce certain order of operations.; > ; > The rename-job-groups-cancelled-column sql should run before other sqls that depend on the modified column name in job_groups_cancelled table, correct?. Migrations are applied successively. You cannot edit a previous migration or the order in which they're applied as they've already been applied to the production database.; That's why I said this:. >> I fear you'll have to take inspiration from rename-job-groups-tables.sql by applying one ALTER TABLE command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. I think you need to find any trigger or stored procedure that references that column, drop it and recreate it with the field renamed. It's a little scary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2339054214:160,update,update,160,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2339054214,1,['update'],['update']
Deployability,"> Do you mean someone watching `hail-is/hail:main` but only deploying occasionally? I feel like we need to separate out publish steps entirely from our CD steps, track our most recently published tag/sha, and spin off publish batches when the tag is no longer the most recent on GitHub. Yeah this is what I had in mind. Agreed, a separate publish vs deploy flow could help. Though, it doesn't address the backwards compatibility issue you mentioned. Ideally we could get all of this right rather than just fixing part of the problem.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985736047:60,deploy,deploying,60,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985736047,2,['deploy'],"['deploy', 'deploying']"
Deployability,"> Even if we don't guarantee stable sort, we might hope to guarantee determinism (no promise what answer you get, but you'll get the same answer each time you run the same pipeline). I think the associative combiner might be enough to ensure determinism. Though if you want to leave it nondeterministic for now, and wait for the better fix of guaranteeing stable sort by appending indices to the sort keys, that's fine with me. Ah, you're right, yeah.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6837#issuecomment-519635282:172,pipeline,pipeline,172,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519635282,1,['pipeline'],['pipeline']
Deployability,"> For an example, let's take DOCKER_PREFIX=gcr.io/hail-vdc. > If a user submits a job using `hailgenetics/python-dill`, what do we do? We see that `image_ref.name()` in `HAIL_GENETICS_IMAGES`, so the current implementation adds the domain gcr.io and image_ref.path becomes `hail-vdc/gcr.io/hailgenetics/python-dill`. Clearly that is incorrect. The proposed change would make the path `hail-vdc/hailgenetics/python-dill`. This does in fact exist! However, this doesn't feel like the intended path based on this part of `publish-public-images.sh`, which suggests it should be `hail-vdc/python-dill`:. Hmm, the proposed implementation actually prints 'hail-vdc/python-dill' for `image_ref.path`:. ```; image_ref = parse_docker_image_reference('hailgenetics/python-dill'); # image_ref.path == 'python-dill'. if image_ref.name() in HAIL_GENETICS_IMAGES:; image_ref.domain = DOCKER_PREFIX.split('/', maxsplit=1)[0] # 'gcr.io'; image_ref.path = '/'.join(DOCKER_PREFIX.split('/')[1:] + [image_ref.path]) . # image_ref.path == 'hail-vdc/python-dill'; ```. Though it should probably be `hail-vdc/hailgenetics/python-dill`? (I also just updated `publish-public-images.sh` to push to `$docker_prefix/hailgenetics/$name`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10382#issuecomment-828117089:1126,update,updated,1126,https://hail.is,https://github.com/hail-is/hail/pull/10382#issuecomment-828117089,1,['update'],['updated']
Deployability,"> Forks would indeed need to overwrite ours, but since the file wouldn't change much it seems like that's not much of a hassle to maintain, right Leo? And ya this seems like a fine change but we would need to follow up right afterward with our own credentials. Yes, alternatively we could also use the GitHub organization name or something similar when constructing the file path to encrypted credentials, to avoid collisions completely. (Forks like the CPG one would only add files to their deployments.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130:492,deploy,deployments,492,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1171065130,1,['deploy'],['deployments']
Deployability,"> Github doesn't have a way to say ""stacked on this PR"", so removal of the stacked tag is a manual gating. I'm surprised it doesn't have a way to do this, since when a PR is mentioned by a link or hash, that dependency is updated with its dependent. . Looks like this may be coming: https://twitter.com/natfriedman/status/1170804894241972224",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314:222,update,updated,222,https://hail.is,https://github.com/hail-is/hail/issues/7417#issuecomment-548479314,1,['update'],['updated']
Deployability,"> Hmm. Maybe the env-setup script should also set the default region/zone to us-central1-a?. Yep, that would remove the need for --zone or --region. We should probably document that part of the configuration, since its likely what prevented the get-credentials command from working for me as provided.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5022#issuecomment-449278737:194,configurat,configuration,194,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449278737,1,['configurat'],['configuration']
Deployability,"> How did you verify only one is created?. I printed in the ReferenceGenome constructor. I guess it could have been serialized, but the issues I was seeing were all on the master, and I don't see how it could have been. I haven't succeeded in making an isolated test case that motivates this change. @chrisvittal has a complicated joint calling pipeline that fails. However, we define equality on ReferenceGenome in terms of value quality, and I think unify should use the same notion of equality so this look like a bug on the face of it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5471#issuecomment-469322872:345,pipeline,pipeline,345,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469322872,1,['pipeline'],['pipeline']
Deployability,"> I agree this is the most compelling critique. I think the Pythonistas would make two points: a) KeyError is the one specific error you get in this case and b) it should be written like this:; > ; > ```python; > try:; > persisted_bm = self._persisted_locations[bm]; > except KeyError as err:; > raise ValueError(f'{bm} is not persisted') from err; > persisted_bm.__exit__(None, None, None); > ```. Updated my comment - I think my argument still holds with `KeyError`. Furthermore, you're still leaving around state in that dict, opening yourself up to calling `__exit__` twice.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12864#issuecomment-1516535470:399,Update,Updated,399,https://hail.is,https://github.com/hail-is/hail/pull/12864#issuecomment-1516535470,1,['Update'],['Updated']
Deployability,"> I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). Makes sense. Could you update the branch in your namespace? If not I can check tomorrow in mine. > Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. Got it. It's just that these changes affect CI and Batch, so I would like to quickly glance at a working example. Again, can do this tomorrow in my namespace. Not entirely comfortable with dev deploy yet, so want to try it when I'm less tired. Sounds like this will go in tomorrow",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757:234,update,update,234,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536244757,3,"['deploy', 'update']","['deploy', 'deployed', 'update']"
Deployability,"> I confirmed that, with this change, #13915 is resolved. In the interest of fixing that for 0.2.125, I'm gonna approve and get it into this release.; > ; > However, I left a comment inline. It seems that the meaning of pathsUsed was always a bit buggy and I think we should kill that tech debt now before it trips us again.; > ; > I'm also still concerned that `test_glob` didn't catch this bug; we should nail down why. We don't catch this in `test_glob` because the matrixread is nested within a `TableKeyByAndAggregate`, which semhash can't handle yet:; ```; 2023-10-26 12:54:40.576 : WARN: Failed to compute SemanticHash: SemanticHash unknown: is.hail.expr.ir.TableKeyByAndAggregate; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13919#issuecomment-1781498654:141,release,release,141,https://hail.is,https://github.com/hail-is/hail/pull/13919#issuecomment-1781498654,1,['release'],['release']
Deployability,"> I did NOT change the navbar to point to the batch docs since it won't exist until the next release. Could you explain this further? The navbar will also not be deployed until the next release, right?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-611733313:93,release,release,93,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-611733313,3,"['deploy', 'release']","['deployed', 'release']"
Deployability,"> I don't know if this adds any value in the containerized/cloud environment, where custom machine images are presumably the way to go. Dan already dockerized our dependencies for the CI setup. I don't quite know what value it would add there, either. It seems good for letting other people install your application in the cloud, but isn't really a thing we do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410368037:291,install,install,291,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410368037,1,['install'],['install']
Deployability,"> I got frustrated with ./gradlew testPython because it requires SPARK_HOME to be set. Can't we just require pyspark 2.2 to be pip-installed and remove that stuff? Should work. I need a bit of time to digest. I was under the impression we were using a pytest recommended layout, and going back to the doc you linked, we have the recommended structure under ""Tests outside application code"":. ```; setup.py; mypkg/; __init__.py; app.py; view.py; tests/; test_app.py; test_view.py; ...; ```. It looks like the `src/` structure is recommended in the following case:. > ... This is problematic if you are using a tool like tox to test your package in a virtual environment, because you want to test the installed version of your package, not the local code from the repository. > In this situation, it is strongly suggested to use a src layout where application root package resides in a sub-directory of your root:. Digging a little bit more, the pytest page references [this blog](https://blog.ionelmc.ro/2014/05/25/python-packaging/#the-structure) for reasons to use `src/`. But some of the comments indicate that `pip install -e` won't work with this structure, since the symlink it creates will be incorrect. It's possible this has been fixed since the issues were posted in 2012. I like the arguments there about testing the installed package so that the deploy is also tested. My reluctance mostly comes from knowing that I spend an hour trying to reconfigure intelliJ every time our directory structure changes. Let's discuss today. As above, I think this is fine but want to think over it a bit more.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503:131,install,installed,131,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454397503,5,"['deploy', 'install']","['deploy', 'install', 'installed']"
Deployability,"> I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our pinned-requirements.txt use pandas 2.0, fix whatever issues arise, but leave requirements.txt flexible for folks?. I think there are compromises either way, but I would be surprised if this just worked. It seems very easy to accidentally adopt new functionality so at that point why even have a lower-bound? I think that, while it's very hard to make sure that all our dependencies are compatible at all possible version combinations, and these things will happen, it just feels like an easily-avoidable lie to say we support 1.x and 2.x but then use functionality exclusive to 2.x. So I'm ok keeping the bounds more relaxed if we can guarantee that *our* usage of pandas is compatible with both. FWIW, I think that our primary dependencies release major versions infrequently enough that it is reasonable to only support the most recent major version, in much the same way that we don't support python versions indefinitely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573:878,release,release,878,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520500573,1,['release'],['release']
Deployability,> I just have one question: Do we need to manually update `estimate-current.sql` with the schema changes from `rename-job-groups-cancelled-column.sql`?. Yes. `estimated-current.sql` is an estimated current schema for documentation purposes only. Please update it to reflect the state of the database once your migration has been applied.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2341484381:51,update,update,51,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2341484381,2,['update'],['update']
Deployability,"> I should fix it!. That'd be awesome!. Yes, the steps are steps from build.yaml. Some background on the current setup, there are two classes of deployed stuff: normal, and ""infrastructure"". Infrastructure is stuff that can't be virtualized (or we've chosen not to) within the k8s cluster and can't be tested with the current CI setup. An example is gateway, that binds the live IP address for hail.is. Infrastructure will need a ""meta"" testing setup that will spin up another k8s cluster or testing in a separate staging cluster, where taking down the test cluster isn't an issue. Obviously, we haven't built this yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541:145,deploy,deployed,145,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540031541,1,['deploy'],['deployed']
Deployability,"> I still have a chunk of work to do on the src/main/c/Makefile and building libraries; which can run on both C++ ABI v2 (gcc-5.x and later) and C++ ABI v1 (gcc-4.x, in particular; gcc-4.8.3 as on CI machines). This seems like a waste. Also, weren't we going to standardize on c++17? We should never see ABI v1, should we? You should upgrade the CI machine (or wait until @danking's new CI goes live.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410112135:334,upgrade,upgrade,334,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410112135,1,['upgrade'],['upgrade']
Deployability,"> I tested that this will succeed with `kubectl can-i --as system:serviceaccount:batch-pods:deploy-svc delete pvc -n test` and `-n batch-pods`. Don't ask my how I found out that the syntax to refer to the deploy-svc service account was that. I don't even remember where I stumbled across that. Yeah, that's some dark magic Dan 👍",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890:92,deploy,deploy-svc,92,https://hail.is,https://github.com/hail-is/hail/pull/5502#issuecomment-468728890,2,['deploy'],['deploy-svc']
Deployability,"> I tested this with a pile of hacks to deploy this into an anonymous namespace in vdc. I'm not ready to PR those changes, they need a clean up before others use them. Sometime next week I hope to get that in. Getting it requires some restructuring of vdc/ and gateway/ to be more modular. Oh, I read that as you weren't ready to merge this. It sounds like you aren't ready to PR the testing stuff?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995:40,deploy,deploy,40,https://hail.is,https://github.com/hail-is/hail/pull/4974#issuecomment-447373995,1,['deploy'],['deploy']
Deployability,"> I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. Do you mean remove all options that are simple pass throughs (such as `--num-worker-local-ssds`) or all options that are also gcloud options (such as `--project`)? The latter could be difficult, since there are some gcloud options that hailctl also needs to read, like `hailctl dataproc start` using `--project` to set requester pays configuration, extending `--initialization-actions` with notebook/VEP init scripts, setting a higher default disk size when `--vep` is specified, etc. or `hailctl dataproc submit` automatically zipping `--py-files`. > I think this also addresses the issue hailctl dataproc submit not supporting --, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters. Nice. That would solve the problem. I would guess submitting script arguments is more common than using gcloud options here, so it would be nice for the gcloud arguments group to be optional, so that `hailctl dataproc submit cluster -- --script-options` would work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479:518,configurat,configuration,518,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758100479,1,['configurat'],['configuration']
Deployability,"> I think I'm going to choose not to support RelationalLetTable at this point; I think we lift relational lets before we lower MatrixTables, so we would never need to support it currently in our normal lowering pipeline. This isn't true -- we generate relational lets in the MatrixLowering pass!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8738#issuecomment-629281049:211,pipeline,pipeline,211,https://hail.is,https://github.com/hail-is/hail/pull/8738#issuecomment-629281049,1,['pipeline'],['pipeline']
Deployability,"> I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first. Can you elaborate more? I'm not sure which code paths you are referring to.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494:145,update,updates,145,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219867494,2,['update'],"['update', 'updates']"
Deployability,"> I think if it's not too hard of a change, we should add the files with encoded secrets to something like `infra/gcp/data/...`. This makes it clear that these files have a different purpose and gives some indication that they're specific to your repo. If you want to also add prefixing the file name with the repo, then that would make it even clearer. But if it's too much work, don't bother. Maybe something like `infra/gcp/data/hail-is/` etc. I've put all deployment-specific configs in an `$ORGANIZATION_DOMAIN` subfolder now, which hopefully should avoid any collisions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731:460,deploy,deployment-specific,460,https://hail.is,https://github.com/hail-is/hail/pull/11724#issuecomment-1173211731,1,['deploy'],['deployment-specific']
Deployability,"> I think pretty strongly that if nPreservedFields==0, you've done something very wrong. I don't disagree, but if you're concerned about this, I think an assert here is both in the wrong place and the wrong solution. I think for the design the IR, we should focus on (1) simple operations (this is already more complicated than I'd like), (2) that are composable, and (3) minimize special cases. By composability, I mean each IR should have a (local) contract, and each program composed from that local contract should be valid. Breaking this introduces a lot of potential bugs that can't be reasoned about locally, which is not good. The IR nodes do what they do. If you don't like what they do, we should probably find different ones. > so how could you possibly join this with another table? You'd have to create a single partition, and we'd never want to do that. Yeah, create a single partition. Or reshuffle if the partitioner has too little information. How much is too little? What if nPreservedFields==1 and we're down to 1 partition? Should that be an error? 2 partitions? How many partitions is too few? Any time nPreservedFields is less than the requested keys, you could get down to 1 partition. This is a continuous issue and rejecting the extreme case doesn't actually solve the problem. I guess isSorted isn't user exposed, but this seems dangerously close to reporting a user error with an assertion. When the service comes up, hopefully not too long, we're going to want to document the IR and make it public. So if we want to reject this case, we should do it early on: when the IR is parsed and/or constructed. (In general I think to give a nice experience we're going to have to do more up-front validation.) This is what I mean when I say ""find another IR"". In summary:; - If we're going to have this assertion, it needs to be in TableKeyBy constructor, and; - This is a complex and serious issue that isn't actually solved by your assertion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940:1219,continuous,continuous,1219,https://hail.is,https://github.com/hail-is/hail/pull/8649#issuecomment-622415940,1,['continuous'],['continuous']
Deployability,"> I think that my unease here is that there's a lot of engineering around the assumption that attempts might not get inserted right away through the intended path (we add attempts in so many different places!). Can you elaborate on this? We indirectly add attempts via SJ, MJS, and MJC via the `add_attempt` stored procedure and now directly in the billing update which should almost never be an insert unless Batch is overwhelmed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11998#issuecomment-1222799909:357,update,update,357,https://hail.is,https://github.com/hail-is/hail/pull/11998#issuecomment-1222799909,1,['update'],['update']
Deployability,> I think this change is no longer needed. Are you sure? https://github.com/hail-is/hail/blob/main/query/deployment.yaml#L36 still references `default_ns.name`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9760#issuecomment-740258327:105,deploy,deployment,105,https://hail.is,https://github.com/hail-is/hail/pull/9760#issuecomment-740258327,1,['deploy'],['deployment']
Deployability,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:691,upgrade,upgrade,691,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039,1,['upgrade'],['upgrade']
Deployability,"> I think we should defer folder creation as an improvement to jgscm. That's fine with me. As a workaround, users can create folders with gsutil for the time being. I think we should just fork it into the hail-is organization, fix it there, install from that repo in our build (don't need to go so far as to publish), and offer changes upstream, which they can take or not. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480347270:241,install,install,241,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480347270,1,['install'],['install']
Deployability,"> I think we should just always mount the deploy config. That might trigger some latent bugs where we don't do quite the right thing, but we can fix those. @danking Just realized you already took care of that in https://github.com/hail-is/hail/pull/9848/files#diff-21972f0c9ab321a85ffb4de98353c86fdea4992a5eb0d48c3489b977513b1da0! Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-773006387:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-773006387,1,['deploy'],['deploy']
Deployability,> I think you missed reference in the python function `delete_prev_cancelled_job_group_cancellable_resources_records`. > I think you missed reference in the python function `delete_prev_cancelled_job_group_cancellable_resources_records`. Good catch! I’ve fixed it and also updated the `id` field referenced by the alias `cancelled_t` to `batch_id` in a few places.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2341935282:273,update,updated,273,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2341935282,1,['update'],['updated']
Deployability,"> I understand the value of conservative updates before public demonstrations. I don't see this as conservative, and I don't think it has to do with the demo. It is to make incremental changes and disentangle unrelated changes. You should think of a PR as being lightweight. There's no reason not to create lots of them. In fact, by creating lots of them, you increase your velocity by removing false dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460386902:41,update,updates,41,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460386902,1,['update'],['updates']
Deployability,"> I'd propose to do an implicit dependency audit every time you push a new commit. > You can still pin versions on published packages, but use unpinned dependencies for CI testing. I think our only option here would be to test twice -- once with the major versions we explicitly depend on, and once with unbounded versions. I don't really like this model, since it basically couples breaking changes in other tools to Hail commits, which isn't how it actually works. I'd much prefer a weekly cron job that tries to update dependencies, I think, but that thing isn't trivial to build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446:515,update,update,515,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542222446,1,['update'],['update']
Deployability,"> I'm confused. What do you mean by this? Are you planning on using Dan's version of notebook (the rainbow gradient notebook.hail.is), or mine? If Dan's, he and I spoke about it, and he specifically stated that his version of Notebook is no longer needed, which is why I deleted the existing notebook. My apologies, @danking was wrong. I'm currently planning to use Dan's version because yours isn't ready. If yours is ready, I will use it. What does ready mean? From our recent email:. - it needs to get in master,; - and and integrated with our CI/CD (we can't be fixing issues and doing manual deployments leading up to or during a tutorial),; - it need to be beaten on by the team to look for issues (including scale issues), and; - it needs to be scale tested (@danking has a script for the old one that fires up N notebooks and reports any failures and summarizes the latency to notebook available),; - @tpoterba and I need to be comfortable enough with it we have confidence we can fix issues that arise during the tutorial. I probably also need time to review the workshop auth flow since I think that changed. If we're requiring login, we'll need to support more social login providers and/or email/password.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358:527,integrat,integrated,527,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-464229358,2,"['deploy', 'integrat']","['deployments', 'integrated']"
Deployability,"> I'm good with this, just one minor request, and a resolution on @daniel-goldstein 's question above.; > ; > Also, need to update the hail version in the changelog text. Sorry, missed that bit about the changelog",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-2050442437:124,update,update,124,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-2050442437,1,['update'],['update']
Deployability,"> I'm inclined to set `HAIL_QUERY_BACKEND=batch` by default, though I can see how this would also be useful to run a local-mode Spark-Hail. I'm happy to be overruled here but I like the ""just copy their config"" approach for configurations like these where both the local and batch backend could make sense, so the behavior is as consistent as is reasonable across environments. It will Just Work in the way you want if the user has the backend set to batch in their config ;)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1323968044:224,configurat,configurations,224,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1323968044,1,['configurat'],['configurations']
Deployability,"> I'm losing track of all the threads. Trying to summarize:; > ; > * Leave the CORS stuff, I don't understand it well enough to have an opinion anyway.; > * Global gzip settings with gzip_min_length.; > * Back out Docker changes, separately PR upgrade to nginx on Debian (would be my preference).; > * Leave auth notebook commented out (although it makes me uncomfortable) and let's keep discussing how to solve it. Got it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460338546:244,upgrade,upgrade,244,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460338546,1,['upgrade'],['upgrade']
Deployability,"> I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`. Sorry, ignore, I misread the comment. Agreed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546:132,deploy,deployed,132,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644228546,1,['deploy'],['deployed']
Deployability,"> I'm seeing issues creating notebooks (500). I can't reproduce this. Can you give me more details? Where are you running it? Note, my namespace is running a different branch that I'm working on (e.g. the menu stuff which doesn't include these changes.). > reaching CI and Batch (502). Is this in my namespace? I don't have CI or Batch deployed. Let me know if you want to test it there and I'll spin up this branch. Is it not working in your namespace?. > Notebook2 link should be changed to redirect to notebook 1. There should be no notebook2 links. I just grepped through the entire codebase, I didn't find anything. We're not asking for notebook2 certs anymore. Notebook2 is dead, long live notebook. Nobody is using it besides us, so I don't see any need for maintaining backward compatibility. In particular, if/when we make this more widely available, there shouldn't be anything2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091:336,deploy,deployed,336,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536150091,1,['deploy'],['deployed']
Deployability,"> I'm worried this PR will break all dev deploys. The migration will bring down the service (batch, etc.) inside the dev namespace, but the dev deploy itself is running in the production batch, so there should be no issue. The deploys will still happen as normal after the migrations, e.g. deploy_batch will run after batch_database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718:41,deploy,deploys,41,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573731718,3,['deploy'],"['deploy', 'deploys']"
Deployability,"> Ideally, I'd release the references to the regions of my producer once I finished constructing the new RegionValue. I think references from a region to other regions should be treated the same as data directly contained in the region. In particular, in the current model it all gets freed at the same time. If we move to a stacked/checkpointed model like Cotton suggested, a checkpoint would remember both where in the region to move back to, and where in the list of region dependencies to move back to, releasing those regions to the right of that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956:15,release,release,15,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-577853956,1,['release'],['release']
Deployability,"> If folks are not continuously deploying every commit, they might miss a tagged commit. @daniel-goldstein , thoughts on how we might support things that should be run on a tagged commit for users who are not deploying every commit?. I don't have a good plan for this at the moment and have not spent much time thinking about it. I will say though that I expect not deploying on every commit while running CI to be painful given our current build process. Sometimes we make ""backwards compatible"" changes (to CI or build.yaml) through multiple PRs that would not be backward compatible if those commits were squashed. We should definitely have a discussion about it, probably with AUS folks since they might be able to offer insight as to what they're planning with deploys. One thing I don't know is, would another hail install only publish some artifacts when *we* tag a commit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985731820:19,continuous,continuously,19,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985731820,6,"['continuous', 'deploy', 'install']","['continuously', 'deploying', 'deploys', 'install']"
Deployability,"> If folks are not continuously deploying every commit, they might miss a tagged commit. Do you mean someone watching `hail-is/hail:main` but only deploying occasionally? I feel like we need to separate out publish steps entirely from our CD steps, track our most recently published tag/sha, and spin off publish batches when the tag is no longer the most recent on GitHub (this could be checked as frequently as they deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985734117:19,continuous,continuously,19,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985734117,4,"['continuous', 'deploy']","['continuously', 'deploy', 'deploying']"
Deployability,"> If we ever ban old versions of Hail from the cluster, then we can also eliminate the log4j2 reconfiguration. New versions of Hail work fine without any runtime log configuration (thanks to QoBAppender). We might want to do this if we get rid of GSA keys. We can't have any more jars that presume the existence of some key file. It would also be a good time to fully delete the `memory` service, even though old jars should be able to tolerate that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692:166,configurat,configuration,166,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527963692,1,['configurat'],['configuration']
Deployability,"> In trying to test this (from your branch, ran pip install on /hail/python just in case). You're running this locally, or with `hailctl dev deploy`? I assume the latter because the former is essentially impossible. ~/.hail/token is no longer used and you can delete it. You'll need a valid tokens.json to run the dev deploy. Once the dev deploy runs, your local configuration is irrelevant. It sounds like your dev deploy was successful. You're getting failures in your deployed services. You need to look into your namespace to debug them. In particular, for auth to run, you're going to need a copy of auth-oauth2-client-secret from the production namespace. To log in inside the dev namespace, you'll have to add the callback to the list of registered callbacks at Google. You can copy mine as an example.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285:52,install,install,52,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532260285,7,"['configurat', 'deploy', 'install']","['configuration', 'deploy', 'deployed', 'install']"
Deployability,"> Inside the cluster, folks no longer need tokens, right? Can we delete all the -tokens secrets?. Clients from before the OAuth change still expect the `tokens.json` file to exist in the job container. So we can only delete these once we drop support for tokens on the server. > It seems to me that this change should also delete hailtop/auth/tokens.py and all uses of it. Clients should unconditionally use OAuth2 now. I think removing client-side token support entirely removes the remaining uses of DeployConfig.default_namespace. This is correct and I'm happy to do this. Do you want me to add that to this change or make a separate PR? I might prefer a separate PR as I will need to come up with a solution for copy-paste tokens which use `tokens.py`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14056#issuecomment-1877749195:502,Deploy,DeployConfig,502,https://hail.is,https://github.com/hail-is/hail/pull/14056#issuecomment-1877749195,1,['Deploy'],['DeployConfig']
Deployability,"> Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency. Agreed. I think our best path for speed is keeping these images totally cacheable so basically dependencies (nothing that will have to change on every commit, e.g. the wheel). Installing the wheels in the image is just adding more latency and work of localization. > The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down).; I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. Totally agree.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182:46,install,installed,46,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,> Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded?. This is an interesting idea. I think it would effectively require merging the database and the deploy steps. We can't re-deploy the existing config because that's the wrong (old) one.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953:104,deploy,deployments,104,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573739953,3,['deploy'],"['deploy', 'deployments']"
Deployability,"> Is this something where you need to add a new docker image to the gcr registry with the package installed? I think for pipeline I had to add new lines to `Dockerfile.pr-builder` and then run `make hail-ci-build-image` and `make push-hail-ci-build-image`. This is just a replacement for notebook, and yeah, basically what you described. This PR adds a dependency (lib sass, corresponding to the `import sass` line in notebook/notebook.py), which will fix the crashloopbackoff currently seen on our cluster (a PR was merged but this was missed).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477:98,install,installed,98,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467611477,2,"['install', 'pipeline']","['installed', 'pipeline']"
Deployability,"> Isn't it also non-associative? We're assuming everything is associative. I was thinking there was still some benefit to using the associative combiner, but I take it back. ApproxCDF is *approximately* commutative, so if we aren't achieving determinism anyways, you're right the commutative combiner is fine. > I don't think we guarantee a stable sort, so I'm inclined to leave it as an unstable sort for now in the name of performance. Even if we don't guarantee stable sort, we might hope to guarantee determinism (no promise what answer you get, but you'll get the same answer each time you run the same pipeline). I think the associative combiner *might* be enough to ensure determinism. Though if you want to leave it nondeterministic for now, and wait for the better fix of guaranteeing stable sort by appending indices to the sort keys, that's fine with me.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6837#issuecomment-519633134:608,pipeline,pipeline,608,https://hail.is,https://github.com/hail-is/hail/pull/6837#issuecomment-519633134,1,['pipeline'],['pipeline']
Deployability,"> It feels a bit odd / bad to copy things into a build directory and build there. the build process drops a bunch of artifacts in the python/ directory (and sub-directories). Isn't it cleaner to do this in a build/ directory?. > It seems like hailctl should be in the hail/python directory?. This seems like a larger organizational decision -- do we want batch/pipeline under there too? It'll be easy to move `hailctl` whenever we want, with just a couple lines of code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6136#issuecomment-494892128:361,pipeline,pipeline,361,https://hail.is,https://github.com/hail-is/hail/pull/6136#issuecomment-494892128,1,['pipeline'],['pipeline']
Deployability,"> It looks good except for the behavior I was seeing. I can try redeploying my version if you think that's what the issue is (the edit page ""update"" doesn't return a page with the description I just added even though it's there on the resources page). This was intentional, but I see it is confusing when you just edited description. I'll add the description to the resource page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-726291462:141,update,update,141,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726291462,1,['update'],['update']
Deployability,"> Just starting to explore this PR. I compared this in-progress CI run to 7126798 (from #12737). The time to service backend starting is ~7 minutes. In the other PR, its ~8 minutes. I suppose that's because this PR isn't hitting any caches, right?; > ; > Hmm, it also seems like the critical path to the service backend test is through `build_hail_jar_and_wheel_only`. I wonder if we double the cores, would the time halve? On my laptop a fresh build is like 3m.; > . Ya I really only focus here on docker image steps, not on the overall critical path. But I like the idea of adding more cores I'll try that. The docker images are hitting cache though since every PR caches from its own previous runs in addition to main. The reason some images still take ~1 minute is because they introduce a new layer (normally the hail_version has changed since the SHA has changed) and they spend most of the time localizing the layer that has the dependencies installed (why I want lazy pulling). A retry of this PR should build the images very quickly, though I haven't tried that recently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223:949,install,installed,949,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458611223,1,['install'],['installed']
Deployability,> Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message. Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1785779141:26,update,update,26,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1785779141,1,['update'],['update']
Deployability,> Mainly that the commit procedure branches on whether the start id is 1. I was trying to avoid an expensive query to update all of the n_pending_parents when we know it's the first update being committed. I think I'd prefer to keep this branching there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221599755:118,update,update,118,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221599755,2,['update'],['update']
Deployability,"> Minor comments. Is this deployed anywhere so I can take a look?. http://34.207.246.132 has the latest, which is this PR + the tutorial page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8920#issuecomment-638971103:26,deploy,deployed,26,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971103,1,['deploy'],['deployed']
Deployability,"> My assumption has been that each fresh pipeline should get a new EmitModuleBuilder and ExecuteContext, if this is the case, I believe what I have done here should be alright. OK, great. I jumped the gun. But something to think about in places we retry to share across invocations of Compile. > The first usage of this feature is to move the FS off of the class itself and onto a container class. If the the FS field in the container class is static, how does it get serialized? The point is the C returned by EmitClassBuilder.resultWithIndex captures and serializes the FS. > Furthermore, we can't have static comparison functions without static reference genomes, so a solution will need to be found. This is a step in that direction. The RG can be serialized. That's what EmitClassBuilder.getReferenceGenome does. So you can put the reference genomes in static classes that decode them lazily, and then the comparison functions and main code can call into those static methods. This doesn't work for FS because (currently) it can't be serialised as code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-652761221:41,pipeline,pipeline,41,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652761221,1,['pipeline'],['pipeline']
Deployability,"> My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. Yeah, I'm sympathetic to this. Our current pinned-version approach is partially a [over-]reaction to builds breaking without any internal change, especially because many new Hail users are also new Python users and not capable of debugging Python dependency hell. > Are you using some continuous integration?. We do use CI, but I'm not exactly sure what you're proposing. An automatic find-the-latest-version-that-works step? It's true that if we were to use unpinned versions, our CI would start failing if a package introduces a breaking change, and we can fix it. However, I think that's too late -- now all old Hail versions will break from fresh installs! I feel that reproducibility demands we pin versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913:372,continuous,continuous,372,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542204913,3,"['continuous', 'install', 'integrat']","['continuous', 'installs', 'integration']"
Deployability,"> Oh, I misunderstood, I thought you were suggesting changing our FROM to stretch/9.6.; > ; > I think that should be fine, but can we do it as a separate PR since it seems orthogonal to this change which we're trying to get in for the demo tomorrow? (And in general orthogonal changes should be separate PRs so discussion on one part doesn't hold up the other parts.). Yes, although the gzip settings issued in this pr will be different between the two version. 1.10.3 doesn't have gzip on by default. I understand the value of conservative updates before public demonstrations, so will do what you ask. Btw, the full config if relying on nginx:10.15.8 goes from:. ```; FROM debian:9.5. RUN apt-get update -y && \; apt-get install -y nginx && \; rm -rf /var/lib/apt/lists/*. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf. RUN ln -sf /dev/stdout /var/log/nginx/access.log; RUN ln -sf /dev/stderr /var/log/nginx/error.log. CMD [""nginx"", ""-g"", ""daemon off;""]; ```. to . ```; FROM nginx:1.15.8. RUN rm -f /etc/nginx/sites-enabled/default; ADD @nginx_conf@ /etc/nginx/conf.d/hail.conf; ADD gzip.conf /etc/nginx/conf.d/gzip.conf; ```. kind of neat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467:541,update,updates,541,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460378467,3,"['install', 'update']","['install', 'update', 'updates']"
Deployability,"> Oh. I remember why now. When we were using rsync I was worried someone would simultaneous try to do this:; > ; > cp gs://bucket/a/b/; > cp gs://bucket/a/; > ; > Then they would clobber each other. Can you flesh this out, I'm not sure I understand the sequence of commands here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701574241:130,a/b,a/b,130,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701574241,1,['a/b'],['a/b']
Deployability,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:650,update,updates,650,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641,1,['update'],['updates']
Deployability,"> Should we rename hailctl dataproc start/stop to create/delete?. I think so. Also, modify to update.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767181420:94,update,update,94,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767181420,1,['update'],['update']
Deployability,"> Slight question about the shuffle ID/client/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:361,pipeline,pipelines,361,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,1,['pipeline'],['pipelines']
Deployability,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:136,deploy,deploy,136,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930,2,['deploy'],['deploy']
Deployability,"> Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`).; > ; > ; > ; > I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this. Right I realize this, why I mentioned unit test (to make unit tests work without calling hail, we would need to wrap each function we want to test in a cli interface, and then containerise those functions...this seems not so friendly to contributors). A 2nd question: how would a contributor submit write hybrid hail query/batch code that called batch from within a hail query script? So hail transforms data, say generates pc's, writes table, and then issues some batch commands that use a pre-built image, as opposed to requiring them to containerise their hail cod3. This will be useful: containerisation is the stated biggest pain point of batch, from my interviews. And one of the biggest hail issues is shuttling data between hail and other processes, which has led people to ask for tighter integration, or more statistical tools (so that they don't need to go out to those other tools, like plink). Showing Batch working from within hail-query scripts would be useful because it will show a much easier integration, and those examples or contributed modules should live somewhere (and that seems not to be hailtop, because this requires batch to be called from code that is calling hail directly, rather than through an image).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432:127,install,installed,127,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671459432,4,"['install', 'integrat']","['installed', 'integration']"
Deployability,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:772,update,update,772,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375,3,['update'],['update']
Deployability,"> The hailctl dataproc subcommand now has --beta, --configuration=, --dry-run, --project= and --zone=. These apply to all commands. There is a GcloudRunner object that takes these options, is set to the click context user obj field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with click, the subcommand options must go on the subcommand, so hailctl dataproc stop --dry-run is an error. Nice. It's great that these are handled at the `hailctl dataproc` level instead of having to remember to account for them in every `hailctl dataproc` subcommand. That's going to resolve a lot of inconsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:52,configurat,configuration,52,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,1,['configurat'],['configuration']
Deployability,"> The one thing I'd say is just that when we make a user facing change, we try to make one of the commits look like:; >; > CHANGELOG: Added or_error method to SwitchBuilder; >; > This ends up being helpful when I have to go through and generate the change log for a new version release. Good to know, updated the commit message. It would be nice to document conventions like this in the [docs for software developers](https://hail.is/docs/0.2/getting_started_developing.html#contributing) and/or a [contributing file](https://docs.github.com/en/free-pro-team@latest/github/building-a-strong-community/setting-guidelines-for-repository-contributors).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468:278,release,release,278,https://hail.is,https://github.com/hail-is/hail/pull/9749#issuecomment-735921468,2,"['release', 'update']","['release', 'updated']"
Deployability,"> The thinking behind this is that there's a huge amount of re-use of code for an individual from one Hail analysis to the next. So I'm not sure I buy this. Hail is a exploratory data analysis platform, it isn't a SQL engine running nightly billing reports. In particular, as we do whole stage optimization and code generation. It isn't clear to me how you do code-reuse when we're specializing each operation into the global context (happy to hear the plan). For example, tables that have many fields, you will need to load different ones for different queries and in general it is infeasible (exponential) to generate them all. Also, to get sharing you need to break the code up and now you're running the compiler multiple times which also seems bad. Given our focus on large-scale analysis, introducing optimization boundaries for code reuse seems like a bad trade off to me. A significant amount of analysis happens in the cloud where $HOME is ephemeral so you won't get savings between sessions. Finally, there are pipelines that are more standardized but it is my impression they are run on extremely large datasets (hours, overnight) in which case compilation speed isn't important. Finally, there's the complexity around locking that isn't easy and have real technical risk. A compiler cache potentially becomes more appealing in the context of an always-on service. There's no locking issue, and you can start to do things like speculative compilation (e.g. immediately start compiling the decoder (the full decoder? Hmm.) when a user opens the dataset.). I would say getting in a 3x decoder improvement is way more important than this. I would have punted it down the road and instrumented to estimate cache hit rates before building this out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596:1021,pipeline,pipelines,1021,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410358596,1,['pipeline'],['pipelines']
Deployability,"> There are other implementations, such as LibreSSL, but they implement roughly the same interface as OpenSSL. LibreSSL introduced a new interface, libtls, that's designed to be easy to use, secure by default, and match the underlying socket semantics as much as possible. If we ever do any C level networking, we should use it. > ad nihilum. You mean ex nihilo, from nothing?. > I intend to eventually require all our services to refuse to speak anything other than TLS 1.3. Can we get a task for this in Asana? And for mTLS?. > Our system is simpler. We have no root certificate.; > Deploy will run create_certs on every master deploy.; > [O]nce incoming trust is fixed, I am unsure how to smoothly upgrade services. I think we should a have a root certificate for all services in the cluster and verify all certificates are signed by the root certificate. I don't know how to handle creating new certs on every deploy, either. I think we should just create them if they don't already exist. It looks like you're pinning keys for incoming and outgoing, which is awesome. It looks like you're duplicating the keys for each incoming/outgoing list it appears in. Alternatively, you could break the cert secret into two parts: the private key/config needed by the service, and the certs needed by the clients/servers. > In the long run, I want to fix batch to use an entirely different network for callbacks. I agree. Can we get a task for this? Using authorization to control who can talk to whom is great. We should enforce that with network policies. Defense in depth. Another task. > Readiness and liveness probes cannot use HTTP.; > Although k8s supports HTTPS, it does not support so-called ""mTLS"" or ""mutual TLS."". Ugh. But probes can be run via a command, so we should be able to use curl and our client certs for this: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/. ```; livenessProbe:; exec:; command:; - cat; - /tmp/healthy; initialDela",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805:585,Deploy,Deploy,585,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-615209805,4,"['Deploy', 'deploy', 'upgrade']","['Deploy', 'deploy', 'upgrade']"
Deployability,"> This complexity is because we allow multiple updates to occur simultaneously to a batch. Right I left this out… I am unsure if we used relative IDs how we would prepare the following update without knowing the IDs of the jobs I'm submitting in this current update, so I think some supplementary request for a reservation of IDs will be necessary anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215954384:47,update,updates,47,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215954384,3,['update'],"['update', 'updates']"
Deployability,"> Unfortunately, the only way to check if this actually works right now is for deploy to succeed. 😞. Can't you check by building the docker image?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5489#issuecomment-468433049:79,deploy,deploy,79,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468433049,1,['deploy'],['deploy']
Deployability,"> We have a 35K cohort. The VCF format of chr1 is 2.4T. Heh. So, yes, ""project"" VCFs grow super-linearly in the number of samples. I (and others) are currently pushing very hard for the VCF spec to support two sparse representations: ""local alleles"" (samtools/hts-specs#434) and ""reference blocks"" (samtools/hts-specs#435). When using these two sparse representations, you should be able to store 35,000 whole genomes in ~10TiB of GZIP-compressed VCF. What is your calling pipeline? Do you generate GVCFs? If yes, I strongly recommend you use the [VDS Combiner](https://hail.is/docs/0.2/vds/hail.vds.combiner.VariantDatasetCombiner.html#hail.vds.combiner.VariantDatasetCombiner) to produce a [VDS](https://hail.is/docs/0.2/vds/index.html). You can read more details in [this recent preprint we wrote](https://www.biorxiv.org/content/10.1101/2024.01.09.574205v1.full.pdf), but a VDS of 35,000 whole genomes should be a few terabytes. I'd guess 4 TiB, but it depends on your reference block granularity. I strongly recommend using size 10 GQ buckets. ---. > I don't know the Kryo JAR. I tested on both docker images hailgenetics/hail:0.2.126-py3.11 and hailgenetics/hail:0.2.127-py3.11. Those should use Kryo 4.0.2. OK. My conclusion is that Kryo still has a bug preventing the serialization of very large objects. This becomes a limitation in Hail: we cannot support PLINK files with tens of millions of variants. Our community is largely transitioning to GVCFs and VDS, so I doubt we'll improve our PLINK1 importer to support such large PLINK1 files. That said, PRs are always welcome if loading such large PLINK1 files is a hard requirement for you all.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1922048526:473,pipeline,pipeline,473,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1922048526,1,['pipeline'],['pipeline']
Deployability,"> We should also probably update the branch protection settings to make this check required. Agreed, I'll add this once it merges to main",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14641#issuecomment-2256699850:26,update,update,26,https://hail.is,https://github.com/hail-is/hail/pull/14641#issuecomment-2256699850,1,['update'],['update']
Deployability,"> We should eventually lift the deploys up to the top-level too, right?. Ya, I decided just to stop just short of that for this PR for brevity and that I'll probably have to update dev docs (currently `make -C batch deploy` still works). I figured there'd be a follow-up PR for moving deploys up here as well as one or two for lifting some stuff over from the `hail/hail` Makefile (which should get reviews from query folks)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12841#issuecomment-1497550505:32,deploy,deploys,32,https://hail.is,https://github.com/hail-is/hail/pull/12841#issuecomment-1497550505,4,"['deploy', 'update']","['deploy', 'deploys', 'update']"
Deployability,"> Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on. yeah, sorry, that's on me. I didn't notice because the pod's log didn't change between the first PR, which didn't have libsass, and the next, which did; assumed CI hadn't deployed it because it was backed up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191:63,deploy,deployed,63,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468435191,3,['deploy'],"['deploy', 'deployed']"
Deployability,"> What is the best way to test gateway?. Deploy the new gateway. Yes, it's live. Yes, it might break stuff. Good luck.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-541104331:41,Deploy,Deploy,41,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-541104331,1,['Deploy'],['Deploy']
Deployability,"> When will it be available to use?. 0.2.129. Should be released soon,",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14375#issuecomment-2025929109:56,release,released,56,https://hail.is,https://github.com/hail-is/hail/pull/14375#issuecomment-2025929109,1,['release'],['released']
Deployability,"> Your tool should also examine the first word of the MAKEFLAGS variable and look for the character n. If this character is present then make was invoked with the ‘-n’ option and your tool should stop without performing any operations. Added. > Your tool should be sure to write back the tokens it read, even under error conditions. This includes not only errors in your tool but also outside influences such as interrupts (SIGINT), etc. You may want to install signal handlers to manage this write-back. I mean, I doubt anyone is sending signals other than SIGKILL to our build system, but I added some signal handlers that just `sys.exit(0)` which triggers the finally (I checked). > We also get a lot of ‘warning: jobserver unavailable: using -j1. Add +' to parent make rule.’warnings when runningmake jvm-test`. This is because our C++ backend uses make to drive compilation (wtf‽). I strip MAKEFLAGS before calling gradle now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104:454,install,install,454,https://hail.is,https://github.com/hail-is/hail/pull/6923#issuecomment-524446104,1,['install'],['install']
Deployability,"> and now directly in the billing update which should almost never be an insert unless Batch is overwhelmed. This is my point. I don't see why we should add another place where we add attempts. I think the billing update should only update attempts that already exist, and the `attempts_after_insert` trigger doesn't have to account for rollups, only the `attempts_after_update` trigger.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11998#issuecomment-1222831447:34,update,update,34,https://hail.is,https://github.com/hail-is/hail/pull/11998#issuecomment-1222831447,3,['update'],['update']
Deployability,> elimination of default_namespace. This is also unfortunately tricky :/// as clients require `default_namespace` exist in container's deploy config,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14056#issuecomment-1879295919:135,deploy,deploy,135,https://hail.is,https://github.com/hail-is/hail/pull/14056#issuecomment-1879295919,1,['deploy'],['deploy']
Deployability,> https://batch.azure.hail.is/batches/3749715 seems to be the last one on april 19 and we pick up again on april 27 https://batch.azure.hail.is/batches/3751384. I only went as far as to check the past deploys to find out this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561235547:201,deploy,deploys,201,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561235547,1,['deploy'],['deploys']
Deployability,"> it seems to provide visibility into what happened during the last run of lets encrypt?. Yes. As far as I know, certbot needs the previous config to do a renew (which I'm not doing yet). > I think the ""sidecar"" approach is simpler than this one (no extra nginx instance, no secrets, no service, no k8s secret creation privileges). We beef up the nginx pod to have a second container sharing a letsencrypt volume (which we've already defined in this PR). You can't mount volumes to multiple pods. You can't even mount volumes to the SAME pod if you want to do rolling updates (because the new instance can't launch because the old one is mounting the volume). I think this means volumes for certs and web root are out. volumes only work for replicated StatefulSets where you can take down one instance at a time for updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868:560,rolling,rolling,560,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432724868,3,"['rolling', 'update']","['rolling', 'updates']"
Deployability,"> make install-wheel and then PYTHONPATH="""" python -c ""import hail; hail.init()"". Should this be an automated test?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6437#issuecomment-504550511:7,install,install-wheel,7,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504550511,1,['install'],['install-wheel']
Deployability,"> nginx should need the services (i.e. domain names) to exist, not the deployments. Ah, that makes sense. I suppose that's why the service definition of `router` used to exist along with gateway's k8s config.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778:71,deploy,deployments,71,https://hail.is,https://github.com/hail-is/hail/pull/10736#issuecomment-891312778,1,['deploy'],['deployments']
Deployability,"> should the `haas/packages` folder be checked in?. I don't quite understand. haas is a monorepo containing all the bits that will make up the web/mobile/desktop/auth api and any other services that allow users to interact with Hail without installing it themselves. . https://danluu.com/monorepo/. Everything in packages is a separate package (web, api1), separately publishable to npm, but which allows dependencies to be shared. Does that help?. Edit: wrong link",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-446604474:241,install,installing,241,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-446604474,1,['install'],['installing']
Deployability,"> what ends up in /home/cotton/hail-20200407-1502-0.2.36-75a0f869d72d.log? What happens to the usual Spark/Hail master logs?. The client and the server are now separated by a machine boundary, so just the Python client logs end up in the client log. That's basically nothing and can probably be removed when using the client with the service. There are no Spark logs, the service is 100% Spark-free. The Hail master logs end up in the query service logs. Obviously a lot of this needs to be rethough and improved. The error checking and reporting needs to get improved at the service boundary, errors should be relative to the input, and clients probably shouldn't get a server-side stack trace. We're going to need additional tools for debugging pipelines on the master, and probably want an admin UI where you get the logs for each query, the IR getting executed, how it was transformed with lowering/optimization, statistics on timing of its execution, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050:747,pipeline,pipelines,747,https://hail.is,https://github.com/hail-is/hail/pull/8485#issuecomment-610628050,1,['pipeline'],['pipelines']
Deployability,"> you also don't need to add another resource VCF to make this fail. Yeah, I wasn't sure if the array_elements_required would be overridden if no missingness was found in the data. I wanted to test that possibility (updated my comment with the results of that test). > ah, crap, there's a simplify rule that turns a TableCount(TableUnion(...)) into the sum of the TableCounts for each child. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805:216,update,updated,216,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-581049805,1,['update'],['updated']
Deployability,">; <h2>Returns</h2>; <a href=""https://python3.org/..../int"">int</a>; {% endraw %}; </div>; {% include ""nav-bottom.html"" %}; {% include ""base-foot.html"" %}; </body>; </html>. ```. The `website` service/k8s-pod/program ""serves the website"" wherein it *runs jinja2 again* and finally produces the HTML that you see on the website. That looks vaguely like this:. ```; <!DOCTYPE html>; <html lang=""en"">; <head>; <title>Hail | {% block title %}{% endblock %}</title>. <link rel=""stylesheet"" href=""/static/css/style.css?v=0.2.64-1ef70187dc78"">; <script src=""https://kit.fontawesome.com/7cdc07e2ca.js"" crossorigin=""anonymous""></script>; <!-- more Hail stuff -->. <link href=""_static/sphinx-style.css"">; <!-- more sphinx stuff here -->; </head>; <body>; <nav class=""navbar align-content-start justify-content-start sticky"" id=""hail-navbar"">; <div class=""container-fluid align-content-start justify-content-start d-flex"" id=""hail-container-fluid"">; <div class=""navbar-header"" id=""hail-navbar-header"">; <a class=""active"" id=""hail-navbar-brand"" href=""/"">; <img alt=""Hail"" height=""30"" id=""logo"" src=""/static/hail-logo-cropped-sm-opt.png"">; </a>; <button type=""button"" id=""navbar-toggler"" class=""navbar-toggler"" data-toggle=""collapse"" data-target=""#hail-navbar-collapse"" aria-expanded=""false"">; <span class=""icon-bar""></span>; <span class=""icon-bar""></span>; <span class=""icon-bar""></span>; </button>; </div>; <div class=""collapse navbar-collapse"" id=""hail-navbar-collapse"">; <span class=""algolia-autocomplete"" ...></span>; <ul class=""nav navbar-nav navbar-right"" id=""hail-menu"">; <li class=""nav-item""><a href=""/docs/0.2/index.html"">Docs</a></li>; <!-- more hail pages -->; </ul>; </div>; </div>; </nav>. <div id=""main'>; <h1><code>my_function</code></h1>; <p>Adds two numbers!</p>; <h2>Parameters</h2>; <ul><li>x : int First number to add.</li><li>y : int Second number to add.</li></ul>; <h2>Returns</h2>; <a href=""https://python3.org/..../int"">int</a>; </div>. <!-- Hail footer things -->; </body>; </html>. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10278#issuecomment-814433087:2779,toggle,toggler,2779,https://hail.is,https://github.com/hail-is/hail/pull/10278#issuecomment-814433087,3,['toggle'],"['toggle', 'toggler']"
Deployability,">Can you help me understand why we can't set a very negative z-index on whatever element is creating the offset or use one of these shorter solutions?. Sure. These solutions don't work because they interfere with the layout of content. What you linked above works fine with 1 element, but not with adjacent elements. All of these solutions try to make up for the fact that the browser will position the element at the top of the browser (has no concept of non-0 offset). The javascript solution fixes this by introducing that non-0-offset. - I tried every permutation of these solutions, including every solution at the link you provided, in the original fix. They all interfere with layout in the presence of nested and adjacent modified elements. There is no z-index solution possible for all combinations, at least without JS (because you need adjacent elements to have descending ordered z-indices). MathJax was handling scrolling at page load. If you remove that line MathJax will scroll the page whenever it detects a hash in the URL. ""Since typesetting usually changes the vertical dimensions of the page, if the URL contains an anchor position, then after the page is typeset, you may no longer be positioned at the correct position on the page. MathJax can reposition to that location after it completes its initial typesetting of the page. This value controls whether MathJax will reposition the browser to the #hash location from the page URL after typesetting for the page"". * https://docs.mathjax.org/en/v2.7-latest/options/hub.html. . The reason we use history instead of say updating location.href is because there is no way to prevent the browser from scrolling to that location when you update that value. It's undefeatable, as mentioned in the comments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748:1704,update,update,1704,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544655748,1,['update'],['update']
Deployability,">Did you test submitting jobs to the cluster itself? This can be quite a different environment than the >tests. Not as such. I ssh'ed in to the master node (after explicitly running a gcloud command to get a cluster with the 1.2 image rather than the default 1.2-deb9). I wasn't confident about ; how to get my own hail.jar to run in the cluster environment instead of the deployed version.; But since NativeModule.cpp is the biggest user of string's, I confirmed that we could run tests; *and* see codegen'ed files showing up in /tmp/hail_*/*.cpp, indicating that C++ decoders were; being generated and were working correctly. >Also, is there a plan about how users (or we) control this in the Dataproc setting? E.g. how do we >submit cluster_sanity_check.py with and without C++ codegen enabled?. I don't have a specific plan, just imagined that it wouldn't be difficult to arrange, and I don't; have any particular opinion about the best way to control it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424835504:373,deploy,deployed,373,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424835504,1,['deploy'],['deployed']
Deployability,"@CDiaz96 @daniel-goldstein @tpoterba @johnc1231 I've updated the document in response to y'all's helpful critique. Please review and approve if you're individually OK with it! Once I have four OKs, I'll remove the WIP tag.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10020#issuecomment-781457932:53,update,updated,53,https://hail.is,https://github.com/hail-is/hail/pull/10020#issuecomment-781457932,1,['update'],['updated']
Deployability,"@DSuveges , we just released a new version of hail, 0.2.73, which contains this fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10682#issuecomment-885125644:20,release,released,20,https://hail.is,https://github.com/hail-is/hail/issues/10682#issuecomment-885125644,1,['release'],['released']
Deployability,"@Sun-shan According to the error message you posted, Spark itself cannot find `/hail/test/BRCA1.raw_indel.vcf`:; ```; py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.; : org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/hail/test/BRCA1.raw_indel.vcf; ```. Looking at that error message, it looks like Spark is interpreting your path as a local file system path, _not_ a hadoop path. Moreover, earlier in your posted output this line:; ```; 17/08/15 08:58:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; ```; suggests that you're not actually connecting to a Spark cluster with a properly configured Hadoop installation. ---. Your Spark cluster appears improperly configured. I'm not sure if `pyspark` is even connecting to your cluster. You might try looking at [this StackOverflow post](https://stackoverflow.com/questions/34642292/cant-connect-pyspark-to-master) about connecting `pyspark` to a Spark cluster. I strongly recommend running `pyspark` again and executing:; ```; spark.sparkContext.master; ```; This should print the URL of your Spark master node. If this prints a String starting with `local`, then you're definitely not connecting to a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635:787,install,installation,787,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-322539635,1,['install'],['installation']
Deployability,@andgan any update on this issue? should it remain open?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/174#issuecomment-279517149:12,update,update,12,https://hail.is,https://github.com/hail-is/hail/issues/174#issuecomment-279517149,1,['update'],['update']
Deployability,@astheeggeggs Thanks for the bug report. It lead to finding a rather serious bug. See https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375 for more details on what other regressions could have been affected. A new release should go out today with the fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8349#issuecomment-617327917:254,release,release,254,https://hail.is,https://github.com/hail-is/hail/issues/8349#issuecomment-617327917,1,['release'],['release']
Deployability,"@bw2 Clicking the edit button on the `hail-is/hail` repo is definitely one of the supported interaction pathways. Each time you click it, you should get a [fresh branch under your hail fork](https://github.com/bw2/hail/branches). So, the particular branch that needs to be squashed is [bw2/hail](https://github.com/bw2/hail/tree/patch-3). Any new commits or force pushes to bw2/hail will automatically update this PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248360486:329,patch,patch-,329,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248360486,2,"['patch', 'update']","['patch-', 'update']"
Deployability,"@bw2 Could you paste the output of `git remote -v`? I suspect your `origin` is the broad institute origin. There's two problems with that:; - we've moved the hail repository to hail-is/hail, and; - you can only modify branches in your fork. So let's start by squashing your commits. Below I'll refer to your fork of hail as `myfork`. ```; git remote add myfork https://github.com/bw2/hail.git; git fetch myfork; git checkout patch-3; git rebase -i ecd1ed9^; git push myfork patch-3; ```. When you return to this page, you should see just one commit listed. As to your origin pointing to broad institute. If your origin looks like:. ```; origin https://github.com/broadinstitute/hail.git (fetch); origin https://github.com/broadinstitute/hail.git (push); ```. Then let's switch it to the new hail-is repository:. ```; git remote remove origin; git remote add origin https://github.com/hail-is/hail.git; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248081179:425,patch,patch-,425,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248081179,2,['patch'],['patch-']
Deployability,"@bw2 if you squash locally and force-push to your branch, this PR will update. No need to close/open a new one",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248069173:71,update,update,71,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248069173,1,['update'],['update']
Deployability,"@catoverdrive Here's the output with docker commands:. ```bash; #!/bin/bash. # change cd to tmp directory; cd /tmp//pipeline.S9YTZap5/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:116,pipeline,pipeline,116,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,4,['pipeline'],['pipeline']
Deployability,@catoverdrive `updateKey` is the method you're looking for in the unsafe case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2424#issuecomment-344368487:15,update,updateKey,15,https://hail.is,https://github.com/hail-is/hail/pull/2424#issuecomment-344368487,1,['update'],['updateKey']
Deployability,"@catoverdrive this came up while Konrad and I were trying to understand a discrepancy with PCA in python sklearn, which automatically mean centers. This simplest solution would be to add a map that mean centers between irm and computeSVD here:; `val svd = irm.computeSVD(k, computeLoadings)`; But this is redundant when the data is already mean-centered, as in pca_of_normalized_genotypes. Let's discuss when you're back and I can make the changes and update the docs which need some work anyhow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966:452,update,update,452,https://hail.is,https://github.com/hail-is/hail/issues/2734#issuecomment-358096966,1,['update'],['update']
Deployability,"@catoverdrive yes, but that would be normal behavior. We register a series of compression codecs when creating the spark configuration/hadoop configuration/HailContext that hadoop uses to dispatch reading of the file to the appropriate input stream class, it does this based on a method in the codec classes like so in `BGZipCodec.java`; ```java; @Override; public String getDefaultExtension() {; return "".bgz"";; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554:121,configurat,configuration,121,https://hail.is,https://github.com/hail-is/hail/pull/5513#issuecomment-469540554,2,['configurat'],['configuration']
Deployability,@chrisvittal I verified the partitioner is no longer broadcast for the combiner pipeline. This should be ready for review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424#issuecomment-466820072:80,pipeline,pipeline,80,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466820072,1,['pipeline'],['pipeline']
Deployability,@chrisvittal can you take a look at the linux installers? installing docker on linux seems really involved so I left that out.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4603#issuecomment-431977104:46,install,installers,46,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-431977104,2,['install'],"['installers', 'installing']"
Deployability,"@chrisvittal not sure what this error is. Doesn’t happen on local (on local all tests pass, besides the one that also fails on master, `is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF`, because I don't have Plink installed). Will try to investigate tomorrow, first step is accessing the log, but if you have suggestions I’m interested!. 2019-05-16 00:23:41 Hail: INFO: test is.hail.expr.ir.ForwardLetsSuite.testAggregators SUCCESS; 2019-05-16 00:23:41 Hail: INFO: starting test is.hail.expr.ir.ForwardLetsSuite.testForwardingOps...; dlopen: /tmp/hail_dJAhNQ/hm_fd419e9b11e18f87ceb4.so: undefined symbol: _ZN4hail2FSC1EP8_jobject",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219:212,install,installed,212,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492892219,1,['install'],['installed']
Deployability,"@cjllanwarne I tested that the `yum` commands work in the shell for the image with the `docker run` command, and the other component of the testing is that the pipeline step is now passing on this branch :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14677#issuecomment-2338994256:160,pipeline,pipeline,160,https://hail.is,https://github.com/hail-is/hail/pull/14677#issuecomment-2338994256,1,['pipeline'],['pipeline']
Deployability,"@cjllanwarne this is a small bugfix that I would like to have in the release, and it is/was a part of #14675 that's you've already approved, so, could I get a thumb?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14699#issuecomment-2377868937:69,release,release,69,https://hail.is,https://github.com/hail-is/hail/pull/14699#issuecomment-2377868937,1,['release'],['release']
Deployability,"@cseed ; I added a secret to default named `ssl-config-hail-root` containing `hail-root-key.pem`, and `hail-root-cert.pem`. Every principal trusts this root. This root trusts every principal. This PR originally prevented clients from speaking to servers with certs they didn't trust. Now everyone trusts everyone. As long as the root key is not leaked this is OK. Only `create_certs` mounts this secret. The key is used to sign every certificate and the cert is included in each principal's incoming and outgoing trust lists. The root certificate and key are never re-created, so our deploys have no downtime and we avoid addressing the rotation problem. I removed all the trust specifications. A later PR will resolve rotation and mTLS. That PR will restore the trust specifications. I didn't change the structure of the secrets (they still have an incoming and outgoing trust list which only contains the root cert) because I need this structure for mTLS anyway. I've updated the PR description with this text so it ends up in the squashed commit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061:584,deploy,deploys,584,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617911061,2,"['deploy', 'update']","['deploys', 'updated']"
Deployability,@cseed ; ```; AccessDeniedException: 403 vdc-sa@hail-vdc.iam.gserviceaccount.com does not have storage.objects.list access to hail-ci-test.; ```. The CI tests use this hail-ci-test bucket as a fake deploy area. We'll need to fix that before we can merge any CI-related PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023:198,deploy,deploy,198,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435938023,1,['deploy'],['deploy']
Deployability,"@cseed @danking . Hi, I tried the following command , and configured the log path , but it still not worked, are there any suggestions?. spark-submit --executor-memory 16g --executor-cores 4 --class org.broadinstitute.hail.driver.Main ******/hail-all-spark.jar --master yarn-client importvcf --log-file /user/hail/hail.log /user/hail/split_test.vcf splitmulti write -o /user/hail/split_test_1_1.vds exportvcf -o /user/hail/split_test_1_1.vcf. **ERROR:**; WARNING: Running spark-class from user-defined location.; hail: info: running: importvcf /user/hail/sample.vcf; hail: info: Coerced sorted dataset; hail: info: running: splitmulti; hail: info: running: write -o /user/hail/sample_1008.vds; hail: write: caught exception: org.apache.spark.SparkException: Job aborted.; .........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 5, bio-x-3): java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN; ...........; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.io.IOException: The file being written is in an invalid state. Probably caused by an error thrown previously. Current state: COLUMN. [splitmulti_1_1.txt](https://github.com/hail-is/hail/files/521087/splitmulti_1_1.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-252825829:803,deploy,deploy,803,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-252825829,2,['deploy'],['deploy']
Deployability,"@cseed All set! Still not sure why 0.0.0.0 was needed in this case, but not Dan's config; first assumption is that JupyterLab sets this as default, and not sure. why listening on localhost was insufficient (first guess is that the docker image didn't specify EXPOSE 8888?). Still need to provide finer-grained status updates, based on more than status.phase (inspect container during the MODIFIED watch event). Also. need to re-implement auth_request to deal with (ignore) the ~30 requests subsequent to the redirect.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832:317,update,updates,317,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097832,1,['update'],['updates']
Deployability,@cseed Can you take a quick look and make sure this is what you were envisioning?. Not sure if the environment passed in the Table and Matrix IR parsing should be updated with the new bindings or if the refMap from the Table or Matrix IR should replace the existing environment's refMap.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4568#issuecomment-430784918:163,update,updated,163,https://hail.is,https://github.com/hail-is/hail/pull/4568#issuecomment-430784918,1,['update'],['updated']
Deployability,"@cseed Here's my pipeline branch. I'm still thinking about how to pass the bucket around. Will keep iterating, but please let me know if these changes aren't compatible with your proposed changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6110#issuecomment-494143351:17,pipeline,pipeline,17,https://hail.is,https://github.com/hail-is/hail/pull/6110#issuecomment-494143351,1,['pipeline'],['pipeline']
Deployability,"@cseed I don't really like how this looks with lists as the inputs to the commands. To me, it's much harder to read and modify. I like the commands looking as much like writing a shell script as possible. Maybe others feel differently though... Also, you can't do something like this ` ' '.join([task.ofile for task in shapeit_tasks])` because you'll lose information about the dependencies before `command` sees the original resource inputs. What I really want is a version of f-string interpolation where I parse and detect the variables (known and unknown), handle them properly by either creating new resources or adding dependencies to the Task, and then execute the Python formatting code inside the curly braces. I'm not sure if it is possible to do this. If it is, it's probably complicated and we'll have to use the Python `ast` and `parser` modules and call `eval` ourselves. ; ```python; from pyapi import Pipeline; p = Pipeline(). bfile_root = 'gs://jigold/input'; bed = bfile_root + '.bed'; bim = bfile_root + '.bim'; fam = bfile_root + '.fam'. p.write_input(bed=bed, bim=bim, fam=fam). subset = p.new_task(); subset = (subset; .label('subset'); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--make-bed', '--out', subset.tmp1]); .command(['awk', ""'{ print $1, $2}'"", subset.tmp1 + '.fam', ""| sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }'"",; '>', subset.tmp2]); .command(['plink', '--bed', p.bed, '--bim', p.bim, '--fam', p.fam, '--remove', subset.tmp2,; '--make-bed', '--out', subset.tmp2])). shapeit_tasks = []; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .command(['shapeit', '--bed-file', subset.ofile, '--chr ', contig, '--out', shapeit.ofile])); shapeit_tasks.append(shapeit). merger = p.new_task(); merger = (merger; .label('merge'); .command(['cat', ' '.join([task.ofile for task in shapeit_tasks]), '>>', merger.ofile)). p.write_output(merger.ofile + "".haps"", ""gs://jigold/final_output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039:917,Pipeline,Pipeline,917,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-447469039,2,['Pipeline'],['Pipeline']
Deployability,"@cseed I know you didn't approve the final design document. In summary,. 1. We print a warning to the user if they specify the memory in the batch user library that we're ignoring this parameter. Memory is deduced from CPU and the worker configuration in the front end ignoring the resource request.; 2. There's a concept of reserved vs. unreserved space. Each job gets 5 Gi per core requested in the reserved space. We try and reserve unreserved space if needed for storage needs that are bigger than the reserved space with a semaphore. If we can't get enough unreserved space, then we still give the user the reserved space at 5Gi per core for their container on the worker data disk in addition to the extra disk at /io that is the full storage request. Example:. Storage request is 375Gi and 1 CPU.; User gets 5 Gi for their container.; We spin up a 375Gi disk. For a local ssd with 16 cores, there's 16*5 Gi or 80 Gi in the reserved space. The reserved space for us is 20 Gi (I can set this back to 25 Gi, but I thought 100 Gi being the minimum disk size for persistent SSD data disks was a nice number). This means the unreserved space that is first come first serve is 275 Gi. If the data disk is a 100Gi persistent SSD, then the unreserved space is 0 Gi and any job that requests more storage than 5 Gi per core will have to spin up a new disk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813:238,configurat,configuration,238,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-714610813,1,['configurat'],['configuration']
Deployability,"@cseed I made these changes apart from splitting out the idiom, which I'd like to do once both this and HailBlockMatrix are in 0.1, before PR against 0.2. I pulled in RichSparkMatrix with asBreeze conversion (also used to convert Spark to Breeze without copy in LMM branch...though maybe that won't be necessary with HailBlockMatrix), so if you don't want to leave that in, I can remove by backing off to `.toArray`, or remove when HailBlockMatrix is in (or wait until the latter is in; Dan just made updates on that which I'll review tomorrow). Is the additional check on kryo registrator a meaningful breaking change on 0.1?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2276#issuecomment-334883633:501,update,updates,501,https://hail.is,https://github.com/hail-is/hail/pull/2276#issuecomment-334883633,1,['update'],['updates']
Deployability,@cseed I'll close this and just ask for feedback on the branch diff once I have a pipeline working end-to-end and thoughts on proper integration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460:82,pipeline,pipeline,82,https://hail.is,https://github.com/hail-is/hail/pull/2890#issuecomment-366455460,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,@cseed I'm also OK approving and addressing these issues in another PR if that unblocks our deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475308077:92,deploy,deploy,92,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475308077,1,['deploy'],['deploy']
Deployability,"@cseed I'm really happy with the interface now! Could you please look over this again and let me know if there are any suggestions you have before I write some tests and give this to someone to code review. I also called this `pyapi` for lack of a better name and it's currently in the batch module... ```python3; from pyapi import Pipeline, resource_group_builder. p = Pipeline() # initialize a pipeline. # Define resource group builders (used with `declare_resource_group`); rgb_bfile = resource_group_builder(bed=""{root}.bed"",; bim=""{root}.bim"",; fam=""{root}.fam""). rgb_shapeit = resource_group_builder(haps=""{root}.haps"",; log=""{root}.log""). # Import a file as a resource; file = p.write_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.write_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .declare_resource_group(tmp1=rgb_bfile, ofile=rgb_bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:332,Pipeline,Pipeline,332,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741,3,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"@cseed Still running the pipeline, but can confirm I got 293 partitions for chr22 and the import step took about 1 minute. ```; 2018-07-18 15:39:30 Hail: INFO: Number of BGEN files parsed: 1; 2018-07-18 15:39:30 Hail: INFO: Number of samples in BGEN files: 487409; 2018-07-18 15:39:30 Hail: INFO: Number of variants across all BGEN files: 1255683; 2018-07-18 15:40:37 Hail: INFO: Coerced almost-sorted dataset; 2018-07-18 15:40:39 Hail: INFO: interval filter loaded 5 of 293 partitions; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3945#issuecomment-405977581:25,pipeline,pipeline,25,https://hail.is,https://github.com/hail-is/hail/pull/3945#issuecomment-405977581,1,['pipeline'],['pipeline']
Deployability,"@cseed The CI tests rely on the service waiting functionality. In particular, they do not have permissions to use `kubectl rollout status` so I can't switch to waiting for a deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7564#issuecomment-557584616:123,rollout,rollout,123,https://hail.is,https://github.com/hail-is/hail/pull/7564#issuecomment-557584616,2,"['deploy', 'rollout']","['deployment', 'rollout']"
Deployability,"@cseed This PR can be merged. I ran the comparison on the cloud between current master and this branch with UKBB Wave 1 Chr21 (20GB) with the exact same cluster configuration (Liam's default settings). Ran this command:. ```; %%timeit -n 1. hc.import_bgen(bgen_file, sample_file = sample_file).count(); ```. Got 3min21sec for master and 3min24sec for my branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2239#issuecomment-337996806:161,configurat,configuration,161,https://hail.is,https://github.com/hail-is/hail/pull/2239#issuecomment-337996806,1,['configurat'],['configuration']
Deployability,@cseed What are you thinking of in pipeline? Both CI and pipeline should use the batch client and thus be abstracted from changes in URLs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6288#issuecomment-501727081:35,pipeline,pipeline,35,https://hail.is,https://github.com/hail-is/hail/pull/6288#issuecomment-501727081,2,['pipeline'],['pipeline']
Deployability,@cseed do you want me to make the changes and push to this branch? Or I can make a patch for you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2958#issuecomment-369635244:83,patch,patch,83,https://hail.is,https://github.com/hail-is/hail/pull/2958#issuecomment-369635244,1,['patch'],['patch']
Deployability,"@cseed got you covered, from a JAR install:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/commit/8269129095ea}}; }; ```; from pip:; ```; In [3]: print(hl.cite_hail_bibtex()) . @misc{Hail,; author = {Hail Team},; title = {Hail},; howpublished = {\url{https://github.com/hail-is/hail/releases/tag/0.2.13}}; }. In [4]: print(hl.cite_hail()) ; Hail Team. Hail 0.2.13-81ab564db2b4. https://github.com/hail-is/hail/releases/tag/0.2.13.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5915#issuecomment-484747010:35,install,install,35,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484747010,3,"['install', 'release']","['install', 'releases']"
Deployability,"@cseed took a different approach, generating a python file which I import in hail initialization. This seemed like the only way to make things work in all the ways Python can be deployed (zip, egg, directory)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4328#issuecomment-422128214:178,deploy,deployed,178,https://hail.is,https://github.com/hail-is/hail/pull/4328#issuecomment-422128214,1,['deploy'],['deployed']
Deployability,"@cseed, details below, exec summary: a series of confusions and confusing interfaces lead to cloud tools deployment issues. The resolution is to fix the deploy script to always deploy `cloudtools` for python 2 and 3 (because *it* works with both). Users will need `python2` somewhere on their PATH, but `gsutil` will guide them through that, i.e. not our problem. There is no problem except that I cannot approve my own PR. I noticed @catoverdrive wasn't actually assigned, so it wasn't in her scorecard queue. ---. There was a bit of confusion around this recently that @catoverdrive figured out. So, `gsutil` only supports `python2`, but it's happy to search through your path for a variety of binaries with suggestive names until it finds a valid python 2.x binary. `cloudtools` itself works with any version of python (there was a recent breaking change in the release of python 3.7, which broke cloud tools [but that's been resolved in an open PR](https://github.com/Nealelab/cloudtools/pull/91/files#diff-7decff7c08c5270a32982ea34483b8cbR11)). Now, wrt PYPI, @catoverdrive discovered that the version of python you use to run `setup.py bdist_wheel` determines which version of python is deployed. This is rather confusing and, since none (or many?) of us did not know this, we accidentally uploaded a variety of [cloud tools versions on pypi, as you've noted](https://pypi.org/simple/cloudtools/). There was some earlier confusion wherein I thought we should only support `python2`, but that was a mistake on my end. We should support both versions, and when the user tries to start a cluster, `gsutil` will provide an error message guiding them to install `python2`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588:105,deploy,deployment,105,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419912588,6,"['deploy', 'install', 'release']","['deploy', 'deployed', 'deployment', 'install', 'release']"
Deployability,"@cseed, not sure if you know, but you can actually run both Spark 1 and 2 on CDH side-by-side (I assume that's what you mean by the on-prem machines). See here for how to install: https://www.cloudera.com/documentation/betas/spark2/latest/topics/spark2_installing.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1124#issuecomment-263318325:171,install,install,171,https://hail.is,https://github.com/hail-is/hail/pull/1124#issuecomment-263318325,1,['install'],['install']
Deployability,"@cseed: It would be great if we could merge this into master soon -- there's a lot of changes here!. Highlight of major changes:; 1. Dosage is implemented in Genotype.scala; - A user can get either dosages `.dosage` or PLs `.pl`; - To go from PLs to Dosages: rescale each PL (10^(-PL/10)), take the sum of the rescaled numbers, then divide by the sum. This is assuming equal weights prior (can incorporate alternate prior later); - To go from Dosages to PLs: same transformation as before; 2. INFO score is implemented in variantqc; - No tests for info score yet as still uncertain which method to use; - My computation agrees with SNPTEST but not QCTOOL; 3. `importgen` and `exportgen` are now implemented; 4. SplitMulti will split dosages correctly except for the setting of false ref. If the original dosage with N genotypes had more than one maximum value [ex: (0.2, 0.2, 0.1, 0.1, 0.1, 0.3)], then the original genotype is -1. But after combining dosages, then there is one unique maximum value. The fakeref flag is not set in this case, but the genotype is > 0.; 5. A randomly generated genotype can have two values very close together (0.4035, 0.4036, 0.2...) that when read back in via gen file or bgen file will have rounding error (0.4035, 0.4035, 0.2...) so there is no maximal genotype anymore (gt = -1). I don't think this is a huge concern as it can only happen if the max dosage is <= 0.5, and these will get filtered out by most users anyways. **To-Do:; 1. Finalize INFO score calculation and write tests; 2. Fix null variant in PLINK code (want to do this in separate branch); 3. Modify variant qc to read parameter about data so info score only calculated for dosage data and likewise for statistics about depth, gq etc.; 4. Handle sex chromosome names in import PLINK properly (do we need to map ""23"" to ""X"", etc.?); 5. Update the readFam function in `importplink` to utilize functionality Jon wrote already",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/243#issuecomment-218212906:1839,Update,Update,1839,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-218212906,1,['Update'],['Update']
Deployability,@daniel-goldstein I added an exit 1 after argument validation and removed the test-dataproc and wheel dependencies in the Makefile to demonstrate the functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=o,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:470,deploy,deploy-,470,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,5,['deploy'],['deploy-']
Deployability,"@daniel-goldstein I noticed a bug in table.py, patched that in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6078#issuecomment-490631176:47,patch,patched,47,https://hail.is,https://github.com/hail-is/hail/pull/6078#issuecomment-490631176,1,['patch'],['patched']
Deployability,@daniel-goldstein should this PR update line 38 of main.tf which uses Ubuntu 20.04 for the batch worker? Do we need to take special action if that happens?. If we do this we also need to change `infra/azure/create_bootstrap_vm.sh` which references `focal`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13440#issuecomment-1679539947:33,update,update,33,https://hail.is,https://github.com/hail-is/hail/pull/13440#issuecomment-1679539947,1,['update'],['update']
Deployability,"@daniel-goldstein, hail/python/requirements.txt still has `google-cloud-storage==1.25.*` in `main`. Wonder if there was a specific reason to pin to the older version? It would be great to update!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520#issuecomment-1123118440:188,update,update,188,https://hail.is,https://github.com/hail-is/hail/pull/11520#issuecomment-1123118440,1,['update'],['update']
Deployability,"@daniel-goldstein, since the `release` step decides whether or not to publish a new release, I decided not to add `is_release` to CI. Instead I've introduced a new file called `release-hail-flag`, writted to by the release step, and used by its dependents to decide if subesquent release actions should be run. What do you think? Is this reasonable?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14398#issuecomment-2026073794:30,release,release,30,https://hail.is,https://github.com/hail-is/hail/pull/14398#issuecomment-2026073794,5,['release'],"['release', 'release-hail-flag']"
Deployability,"@danking , I'm a bit stuck on how to proceed with the credential refreshing. Here's the layout of the problem:. 1. In normal Azure, we accept user-provided SAS tokens. Since they are user-provided, we have no way of obtaining new ones and the onus is on the user to obtain a SAS token for however long they expect to need to use it.; 2. This current design in Terra is to not make the user have to do that, because that seems annoying, and for terra-controlled ABS containers we have an endpoint we can hit to get a SAS token. Ok, but now we need to update our Azure FS infrastructure to refresh a credential if it expires. But, we use the azure client lib and don't control all http requests. For example, for `AzureStorageFS.open`, we call `downloader.readall()` if we want to load the whole file into memory. I went spelunking through their source and looks like `readall` mostly wraps a sequence of range reads, but regardless if we were to use that method we would have to catch credential expiration errors, reset credentials on the blob client and retry hoping that we didn't break any invariants -- I don't want to do that as I wouldn't trust a stream that encountered a non-transient error like that. It could be that getting rid of `downloader.readall` is the only thing we have to worry about, but it makes me uneasy not having control of the http requests we're making to ABS. Do you see a solution other than raking through our `aioazure.fs` and making sure that we only use ""quick"" methods and possibly retrying 401s? It just seems to me like we're going against the grain and even though it feels user-hostile the intention of SAS tokens are to have users own credential expiration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715:550,update,update,550,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1930482715,1,['update'],['update']
Deployability,"@danking - At first glance I do not see any installation of pyspark. * `pip show pyspark` -> WARNING: Package(s) not found: pyspark. * I see mention of `pyspark 3.3.3` in `/hail/hail/python/pinned-requirements.txt` but it seems not installed as per the logs (see below); ```; pyspark==3.3.3; # via -r hail/hail/python/requirements.txt`; ``` . * I do not see any `pyspark` in `/hail/hail/python/hailtop/hailctl/deploy.yaml`. * Checking in `/usr/lib/spark` I see reference of scala 2.12.15 same as in the hail logs; ```sh; $ cat /usr/lib/spark/RELEASE ; Spark 3.3.2-amzn-0.1 built for Hadoop 3.3.3-amzn-3.1; Build flags: -Divy.home=/home/release/.ivy2 -Dsbt.ivy.home=/home/release/.ivy2 -Duser.home=/home/release -Drepo.maven.org= -Dreactor.repo=file:///home/release/.m2/repository -Dhadoop.version=3.3.3-amzn-3.1 -Dyarn.version=3.3.3-amzn-3.1 -Dhive.version=2.3.9-amzn-3 -Dparquet.version=1.12.2-amzn-3 -Dprotobuf.version=2.5.0 -Dfasterxml.jackson.version=2.13.4 -Dfasterxml.jackson.databind.version=2.13.4 -Dcommons.httpclient.version=4.5.9 -Dcommons.httpcore.version=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" whic",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:44,install,installation,44,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,8,"['RELEASE', 'deploy', 'install', 'release']","['RELEASE', 'deploy', 'installation', 'installed', 'release']"
Deployability,@danking - Do you have an idea from where this problematic java class should be sourced from ?; I do recall needed to mandle with some symlinks between java directories to install an old version of Hail as JDK was installed properly but some jar from JRE was needed and not present in the JDK directory. ```sh; # for Hail v0.2.32; sudo ln -s /etc/alternatives/java_sdk/include /etc/alternatives/jre/include; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777469472:172,install,install,172,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777469472,2,['install'],"['install', 'installed']"
Deployability,"@danking -; Let's compare with & without Hail install; I don't know where to find `load-spark-env.sh` so I only print the env once. ## Without Hail. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; /usr/bin/spark-shell; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; /usr/bin/spark-submit; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; /usr/bin/spark-class; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; SPARK_SCALA_VERSION=; ```; <details><summary>>>>>>>>>>> before load-spark-env.sh <<<<<<<<<</summary>; <p>; ```sh; XDG_SESSION_ID=38; HOSTNAME=ip-192-168-96-172; TERM=xterm-256color; SHELL=/bin/bash; HISTSIZE=1000; SSH_CLIENT=103.37.196.84 57805 22; QTDIR=/usr/lib64/qt-3.3; QTINC=/usr/lib64/qt-3.3/include; SSH_TTY=/dev/pts/0; USER=hadoop; LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:46,install,install,46,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,2,"['install', 'release']","['install', 'release']"
Deployability,"@danking . This should be ready to look at. Stacks on #5452. Once that PR is merged, changes are:. 1) notebook.html: organize into form (notebook-form.html) and notebook state (notebook-state.html) components.; 2) add modified versions of notebook-api. Namely I don't use the marshaling procedure you didn't like, and refactor as much of the JS stuff as I can into synchronous http requests.; * modifies /notebook routes , adds `marshall_notebook`, `get_live_user_notebooks`, `wait_websocket`, and replace any calls to `session['pod_name']` and `session['svc_name`] with equivalent versions based on `session['notebook']`, which contains the notebook object of the existing session. Changes mainly contained within commit: https://github.com/hail-is/hail/pull/5476/commits/2f180ed0bfb3b0dfb7224df1ef6afba0e1a9cbfc (the following pr only renames notebook-obj.html to notebook-state.html). Basically feels like a synchronous / refresh-based version of what we had on app.hail.is, with less state insight (uses only the websocket-based reachability check). Upcoming PR will restore fine-grained state updates via JS/websocket. cc @cseed. Images:; <img width=""1302"" alt=""screen shot 2019-02-28 at 3 06 46 pm"" src=""https://user-images.githubusercontent.com/5543229/53595163-88d8ea00-3b6a-11e9-841b-7dbf6981c990.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 06 51 pm"" src=""https://user-images.githubusercontent.com/5543229/53595148-7ced2800-3b6a-11e9-9428-5290b5ee1dc7.png"">. <img width=""1301"" alt=""screen shot 2019-02-28 at 3 09 17 pm"" src=""https://user-images.githubusercontent.com/5543229/53595276-d35a6680-3b6a-11e9-930e-5ef0757e181e.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208:1098,update,updates,1098,https://hail.is,https://github.com/hail-is/hail/pull/5476#issuecomment-468419208,1,['update'],['updates']
Deployability,"@danking @cseed should be ready for testing soon. pruned portions we're not using atm, wrote docker files, tested. if you want to run both packages on your local machine, you could use the top-level docker file (or each package's). To start a local instance of the web app, simply run:; `npm install && npm bootstrap`. To get a hot-reloading version of the web app (links to your browser, refreshes all changes): `cd packages/public && npm run dev`. To start the gateway: `cd packages/hail-api-gateway && nodemon index.js`. Dev mode routing is slow. To see a production, minified build: `cd packages/public && npm run build && npm run prod-test`.; * Build is a kind of compilation process. Dev dependencies are pruned, the app is split into static bundles, and minified. Some optimizations, like inlining of some React functions also occurs. This is independent of anything V8 does . This will also show a neat readout of all bundles:; ```; Browser assets sizes after gzip:. 2.79 kB .next/static/gZEz****/pages/_app.js; 2.42 kB .next/static/gZEz****/pages/_error.js; 502 B .next/static/gZEz****/pages/auth0callback.js; 349 B .next/static/gZEz****/pages/index.js; 745 B .next/static/gZEz****/pages/notebook.js; 856 B .next/static/gZEz****/pages/scorecard.js; 243 B .next/static/gZEz****/pages/tutorial.js; 99.4 kB .next/static/chunks/commons.294f****.js; 101 B .next/static/chunks/styles.9f25****.js; 450 B .next/static/css/commons.b770adbe.chunk.css; 5.74 kB .next/static/css/styles.4f393762.chunk.css; 6.93 kB .next/static/runtime/main-76ed****.js; 737 B .next/static/runtime/webpack-8917****.js; ```. Bundling cutoffs can be tweaked, but basically any common dependencies between pages are placed into one chunk. Chunks are loaded in parallel, and no chunks are needed to load the page; it's just HTML on initial render. At least some of the chunks could theoretically be served from a CDN (styles of course, some js). Each package expects a .env file, which organizes the environment variables used",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935:292,install,install,292,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454271935,1,['install'],['install']
Deployability,"@danking Are there any releases of Hail that support 3.2? It seems like hail went from 3.1 -> 3.3 and that no builds hosted on Github support 3.2, and that because pyspark is pinned to 3.3+ we'd need to fork it to install hail in any projects that use spark 3.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11707#issuecomment-1597830010:23,release,releases,23,https://hail.is,https://github.com/hail-is/hail/issues/11707#issuecomment-1597830010,2,"['install', 'release']","['install', 'releases']"
Deployability,"@danking Failing check_services, let's fix and release",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11691#issuecomment-1082139926:47,release,release,47,https://hail.is,https://github.com/hail-is/hail/pull/11691#issuecomment-1082139926,1,['release'],['release']
Deployability,@danking I added a change log for Batch. I assume this change will be released in the next version 0.2.42.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8795#issuecomment-630925598:70,release,released,70,https://hail.is,https://github.com/hail-is/hail/pull/8795#issuecomment-630925598,1,['release'],['released']
Deployability,"@danking I ended up rewriting this a bit to make it work with the nginx timeout (instead of getting rid of the timeout, since having a heartbeat seems like a pretty reasonable thing); updated the PR description to match.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840:184,update,updated,184,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-717406840,1,['update'],['updated']
Deployability,"@danking I have unit + integration tests for 2 methods in this package. Do you want me to add them here, or in a separate PR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000:23,integrat,integration,23,https://hail.is,https://github.com/hail-is/hail/pull/5369#issuecomment-465392000,1,['integrat'],['integration']
Deployability,@danking I just want to double check you are okay with this change before it goes on. I'm going to do one more pass on dev deploy to make sure everything still looks good. I'll update the mailing list and then send out an announcement on both Zulip and the mailing list notifying everyone that the pricing is changing.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1142316825:123,deploy,deploy,123,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1142316825,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,"@danking I think I addressed most of your comments. Everything is passing. Can you look if you like the interface for `sync_check_call` etc.? If so, then can I translate your TLS commands in lists into a single string? Is there ever a reason to want to pass the list explicitly in this case?. Once you're happy with the changes, then I'll do another test round with dev deploy to make sure the cache is actually doing its thing and the garbage collection loop is working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101:370,deploy,deploy,370,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-665855101,1,['deploy'],['deploy']
Deployability,"@danking I try from the last commin of Hail. That seems to solve the issue of Spark version but not the java error... ```; // Setup EMR + python 3.9 + java 11 without installin hail; // Check pyspark; $ pyspark --version; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21. // Clone last commit of Hail & install; $ export PATH=$PATH:/home/hadoop/.local/bin; $ cd /tmp; $ git clone --depth 1 https://github.com/broadinstitute/hail.git; $ cd hail/hail/; $ make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; // Check pyspark; $ pyspark --version; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21. // Create symlink to hail-all-spark.jar; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python${PYTHON_VERSION}/site-packages/hail/backend /opt/hail/backend; // Launch spark-shell; $ spark-shell; Exception in thread ""main"" java.lang.NoSuchMethodError: 'scala.reflect.internal.settings.MutableSettings scala.reflect.internal.settings.MutableSettings$.SettingsOps(scala.reflect.internal.settings.MutableSettings)'; ```. Could it be a problem of PATH ? issue with where Hail is installed ?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1775200581:167,install,installin,167,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1775200581,4,['install'],"['install', 'install-on-cluster', 'installed', 'installin']"
Deployability,"@danking I'd apply this patch. Not sure why we install those dependencies in the auth dockerfile, they're already in the docker requirements:. ```diff; diff --git a/auth/Dockerfile b/auth/Dockerfile; index 0c2bfa4dad..a94928c697 100644; --- a/auth/Dockerfile; +++ b/auth/Dockerfile; @@ -1,9 +1,5 @@; FROM {{ service_base_image.image }}; ; -RUN hail-pip-install \; - google-auth-oauthlib==0.4.6 \; - google-auth==1.25.0; -; COPY auth/setup.py auth/MANIFEST.in /auth/; COPY auth/auth /auth/auth/; RUN hail-pip-install /auth && rm -rf /auth; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12544#issuecomment-1349545278:24,patch,patch,24,https://hail.is,https://github.com/hail-is/hail/pull/12544#issuecomment-1349545278,4,"['install', 'patch']","['install', 'patch']"
Deployability,@danking Looks like we need to upgrade `pytest_asyncio`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14097#issuecomment-1881181045:31,upgrade,upgrade,31,https://hail.is,https://github.com/hail-is/hail/pull/14097#issuecomment-1881181045,1,['upgrade'],['upgrade']
Deployability,"@danking Pushed a version that should work on local, now focusing on deployment changes. This is a clean fork; I rolled back all notebook changes to master. notebook-api/notebook/notebook.py is the file to review. Corresponding client pr commit: https://github.com/hail-is/hail/pull/5162/commits/7afc4a5b599a233a4e4b40bb9c7a260b062dd925; - This also includes all CORS bits. With the caveat that this is my first attempt at Kubernetes events, I think this moves things in the right direction. We now have an authenticated, push-notification system for an arbitrary number of notebooks. There are a few issues with it currently, mostly in handling closed web socket connections in gevent, which I will move away from in the iteration after Wednesday, but I handle dead socket errors and they don't *seem* to accumulate over time. I also need to implement a reconnection system on the client. The neat thing about this synchronizes sessions between refresh. So if you have N collaborators all on the same window (or more likely, you have 2 windows open), they will all get consistent state as quickly as Kubernetes knows it. This may not seem useful atm, but it allows us to get really fine-grained view into svc/pod uptime. This also should be much faster, provided we don't overburden the server with watchers (can be solved using server implementation as well), say by using an interval of a second, because we query kubernetes directly, rather than hitting the liveness endpoint by traveling over public internet and then being proxied at the boundary by nginx. We know within ms of the true state. Remaining q is whether this completely replicates the liveness endpoint. I also tried to make the serializing the kubernetes object the domain of the caller; I like this because the called can stop thinking about whether something implements __getitem__, and can specify pretty arbitrary transformations on that data (see lines 172-210, 331, 360, and all other calls to marshall_json), and allows us t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071:69,deploy,deployment,69,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459839071,1,['deploy'],['deployment']
Deployability,@danking Sorry for the delay on this. Trying to move this along faster now to unblock the release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1344624461:90,release,release,90,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1344624461,1,['release'],['release']
Deployability,"@danking Still needs some updates. I think it's low priority. Would it be more appropriate to close this (and multiple notebooks pr), and re-issue when ready?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5540#issuecomment-481370104:26,update,updates,26,https://hail.is,https://github.com/hail-is/hail/pull/5540#issuecomment-481370104,1,['update'],['updates']
Deployability,"@danking Tests are passing. What's the problem?. Strange, cloudtools is showing 2.0.0 available, but pip is installing 1.1.6. ```; + pip search cloudtools; cloudtools (2.0.0) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/47/f1/bec895151ea74b2117c66620840e9a86436b376927b557b080289b61f754/cloudtools-1.1.16-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.16; ```. Ah, cloudtools 1.2.0 and 2.0.0 were set up as python 2 packages, see https://pypi.org/simple/cloudtools/:. ```; cloudtools-1.1.16-py2-none-any.whl; cloudtools-1.1.16-py3-none-any.whl; cloudtools-1.2.0-py2-none-any.whl; cloudtools-2.0.0-py2-none-any.whl; ```. 1.1.16 is set up for both. @liameabbott I assume we're only supporting Python 3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154:108,install,installing,108,https://hail.is,https://github.com/hail-is/hail/pull/4240#issuecomment-419786154,4,"['Install', 'install']","['Installing', 'install', 'installed', 'installing']"
Deployability,@danking Thank you. This looks good for now. I'll prepare a follow on for linux installation.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4603#issuecomment-435163425:80,install,installation,80,https://hail.is,https://github.com/hail-is/hail/pull/4603#issuecomment-435163425,1,['install'],['installation']
Deployability,@danking That's because we weren't actually respecting the `HAIL_GENETICS_HAIL_IMAGE` for the default python image and always referencing Dockerhub. This didn't manifest until now because we were using the `python-dill` images which do not change every release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13659#issuecomment-1725696869:253,release,release,253,https://hail.is,https://github.com/hail-is/hail/pull/13659#issuecomment-1725696869,1,['release'],['release']
Deployability,@danking This should be really close to having the tests passing. The only other thing left to do once it passes in GCP is to copy the data over to Azure and make a default Azure configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1339804624:179,configurat,configuration,179,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1339804624,1,['configurat'],['configuration']
Deployability,@danking What do you think about having a version ID inside the JAR file (MANIFEST???). We already download the JAR file on the worker. Not sure how much extra time it would be to look for the version inside the JAR (maybe cache this?) and then pass the right argument configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909:269,configurat,configuration,269,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1258646909,1,['configurat'],['configuration']
Deployability,@danking back to you. restarted the integration test.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026:36,integrat,integration,36,https://hail.is,https://github.com/hail-is/hail/pull/1748#issuecomment-298957026,1,['integrat'],['integration']
Deployability,@danking can you edit the title / commit message to something more patch-notes-y? 🙏,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5421#issuecomment-467106563:67,patch,patch-notes-y,67,https://hail.is,https://github.com/hail-is/hail/pull/5421#issuecomment-467106563,1,['patch'],['patch-notes-y']
Deployability,@danking suggested we move this to a separate project from `batch`. Possibly call it `pipeline`.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452825568:86,pipeline,pipeline,86,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452825568,1,['pipeline'],['pipeline']
Deployability,@danking thanks for the update. Looks good to me!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1452#issuecomment-290754177:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290754177,1,['update'],['update']
Deployability,"@danking what do you think is best? I installed them from `R 3.4.0` with:; ```; source(""https://bioconductor.org/biocLite.R""); biocLite(""GENESIS""); biocLite(""SNPRelate""); biocLite(""GWASTools""); ```; But you also mentioned some incompatibility with the latest version of R.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377699456:38,install,installed,38,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377699456,1,['install'],['installed']
Deployability,"@danking, @cseed An attempt to use --notebook-dir failed (Didn't understand the path). Will make another attempt to set this as a config, but if not, I think we should defer folder creation as an improvement to jgscm, I'll open an issue. Have forked jgscm, and have identified what appears a likely path to the fix (they don't specify the full blob path, gs://bucket/blob). As an aside, jgscm is effectively unmaintained. 2 of the problems I've encountered have issues dating to May & August (last accepted PR was April 2018). After we patch in the fixes needed (dependencies, folder creation), I think we should consider publishing a separate package from our fork (say jgscm2), unless we want to maintain jgscm in our repo, which may be less desirable from a licensing perspective based on our earlier convos (jgscm is MIT, but I believe you still may prefer to not mix codebases?)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780:536,patch,patch,536,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480335780,1,['patch'],['patch']
Deployability,"@danking, I've addressed your comments. Unfortunately w.r.t your time, I've removed all mysql components. Once I get websocket code working (some issue right now, 402, gevent ws doens't like my headers, maybe cors related), I will push an update. The update will revert all changes to notebook.py, and add a completely separate folder, notebook-api, in which all further changes will be made. I want to keep your (working) notebook code completely distinct, so that Cotton has a fallback for Feb 5 if needed. After Wed we can remove the old code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-459535569:239,update,update,239,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-459535569,2,['update'],['update']
Deployability,"@danking. Here what I have done in my environment ( AWS EMR ); * Create EMR without installing hail; * Update PATH ( this is needed or I get an error with `hailctl not found` at the installation step); ```sh; export PATH=$PATH:/home/hadoop/.local/bin; ```; * Clone latest commit of Hail; ```sh; cd /tmp; git clone --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; ```; * Edit `build.gradle` and add `exclude group: 'org.scala-lang', module: 'scala-reflect'`; * Build Hail; ```sh; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```; * Symlink hail-all-spark.jar into /opt ( At the EMR creation step (before hail installation) I edit the `spark-defaults` properties in order to link `hail-all-spark.jar`... This config was needed & works successfuly for an old version of Hail (0.2.60)... can be revisit if not appropriate for recent version; ```sh; sudo mkdir /opt/hail/; sudo ln -sf /home/hadoop/.local/lib/python3.9/site-packages/hail/backend /opt/hail/backend; ```; * start pyspark; ```sh; $ pyspark; Python 3.9.18 (main, Oct 25 2023, 05:26:35) ; [GCC 7.3.1 20180712 (Red Hat 7.3.1-17)] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; SLF4J: No SLF4J providers were found.; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See https://www.slf4j.org/codes.html#noProviders for further details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:84,install,installing,84,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,5,"['Update', 'install']","['Update', 'install-on-cluster', 'installation', 'installing']"
Deployability,"@danking; ```; [root@mg hail]# echo $HAIL_HOME; /opt/Software/hail; [root@mg hail]# echo $PYTHONPATH; :/opt/Software/hail/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python:/opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/py4j-0.10.4-src.zip; [root@mg hail]# cd /opt/Software/hail/python; [root@mg python]# ls; hail; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python; [root@mg python]# ls; docs lib MANIFEST.in pylintrc pyspark README.md run-tests run-tests.py setup.cfg setup.py test_support; [root@mg python]# cd /opt/cloudera/parcels/SPARK2/lib/spark2/python/lib/; [root@mg lib]# ls; py4j-0.10.4-src.zip PY4J_LICENSE.txt pyspark.zip; [root@mg lib]# echo $SPARK_CLASSPATH; /opt/Software/hail/build/libs/hail-all-spark.jar; [root@mg lib]# cd /opt/Software/hail/build/libs/; [root@mg libs]# ls; hail-all-spark.jar; ```; the configuration file:; ```; export SPARK_HOME=/opt/cloudera/parcels/SPARK2/lib/spark2; export HAIL_HOME=/opt/Software/hail; export PYTHONPATH=""$PYTHONPATH:$HAIL_HOME/python:$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j*-src.zip`""; export SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177:847,configurat,configuration,847,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337442177,1,['configurat'],['configuration']
Deployability,"@ehigham Also another question is how does the schema update enforce certain order of operations. . The `rename-job-groups-cancelled-column` sql should run before other sqls that depend on the modified column name in `job_groups_cancelled` table, correct?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2334796201:54,update,update,54,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334796201,1,['update'],['update']
Deployability,@gtiao I think the `gcp-public-data--gnomad` bucket still hasn't been updated. Do you have a tentative date for the update?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-811255128:70,update,updated,70,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-811255128,2,['update'],"['update', 'updated']"
Deployability,"@gtiao, great!. We haven't released yet, so we can still fix. It seems the right fix is to re-generate that file as a Table rather than a MatrixTable because it doesn't have any entry data. @pwc2 could you furnish a list of all the misnamings?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9955#issuecomment-771159438:27,release,released,27,https://hail.is,https://github.com/hail-is/hail/pull/9955#issuecomment-771159438,1,['release'],['released']
Deployability,@ihelbig Did you install through brew or pip? Normally we recommend downloading the official Spark distribution. @danking Did you install through brew? Did you see anything like this?. /Users/ih/hailenv/lib/python2.7/site-packages/pyspark/java_gateway.py:77 is trying to start Spark by invoking $SPARK_HOME/bin/spark-submit. What is $SPARK_HOME?. You might modify java_gateway.py before like 77 to print out `command` to see what command in detail it is trying to invoke.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319701721:17,install,install,17,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319701721,2,['install'],['install']
Deployability,"@iris-garden I think you make great points! And I agree, across many PRs we probably do want to be analyzing the security impacts at every stage, not just as a one-off ""when we're done it will be X"" analysis in the ticket... So I guess in my mind the _only_ real reason for using the issue-level review would be for tracking the impact of non-code changes (like configuration updates to production). I will try to make the templates reflect that distinction",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981:362,configurat,configuration,362,https://hail.is,https://github.com/hail-is/hail/pull/14666#issuecomment-2329290981,2,"['configurat', 'update']","['configuration', 'updates']"
Deployability,@iris-garden Would you be interested in collaborating on this set of changes? I know you were starting to think about environment variables and configuration as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13224#issuecomment-1663830418:144,configurat,configuration,144,https://hail.is,https://github.com/hail-is/hail/pull/13224#issuecomment-1663830418,1,['configurat'],['configuration']
Deployability,"@jbloom22 @tpoterba this makes me uncomfortable, we have no canary if PCRelate starts failing or if any of the infrastructure on which it depends starts failing. What is the long term plan?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3274#issuecomment-377931559:60,canary,canary,60,https://hail.is,https://github.com/hail-is/hail/pull/3274#issuecomment-377931559,1,['canary'],['canary']
Deployability,"@jbloom22 This line is wrong:; https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/utils/Graph.scala#L80; `l` and `r` should be tuples (i.e. rows). When the IR route was added, it was modified to take tuples, but the AST route was not updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3704#issuecomment-394407542:250,update,updated,250,https://hail.is,https://github.com/hail-is/hail/pull/3704#issuecomment-394407542,1,['update'],['updated']
Deployability,"@jbloom22 can you review this? You know most about Nirvana interop. Also, how do we get it integrated into cloudtools?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-377697212:91,integrat,integrated,91,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-377697212,1,['integrat'],['integrated']
Deployability,"@jigold ; this should be working, not sure why it's stuck in pending. If there is an issue I suspect it's my use of pip3 install. I'm also going to split out the /docker changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-478592569:121,install,install,121,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-478592569,1,['install'],['install']
Deployability,"@jigold I stood up batch/ci in my own project from `hail-is/hail:main` and then deployed this branch, taking notes of any changes I needed to make and all seemed to work out OK. I think that's about as much as I can properly test this without trying things out in haildev/hail-vdc. The steps were as follows:. 1. Generate the configmaps used by gateway/internal-gateway. These will have the routing configuration for production services (I've edited the bootstrap instructions to match); `make -C gateway envoy-xds-config && make -C internal-gateway envoy-xds-config`; 2. … wait a few seconds for CI to quietly update these configmaps with information about testing namespaces … (can manually verify changes with `download-configmap gateway-xds-config`); 3. Deploy the new versions of gateway/internal-gateway; `make -C gateway deploy NAMESPACE=default && make -C internal-gateway NAMESPACE=default`. This worked for me in my project with no downtime, but either way I would probably do the same thing as with the previous PR where I test it in azure before making changes to hail-vdc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346:80,deploy,deployed,80,https://hail.is,https://github.com/hail-is/hail/pull/12095#issuecomment-1293703346,5,"['Deploy', 'configurat', 'deploy', 'update']","['Deploy', 'configuration', 'deploy', 'deployed', 'update']"
Deployability,"@jigold Notebook has no tests, so it doesn't have an environment.yml that needs to be updated in the hail-ci-build-image. This configuration is only for the run-time service.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462:86,update,updated,86,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467637462,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"@jigold OK, so here's the summary of what I learned:. We don't have tabix files for GRCh38 and we only test on small positions. Many large positions without tabix files seems to cause a problem for VEP (and make it slow, unsurprisingly). Fix seems to be to download the *indexed* homo_sapiens cache https://ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/ and upload that to our QoB VEP bucket. I presume you copied from the data we use in Dataproc? If yes, we should update that to also have tabix files. Also, in Dataproc, we use highmem machines for VEP. We should change _service_vep to also use highmem machines. <details><summary>Listing the tabix files for GRCh38 and GRCh37</summary>. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch38-us-central1/homo_sapiens/95_GRCh38/\*/\*.tbi; CommandException: One or more URLs matched no objects.; ```. ```; (base) dking@wm28c-761 /tmp % gsutil ls gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/\*/\*.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/1/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/10/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/11/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/12/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/13/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/14/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/15/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/16/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/17/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/18/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/19/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapiens/85_GRCh37/2/all_vars.gz.tbi; gs://hail-qob-vep-grch37-us-central1/homo_sapi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145:327,release,release-,327,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1830868145,2,"['release', 'update']","['release-', 'update']"
Deployability,"@jigold Ok I changed this in a way that will hopefully be more clear. The rules are as follows:. 1. We only use `ci-intermediate` for anonymous images. Images are named auth, batch, etc. even when they are in tests or dev deploys.; 1. Every image draws from the main branch cache tag, named `<DOCKER_PREFIX>/<image_name>:cache`; 2. Every image has an additional cache tag that it draws from and pushes to. For deploys, that is the same as the main branch cache, for PRs, it is `cache-pr-<pr_number>`, for dev deploys it is `cache-<dev_username>`, and for deploys conducted by CIs in a non-default namespace, it is `cache-<namespace-CI-is-in>-deploy`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450:222,deploy,deploys,222,https://hail.is,https://github.com/hail-is/hail/pull/11999#issuecomment-1177641450,5,['deploy'],"['deploy', 'deploys']"
Deployability,"@jigold This PR allows us to create fully-pinned, deterministic, requirements files for our pip dependencies. This uses those fully-pinned versions in our docker images so that breaking releases of transitive dependencies don't spontaneously break our CI pipelines. If we want to add or upgrade a dependency, someone needs to:. 1. Update the relevant `requirements.txt` file just as we would do currently.; 2. Run `pip-compile` to generate new lock `pinned-requirements.txt` files and check in the changes to a PR. The added check step verifies that you can't do 1 and forget to do 2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1145288523:186,release,releases,186,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1145288523,4,"['Update', 'pipeline', 'release', 'upgrade']","['Update', 'pipelines', 'releases', 'upgrade']"
Deployability,"@jigold Yes, since the CompileWithAggregators interface changed. Whichever goes in first, the other can rebase and update?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3422#issuecomment-383611333:115,update,update,115,https://hail.is,https://github.com/hail-is/hail/pull/3422#issuecomment-383611333,1,['update'],['update']
Deployability,@jigold can I remove the WIP tag? I'd like this to make it into the 0.2.125 release so that AUS can check out a tagged release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769060386:76,release,release,76,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769060386,2,['release'],['release']
Deployability,@jigold can you check batch in an hour and/or tomorrow morning to make sure its all good after deploy too?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5615#issuecomment-477358846:95,deploy,deploy,95,https://hail.is,https://github.com/hail-is/hail/pull/5615#issuecomment-477358846,1,['deploy'],['deploy']
Deployability,"@jigold looks good after patch, thanks! back to you",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2205#issuecomment-328296634:25,patch,patch,25,https://hail.is,https://github.com/hail-is/hail/pull/2205#issuecomment-328296634,1,['patch'],['patch']
Deployability,"@jigold this needs gcloud installed and configured to work in cluster, specifically 'GOOGLE_APPLICATION_CREDENTIALS'. Will work on that after some higher priority items are in, I believe it is sufficient to have this working on our local machines for now (manual user creation). Will unassign for now, and re-assign when gcloud is configured. Any suggestions on how to get that configured on the cluster would be much appreciated too :). cc @cseed, @danking",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954:26,install,installed,26,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-479316954,1,['install'],['installed']
Deployability,"@jigold this should pass now. I also learned that Mypy checks each command line argument as an independent module. Instead of specifying individual files, we have to tell it to check `batch`. I also added `google_storage.py` in an ill-fated attempt to let batch use that. I think we can only type check modules that depend on one another by installing them first. We'll leave that for future work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859:341,install,installing,341,https://hail.is,https://github.com/hail-is/hail/pull/8968#issuecomment-662143859,1,['install'],['installing']
Deployability,"@jigold to address your points from before:; 1. Pairing db update and/or self._pod_name with the k8s call, this might be a good idea but I think is orthogonal to this change. I want to focus on cleaning up k8s use and getting pods deleted, then think a little harder about how we should restructure the code more generally to be more understandable; 2. I always return at least the error so that the calling code has to decide what an error means; 3. I now always delete the pod after the db update, which I think is right b/c we don't want the refresh loop to recreate the pod.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954:59,update,update,59,https://hail.is,https://github.com/hail-is/hail/pull/6175#issuecomment-495393954,2,['update'],['update']
Deployability,@jmarshall it sounded when we caught up in person like you agreed that skipping empty command strings at source (and emitting a warning) during the original `job.command` call would be the way forward here... were you planning to update this PR with that change?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2383405458:230,update,update,230,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2383405458,1,['update'],['update']
Deployability,@jmarshall thanks again for the feedback 0.2.128 should now be fixed: https://github.com/hail-is/hail/releases/tag/0.2.128 .,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14322#issuecomment-1967719780:102,release,releases,102,https://hail.is,https://github.com/hail-is/hail/pull/14322#issuecomment-1967719780,1,['release'],['releases']
Deployability,"@johnc1231 I updated this, after discovering that HadoopFS does mkdir -p, but only during writes. I also made some small changes related to backend.py and ServiceBackend, to make it at least formally possible to use ServiceBackend (issue opened to this effect). Given the state of ServiceBackend and apiserver, I did not update test_google_fs_utils.py to explicitly check mkdir -p behavior, but this is implicitly checked in the local version of the file creation test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6887#issuecomment-531924944:13,update,updated,13,https://hail.is,https://github.com/hail-is/hail/pull/6887#issuecomment-531924944,2,['update'],"['update', 'updated']"
Deployability,"@johnc1231 I've backed out the removal of _jbm etc. since it's not necessary for this change (I just wanted to convince myself that it was, in fact, unnecessary now) with the intention of opening a separate PR for it once Konrad and Jacob are no longer relying on it for their pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8115#issuecomment-588323242:277,pipeline,pipelines,277,https://hail.is,https://github.com/hail-is/hail/pull/8115#issuecomment-588323242,1,['pipeline'],['pipelines']
Deployability,@johnc1231 I've made a PR against your branch that rebases on is.hail and suggests other updates.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1264#issuecomment-273365626:89,update,updates,89,https://hail.is,https://github.com/hail-is/hail/pull/1264#issuecomment-273365626,1,['update'],['updates']
Deployability,@johnc1231 Thoughts on including; ```; pip install gnomad; ```. The package name isn't actually included anywhere on that page.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8452#issuecomment-608620168:43,install,install,43,https://hail.is,https://github.com/hail-is/hail/pull/8452#issuecomment-608620168,1,['install'],['install']
Deployability,"@johnc1231 You can approve. It's actually already deployed, I did that to test it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6973#issuecomment-526698057:50,deploy,deployed,50,https://hail.is,https://github.com/hail-is/hail/pull/6973#issuecomment-526698057,1,['deploy'],['deployed']
Deployability,@johnc1231 has started to do this and will update soon here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/323#issuecomment-279525226:43,update,update,43,https://hail.is,https://github.com/hail-is/hail/issues/323#issuecomment-279525226,1,['update'],['update']
Deployability,@johnc1231 update this one your PR is in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/863#issuecomment-279597753:11,update,update,11,https://hail.is,https://github.com/hail-is/hail/issues/863#issuecomment-279597753,1,['update'],['update']
Deployability,"@konradjk Yes, sadly I can reproduce it 100% of the time. I won't be actively working on this for the time being, and I'll hope that when I come back to it a JDK and/or OS upgrade on the dataproc side has fixed it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419967871:172,upgrade,upgrade,172,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419967871,1,['upgrade'],['upgrade']
Deployability,"@konradjk ah, I meant does Pan-UKB itself have a release version? I guess we should just call this v1?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10186#issuecomment-804357789:49,release,release,49,https://hail.is,https://github.com/hail-is/hail/pull/10186#issuecomment-804357789,1,['release'],['release']
Deployability,"@konradjk pssh, what is this ""server"" you speak of. The kubernetes pod indeed needs a newer version of cloud tools. We always grab the latest when running the hail PR jobs. Unfortunately the deploy for cloud tools on python3 was broken. That's being fixed by https://github.com/Nealelab/cloudtools/pull/101. `pip`, unhelpfully, tells you the latest version is X.Y even if X.Y isn't available for your version of python.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422134593:191,deploy,deploy,191,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422134593,1,['deploy'],['deploy']
Deployability,"@lfrancioli `Slope` was introduced to bokeh in a later version. I'll go through and update some of our python dependencies when I get a chance this week, which should mean this can just go in at that point.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6162#issuecomment-497035653:84,update,update,84,https://hail.is,https://github.com/hail-is/hail/pull/6162#issuecomment-497035653,1,['update'],['update']
Deployability,"@mhebrard I notice you're using `sudo make`, I suspect this means that Hail's code is running under a modified `PATH` that lacks `pip-compile`. We'll fix our install-on-cluster target to have a ""make the artifact"" and an ""install"" step that are separate (so you can install as root but build as a normal user). In the mean time, apply this patch:. ```; diff --git a/hail/Makefile b/hail/Makefile; index dabe146d3a..e12ac791c4 100644; --- a/hail/Makefile; +++ b/hail/Makefile; @@ -349,7 +349,7 @@ install: $(WHEEL); hailctl config set query/backend spark; ; .PHONY: install-on-cluster; -install-on-cluster: $(WHEEL) check-pip-lockfile; +install-on-cluster: $(WHEEL); sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; -$(PIP) uninstall -y hail; $(PIP) install $(WHEEL) --no-deps. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13445#issuecomment-1690045571:158,install,install-on-cluster,158,https://hail.is,https://github.com/hail-is/hail/issues/13445#issuecomment-1690045571,10,"['install', 'patch']","['install', 'install-on-cluster', 'patch']"
Deployability,"@mhebrard I would expect that `MutableSettings` to come from your Spark installation. I think there are two problems:; 1. For some reason, our gradle configuration is including `MutableSettings`. We should get rid of that, but I haven't yet figured out how to do that.; 2. Normally (1) isn't a problem because your Spark installation appears on the class path before Hail does. It seems to me that this isn't true in your case. This is probably caused by linking the Hail JAR into `/opt`. Is `/opt/hail/backend` on your class path? Why do you link the backend directory into `/opt`? The Hail JAR should be distributed automatically by Spark. You shouldn't need to do anything special after you `pip install` / `make install-on-cluster`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777706782:72,install,installation,72,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777706782,5,"['configurat', 'install']","['configuration', 'install', 'install-on-cluster', 'installation']"
Deployability,"@mhebrard The only way I can imagine that we would mutate your environment is if we are accidentally installing `pyspark`. `install-on-cluster` takes pains to avoid that:; ```; install-on-cluster: $(WHEEL) check-pip-lockfile; 	sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$$' | tr '\n' '\0' | xargs -0 $(PIP) install -U; 	-$(PIP) uninstall -y hail; 	$(PIP) install $(WHEEL) --no-deps; 	hailctl config set query/backend spark; ```. But that is broken somehow? When you ran `install-on-cluster` did you see a `pip` output indicating that pyspark got installed?. Can you check if pyspark is installed via pip now? `pip show pyspark` (should say its not installed). If it is installed, try uninstalling it `pip uninstall pyspark`. You might also try removing the first line of `install-on-cluster` entirely. That will leave you without Hail's dependencies installed, but if `pyspark-shell` is still the right version of Scala, then I suspect the issue is that line.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906:101,install,installing,101,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1769053906,12,['install'],"['install', 'install-on-cluster', 'installed', 'installing']"
Deployability,"@mhebrard can you try applying this diff and then building?. ```diff; diff --git a/hail/build.gradle b/hail/build.gradle; index d4bdd879f0..1b65904484 100644; --- a/hail/build.gradle; +++ b/hail/build.gradle; @@ -100,6 +100,7 @@ configurations {; hailJar.extendsFrom implementation; hailJar {; exclude group: 'org.scala-lang', module: 'scala-library'; + exclude group: 'org.scala-lang', module: 'scala-reflect'; exclude group: 'org.apache.spark'; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1777721812:229,configurat,configurations,229,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1777721812,1,['configurat'],['configurations']
Deployability,"@mhebrard. Just to be clear: *after* installing Hail, `/usr/bin/spark-shell --version` now shows `2.12.13` but `pip3 show pyspark` still shows ""Warning: Package(s) not found: pyspark""?. Is `/usr/bin/spark-shell` a symlink? What does it point to? Is `/usr/lib/spark` a symlink? Does it point to the same place? Actually, let's just check a bunch of things:; ```; ls -al /usr/bin/spark-shell; echo $(which spark-shell); ls -al $(which spark-shell); spark-shell --version. ls -al /usr/bin/spark-submit; echo $(which spark-submit); ls -al $(which spark-submit); spark-submit --version. ls -al /usr/bin/spark-class; echo $(which spark-class); ls -al $(which spark-class). echo SPARK_SCALA_VERSION=$SPARK_SCALA_VERSION. echo "">>>>>>>>>> before load-spark-env.sh <<<<<<<<<""; env. load-spark-env.sh. echo "">>>>>>>>>> after load-spark-env.sh <<<<<<<<<""; env. which scala; ls -al $(which scala); cat $(which scala); ```. And one more thing, can you edit `$(which spark-shell)` to add `set -x` then try `spark-shell` and see if there's anything mysterious?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222:37,install,installing,37,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1771296222,1,['install'],['installing']
Deployability,@natestockham As to your more specific issue can you tell me the output of this:. ```bash; echo $SPARK_HOME; echo $HAIL_HOME; echo $PYTHONPATH; ```. Can you also post the invocation you're using to trigger this test failure? I assume you're in a clone of the Hail repository and running:. ```bash; ./gradlew test -Dspark.version=2.1.0; ```. in a shell with `$SPARK_HOME` pointing to a `2.1.0` installation of Spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119:393,install,installation,393,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281828119,1,['install'],['installation']
Deployability,@patrick-schultz I've updated this PR so that OrderedRVD.union() uses better logic to avoid the shuffling the RVDs that it's trying to merge. I did this by merging the partitioners so that each resulting partition only gets information from one original partition per input RVD. I've removed the unionDisjoint stuff because this merge logic should automatically do that in the case of disjoint RVDs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487:22,update,updated,22,https://hail.is,https://github.com/hail-is/hail/pull/4043#issuecomment-409679487,1,['update'],['updated']
Deployability,"@patrick-schultz do we expect that after the randomness PR we need to update random tests (in this case, balding Nichols) because the sequence of generated numbers has changed?. (Context: it appears the randomness PR accidentally disabled the test_statgen.py tests which included a balding Nichols test)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397329460:70,update,update,70,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397329460,1,['update'],['update']
Deployability,@patrick-schultz sounds like an opportunity to learn how to do a deploy from Tim 😉,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509642293:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642293,1,['deploy'],['deploy']
Deployability,"@pyousefi could you help me replicate this? What system are you using, and which specific Python installation?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-522487262:97,install,installation,97,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-522487262,1,['install'],['installation']
Deployability,"@ryerobinson, ah! This is because of xargs. You're building a Dockerfile, is that right? From where did you get this Dockerfile?. I think you need `xargs -d '\n'` or `xargs -0` to prevent xargs from splitting on the space between the semicolon and the sys_platform. e.g. in the python:3.8 container, this works:; ```; root@0c1415c108de:/# echo ""uvloop==0.16.0; sys_platform!='win32'"" | xargs -0 python3 -m pip install -U; Collecting uvloop==0.16.0; Downloading uvloop-0.16.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.7 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.7/4.7 MB 3.6 MB/s eta 0:00:00; Installing collected packages: uvloop; Successfully installed uvloop-0.16.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.; You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command.; ```; but this does not:; ```; root@0c1415c108de:/# echo ""uvloop==0.16.0; sys_platform!='win32'"" | xargs python3 -m pip install -U; Requirement already satisfied: uvloop==0.16.0 in /usr/local/lib/python3.8/site-packages (0.16.0); ERROR: Could not find a version that satisfies the requirement sys_platform!=win32 (from versions: none); ERROR: No matching distribution found for sys_platform!=win32; WARNING: You are using pip version 22.0.4; however, version 22.2.2 is available.; You should consider upgrading via the '/usr/local/bin/python3 -m pip install --upgrade pip' command. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255256786:410,install,install,410,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255256786,8,"['Install', 'install', 'upgrade']","['Installing', 'install', 'installed', 'upgrade']"
Deployability,@sjparsa I invited you to the Hail repo so that the CodeQL pipelines will run. Your GitHub inbox should contain an invite message that you need to accept.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13258#issuecomment-1644149775:59,pipeline,pipelines,59,https://hail.is,https://github.com/hail-is/hail/pull/13258#issuecomment-1644149775,1,['pipeline'],['pipelines']
Deployability,"@tpoterba . Thanks for pointing out the extra step. ; So I have compiled hail to run on Centos 6 and it is running python scripts fine locally (master=local[*]). However, the following error occurs when running it with yarn. Any suggestions on this? . ```; [Stage 0:> (0 + 1) / 292]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/bgen_count.py in <module>; 10 mt=hl.import_bgen(bgen,sample_file=sample,entry_fields=['GT','GP','dosage']); 11 mt.describe(); ---> 12 print(""Count:"",mt.count()); 13 mt.s.show(); 14. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/matrixtable.py in count(self); 2129 Number of rows, number of cols.; 2130 """"""; -> 2131 r = self._jmt.count(); 2132 return r._1(), r._2(); 2133. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 6, scc-q06.scc.bu.edu, executor 7): ExecutorLostFailure (executor 7 exited caused by one of the running tasks) Reason: Container marked as failed: container_e2435_1542127286896_0109_02_000004 on host: scc-q06.scc.bu.edu. Exit status: 1. Diagnostics: Exception from containe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705:891,install,install,891,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-446057705,1,['install'],['install']
Deployability,"@tpoterba ; Hi Tim , thank you ,I tried the plink1.9, and it works. but when I use the ""importvcf"" command, there are some issues, I took the advice in ""http://www.slf4j.org/codes.html"", added one of the jars in my classpath,but the issue still appeared. (1) command and the info:; root hail $ ./build/install/hail/bin/hail importvcf src/test/resources/sample.vcf.gz -f write -o sample_4.vds; hail: info: running: importvcf src/test/resources/sample.vcf.gz -f; hail: info: running: write -o sample_4.vds; SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; hail: info: while importing:; file:/***/hail/src/test/resources/sample.vcf.gz import clean; hail: info: timing:; importvcf: 736.849ms; write: 2.463s. (2) modify the classpath; I add the ""slf4j-nop.jar"" in the CLASSPATH,as follows:; root hail $ echo $CLASSPATH; .:/usr/share/java/slf4j/slf4j-nop.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/dt.jar:/opt/BioDir/jdk/jdk1.8.0_91/lib/tools.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230429438:302,install,install,302,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230429438,1,['install'],['install']
Deployability,"@tpoterba ; Hi, Tim; I installed plink, and set the pathas follows,but when excute ""gradle check"",still have problems,I don't know how I should do ?; ------------------(1) ; root ***\* $ plink --file test. @----------------------------------------------------------@; | PLINK! | v1.07 | 10/Aug/2009 |; |----------------------------------------------------------|; | (C) 2009 Shaun Purcell, GNU General Public License, v2 |; |----------------------------------------------------------|; | For documentation, citation & bug-report instructions: |; | http://pngu.mgh.harvard.edu/purcell/plink/ |; @----------------------------------------------------------@. Web-based version check ( --noweb to skip ); ......-----------------------------------------------------------------------------------; (2) The path; export PLINK_HOME=/***/plink. ## export PATH=$PLINK_HOME:$PATH. (3) The errors:; Gradle suite > Gradle test > org.broadinstitute.hail.methods.ExportPlinkSuite.testBiallelic FAILED; java.io.FileNotFoundException at ExportPlinkSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.methods.ExportSuite); Gradle suite > Gradle test > org.broadinstitute.hail.driver.GRMSuite.test FAILED; java.io.FileNotFoundException at GRMSuite.scala:20; Running test: Test method testGenotypeStream(org.broadinstitute.hail.variant.GenotypeStreamSuite); Gradle suite > Gradle test > org.broadinstitute.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; java.io.FileNotFoundException at ImputeSexSuite.scala:17; Running test: Test method test(org.broadinstitute.hail.variant.IntervalListSuite)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230191234:23,install,installed,23,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230191234,1,['install'],['installed']
Deployability,"@tpoterba @cseed . Has the latest 0.2 version been tested on Centos 6 yet? While we have compiled and run it locally, we are unable to get the compiled version running on our Centos 6 hadoop cluster. An upgrade to Centos 7 is not planned to the spring so it would be great to verify that this can run on Centos 6.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-453607151:203,upgrade,upgrade,203,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-453607151,1,['upgrade'],['upgrade']
Deployability,"@tpoterba @danking We at Databricks are still interested in this. Although Hail's frontend is in Python, it's still useful to publish to maven central. First, it makes the dependency information available. I've seen people write pipelines that are partly in Hail and partly in PySpark and can include Java libraries for things like data sources. There's a lot of tooling for resolving dependency conflicts between different libraries, but they're not very accessible unless all your dependencies are published to maven repos and have dependency poms available. It's also easier to update pipelines to the latest Hail version if the artifacts published to a standard location.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319:229,pipeline,pipelines,229,https://hail.is,https://github.com/hail-is/hail/issues/1963#issuecomment-481354319,3,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,@tpoterba @jigold committed some updates based on your feedback.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1936#issuecomment-314784714:33,update,updates,33,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-314784714,1,['update'],['updates']
Deployability,"@tpoterba I added `hl.debug_info()` which is maybe helpful?. I guess we're really talking about something that sanity checks the environment but assuming we already have hail pip-installed. What would you want to do? I guess we could check some spark stuff, try to create a context, if that fails tell the user its a spark problem? We could just do that in init though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5910#issuecomment-484750191:179,install,installed,179,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484750191,1,['install'],['installed']
Deployability,"@tpoterba I also added a `hail-ci-build.sh` so we'll build the images in the PRs, ensuring we don't get deploy problems from image build failures.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4965#issuecomment-447034807:104,deploy,deploy,104,https://hail.is,https://github.com/hail-is/hail/pull/4965#issuecomment-447034807,1,['deploy'],['deploy']
Deployability,"@tpoterba I had everything working locally, but hadn't updated master. Unfortunately, there's a test failure in the new test you added for `index_globals` and I can't figure out how to fix it. I tried creating the environment separately and the `pli` field was there. Could you please take a look?. ```; _____________ [doctest] hail.matrixtable.MatrixTable.index_globals _____________; [gw0] darwin -- Python 3.6.1 //anaconda/envs/py36/bin/python; UNEXPECTED EXCEPTION: AttributeError(""StructExpression instance has no field, method, or property 'pli'\n Hint: use 'describe()' to show the names of all data fields."",); Traceback (most recent call last):. File ""//anaconda/envs/py36/lib/python3.6/doctest.py"", line 1330, in __run; compileflags, 1), test.globs). File ""<doctest hail.matrixtable.MatrixTable.index_globals[0]>"", line 1, in <module>. File ""/Users/jigold/hail/build/tmp/doctest/python/hail/expr/expressions/typed_expressions.py"", line 1161, in __getattr__; raise AttributeError(get_nice_attr_error(self, item)). AttributeError: StructExpression instance has no field, method, or property 'pli'; Hint: use 'describe()' to show the names of all data fields. /Users/jigold/hail/build/tmp/doctest/python/hail/matrixtable.py:2197: UnexpectedException; ============== 1 failed, 420 passed, 13 skipped in 79.03 seconds ===============; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3620#issuecomment-390841993:55,update,updated,55,https://hail.is,https://github.com/hail-is/hail/pull/3620#issuecomment-390841993,1,['update'],['updated']
Deployability,@tpoterba I made it lazily check for nullness so that users can still run the Java & Scala tests without installing spark.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1119#issuecomment-264290702:105,install,installing,105,https://hail.is,https://github.com/hail-is/hail/pull/1119#issuecomment-264290702,1,['install'],['installing']
Deployability,@tpoterba I updated the main message with a better explainer on the changes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454592988:12,update,updated,12,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454592988,1,['update'],['updated']
Deployability,@tpoterba Instead of using `hailtop/requirements.txt` + `hail/requirements.txt` you can use `hail/pinned-requirements.txt`. It includes all the fully-pinned requirements to install hail (it is the union of the hailtop and hail requirements) and is the version of the requirements that we test with.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12829#issuecomment-1500285929:173,install,install,173,https://hail.is,https://github.com/hail-is/hail/pull/12829#issuecomment-1500285929,1,['install'],['install']
Deployability,"@tpoterba OK, this is ready for final review. Flags are now duplicated in Python so that service backend can perform all actions without starting a JVM. I have a test that verifies the flag sets, their envvars, and the default values, are all the same. I preserved the randomness behavior. We can address that in a separate PR. The flags now use the Hail `configuration_of` machinery which checks, in order:; - an explicit value (not relevant to flags); - a deprecated environment variable (these are the current flag envvars); - an environment variable with a mechanically derived name (e.g. `HAIL_QUERY_NO_WHOLE_STAGE_CODEGEN`); - the hail configuration file (usually: ""~/.config/hail/config.ini"") under the section ""query"". FWIW, hail configuration files look like this:. ```; (base) dking@wm28c-761 hail % cat ~/.config/hail/config.ini ; [query]; backend = spark; jar_url = gs://hail-query/jars/dking/0wfcw2e6sma9/f4fb19e3d387d6efc6cf0f19b95bec59c95b793a.jar. [batch]; remote_tmpdir = gs://1-day/dktmp/; billing_project = hail. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434:642,configurat,configuration,642,https://hail.is,https://github.com/hail-is/hail/pull/12423#issuecomment-1411200434,2,['configurat'],['configuration']
Deployability,"@tpoterba We should just make a gradle target for end-users, like: ""releaseJar"" that just depends on the right targets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6132#issuecomment-494049017:68,release,releaseJar,68,https://hail.is,https://github.com/hail-is/hail/issues/6132#issuecomment-494049017,1,['release'],['releaseJar']
Deployability,@tpoterba We're stuck with 8 [due to spark](https://issues.igniterealtime.org/browse/SPARK-2017). I'll update the check. Thanks for the report @dekinsitro,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896#issuecomment-444557619:103,update,update,103,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444557619,1,['update'],['update']
Deployability,@tpoterba this PR now also contains updates to the Python docs for additional plotting functions,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3972#issuecomment-407388967:36,update,updates,36,https://hail.is,https://github.com/hail-is/hail/pull/3972#issuecomment-407388967,1,['update'],['updates']
Deployability,"@tpoterba when I changed ttuple from a mapping to a sequence, I lost `.values()` as a uniform way to get the types of a tstruct or ttuple. I've updated this PR to modify tstruct to have a `types()` which is analogous to ttuple's `types()`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7178#issuecomment-545569253:144,update,updated,144,https://hail.is,https://github.com/hail-is/hail/pull/7178#issuecomment-545569253,1,['update'],['updated']
Deployability,"@vladsaveliev , we've had a bit of CI instability here which is causing tests to fail. Looks like it's merged now, I'll do a release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11435#issuecomment-1059199901:125,release,release,125,https://hail.is,https://github.com/hail-is/hail/pull/11435#issuecomment-1059199901,1,['release'],['release']
Deployability,"@vladsaveliev no specific reason, we were just trying to roll up a bunch of updates at once instead of tons of tiny PRs and then got bogged down with some changes that needed to be made. Feel free to PR any version updates and I'll see them through!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520#issuecomment-1123673359:76,update,updates,76,https://hail.is,https://github.com/hail-is/hail/pull/11520#issuecomment-1123673359,2,['update'],['updates']
Deployability,"@zenghz What version of Hail are you using? It must be ancient. We no longer support Spark 1 and haven't for quite some time. We saw this in sporadically in some deployments, but never understood it or developed a reliable workaround. My advice would be to upgrade to Spark 2 and the latest version of Hail if possible. You might search on other forums for ideas/work arounds, for example: https://stackoverflow.com/questions/29960686/parquet-error-when-saving-from-spark",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578:162,deploy,deployments,162,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-319526578,2,"['deploy', 'upgrade']","['deployments', 'upgrade']"
Deployability,"@zillurbmb51 You need to install Java 8, you have Java 12.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-516543503:25,install,install,25,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-516543503,1,['install'],['install']
Deployability,"@zyd14 There are no releases of Hail that support Spark 3.2.x. We very closely follow Google Dataproc's release cycle. If you want Spark 3.2.x support, you'll need to fork, edit the code to handle changes to the APIs of dependencies, and recompile.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11707#issuecomment-1598879560:20,release,releases,20,https://hail.is,https://github.com/hail-is/hail/issues/11707#issuecomment-1598879560,2,['release'],"['release', 'releases']"
Deployability,A Hail pipeline:; ```; read profile225-with-SEX_FROM_STAT.vds \; filtergenotypes -c ' g.dp > 400 ||; (g.isHomRef && (g.ad[0] / g.dp < 0.9 || g.gq < 20)) ||; (g.isHomVar && (g.ad[1] / g.dp < 0.9 || g.pl[0] < 20)) ||; (g.isHet && ( (g.ad[0] + g.ad[1]) / g.dp < 0.9 || g.ad[1] / g.dp < 0.20 || g.pl[0] < 20 ))' --keep count -g; ```. `master`/`c5bdcfd`:; ```; hail: info: timing:; read: 4.795s; filtergenotypes: 254.277ms; count: 1m40.8s; total: 1m45.8s; ```; this branch:; ```; hail: info: timing:; read: 5.822s; filtergenotypes: 617.229ms; count: 53.545s; total: 59.984s; ```. Ignoring read time: the compiled version takes 53% of the time. ![The Flash](https://media.giphy.com/media/lRnUWhmllPI9a/giphy.gif),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1254#issuecomment-274951875:7,pipeline,pipeline,7,https://hail.is,https://github.com/hail-is/hail/pull/1254#issuecomment-274951875,1,['pipeline'],['pipeline']
Deployability,A fix to this issue would detect which FASTAs and which chain files (for liftovers) are needed by a pipeline (a call to `ServiceBackend.execute`) and only mount the necessary ones.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13416#issuecomment-1679191381:100,pipeline,pipeline,100,https://hail.is,https://github.com/hail-is/hail/issues/13416#issuecomment-1679191381,1,['pipeline'],['pipeline']
Deployability,"A newer version of protobuf exists, but since this PR has been edited by someone other than Dependabot I haven't updated it. You'll get a PR for the updated version as normal once this PR is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12518#issuecomment-1334218563:113,update,updated,113,https://hail.is,https://github.com/hail-is/hail/pull/12518#issuecomment-1334218563,2,['update'],['updated']
Deployability,"A newer version of pylint exists, but since this PR has been edited by someone other than Dependabot I haven't updated it. You'll get a PR for the updated version as normal once this PR is merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11980#issuecomment-1188158089:111,update,updated,111,https://hail.is,https://github.com/hail-is/hail/pull/11980#issuecomment-1188158089,2,['update'],['updated']
Deployability,"AFAICT, FASTAs live at:; ```; ftp.ensembl.org/pub/release-95/fasta/homo_sapiens/dna_index/; ```; whereas the VEP cache lives at; ```; ftp.ensembl.org/pub/release-95/variation/indexed_vep_cache/homo_sapiens_merged_vep_95_GRCh38.tar.gz; ```; These seem to be two distinct sources of data, so my inclination is to not move the FASTAs inside the cache folder. That seems likely to cause confusion for ourselves in the future. Seems very reasonable to have `gs://bucket/cache/95_GRCh38/homo_sapiens_merged/...` and `gs://bucket/fasta/95_GRCh38/homo_sapiens/...`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1846193943:50,release,release-,50,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1846193943,2,['release'],['release-']
Deployability,"AFAICT, you didn't edit the release.sh script; do I misunderstand what you're worried about?. Can you run the dataproc tests via dev deploy and post the batch links here? I think this should do it. ```; hailctl dev deploy --branch jigold/fix-vep-grch38-cache -s test_dataproc-38 -s test_dataproc-37; ```. If those pass then I'm confident `vep-GRCh38.sh` is correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1883942042:28,release,release,28,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1883942042,3,"['deploy', 'release']","['deploy', 'release']"
Deployability,AFAIK there are no filter_entries in @gtiao's pipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384674539:46,pipeline,pipeline,46,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384674539,1,['pipeline'],['pipeline']
Deployability,"AFAIK, everything has been addressed. Are you referring to [this](https://github.com/hail-is/hail/pull/7875#discussion_r367430472)? Now we user `insert` and catch the duplicate key error instead of the previous `select ... for update` followed by an `insert`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575750888:227,update,update,227,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575750888,1,['update'],['update']
Deployability,"According to @johnc1231 in https://hail.zulipchat.com/#narrow/stream/123010-Hail-0.2E2.20support/topic/Troubles.20getting.20started.20(Python.203.2E8):. > It's intentionally limited to 3.7 because of precisely that error; >; > John Compitello: This is because of our dependence on spark 2. Spark 3 won't have that restriction, we are in the process of upgrading, hopefully should be fixed next week. I'll close this and wait for the release with Spark 3.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10197#issuecomment-800706947:433,release,release,433,https://hail.is,https://github.com/hail-is/hail/issues/10197#issuecomment-800706947,1,['release'],['release']
Deployability,"Ack. The problem here is that the version on PyPI is a bit old. I changed the behavior of `get_1kg` to also keep the VCF, and changed the tutorial, a few weeks ago. I'm hoping to deploy a new version to PyPI by the end of this week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4775#issuecomment-438644309:179,deploy,deploy,179,https://hail.is,https://github.com/hail-is/hail/issues/4775#issuecomment-438644309,1,['deploy'],['deploy']
Deployability,"Actually also fixed in 3.0.2, so will test with that for now. Trying to get all tests passing in anticipation of Google's dataproc image update and Spark 3.1.1 release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10054#issuecomment-788986746:137,update,update,137,https://hail.is,https://github.com/hail-is/hail/pull/10054#issuecomment-788986746,2,"['release', 'update']","['release', 'update']"
Deployability,"Actually, I have a feeling this bulk update will lock the entire table for 30 mins to an hour since there's no use of a where condition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1934192425:37,update,update,37,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1934192425,1,['update'],['update']
Deployability,"Actually, I realized I need this for aggregators... will update PR. Sorry.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1833#issuecomment-302138783:57,update,update,57,https://hail.is,https://github.com/hail-is/hail/pull/1833#issuecomment-302138783,1,['update'],['update']
Deployability,"Actually, maybe we just don't want to push to any cache at all for test deployments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152522572:72,deploy,deployments,72,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152522572,1,['deploy'],['deployments']
Deployability,"Added Pruner tests, and update the add-ir-checklist.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8520#issuecomment-612126069:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/8520#issuecomment-612126069,1,['update'],['update']
Deployability,"Added a region argument, and mandated that users have either configured a region or are using `--region`. I believe older versions of gcloud assume the region is the default region for your project if you don't have anything specified, which causes users to have random errors when they update gcloud that they ask about on Zulip. This way, everyone gets a good error message",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671:287,update,update,287,https://hail.is,https://github.com/hail-is/hail/pull/8281#issuecomment-597307671,1,['update'],['update']
Deployability,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:1396,deploy,deployed,1396,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516,1,['deploy'],['deployed']
Deployability,"Added support for a few more nodes, including {Insert, Select}Fields and Array{Range, Map, Filter} (deforested). I'm going to do ordering next. That should give us enough interesting stuff to play with when simple Table pipeline start working.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4663#issuecomment-433775154:220,pipeline,pipeline,220,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-433775154,1,['pipeline'],['pipeline']
Deployability,Added to the omnibus I'll dev deploy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11541#issuecomment-1062274709:30,deploy,deploy,30,https://hail.is,https://github.com/hail-is/hail/pull/11541#issuecomment-1062274709,1,['deploy'],['deploy']
Deployability,Adding to list of PRs I'm going to dev deploy to validate since this is many major versions apart from what we have now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11517#issuecomment-1061043455:39,deploy,deploy,39,https://hail.is,https://github.com/hail-is/hail/pull/11517#issuecomment-1061043455,1,['deploy'],['deploy']
Deployability,"Addressed @danking's comments, though I'm not going to attempt to add type information in this PR, given intended timeline for release and other commitments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11247#issuecomment-1020493898:127,release,release,127,https://hail.is,https://github.com/hail-is/hail/pull/11247#issuecomment-1020493898,1,['release'],['release']
Deployability,"Addressed comments. In addition to your other comments, I required unique IDs and updated the docs to reflect the chain in variant join.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2724#issuecomment-358734769:82,update,updated,82,https://hail.is,https://github.com/hail-is/hail/pull/2724#issuecomment-358734769,1,['update'],['updated']
Deployability,Admittedly a debatable choice but worthwhile in the context of tests which generate tiny pipelines dominated by CDA overhead,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1485548034:89,pipeline,pipelines,89,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1485548034,1,['pipeline'],['pipelines']
Deployability,"After the first commit, lots of unrelated type errors popped up from mypy. I think this is because I have the new mypy installed and it's actually catching more errors that were there all along. It might help to see the errors that it gave me (also visible in old CI builds of the PR:. ```; ci/github.py:508: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:551: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:554: error: Incompatible types in assignment (expression has type ""bool"", variable has type ""Optional[str]""); ci/github.py:574: error: Unsupported operand types for > (""int"" and ""None""); ci/github.py:574: note: Left operand is of type ""Optional[int]""; ci/github.py:575: error: Unsupported operand types for + (""None"" and ""int""); ci/github.py:575: note: Left operand is of type ""Optional[int]""; ci/github.py:817: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:828: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:840: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:842: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; ci/github.py:849: error: Item ""MergeFailureBatch"" of ""Union[Batch, Any, MergeFailureBatch]"" has no attribute ""id""; ci/github.py:849: error: Item ""None"" of ""Optional[Dict[str, PR]]"" has no attribute ""values""; Found 11 errors in 1 file (checked 19 source files); ```. It might be helpful to look at the first commit and last commit in isolation. Or if you'd like I can make a separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775:119,install,installed,119,https://hail.is,https://github.com/hail-is/hail/pull/11530#issuecomment-1062214775,1,['install'],['installed']
Deployability,"Agh, so we won't be able to use Python 3.8 with hail at all until switching to Spark 3.0? And that will be locked up in waiting for dataproc / Terra updates as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7513#issuecomment-555075049:149,update,updates,149,https://hail.is,https://github.com/hail-is/hail/issues/7513#issuecomment-555075049,1,['update'],['updates']
Deployability,"Ah I didn't see you had posted this a couple of days ago. I'm also getting this error on the pipeline below (which works on ~15K genomes, but not ~125K exomes):; ```; mt = mt.select_cols(group_membership=tuple(x[1] for x in sample_group_filters), project_id=mt.meta.project_id); mt = mt.select_rows(*mt.row_key); mt = mt.select_entries(n_alt=mt.GT.n_alt_alleles(), adj=mt.adj). frequency_expression = []; for i in range(len(sample_group_filters)):; subgroup_dict = sample_group_filters[i][0]; subgroup_dict['group'] = 'adj'. freq_expression = hl.struct(; ac=hl.agg.sum(hl.agg.filter(mt.group_membership[i] & mt.adj, mt.n_alt)),; an=2 * hl.agg.count_where(mt.group_membership[i] & mt.adj & hl.is_defined(mt.n_alt)),; hom=hl.agg.count_where(mt.group_membership[i] & mt.adj & (mt.n_alt == 2)),; meta=subgroup_dict; ); frequency_expression.append(freq_expression); freq_expression = hl.struct(; ac=hl.agg.sum(mt.n_alt),; an=2 * hl.agg.count_where(hl.is_defined(mt.n_alt)),; hom=hl.agg.count_where(mt.n_alt == 2),; meta={'group': 'raw'}; ); frequency_expression.insert(1, freq_expression). mt = mt.annotate_rows(freq=frequency_expression); mt.rows().write(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-386942393:93,pipeline,pipeline,93,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-386942393,1,['pipeline'],['pipeline']
Deployability,"Ah I see what you’re talking about; I was focused on the returned object’s schema. Yep I will remove if possible, or update the signature of PR and WartchedBranch implementations",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6142#issuecomment-497845843:117,update,update,117,https://hail.is,https://github.com/hail-is/hail/issues/6142#issuecomment-497845843,1,['update'],['update']
Deployability,"Ah I see, all these steps cut out early for non-release deploys. So we wouldn't have released a broken package which is good, but we'd still end up with a broken deploy, which is annoying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1329699051:48,release,release,48,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1329699051,4,"['deploy', 'release']","['deploy', 'deploys', 'release', 'released']"
Deployability,"Ah shit, that's in `-Wextra`? @chrisvittal do you know if that patch also passes the tests? I was worried that change would break something.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5354#issuecomment-464148672:63,patch,patch,63,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464148672,1,['patch'],['patch']
Deployability,"Ah sorry, I forgot to update the original commit message as that's not actually correct. The optimization is that we don't need to iterate through all blobs until we find the exact blob matching our path name. The list operation returns all blobs that start with the prefix of that path. If we see a blob with a different name that is a child of our path `f'{path}/foo`, then we know it's a directory and don't need to iterate anymore (although it could be a file as well, but in Scala we don't currently throw errors on paths that are both files and directories, so we just choose the first we see). If we see a blob that matches the path exactly, then we know it's a file and stop iterating. The only reason we need to iterate through more than one blob is if there's blobs that are like `'{path}zzzzz/foo` or `'{path}szzzzz`. We need to ignore these as they don't provide any information on whether `{path}` is a file or directory. This is where `isChildOf` is needed because we need to make sure the blob is actually a child of the path such as `'{path}/file` and not `{path}zzzzz/file`. As for `getValues` versus `iterateAll`, I just used the one that was in the Java documentation for using the `list` method.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/13390#issuecomment-1673853274,1,['update'],['update']
Deployability,"Ah the timings were slightly off as I had not downloaded all the data and was using some from `hail_search/fixtures`.; After pulling down the `SNV_INDELS` data, my updated timings are:. | query | results | elapsed |; | ----- | ------- | ------- |; | 0 | 4 | 7s |; | 1 | 83 | 50s |",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1821595905:164,update,updated,164,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1821595905,1,['update'],['updated']
Deployability,Ah ya didn't comment on this one. Ya it is. `make -C admin-pod deploy NAMESPACE=default` should be broken in main but functional in this branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14627#issuecomment-2251051950:63,deploy,deploy,63,https://hail.is,https://github.com/hail-is/hail/pull/14627#issuecomment-2251051950,1,['deploy'],['deploy']
Deployability,"Ah yes, this is what I meant by deploy. Patrick can learn how to do a release / pip deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864:32,deploy,deploy,32,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509643864,3,"['deploy', 'release']","['deploy', 'release']"
Deployability,"Ah! Because creating a pod can fail with a non-recoverable error. Then I think it is:. Pending -> Ready -> Error, Created; Created -> Running; Running -> Error, Failed, Success. Question is, what causes Created -> Running?. If it is seeing the pod running, then when the pod gets deleted (e.g. preemption), you'll also need:; Running -> Ready. You might also need:; Running -> Created; if you update the pod state and it exists but isn't actually running yet. I'm not sure if that can happen, but seems safe. I would probably just have Running (= Created), and I wouldn't track if the pod is running or not (if we're trying to inform the user, let's just make the batch UI better, e.g. give more information about the pod ... aside: I have some awesome ideas for this)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683:393,update,update,393,https://hail.is,https://github.com/hail-is/hail/pull/6268#issuecomment-499339683,1,['update'],['update']
Deployability,"Ah, I bet that service accounts default all their API requests to the namespace they are a member of. For `deploy-svc` that's `batch-pods`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432384646:107,deploy,deploy-svc,107,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432384646,1,['deploy'],['deploy-svc']
Deployability,"Ah, I remember why this is. Here's a diagram of the current and proposed scenarios that I hope helps:. ### Normal services (current `main`); 1. gateway receives a request destined for `batch.hail.is`; 2. gateway intends to forward this request to `batch.default:443`; 3. gateway makes a DNS request to resolve `batch.default`. gateway receives IP `A.A.A.A` which is the cluster IP of the batch Kubernetes Service; 4. gateway forwards the request to `A.A.A.A:443`; 5. The Kubernetes Service (really kube-proxy) receives the request, selects a pod with IP `X.X.X.X` and forwards the request to `X.X.X.X:5000`. ### Proposed headless service approach; 1. gateway receives a request destined for `batch.hail.is`; 2. gateway intends to forward this request to `batch.default:443`; 3. gateway makes a DNS request to resolve `batch.default`. gateway receives multiple DNS records back saying that `batch.default` corresponds to the IP addresses `X.X.X.X`, `Y.Y.Y.Y`, and `Z.Z.Z.Z` (assuming there are 3 pods in the deployment).; 4. gateway gets its pick out of the pods (this is really important and is why envoy needs all the IPs to properly load balance!) and decides to forward the request directly to pod `X.X.X.X:443`. So in the second scenario, it is necessary that the pod itself be listening on 443 because that is where gateway is going to send the request. It is not exactly a permissions issue, but upon writing this I am now realizing that by doing so we *require* that the service pods like `auth` and `batch` be running as root in order to bind on port 443. I think the port specified in the `Service` yaml is actually useless now. So two actionable options are:. 1. Remove the useless `port` field on the `Service` yaml for auth, batch, etc.; 2. Keep all of our services on unprivileged ports (5000) and have gateway forward traffic to `batch.default:5000` instead of `batch.default:443`. Keeping our services on port 5000 could allow us to run those services as non-root users. I guess k8s has",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12094#issuecomment-1283052287:1007,deploy,deployment,1007,https://hail.is,https://github.com/hail-is/hail/pull/12094#issuecomment-1283052287,1,['deploy'],['deployment']
Deployability,"Ah, I was accidentally modifying an installed version of Hail. I recovered the files and brought them in. I deleted that other debug_info. It doesn't include the batch information. Now every batch test in batch/ and in hail/ should be consistently using `Batch.debug_info`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10953#issuecomment-939112813:36,install,installed,36,https://hail.is,https://github.com/hail-is/hail/pull/10953#issuecomment-939112813,1,['install'],['installed']
Deployability,"Ah, OK, this is actually quite sensible. I have to tell gradle that this is a test jar, I do that by saying I want my class path to look like the test class path at runtime. A wrinkle is that I have to explicitly request our own code too (that's the first diff line). I'm actually quite pleased that our grade file has become a bit more standard and less custom.; ```diff; task shadowTestJar(type: ShadowJar) {; archiveClassifier = 'spark-test'; + from sourceSets.test.output; + configurations = [project.configurations.testRuntimeClasspath]; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710347695:479,configurat,configurations,479,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710347695,2,['configurat'],['configurations']
Deployability,"Ah, perfect. I'm a bit worried that the URLs aren't themselves versioned. Konrad, will subsequent releases use the same URL?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10186#issuecomment-804359604:98,release,releases,98,https://hail.is,https://github.com/hail-is/hail/pull/10186#issuecomment-804359604,1,['release'],['releases']
Deployability,"Ah, right, OK, I forget that we have two parallel pipelines here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13749#issuecomment-1743497768:50,pipeline,pipelines,50,https://hail.is,https://github.com/hail-is/hail/pull/13749#issuecomment-1743497768,1,['pipeline'],['pipelines']
Deployability,"Ah, sorry, I understand now. It's this line: https://github.com/hail-is/hail/blob/57537fea08d4dcb1548a4ab1f55f093eae9bd016/hail/Makefile#L259 . Sorry, I got my head in the wrong space with the `amazon-ebs` output. You're totally right, we need to fix `install-on-cluster`. The line you reference is actually correct for complicated reasons. It gets read on the cluster by init_notebook.py which knows to split on newlines. . I created [a new PR and listed you as a co-author on the commit](https://github.com/hail-is/hail/pull/12216) with the change. Thank you much for your patience and report!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255387962:252,install,install-on-cluster,252,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255387962,1,['install'],['install-on-cluster']
Deployability,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601:64,pipeline,pipeline,64,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601,1,['pipeline'],['pipeline']
Deployability,"Ah. We haven't tested against Spark 2.2 yet, so that could be the issue. We currently test / deploy against 2.0.2 and 2.1.0, though I imagine we'll update these versions to 2.1.x and 2.2.x soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319677826:93,deploy,deploy,93,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319677826,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,"Alright awesome. I'm good to approve this, but if you could, please rebase this off of branch ""breakingbad"" and PR into that. We're currently putting all potentially pipeline breaking changes into that branch until we merge that whole thing into master at once for 0.1 release. breakingbad will be merged into master tonight or tomorrow in all liklihood.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1779#issuecomment-301174135:166,pipeline,pipeline,166,https://hail.is,https://github.com/hail-is/hail/pull/1779#issuecomment-301174135,2,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"Alright, I'm OK with that. Let's see if we can remove these before 0.2 is released. If not, that's fine too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2649#issuecomment-355363719:74,release,released,74,https://hail.is,https://github.com/hail-is/hail/pull/2649#issuecomment-355363719,1,['release'],['released']
Deployability,"Alright, rebased and made the requested change. Noticed one strange thing though. When I do ; `make install-deps`, the print out starts with:. ```; cat: /secrets//pypi-username: No such file or directory; cat: /secrets//pypi-password: No such file or directory; python3 -m pip install -U -r python/requirements.txt -r python/dev-requirements.txt; ```; Obviously that third line makes sense but I don't see why first 2 happen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985:100,install,install-deps,100,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501311985,2,['install'],"['install', 'install-deps']"
Deployability,Alright. There's been some significant updates since this was last touched. I'm closing this and will be opening a new series of PRs with the new end product.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4396#issuecomment-435198817:39,update,updates,39,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-435198817,1,['update'],['updates']
Deployability,"Alrighty @danking, I was able to spin up a dataproc cluster and open a notebook:. ```python; hailctl dataproc start --region us-central1 my-first-cluster-2; hailctl dataproc connect my-first-cluster-2 notebook; ```. and paste the example analysis script from the [dataproc page](https://hail.is/docs/0.2/install/dataproc.html) in the docs into the notebook, and it ran successfully!. Any other tests you think are warranted here, or is this fix all set?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12965#issuecomment-1536483087:304,install,install,304,https://hail.is,https://github.com/hail-is/hail/pull/12965#issuecomment-1536483087,1,['install'],['install']
Deployability,Also I updated the pr-builder base image to miniconda3 (needs-redeploy.py requires Python 3) and made the version explicit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4400#issuecomment-423773748:7,update,updated,7,https://hail.is,https://github.com/hail-is/hail/pull/4400#issuecomment-423773748,1,['update'],['updated']
Deployability,Also added deployment for scorecard. I'm done making changes now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4408#issuecomment-424304088:11,deploy,deployment,11,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424304088,1,['deploy'],['deployment']
Deployability,Also added upload to projects and added upload get-deployed-sha and deploy scripts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-428393425:51,deploy,deployed-sha,51,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393425,2,['deploy'],"['deploy', 'deployed-sha']"
Deployability,Also can you dev deploy `mirror_hailgenetics_images` (maybe change that name too) to make sure that still works?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14223#issuecomment-1917966825:17,deploy,deploy,17,https://hail.is,https://github.com/hail-is/hail/pull/14223#issuecomment-1917966825,1,['deploy'],['deploy']
Deployability,Also finally I was able to compile it easily with 2.1.0.; Does compiling with 1.6.2 need some special patch as I couldn't find in in the code source tree,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-277495418:102,patch,patch,102,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-277495418,1,['patch'],['patch']
Deployability,"Also in this PR, I've started using [numpy style docstrings](http://www.sphinx-doc.org/en/stable/ext/example_numpy.html#example-numpy), and installed `sphinxcontrib-napoleon` on the CI server to support them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2551#issuecomment-350557712:140,install,installed,140,https://hail.is,https://github.com/hail-is/hail/pull/2551#issuecomment-350557712,1,['install'],['installed']
Deployability,Also looks like we might need to go to 1.1.0 b/c that’s all that’s been released to Apt https://packages.cloud.google.com/apt/dists/gcsfuse-jammy/main/binary-amd64/Packages,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1738332294:72,release,released,72,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1738332294,1,['release'],['released']
Deployability,Also moved pipeline to hailtop so it is including in the Hail package. The tests remain in the pipeline/ subproject directory. @konradjk You'll have to import pipeline with `from hailtop import pipeline` after this. FYI @jigold,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295:11,pipeline,pipeline,11,https://hail.is,https://github.com/hail-is/hail/pull/6445#issuecomment-504701295,4,['pipeline'],['pipeline']
Deployability,Also needs doc update?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/852#issuecomment-250204532:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/852#issuecomment-250204532,1,['update'],['update']
Deployability,Also removed 2.1.0 from deployed versions. When this goes in I'll remove the CI tests againt 2.1.0.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2656#issuecomment-355463197:24,deploy,deployed,24,https://hail.is,https://github.com/hail-is/hail/pull/2656#issuecomment-355463197,1,['deploy'],['deployed']
Deployability,Also updated binary ops to promote arg types to the return type (e.g. int32 -> float64) if the return type is also numeric (not bools though). This eliminates the need to double-check the signature of a division both in the IR and expressions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5737#issuecomment-479530162:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/5737#issuecomment-479530162,1,['update'],['updated']
Deployability,"Also, I forgot making sure all of the Docker images are valid and integrated into CI like the vep images.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13588#issuecomment-1710521152:66,integrat,integrated,66,https://hail.is,https://github.com/hail-is/hail/pull/13588#issuecomment-1710521152,1,['integrat'],['integrated']
Deployability,"Also, I'm gonna add parameters to `hl.init` and set the flags in there. That punts the interface decision down the road by slightly restricting users (you can't change user project mid-pipeline).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1230490168:185,pipeline,pipeline,185,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1230490168,1,['pipeline'],['pipeline']
Deployability,"Also, I'm not opposed to doing a migration of the existing batches to having rows in the updates table. It will just take some work to write a chunking script because we don't want to do the migration all in one transaction. Luckily this code doesn't have to be as well tested as other migrations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219917934:89,update,updates,89,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219917934,1,['update'],['updates']
Deployability,"Also, back-compatible with VDS on-disk representation, not necessary pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1323#issuecomment-277957797:69,pipeline,pipelines,69,https://hail.is,https://github.com/hail-is/hail/pull/1323#issuecomment-277957797,1,['pipeline'],['pipelines']
Deployability,"Also, just to make sure, you tested this with dev deploy?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11798#issuecomment-1125272065:50,deploy,deploy,50,https://hail.is,https://github.com/hail-is/hail/pull/11798#issuecomment-1125272065,1,['deploy'],['deploy']
Deployability,"Also, there were three updates to the prices when I checked earlier, but no values actually changed. All of the rates for us-central1 are the same as what we currently have in the database. I checked the other regions and they seemed good too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1170495764:23,update,updates,23,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1170495764,1,['update'],['updates']
Deployability,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:1104,update,update,1104,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,1,['update'],['update']
Deployability,"An update. I'm working with debugging info from the AoU VDS creation cluster. A VDS creation was run using an n1-highmem-8 driver. The cluster is created by hailctl with no custom driver settings; <details><summary>template for hailctl dataproc start</summary>. [Source](https://github.com/broadinstitute/gatk/blob/ah_var_store/scripts/variantstore/wdl/extract/run_in_hail_cluster.py#L36C1-L48C1). ```; hailctl dataproc start ; --autoscaling-policy={autoscaling_policy}; --worker-machine-type {worker_machine_type}; --region {region}; --project {gcs_project}; --service-account {account}; --num-master-local-ssds 1; --num-worker-local-ssds 1 ; --max-idle=60m; --max-age=1440m; --subnet=projects/{gcs_project}/regions/{region}/subnetworks/subnetwork; {cluster_name}; ```. </details>. I have the driver node syslogs as well as the Hail log file. For some reason all logs other than the Hail logs are missing from this file. We separately need to determine why all the Spark logs etc. are missing. Based on the syslog, after system start up and just before the Jupyter notebook starts, the system is already using ~8,500MiB:; ```; Nov 22 14:29:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 43808 of 52223 MiB (83.89%), swap free: 0 of 0 MiB ( 0.00%); ```; So, the effective maximum memory that Hail could possibly use is around 43808MiB. After the Notebook and Spark initialize we're down to 42,700 MiB (about ~1000MiB more in use).; ```; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. `hailctl` sets the VM RAM limit to 80% of the instance type's memory, so 80% * 52GiB = 42598MiB. This means the JVM is permitted to effectively use all the remaining memory. At time of sigkill the total memory allocated by the JVM was about 2000MiB below the max heap size. Note that the heap is contained within all memory allocated by the JVM.; ```; Nov 22 15:31:05 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419:3,update,update,3,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832531419,1,['update'],['update']
Deployability,"An update: I missed an additional `annotate_variants_table(kt_keyed_on_variant)` in my example above. Interestingly, it seems to work if I do:; ```; vds.annotate_variants_table(kt_keyed_on_variant).annotate_variants_table(kt_keyed_on_locus).write(); ```; but fails if I:; ```; vds.annotate_variants_table(kt_keyed_on_locus).annotate_variants_table(kt_keyed_on_variant).write(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1823#issuecomment-302457141:3,update,update,3,https://hail.is,https://github.com/hail-is/hail/issues/1823#issuecomment-302457141,1,['update'],['update']
Deployability,"An update: this seems to happen when running from an interactive shell as I'm setting up my own jar and python directory, so maybe I'm doing something wrong? Happens on the cloud but not locally...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1360#issuecomment-278532107:3,update,update,3,https://hail.is,https://github.com/hail-is/hail/issues/1360#issuecomment-278532107,1,['update'],['update']
Deployability,"And here's a dev deploy that runs the dataproc tests. Don't approve until these tests pass! We don't run them on ordinary PRs because they're expensive and slow. We do run them on main commits. For this PR, the chance of having broken dataproc is high enough that we should ensure the tests pass before merging into main. https://ci.hail.is/batches/8121061",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1930642516:17,deploy,deploy,17,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1930642516,1,['deploy'],['deploy']
Deployability,"And in the off chance that you're using ArchLinux, can you try installing `openblas-lapack` from the AUR?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-238881210:63,install,installing,63,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238881210,1,['install'],['installing']
Deployability,And let's update the navbar since we're making the above change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-612155649:10,update,update,10,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612155649,1,['update'],['update']
Deployability,Another follow up PR: update the ggplot tutorial to include the use of cross-product legends!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12254#issuecomment-1267273283:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/12254#issuecomment-1267273283,1,['update'],['update']
Deployability,"Another issue I noticed: viz.min.js isn't found in the deployed internal copy. Assuming the vendors directory was copied, you may have deployed before adding execute bit to that folder.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639030337:55,deploy,deployed,55,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639030337,2,['deploy'],['deployed']
Deployability,"Another very simple pipeline reported https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/zip.3A.20length.20mismatch . We can get access to these files via Sam B. ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.annotate(; uniprot = ensp2uniprot_ht[context_mis_freq_ht.ensp].uniprot); ```. notice that the error is removed if you instead use:; ```python3; context_mis_freq_ht = hl.read_table(""gs://epi25/misc-data/gnomAD_v4/grch38_context_vep_annotated.v105.prefiltered.missense_freq_ensp.ht""); ensp2uniprot_ht = hl.import_table(""gs://epi-mis-3d/misc/ensp2uniprot_mart_export.ensp2uniprot.txt""). context_mis_freq_ht = context_mis_freq_ht.key_by(""ensp""); ensp2uniprot_ht = ensp2uniprot_ht.key_by(""ensp""). context_mis_freq_ht = context_mis_freq_ht.join(ensp2uniprot_ht,'left'). ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858:20,pipeline,pipeline,20,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1883607858,1,['pipeline'],['pipeline']
Deployability,Arcturus has been without internet and unable to review #6969 - OK if we release tomorrow?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-527601168:73,release,release,73,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527601168,1,['release'],['release']
Deployability,Are you asking how to detect that one command of many in a bash *pipeline* failed? We need pipe fail enabled for that:. ```bash; # bad; (base) dking@wm28c-761 ~ % ls fdsafds | xargs echo hello; ls: fdsafds: No such file or directory; (base) dking@wm28c-761 ~ % echo $?; 0; ```; ```bash; # good; (base) dking@wm28c-761 ~ % set -o pipefail; (base) dking@wm28c-761 ~ % ls fdsafds | xargs echo hello; ls: fdsafds: No such file or directory; (base) dking@wm28c-761 ~ % echo $?; 1; ```. Permissions issues indeed fail the `gcloud` command:; ```; (base) dking@wm28c-761 ~ % gcloud compute instances list --project notmyproject; API [compute.googleapis.com] not enabled on project [notmyproject]. Would you ; like to enable and retry (this will take a few minutes)? (y/N)? y. Enabling service [compute.googleapis.com] on project [notmyproject]...; ERROR: (gcloud.compute.instances.list) PERMISSION_DENIED: Permission denied to enable service [compute.googleapis.com]; Help Token: AVzH8v0NCN6UR5g5Xtu_gFde3SeZCmToYDDOlz7hp5HiVvGHKX8aeJ-kn0N0n72nMovbuw4ksm8MB0OifqPrdxlc6lWwJJKi0CsIJon1a7SSlF_H; - '@type': type.googleapis.com/google.rpc.PreconditionFailure; violations:; - subject: ?error_code=110002&service=serviceusage.googleapis.com&permission=serviceusage.services.enable&resource=notmyproject; type: googleapis.com; - '@type': type.googleapis.com/google.rpc.ErrorInfo; domain: serviceusage.googleapis.com; metadata:; permission: serviceusage.services.enable; resource: notmyproject; service: serviceusage.googleapis.com; reason: AUTH_PERMISSION_DENIED; (base) dking@wm28c-761 ~ % echo $?; 1; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1743472268:65,pipeline,pipeline,65,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1743472268,1,['pipeline'],['pipeline']
Deployability,Are you planning on adding the linters to ci2 and pipeline as well?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6149#issuecomment-494519184:50,pipeline,pipeline,50,https://hail.is,https://github.com/hail-is/hail/pull/6149#issuecomment-494519184,1,['pipeline'],['pipeline']
Deployability,"Are you sure that you installed all the necessary packages listed here: https://hail.is/docs/0.2/install/linux.html ? In particular this kind of error can happen if you did not install openblas. In the future, please use https://discuss.hail.is for support questions, we don't monitor GitHub issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9939#issuecomment-786236682:22,install,installed,22,https://hail.is,https://github.com/hail-is/hail/issues/9939#issuecomment-786236682,3,['install'],"['install', 'installed']"
Deployability,"Are you using Python 3? Hail currently only supports Python 2, though we expect to be either compatible with both or _only_ compatible with Python 3 in the next stable release",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321211196:168,release,release,168,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321211196,1,['release'],['release']
Deployability,"Are you working with another branch at the same time in dev? If not, I feel like `hailctl dev deploy -b jigold:region-job-queue-fast-ci -s test_batch,test_hailtop_batch -e deploy_batch` would be a faster feedback loop and not take up a spot in the PR queue",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507:94,deploy,deploy,94,https://hail.is,https://github.com/hail-is/hail/pull/12263#issuecomment-1267535507,1,['deploy'],['deploy']
Deployability,"As a data point, we saw this error for IR sizes beyond 20000000 in our 0.2.132ish deployment too, when using ServiceBackend. PR #14567 was present in 0.2.132, so unless something weird was going on with our installation (certainly a possibility) I think this means #14567 wasn't effective for us either. We're very much looking forward to giving #14750 a try.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14749#issuecomment-2458392840:82,deploy,deployment,82,https://hail.is,https://github.com/hail-is/hail/issues/14749#issuecomment-2458392840,2,"['deploy', 'install']","['deployment', 'installation']"
Deployability,"As an example of this slash issue, the following config (deployed right now) doesn't work. ```; location /monitoring/grafana {; proxy_pass http://grafana/;; }. location /monitoring/grafana/ {; proxy_pass http://grafana/;; }; ```. Routing to https://internal.hail.is/monitoring/grafana appears to not hit the router (`k logs router-759c675b98-8mp67 -n monitoring -f`). Suggests problem is upstream of router-759. https://internal.hail.is/monitoring/grafana/ works fine, as expected. Trailing slash on GF_SERVER_ROOT_URL has no effect, as expect, since before grafana gets anything, the router should receive the request.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336:57,deploy,deployed,57,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540645336,1,['deploy'],['deployed']
Deployability,"As soon as Research Computing installs the new Hail version, I will rerun that script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-689714135:30,install,installs,30,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-689714135,1,['install'],['installs']
Deployability,Assigning to Tim so it shows up in his review queue. He has some fixes he wanted to get released that probably need to be added to the change log.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12891#issuecomment-1514980397:88,release,released,88,https://hail.is,https://github.com/hail-is/hail/pull/12891#issuecomment-1514980397,1,['release'],['released']
Deployability,"At some point, we should think about how to improve the discoverability and machine-verifiability of our APIs. Currently the tightest type of job log is rather complex. If the performance is OK, I think we should move towards classes that define the request and response types of each call. ---. The main difference is `hail-pip-install` having `retry`. If pip exits with a non-zero exit code, we'll just rerun the command exactly, at most four more times. This mitigates missing retry logic in `pip` itself. For example, [this job](https://ci.hail.is/batches/167314/jobs/27) failed because pip encountered a connection reset while downloading a file. Ideally, pip would simply retry the download. Since we don't control the pip source code, I use a retry that treats all of pip as a black box. There's definitely a failure mode: if you specify a package that doesn't exist, pip will error five times in a row and take ~30 seconds before the retry logic gives up. I'm OK with this because pip should basically never fail for legitimate reasons.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278:329,install,install,329,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-775241278,1,['install'],['install']
Deployability,Awaiting confirmation that #13899 is not meant to go in this release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13905#issuecomment-1780008230:61,release,release,61,https://hail.is,https://github.com/hail-is/hail/pull/13905#issuecomment-1780008230,1,['release'],['release']
Deployability,"Awesome, thanks for the close attention, @nawatts ! @pwc2 let's update the path and get this merged!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-820650510:64,update,update,64,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-820650510,1,['update'],['update']
Deployability,"Azure is currently running this internal-gateway/gateway, and PRs seem to be doing no worse, and sometimes better afaict, than the nginx in GCP, save for the Connection reset retrying which once in a handful of PRs will stall the test until timeout. If we decide to merge this I would want to watch a few subsequent PRs to confirm that they're not stalling and if so I would feel confident rolling out envoy to gcp as well (and can resize the k8s pool immediately after!)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12425#issuecomment-1303825690:390,rolling,rolling,390,https://hail.is,https://github.com/hail-is/hail/pull/12425#issuecomment-1303825690,1,['rolling'],['rolling']
Deployability,"BLAS is actually installed in /usr/lib64/atlas. Spark was not finding the lib for some reason. ; ; The solution was to add the following config to the spark-submit command line. --conf spark.executor.extraClassPath=""/usr/lib64/atlas"" . It would be useful to add this to the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667:17,install,installed,17,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529740667,1,['install'],['installed']
Deployability,"Because node selectors are ""recommended"": https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ ""the recommended approaches all use label selectors to make the selection."" ""nodeSelector is the simplest recommended form of node selection constraint."". The taint/toleration documentation use no such language and their suggested use cases don't match ours: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/. I'm not sure what to read into this, if anything. I had a mark set in my mind against taints since I remember reading that some scheduler features (maybe eviction or downsizing?) were disabled with taints. I can't find this in the docs anymore, so it was probably fixed (or I'm not searching hard enough?), but the bad feeling remains. I see your argument, although missing the tag means either paying too much (running a preemptible pod on a non-preemptible node) which we should discovery by monitoring the non-preemptible node workload, or we get excessive downtime on preemptions which we should notice through uptime monitoring. I'm mostly just frustrated with the autoscheduler and trying to simplify things to get it to behave reasonably before I end up writing our own.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683:78,configurat,configuration,78,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-560696683,2,['configurat'],['configuration']
Deployability,"Because of the unpredictable way that git clone might realize the requirements files, I removed the pinned-requirements file as a dependency of the changed targets. In both cases, regenerating that file (either in CI as part of the deploy.yaml target or on a cluster for `install-on-cluster`) could cause a dataproc cluster running with untested dependency versions even if the requirements.txt files are unchanged. I do, however, require that the pinned-requirements files be compatible using the same check we do in CI. I performed the following manual testing:; 1. Creating a dataproc cluster through `hailctl dataproc start`; 2. ssh'ing into said cluster, cloning this branch and running `make -C hail install-on-cluster` to completion; 3. Updating the requirements.txt file to something incompatible and successfully installing on cluster again with updated pinned requirements. However, I'm not sure I'm actually doing this right. I checked that in step 2 I was *not* regenerating any pinned-requirments files, but in step 3 make updated the pinned requirements without me telling it to, I'm guessing because of the wheel's dependence on `PY_FILES` and I changed the source under hail/python. So I don't entirely understand why I have this desirable result.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1338482256:232,deploy,deploy,232,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1338482256,6,"['deploy', 'install', 'update']","['deploy', 'install-on-cluster', 'installing', 'updated']"
Deployability,"Before this problem occurred, Hail was installed in an iPython notebook with the magic command `%pip install hail`. After this problem, I then re-installed it on my command line with `pip install hail`; the problem then seemed to go away. Are you aware of any differences between installing Hail with pip via iPython versus via command line?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13230#issuecomment-1629820723:39,install,installed,39,https://hail.is,https://github.com/hail-is/hail/issues/13230#issuecomment-1629820723,5,['install'],"['install', 'installed', 'installing']"
Deployability,"Before we merge this, I'd like to have the new Artifact Registry in hail-vdc setup and have configured a cloud run job for the cleanup script. I think a daily run is good enough. https://github.com/GoogleCloudPlatform/gcr-cleaner/blob/main/docs/deploy-cloud-run.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255117035:245,deploy,deploy-cloud-run,245,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255117035,1,['deploy'],['deploy-cloud-run']
Deployability,"Ben, can you reopen the PR from a branch on your fork of Hail? We're deleting all non-deployment branches from the main repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1987#issuecomment-317845459:86,deploy,deployment,86,https://hail.is,https://github.com/hail-is/hail/pull/1987#issuecomment-317845459,1,['deploy'],['deployment']
Deployability,"Better place to post things like this would be discuss.hail.is (because it's probably a configuration issue with your cluster and not a bug in hail). I'd guess you don't have BLAS installed on your cluster. If you check the hail.log file, do you have lines like. ```; Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK; Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS; ```. ?. See here: https://hail.is/docs/0.2/getting_started.html#common-installation-issues",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947:88,configurat,configuration,88,https://hail.is,https://github.com/hail-is/hail/issues/7008#issuecomment-529008947,3,"['configurat', 'install']","['configuration', 'installation-issues', 'installed']"
Deployability,"Blocked by matplotlib, which has not yet released a compatible version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13376#issuecomment-1668462300:41,release,released,41,https://hail.is,https://github.com/hail-is/hail/pull/13376#issuecomment-1668462300,1,['release'],['released']
Deployability,"Bowing out of this one as it is too spicy for my taste. Happy to test it on my pipelines once its passing tests and ready, but someone else should probably do the code review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-413510162:79,pipeline,pipelines,79,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-413510162,1,['pipeline'],['pipelines']
Deployability,"Buy it, use it, break it, fix it; Trash it, change it, mail - upgrade it; Charge it, point it, zoom it, press it; Snap it, work it, quick - erase it; Write it, cut it, paste it, save it; Load it, check it, quick - rewrite it; Plug it, play it, burn it, rip it; Drag and drop it, zip - unzip it; Lock it, fill it, call it, find it; View it, code it, jam - unlock it; Surf it, scroll it, pause it, click it; Cross it, crack it, switch - update it; Name it, read it, tune it, print it; Scan it, send it, fax - rename it; Touch it, bring it, pay it, watch it; Turn it, leave it, start - format it",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817:62,upgrade,upgrade,62,https://hail.is,https://github.com/hail-is/hail/pull/2036#issuecomment-318518817,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,CI deploy is broken due to checkout failure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8577#issuecomment-616211238:3,deploy,deploy,3,https://hail.is,https://github.com/hail-is/hail/pull/8577#issuecomment-616211238,1,['deploy'],['deploy']
Deployability,CI deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8500#issuecomment-611051863:3,deploy,deployed,3,https://hail.is,https://github.com/hail-is/hail/pull/8500#issuecomment-611051863,1,['deploy'],['deployed']
Deployability,Can be overcome by:; pip install wheel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742#issuecomment-734127073:25,install,install,25,https://hail.is,https://github.com/hail-is/hail/issues/9742#issuecomment-734127073,1,['install'],['install']
Deployability,Can one of the admins verify this patch?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-220609757:34,patch,patch,34,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220609757,9,['patch'],['patch']
Deployability,Can see an example of this running (with the import / export VCF pipeline) [here](https://batch.azure.hail.is/batches/3423869),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12593#issuecomment-1381008906:65,pipeline,pipeline,65,https://hail.is,https://github.com/hail-is/hail/pull/12593#issuecomment-1381008906,1,['pipeline'],['pipeline']
Deployability,Can we double check with @bw2 that this won't break his pipelines?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12324#issuecomment-1279117052:56,pipeline,pipelines,56,https://hail.is,https://github.com/hail-is/hail/pull/12324#issuecomment-1279117052,1,['pipeline'],['pipelines']
Deployability,Can we follow this up with getting the file into git and checking that import-count works before deploying it?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4716#issuecomment-435389708:97,deploy,deploying,97,https://hail.is,https://github.com/hail-is/hail/issues/4716#issuecomment-435389708,1,['deploy'],['deploying']
Deployability,Can we get #9986 into this one as well? Qi asking for some version updates for Terra folks,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9984#issuecomment-773360730:67,update,updates,67,https://hail.is,https://github.com/hail-is/hail/pull/9984#issuecomment-773360730,1,['update'],['updates']
Deployability,"Can we make it a job and/or batch level configuration? The user obviously can do whatever they like with their tokens. However, since most users don't need them, I prefer our default to be to not expose them. I think we just need a new method and attribute on `Job` and `Batch` that mirrors, say, `image` or `memory`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020:40,configurat,configuration,40,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767739020,1,['configurat'],['configuration']
Deployability,Can you add a pipeline/dataset that demonstrates this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4506#issuecomment-427574034:14,pipeline,pipeline,14,https://hail.is,https://github.com/hail-is/hail/issues/4506#issuecomment-427574034,1,['pipeline'],['pipeline']
Deployability,"Can you post the results of. ```; yum info atlas-devel; ```. If `atlas-devel` is not installed, can you try installing it?. ```; yum install atlas-devel; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239495713:85,install,installed,85,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239495713,3,['install'],"['install', 'installed', 'installing']"
Deployability,Can you refresh my memory on what this is about? I basically thought everything in vdc/ was complete bitrot. This is for deploying ... a new Hail Python package?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8178#issuecomment-599780653:121,deploy,deploying,121,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599780653,1,['deploy'],['deploying']
Deployability,"Can you try it again? @cseed updated the JAR file on data flow. Also, your code is not correct -- should be this:. ```; hail importbgen -s impv1.hail.sample chr21impv1.bgen \ variantqc \ annotatevariants expr -c 'va.infoMetrc = gs.infoScore()' \ exportvariants -c 'Chrom=v.contig,rsID = va.rsid,info= va.infoMetrc.score,Pos=v.start,Ref=v.ref,Alt=v.alt,MAF=va.qc.AF' -o file:///medpop/afib/schoi/projects/ukbb/Result/QC/variantQC.tsv; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/685#issuecomment-251708654:29,update,updated,29,https://hail.is,https://github.com/hail-is/hail/issues/685#issuecomment-251708654,1,['update'],['updated']
Deployability,Caused by a pandas issue: https://github.com/pandas-dev/pandas/issues/18843#issuecomment-1765218145. Easy fix seems to be to monkey patch pandas `is_sequence` to not treat a Struct as a sequence so that the printed from looks like `Struct(x=0)` etc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13512#issuecomment-1765221174:132,patch,patch,132,https://hail.is,https://github.com/hail-is/hail/issues/13512#issuecomment-1765221174,1,['patch'],['patch']
Deployability,Certainly appears to have fixed my issue. Pipeline that went from 11 to 25 mins is now back down to 11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4028#issuecomment-410786746:42,Pipeline,Pipeline,42,https://hail.is,https://github.com/hail-is/hail/issues/4028#issuecomment-410786746,1,['Pipeline'],['Pipeline']
Deployability,"Changed function in another branch with more current updates, so I closed this one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11473#issuecomment-1066891312:53,update,updates,53,https://hail.is,https://github.com/hail-is/hail/pull/11473#issuecomment-1066891312,1,['update'],['updates']
Deployability,Changed to use the environment variable directly. I also updated the PR description with details on how to install mkl and make Hail load it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440:57,update,updated,57,https://hail.is,https://github.com/hail-is/hail/pull/10770#issuecomment-897042440,2,"['install', 'update']","['install', 'updated']"
Deployability,"Changes since last review:; - Method now takes expressions for call and (optionally) scores.; - Block matrix and table of scores annotated and collected from source.cols() sent to Python, processed using int indices, column names restored on python side (thanks @tpoterba); - Fixed bug that silently dropped `n_samples / block_size` proportion of pairs, Python test checks it; - Extended Python tests to compare k and scores paths, test counts, min_kinship, maf, block_size; - Tuned tolerances on comparison with R from Python; - Extended to general column key, removing unique key check, noted in docs; - MEMORY_AND_DISK caching as default (thanks @konradjk) on Scala side; - The diagonal fix meant phi is computed with parallelism up to the number of diagonal blocks, rather than parallelism 1. But that's still likely a bottleneck as phi requires computing and point-wise dividing two big gram matrices. I now write phi to disk and read it back in, which squares the parallelism up to the number of blocks in phi. I think this should also improve the stability of the many downstream calculations derived from phi, esp. if pre-emptibles are used. No longer cacheing phi, but I left caching on the other matrices. @konradjk let us know how this version compares next time you run it.; - Noted in FIXME room for further improvement when fusing blocks: `replace join with zipPartitions, throw away lower triangular blocks sooner, avoid the nulls`; - Updated docs accordingly; - Deleted a bunch of code in PCRelate and PCRelateSuite",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104:1450,Update,Updated,1450,https://hail.is,https://github.com/hail-is/hail/pull/3211#issuecomment-376725104,1,['Update'],['Updated']
Deployability,"Changes to the batch format (specifically around resources, but also a bug related to unused secrets namespace field) broke CI. I had deployed this as CI, and it looks it will be able to self-build.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7324#issuecomment-543186317:134,deploy,deployed,134,https://hail.is,https://github.com/hail-is/hail/pull/7324#issuecomment-543186317,1,['deploy'],['deployed']
Deployability,"Changes:. - Rename Pipeline class to Batch; - Rename Task class to Job; - Rename pipeline module to batch; - Update getting started page to reflect module is at hailtop.batch; - Update website location (build.yaml, Makefile, web_common); - Update conf.py to use batch as the docs name; - Fix all examples in docs to use batch, job; - Go through docs text and change references to pipelines to batches and tasks to jobs; - Move pipeline/test/test_pipeline.py to hail/python/test/hailtop/batch/; - Update tests to use batch, job; - Update benchmark suite; - Rename BatchBackend to ServiceBackend; - Consolidate test_pipeline into test_hailtop_batch; - Update Hail makefile to build batch docs rather than pipeline docs. I did *NOT* change the navbar to point to the batch docs since it won't exist until the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606:19,Pipeline,Pipeline,19,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-610595606,12,"['Pipeline', 'Update', 'pipeline', 'release']","['Pipeline', 'Update', 'pipeline', 'pipelines', 'release']"
Deployability,"Changing the `dev-environment.yaml` requires you to build a new CI image. You can do this with `make push-hail-ci-build-image`. This should update the file `hail-ci-build-image`. If you check that change into this branch, the CI system will use that image, with the new conda environment, instead.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4139#issuecomment-412612911:140,update,update,140,https://hail.is,https://github.com/hail-is/hail/pull/4139#issuecomment-412612911,1,['update'],['update']
Deployability,"Chris, could you do a release next week to get this fix out? Thank you much!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11345#issuecomment-1059678104:22,release,release,22,https://hail.is,https://github.com/hail-is/hail/pull/11345#issuecomment-1059678104,1,['release'],['release']
Deployability,Citation for log4j1 programmatic configuration breaking log4j2: https://logging.apache.org/log4j/2.x/manual/migration.html#limitations-of-the-log4j-1-x-bridge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047:33,configurat,configuration,33,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1524328047,1,['configurat'],['configuration']
Deployability,Closing for now while I dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9074#issuecomment-658273654:28,deploy,deploy,28,https://hail.is,https://github.com/hail-is/hail/pull/9074#issuecomment-658273654,1,['deploy'],['deploy']
Deployability,"Closing in favor of https://github.com/hail-is/hail/pull/12386, which uses my fork since (I think?) CI only lets authorized users run pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11800#issuecomment-1294024957:134,pipeline,pipelines,134,https://hail.is,https://github.com/hail-is/hail/pull/11800#issuecomment-1294024957,1,['pipeline'],['pipelines']
Deployability,"Closing the loop, passed in deploy build:; https://batch.hail.is/batches/8128498/jobs/208",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14198#issuecomment-1946416613:28,deploy,deploy,28,https://hail.is,https://github.com/hail-is/hail/pull/14198#issuecomment-1946416613,1,['deploy'],['deploy']
Deployability,Closing the loop: this was released into 0.2.110!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11738#issuecomment-1467000303:27,release,released,27,https://hail.is,https://github.com/hail-is/hail/issues/11738#issuecomment-1467000303,1,['release'],['released']
Deployability,Closing this while I debug with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7333#issuecomment-545621097:36,deploy,deploy,36,https://hail.is,https://github.com/hail-is/hail/pull/7333#issuecomment-545621097,3,['deploy'],['deploy']
Deployability,Closing this while I dev deploy test it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7910#issuecomment-576348264:25,deploy,deploy,25,https://hail.is,https://github.com/hail-is/hail/pull/7910#issuecomment-576348264,1,['deploy'],['deploy']
Deployability,"Closing to refactor, will open updated PR shortly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3397#issuecomment-386605198:31,update,updated,31,https://hail.is,https://github.com/hail-is/hail/pull/3397#issuecomment-386605198,1,['update'],['updated']
Deployability,Closing while I dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9095#issuecomment-672126602:20,deploy,deploy,20,https://hail.is,https://github.com/hail-is/hail/pull/9095#issuecomment-672126602,2,['deploy'],['deploy']
Deployability,Closing while updates are made.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-379095695:14,update,updates,14,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-379095695,1,['update'],['updates']
Deployability,Closing with 0.1 release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1396#issuecomment-301546353:17,release,release,17,https://hail.is,https://github.com/hail-is/hail/issues/1396#issuecomment-301546353,1,['release'],['release']
Deployability,"Code looks good. Can you update the VariantDataset.vep function documentation? In particular, note that plugin overrides human_ancestor and conservation file. plugin in cleaner. We should remove the latter when we start version 0.2: https://github.com/hail-is/hail/issues/1728",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358:25,update,update,25,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-297784358,1,['update'],['update']
Deployability,Code looks good. It looks like there's something wrong with the gradle cpp test configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340:80,configurat,configuration,80,https://hail.is,https://github.com/hail-is/hail/pull/4594#issuecomment-433123340,1,['configurat'],['configuration']
Deployability,"Compared to the baseAddress stuff, a more exotic change would be to change the layout of a PCanonicalArray, as well as update the notion of a 'pointer' to a PCanonicalArray. The result would be something like [sds](https://github.com/antirez/sds) which is a dynamic string library, where the public api works on char pointers, and all the metadata is kept in a prefix to that pointer.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10579#issuecomment-859253106:119,update,update,119,https://hail.is,https://github.com/hail-is/hail/pull/10579#issuecomment-859253106,1,['update'],['update']
Deployability,Completely missed this . > This is failing style checks. There need to be two blank lines before the main function and only one blank line after the added if statement. sorry completely missed this rev. comment. Just noticed when colleague tried to launch a pipeline with this feature assumed in the submitter. Fixed style issues now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10900#issuecomment-978174752:258,pipeline,pipeline,258,https://hail.is,https://github.com/hail-is/hail/pull/10900#issuecomment-978174752,1,['pipeline'],['pipeline']
Deployability,Configuration can now be stored in the metadata top-level JSON (e.g. split).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/368#issuecomment-236621515:0,Configurat,Configuration,0,https://hail.is,https://github.com/hail-is/hail/issues/368#issuecomment-236621515,1,['Configurat'],['Configuration']
Deployability,Confirmed that Miniconda installer appends the necessary to bash_profile (both GUI package installer and bash installer),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6738#issuecomment-515167477:25,install,installer,25,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515167477,3,['install'],['installer']
Deployability,"Confirmed that this prebuilt libhail.so can run tests with HAIL_ENABLE_CPP_CODEGEN=1; on a dataproc node with the default 1.2 image (debian8 and g++-4.9.2). The dataproc nodes have g++ and make already installed, so this PR should suffice to; make them work with C++ codegen.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424477261:202,install,installed,202,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424477261,1,['install'],['installed']
Deployability,"Confirmed this is working. If I introduce a segfault to some code, then run `make install-editable HAIL_DEBUG_MODE=1`, then execute the python code that would cause a segfault, it throws a RuntimeError.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9217#issuecomment-669210874:82,install,install-editable,82,https://hail.is,https://github.com/hail-is/hail/pull/9217#issuecomment-669210874,1,['install'],['install-editable']
Deployability,"Copying from issue #4609:; > Ah, I bet that service accounts default all their API requests to the namespace they are a member of. For deploy-svc that's batch-pods",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4618#issuecomment-432384741:135,deploy,deploy-svc,135,https://hail.is,https://github.com/hail-is/hail/pull/4618#issuecomment-432384741,1,['deploy'],['deploy-svc']
Deployability,Cotton had an old comment that's now disappeared saying some of the SELECTs in the Python code should have been SELECT FOR UPDATE. I think it's okay to do that in a separate PR. Can we get a concise list of things that still need to be addressed later from this PR?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575720943:123,UPDATE,UPDATE,123,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575720943,1,['UPDATE'],['UPDATE']
Deployability,"Cotton, mostly looks great, I haven't taken a look at the deployment configs yet. I would like to do that, and if you're ok with this from a time standpoint, spin up a locally deployed version to play with. . Nice work!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663:58,deploy,deployment,58,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528373663,2,['deploy'],"['deployed', 'deployment']"
Deployability,"Create a new bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: y; What is the name of the new bucket (Example: hail-batch-jigold-yrxul): hail-batch-jigold-yrxul; Which google project should hail-batch-jigold-yrxul be created in? This project will incur costs for storing your Hail generated data. (Example: hail-jigold): hail-jigold; Which region does your data reside in? (Example: us-central1): us-central1; Do you want to set a lifecycle policy (automatically delete files after a time period) on the bucket hail-batch-jigold-yrxul? [y/n]: y; After how many days should files be automatically deleted from bucket hail-batch-jigold-yrxul? (30): 15; Created bucket hail-batch-jigold-yrxul in project hail-jigold.; Updated bucket hail-batch-jigold-yrxul in project hail-jigold with lifecycle rule set to 15 days and labels {'bucket': 'hail-batch-jigold-yrxul', 'owner': 'jigold', 'data_type': 'temporary'}.; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-yrxul in project hail-jigold.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gservic",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:845,Update,Updated,845,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['Update'],['Updated']
Deployability,Cross linking to an issue on the same partition that the gnomAD team discovered when running a different pipeline: https://github.com/hail-is/hail/issues/13584,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1710193006:105,pipeline,pipeline,105,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1710193006,1,['pipeline'],['pipeline']
Deployability,"Cross-posting from Zulip:. Perhaps unfilter_entries should take a condition and values for the fields:. ```; mt = mt.unfilter_entries(hl.is_defined(mt2[mt.row_key, mt.col_key]); foo = mt2[mt.row_key, mt.col_key].foo); ```. This is relevant to a situation where Laurent wanted to update the filtered cells based on data from another matrix table.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8041#issuecomment-582177354:279,update,update,279,https://hail.is,https://github.com/hail-is/hail/issues/8041#issuecomment-582177354,1,['update'],['update']
Deployability,"Current Plan:. - input pod; exit 0 if any files exist in /io/; if pod is preempted, delete the pvc and start a new pod. - main pod; init container touches /io/some_file_main; if /io/some_file_main exists, exit 0; If pod is preempted, delete pvc and start a new input pod (rollback). - output pod; init container checks /io/some_file_output; if /io/some_file_output exists, exit 0; touch /io/some_file_output at the end of running the output pod successfully",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6494#issuecomment-506534967:272,rollback,rollback,272,https://hail.is,https://github.com/hail-is/hail/issues/6494#issuecomment-506534967,1,['rollback'],['rollback']
Deployability,"Currently the` nvidia-container-toolkit` is installed both in the vm startup script and in the worker docker image. The toolkit must be installed in the startup script to be able to configure docker with the command `nvidia-ctk runtime configure --runtime=docker`. This command cannot be run from Dockerfile.worker because it gets the error `""unable to flush config: unable to open /etc/docker/daemon.json for writing: open /etc/docker/daemon.json: no such file or directory""`.; The toolkit also has to be installed in Dockerfile.worker since that is where crun is invoked from. To execute the nvidia hook, the toolkit needs to be installed in that container. We could probably find a workaround to this if you would like but it only increased the worker image from 1.47Gb to 1.55Gb so it seems pretty small.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481:44,install,installed,44,https://hail.is,https://github.com/hail-is/hail/pull/13430#issuecomment-1710796481,4,['install'],['installed']
Deployability,"Dan, I went down a rabbit hole with this one. Updated bootstrap (XSS exploit protection, not EOL), jQuery (a bunch of security patches), focused on using flex box for layout, and fixed many of the inconsistencies I found on the docs page (the way the header was laid out, namely lack of element alignment with rest of docs and odd centering, the weirdness of having two home buttons named Hail, Annotation Database didn't scope styles so changed docs nav layout, broken navbar menu, etc).; - Also removes navbar code duplication in docs. Also after speaking with Jackie, restored fixed navbar on docs (so that it stays in place during scrolling). This may cause issues with (especially older) mobile devices, but those probably aren't spending much time on the docs page anyway. Works great on narrow views as well. Since 0.1 doesn't appear to be built, if these changes can affect that will need to be addressed. Before: https://youtu.be/I-Awgx3spnQ; After: https://youtu.be/ff1387vDsQ8. cc @cseed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293:46,Update,Updated,46,https://hail.is,https://github.com/hail-is/hail/pull/6789#issuecomment-522140293,2,"['Update', 'patch']","['Updated', 'patches']"
Deployability,Daniel -- I haven't tested this beyond dev deploy. Can you take a look first to make sure everything makes sense before I do the last test of both the attempt_resources format version < 3 PR changes and these changes to populate the by_date tables?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1178112385:43,deploy,deploy,43,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1178112385,1,['deploy'],['deploy']
Deployability,Dataproc passed on this release. https://ci.hail.is/batches/8105133 as of 5f793639264adf0990a2189dd6d34ba67d049b9f which is based on 1a5f4851149a84fd210c694053fb5c593cf27d07 in hi/main.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14086#issuecomment-1887561566:24,release,release,24,https://hail.is,https://github.com/hail-is/hail/pull/14086#issuecomment-1887561566,1,['release'],['release']
Deployability,"Delay merging until https://github.com/broadinstitute/install-gcs-connector/pull/6 is merged. Without that PR, users will not have access to a version of the GCS Hadoop connector that does not use tons of memory in JVM 11.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1931024352:54,install,install-gcs-connector,54,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1931024352,1,['install'],['install-gcs-connector']
Deployability,"Dependabot tried to update this pull request, but something went wrong. We're looking into it, but in the meantime you can retry the update by commenting `@dependabot rebase`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12476#issuecomment-1319054705:20,update,update,20,https://hail.is,https://github.com/hail-is/hail/pull/12476#issuecomment-1319054705,10,['update'],['update']
Deployability,"Depending on which PR of #8844 goes in first, I'll need to update the docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8850#issuecomment-635432823:59,update,update,59,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-635432823,1,['update'],['update']
Deployability,"Deploy commits don't need to cleanup which adds some latency to this PR. We should probably use xargs -P4 to delete instances 4-way parallel. This PR is ~46 minutes, including all the cleanup time, where as deploys are 46 minutes *without the cleanup time*. Notice two things: (1) the service backend is again the critical path (2) some local backend tests took quite a while to get scheduled. Seems fishy to me that it took ~16 minutes to find a core for the local backend tests to run on. Anyway, seems good to use more fine-grained parallelism. This should help keep the cluster large-ish and turning over fast so that users get a great experience during the work day. ---. A deploy commit:. <img width=""2032"" alt=""Screen Shot 2023-05-17 at 17 30 55"" src=""https://github.com/hail-is/hail/assets/106194/9c00365e-1079-451c-bd85-e10561e715c1"">. This PR:. <img width=""2032"" alt=""Screen Shot 2023-05-17 at 17 34 40"" src=""https://github.com/hail-is/hail/assets/106194/fa7751be-3986-4361-89ea-e322760176bf"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1552119464:0,Deploy,Deploy,0,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1552119464,3,"['Deploy', 'deploy']","['Deploy', 'deploy', 'deploys']"
Deployability,"Deploy happened, closing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8336#issuecomment-602797692:0,Deploy,Deploy,0,https://hail.is,https://github.com/hail-is/hail/pull/8336#issuecomment-602797692,1,['Deploy'],['Deploy']
Deployability,"Deployed, branch protections reenabled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9728#issuecomment-730651886:0,Deploy,Deployed,0,https://hail.is,https://github.com/hail-is/hail/pull/9728#issuecomment-730651886,1,['Deploy'],['Deployed']
Deployability,Deploying and taking a look!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10060#issuecomment-786227812:0,Deploy,Deploying,0,https://hail.is,https://github.com/hail-is/hail/pull/10060#issuecomment-786227812,1,['Deploy'],['Deploying']
Deployability,Deploys tend to be a live debugging experience when there are new changes anyway 😉,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11305#issuecomment-1028150772:0,Deploy,Deploys,0,https://hail.is,https://github.com/hail-is/hail/pull/11305#issuecomment-1028150772,1,['Deploy'],['Deploys']
Deployability,"Description updated. > Also, it would be helpful to record for history what you ran to update all the pinned-requirements.txt files. ```bash; $ make generate-pip-lockfiles; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14631#issuecomment-2256776992:12,update,updated,12,https://hail.is,https://github.com/hail-is/hail/pull/14631#issuecomment-2256776992,2,['update'],"['update', 'updated']"
Deployability,Dev deploy still coming up...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8805#issuecomment-629258538:4,deploy,deploy,4,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629258538,1,['deploy'],['deploy']
Deployability,"Discussion in a different forum sounds good. This came up with the GPU branch as there was a question on whether we can trust what we are parsing as a resource or did we need to hardcode the SKUs explicitly. This was my proposed solution for us at least knowing that something has changed with lots of driver error messages and to not try and do any updates if the invariants of the ""SKU"" don't hold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579:350,update,updates,350,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1736209579,1,['update'],['updates']
Deployability,Do not merge until addressing concerns raised by Chris in the release thread of #team,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13171#issuecomment-1586440331:62,release,release,62,https://hail.is,https://github.com/hail-is/hail/pull/13171#issuecomment-1586440331,1,['release'],['release']
Deployability,Do we have a logging agent installed inside the containers themselves? I'd have thought that was on the worker VM. Seems like we could go to 22.04 in all the containers. It does mean our worker VM is out of sync with the containers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11859#issuecomment-1138723739:27,install,installed,27,https://hail.is,https://github.com/hail-is/hail/pull/11859#issuecomment-1138723739,1,['install'],['installed']
Deployability,"Do you have an error message other than that failed post? I'm not seeing why that would stop a deploy. In #13115, the PR healing code is after the `_heal_deploy` code, so I don't know why an exception when healing a PR would stop a deploy from occurring. This POST error is also occurring on GCP. Definitely something to fix but I'm not sure why it's related. > This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. Unfortunately I'm not sure if this is relevant in Azure. Azure CI thinks about merge candidate when it comes to testing PRs, but it doesn't merge any PRs and whether or not it does a deploy just depends if there's a new commit on `main`, it shouldn't have to do with PRs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065:95,deploy,deploy,95,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561903065,3,['deploy'],['deploy']
Deployability,"Docs and a bunch of other updates are done. Ready for review!. Once it's in, I'll refactor linreg, logreg, and lmmreg commands to put the phenotype and covariate extraction logic in one place under stats.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957:26,update,updates,26,https://hail.is,https://github.com/hail-is/hail/pull/1064#issuecomment-260219957,1,['update'],['updates']
Deployability,"Does not work (passes) with PL present in all rows, which surprised me since type should be virtual type should be taken as stated, I believe. Haven't investigated further (stats day) to see what the IR generated is. Also does not work if I edit the VCF file and insert a bogus PL of .,.,. for each sample. An upcast seems to be happening in the mt1 child, because PL is clearly missing in mt2:. <img width=""705"" alt=""Screenshot 2020-01-31 12 40 13"" src=""https://user-images.githubusercontent.com/5543229/73561429-f9e1eb00-4426-11ea-9bb8-0cec77398d92.png"">. code in updated, pushed test. edit, to show that mt1 does have expected entries (though this shouldn't matter unless array_elements_required doesn't loosen requiredeness over the imputed type):. MT1:; <img width=""170"" alt=""Screenshot 2020-01-31 12 47 00"" src=""https://user-images.githubusercontent.com/5543229/73561943-07e43b80-4428-11ea-847e-65f2f3771af8.png"">; MT2:; <img width=""208"" alt=""Screenshot 2020-01-31 12 47 05"" src=""https://user-images.githubusercontent.com/5543229/73561944-087cd200-4428-11ea-8968-6daf53291d83.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992:566,update,updated,566,https://hail.is,https://github.com/hail-is/hail/pull/8008#issuecomment-580834992,1,['update'],['updated']
Deployability,"Does the version of spark matter? such as apache spark 2.0.2 and the cloudera spark?; We use the cloudera hadoop,but for hail, the cloudera'spark can't work,so in the configuration we replaced the cloudera spark with the apache spark2.0.2,and this works in local mode,but have errors in cluster mode",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969:167,configurat,configuration,167,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321241969,2,['configurat'],['configuration']
Deployability,Doesn't the batch driver need to schedule the `deploy_batch` job to update itself? How can it eventually get to deploying the new / compatible code?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812763995:68,update,update,68,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812763995,2,"['deploy', 'update']","['deploying', 'update']"
Deployability,Don't let me forget to announce this on Zulip! We need to both announce we're doing an upgrade and that we are enforcing quotas for the root file system.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10648#issuecomment-879409551:87,upgrade,upgrade,87,https://hail.is,https://github.com/hail-is/hail/pull/10648#issuecomment-879409551,1,['upgrade'],['upgrade']
Deployability,"During implementation, I noticed some behaviors that were not fully specified. If an allele is filtered, we must address the genotypes that reference that allele. A genotype consists of five parts. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | if the filtered allele is `a` then `forall b.` `b/a` and `a/b` are converted, respectively, to `b/0` and `0/b` |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | convert the allele-pair & likelihood pairs (e.g. `(0/1, 10)`) according to the GT rule. This yields a bag of (possibly duplicated) allele-likelihood pairs. We reduce back to unique allele-pairs by taking the `min` likelihood for each allele-pair |; | GQ | genotype quality | set to the second lowest value in the modified PL |. I'm a tad uneasy about the actions for AD, DP, and PL. For AD, should we shift the depth to the reference? For DP, should we subtract the removed depths? For PL, `min` should be ok when the values have differing orders of magnitude, but if two values are similar, should we convert to probabilities, sum, and convert back?. @cseed @monkollek @konradjk",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240788265:333,a/b,a/b,333,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240788265,1,['a/b'],['a/b']
Deployability,E is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6594,release,release,6594,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,E=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4503,release,release,4503,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,EADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.p,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3210,deploy,deploy-,3210,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,EEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:14082,release,release,14082,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,ENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: rel,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3550,deploy,deploy-,3550,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,ENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']';,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9026,deploy,deploy-,9026,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,ERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_A,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:12099,deploy,deploy,12099,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy']
Deployability,ETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2430,deploy,deploy-,2430,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,"Er, wait, sorry, this thread was about the JAR. That indeed should have been deployed under the current steps (because we run QoB tests). So this is a separate issue, but leaving my comment up for posterity",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1572661584:77,deploy,deployed,77,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572661584,1,['deploy'],['deployed']
Deployability,Erm. I guess that's OK as long as we ensure the labels are the same as the service/deployment names.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6094#issuecomment-491423751:83,deploy,deployment,83,https://hail.is,https://github.com/hail-is/hail/pull/6094#issuecomment-491423751,1,['deploy'],['deployment']
Deployability,"Erm. This won't work unless I add the nginx package from apt to the pr-builder image. I guess that's better than nothing? But really I want to use the docker image we'll deploy. Once we're building docker images ourselves, this will be much easier.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4429#issuecomment-424416553:170,deploy,deploy,170,https://hail.is,https://github.com/hail-is/hail/pull/4429#issuecomment-424416553,1,['deploy'],['deploy']
Deployability,Ew8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with status 'done'; Collecting azure-common==1.1.28; Using cached azure_common-1.1.28-py2.py3-none-any.whl (14 kB); Collecting azure-core==1.29.3; Using cached azure_core-1.29.3-py3-none-any.whl (191 kB); Collecting azure-identity==1.14.0; Using cached azure_identity-1.14.0-py3-none-any.whl (160 kB); Collecting azure-mgmt-core==1.4.0; Using cached azure_mgmt_core-1.4.0-py3-none-any.whl (27 kB); Collecting azure-mgmt-storage==20.1.0; Using cached azure_mgmt_storage-20.1.0-py3-none-any.whl (2.3 MB); Collecting azure-storage-blob==12.17.0; Using cached azure_storage_blob-12.17.0-py3-none-any.whl (388 kB); Collecting bokeh==3.2.2; Using cached bokeh-3.2.2-py3-none-any.whl (7.8 MB); Collecting boto3==1.28.41; Using cached boto3-1.28.41-py3-none-any.whl (135 kB); Collecting botocore==1.31.41; Using cached ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:32573,Install,Installing,32573,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['Install'],['Installing']
Deployability,Example CI job (dev deploy) https://ci.hail.is/batches/8152856,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14398#issuecomment-2018888904:20,deploy,deploy,20,https://hail.is,https://github.com/hail-is/hail/pull/14398#issuecomment-2018888904,1,['deploy'],['deploy']
Deployability,"Excellent work! Looks much better to me in a dev deploy:. <img width=""885"" alt=""Screen Shot 2021-03-29 at 3 06 16 PM"" src=""https://user-images.githubusercontent.com/106194/112886867-7cece880-90a0-11eb-8b91-718bb1df4263.png"">; <img width=""885"" alt=""Screen Shot 2021-03-29 at 3 06 14 PM"" src=""https://user-images.githubusercontent.com/106194/112886869-7eb6ac00-90a0-11eb-8b4d-8fc79b6e7892.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10239#issuecomment-809637141:49,deploy,deploy,49,https://hail.is,https://github.com/hail-is/hail/pull/10239#issuecomment-809637141,1,['deploy'],['deploy']
Deployability,"Exciting, first code-related pull request review! It seems correct. I was wondering how you're testing table.py, backend.py, java.py, and would it be worthwhile to write unit or integration tests for these sections? I'd be happy to work on that if desired.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617:178,integrat,integration,178,https://hail.is,https://github.com/hail-is/hail/pull/5019#issuecomment-449156617,1,['integrat'],['integration']
Deployability,"FWIW, I finally found a simpler reproducer. It really takes some doing to convince the simplifier to apply this rule. This operation should use a constant ~1GiB of RAM (in reality, in a non-broken pipeline it uses closer to 8GiB, but, still, a constant amount of RAM), but in reality memory use grows with each row processed. ```python3; import hail as hl; ht = hl.utils.range_table(1); ht = ht.key_by(); ht = ht.select(rows = hl.range(10)); ht = ht.explode('rows'); ht = ht.annotate(garbage=hl.range(1024 ** 3)); ht.write('/tmp/foo.ht', overwrite=True); ```; The simplifier cannot simplify the pipeline if the key is still present so this pipeline is sufficient to restore normal memory usage:; ```python3; import hail as hl; ht = hl.utils.range_table(1); ht = ht.select(rows = hl.range(10)); ht = ht.explode('rows'); ht = ht.annotate(garbage=hl.range(1024 ** 3)); ht.write('/tmp/foo.ht', overwrite=True); ```. The ""bad"" `WritePartition` body IR looks like this:; ```; (StreamFlatMap __iruid_447; (StreamRange -1 True; (GetField start (Ref __iruid_446)); (GetField end (Ref __iruid_446)); (I32 1)); (StreamMap __iruid_448; (StreamRange 1 False (I32 0) (I32 10) (I32 1)); (InsertFields; (Literal Struct{} <literal value>); (""rows"" ""garbage""); (rows (Ref __iruid_448)); (garbage; (ToArray; (StreamRange 2 False; (I32 0); (I32 1073741824); (I32 1))))))); ```; The ""good"" IR looks like this:; ```; (StreamFlatMap __iruid_480; (StreamRange -1 True; (GetField start (Ref __iruid_479)); (GetField end (Ref __iruid_479)); (I32 1)); (Let __iruid_481; (MakeStruct; (idx (Ref __iruid_480)); (rows; (ToArray; (StreamRange 1 False (I32 0) (I32 10) (I32 1))))); (StreamMap __iruid_482; (ToStream True (GetField rows (Ref __iruid_481))); (InsertFields; (Ref __iruid_481); (""idx"" ""rows"" ""garbage""); (rows (Ref __iruid_482)); (garbage; (ToArray; (StreamRange 2 False; (I32 0); (I32 1073741824); (I32 1)))))))); ```. Notice, in particular, that the `StreamMap` inside the `StreamFlatMap` uses memory management because",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13619#issuecomment-1720022103:197,pipeline,pipeline,197,https://hail.is,https://github.com/hail-is/hail/pull/13619#issuecomment-1720022103,3,['pipeline'],['pipeline']
Deployability,"FWIW, this pipeline was performing these checks perhaps as many as 10 times per genotype, which is obviously unreasonable. Nonetheless, sending the RG along as a literal should improve the speed of these operations.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13862#issuecomment-1773407789:11,pipeline,pipeline,11,https://hail.is,https://github.com/hail-is/hail/issues/13862#issuecomment-1773407789,1,['pipeline'],['pipeline']
Deployability,"FWIW, when I added the MySQL pods I made sure to install the client config in them so you don't need the admin-pod for test namespaces, just `kssh db <NAMESPACE>` and `mysql` should work",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13150#issuecomment-1581414024:49,install,install,49,https://hail.is,https://github.com/hail-is/hail/pull/13150#issuecomment-1581414024,1,['install'],['install']
Deployability,"FYI @danking for setting up deployment in ci, last comment in particular.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4408#issuecomment-424188007:28,deploy,deployment,28,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424188007,1,['deploy'],['deployment']
Deployability,"FYI, I had to add a default/gce-deploy-config for the deploy config for tasks running on batch2 workers.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196:32,deploy,deploy-config,32,https://hail.is,https://github.com/hail-is/hail/pull/7507#issuecomment-553959196,2,['deploy'],"['deploy', 'deploy-config']"
Deployability,"FYI, I need to update developer getting started.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4301#issuecomment-420022480:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/4301#issuecomment-420022480,1,['update'],['update']
Deployability,"FYI, I was seeing some intermittent test_batch failures where the service wasn't quite up yet in spite of the deployment being available. I added a wait for service option which just hits the /healtcheck endpoint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769:110,deploy,deployment,110,https://hail.is,https://github.com/hail-is/hail/pull/5992#issuecomment-487818769,1,['deploy'],['deployment']
Deployability,"FYI, Liam made matrix tables from the files direct from EBI, which have this header line:; ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/; https://console.cloud.google.com/storage/browser/hail-datasets/raw-data/?project=broad-ctsa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4160#issuecomment-413977183:132,release,release,132,https://hail.is,https://github.com/hail-is/hail/issues/4160#issuecomment-413977183,1,['release'],['release']
Deployability,"FYI, https://github.com/hail-is/hail/pull/9786 adds another field, batch_gcp_regions. I updated our cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9777#issuecomment-738494376:88,update,updated,88,https://hail.is,https://github.com/hail-is/hail/pull/9777#issuecomment-738494376,1,['update'],['updated']
Deployability,Failing due to some mypy silliness but only in the pip installed images. Need to somehow modify this to make sure we have all appropriate stubs available and/or ignore missing stubs?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1059677772:55,install,installed,55,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1059677772,1,['install'],['installed']
Deployability,"Finally, it'd be nice if you can post timings (which I think you have) in the PR: master, vs this branch with a brief description of the timing setup (and maybe the pipeline you're using to time). Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3335#issuecomment-380555221:165,pipeline,pipeline,165,https://hail.is,https://github.com/hail-is/hail/pull/3335#issuecomment-380555221,1,['pipeline'],['pipeline']
Deployability,"Finding:. Before PruneDeadFields (after ForwardRelationalLets) we see the following Let:. <img width=""759"" alt=""Screenshot 2020-02-17 16 17 55"" src=""https://user-images.githubusercontent.com/5543229/74686967-1c069780-51a1-11ea-8acf-ef6c6d313c2f.png"">. After:; <img width=""419"" alt=""Screenshot 2020-02-17 16 14 08"" src=""https://user-images.githubusercontent.com/5543229/74686973-1f018800-51a1-11ea-8f98-f3b15a16dbfa.png"">. The MakeTuple is being modified, one field removed. If this is the correct action, then it is a bug to not update GetTupleElement to reflect the fact that the desired element came after the removed field, therefore requiring the index of that element to be shifted, or the type of the tuple to be shifted. Ah, I suppose what is happening is that this could be a problem with InferPType, since my MakeTuple inference should map the old index to the new (old: 1, new: 0), via fieldIdx. Ok, looking at this, it appears InferPType is getting the wrong MakeTuple, with 2 elements:. <img width=""998"" alt=""Screenshot 2020-02-17 16 30 31"" src=""https://user-images.githubusercontent.com/5543229/74687628-ee225280-51a2-11ea-9a9d-a3813f587d1d.png"">. If I fix this, I fix the problem. Somehow InferPType is getting the wrong IR. First thought, maybe we need a deepCopy, regardless of ir sharing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-587165563:529,update,update,529,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-587165563,1,['update'],['update']
Deployability,"FitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a problem as the py4j versions via pip and and SPARK_HOME are both 10.4, and moreover this setup worked with spark 2.0.2, but a possible confound. Perhaps change the getting started docs so the PYTHONPATH is always defined to point to the spark version of py4j?. Anyway, here are the current paths as you requested. . `echo $SPARK_HOME /share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7`. `echo $PYTHONPATH; /home/stockham/bin/python/Python-2.7.12:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python:/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip:/scratch/PI/dpwall/computeEnvi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:1571,install,install,1571,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721,1,['install'],['install']
Deployability,Fix has been deployed and I confirmed it resolves the issue in a freshly created cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505#issuecomment-468807129:13,deploy,deployed,13,https://hail.is,https://github.com/hail-is/hail/issues/5505#issuecomment-468807129,1,['deploy'],['deployed']
Deployability,Fix is to update to 2.29.1.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13937#issuecomment-1821482163:10,update,update,10,https://hail.is,https://github.com/hail-is/hail/issues/13937#issuecomment-1821482163,1,['update'],['update']
Deployability,Fixed and test updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12990#issuecomment-1540350149:15,update,updated,15,https://hail.is,https://github.com/hail-is/hail/pull/12990#issuecomment-1540350149,1,['update'],['updated']
Deployability,"Fixed by #10781, website will update when we release hail 0.2.75",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10676#issuecomment-899588096:30,update,update,30,https://hail.is,https://github.com/hail-is/hail/issues/10676#issuecomment-899588096,2,"['release', 'update']","['release', 'update']"
Deployability,Fixed in tutorial update. PR 1298,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1237#issuecomment-275417860:18,update,update,18,https://hail.is,https://github.com/hail-is/hail/issues/1237#issuecomment-275417860,1,['update'],['update']
Deployability,"Fixed, thanks for the push, that was easy to do and is so much better than the old thing. I can't delete the powerpoints from their old location on github yet since the current website still links to them, but after 0.2.32 release I'll delete the old ones.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7950#issuecomment-577856190:223,release,release,223,https://hail.is,https://github.com/hail-is/hail/pull/7950#issuecomment-577856190,1,['release'],['release']
Deployability,Fixing with a change to dataproc image version requires a cloud tools upgrade form all our users. Perhaps we should start a new latest-hash file and push a cloud tools update that looks there and uses `1.2-deb9`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405:70,upgrade,upgrade,70,https://hail.is,https://github.com/hail-is/hail/pull/4239#issuecomment-417436405,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,"For future upgraders, there's something about pandas 1.1.5 that causes `pylint` to fail. It seems like underlying ast processing library `astroid` enters infinite recursive loop when pandas 1.1.5 is installed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9804#issuecomment-741921520:11,upgrade,upgraders,11,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-741921520,2,"['install', 'upgrade']","['installed', 'upgraders']"
Deployability,"For now, downgrade:; ```; pip3 install 'ipython<8.17'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300:31,install,install,31,https://hail.is,https://github.com/hail-is/hail/issues/14166#issuecomment-1896447300,1,['install'],['install']
Deployability,For some reason this is timing out after a minute in my dev deploy even though I've removed the heartbeat and it's working on a local server I have running. Need to investigate further.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047:60,deploy,deploy,60,https://hail.is,https://github.com/hail-is/hail/pull/9636#issuecomment-715604047,1,['deploy'],['deploy']
Deployability,"For testing, we start a server with `python ci/ci.py`. That doesn't appear to work. Do we need to pip install it instead?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4709#issuecomment-435169908:102,install,install,102,https://hail.is,https://github.com/hail-is/hail/pull/4709#issuecomment-435169908,1,['install'],['install']
Deployability,For that reason I'm somewhat skeptical and given the future of our scala code feel like we shouldn't spend much effort on non-trivial updates,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12226#issuecomment-1259454277:134,update,updates,134,https://hail.is,https://github.com/hail-is/hail/pull/12226#issuecomment-1259454277,1,['update'],['updates']
Deployability,"For the partitioning algorithm, I updated the test to confirm that all the individuals in the unrelated set are mutually unrelated. For PC-AiR, I updated the test to compare the loadings to PCA on just the unrelated individuals. The loadings are NumPy close. The scores are slightly different though because they are calculated differently. When there are related individuals, the scores are calculated by multiplying the loadings and the standardized genotypes. When there are no related individuals, the scores are calculated by multiplying the columns of the appropriate singular matrix with the eigenvalues. So for the scores, I just add a regression test. (In my testing, I observed that most of the scores were less than 1% different. However, there were a few differences that were larger around 20% or 30%. Not sure if this is a cause for concern because the calculation approaches are different and the SVD is approximate.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1977943502:34,update,updated,34,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1977943502,2,['update'],['updated']
Deployability,Force merging due to broken batch. Hand deploying CI now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8424#issuecomment-607411084:40,deploy,deploying,40,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607411084,1,['deploy'],['deploying']
Deployability,"From a fresh clone, the above (modified with `rm -f`) fails with: ; ```bash; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; /home/edmund/.local/src/hail/.venv/bin/python3: Error while finding module specification for 'hailtop.aiotools.copy' (ModuleNotFoundError: No module named 'hailtop'); make: *** [Makefile:355: upload-remote-test-resources] Error 1; make: Leaving directory '/home/edmund/.local/src/hail/hail'; ```. I'll try again with `hailtop` installed - just wanted to point out the dependency failure in `Makefile`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777:1100,install,installed,1100,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887719777,1,['install'],['installed']
Deployability,"From that VM, I can get into the container, install the jdk, then run jstack on one of the hung JVMs.; ```; # jstack 1433; ...; ""pool-1-thread-1"" #18 prio=5 os_prio=0 tid=0x00007f50c4f23000 nid=0x82c waiting on condition [0x00007f5084eeb000]; java.lang.Thread.State: WAITING (parking); 	at sun.misc.Unsafe.park(Native Method); 	- parking to wait for <0x00000000e8ddaea0> (a scala.concurrent.impl.Promise$CompletionLatch); 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); 	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); 	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242); 	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258); 	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263); 	at scala.concurrent.Await$.$anonfun$result$1(package.scala:220); 	at scala.concurrent.Await$$$Lambda$2201/1092639564.apply(Unknown Source); 	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:57); 	at scala.concurrent.Await$.result(package.scala:146); 	at is.hail.backend.service.ServiceBackend.parallelizeAndComputeWithIndex(ServiceBackend.scala:145); ...; ```. This is the line that waits to upload the compiled code for the workers to Google Cloud Storage. The other threads appear to be waiting on the memory service:; ```; ""pool-2-thread-2"" #27 prio=5 os_prio=0 tid=0x00007f5028ad9000 nid=0x88d waiting on condition [0x00007f50274fc000]; java.lang.Thread.State: TIMED_WAITING (sleeping); 	at java.lang.Thread.sleep(Native Method); 	at is.hail.services.package$.sleepAndBackoff(package.scala:32); 	at is.hail.services.package$.retryTransientErrors(package.scala:86); 	at is.hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903:44,install,install,44,https://hail.is,https://github.com/hail-is/hail/pull/11471#issuecomment-1059453903,1,['install'],['install']
Deployability,Fully deploy is now working again.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-432499924:6,deploy,deploy,6,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-432499924,1,['deploy'],['deploy']
Deployability,"Further pruned. Removed all GraphQL libraries, besides graphql-tag, which I like, because 1) simple hash-based cache: no need to walk complex graph to normalize cache, because in most cases I'm perfectly fine with not re-using cache across different queries (that may have some shared fields). Apollo does something ""smarter"", but much slower: walks a query, checks that the requested fields for a node are the same, and that the node's id is the same, as some other query. 2) no runtime validation of query shape via graphql-tag...uses simple template strings, which are free. We don't care about schema validation in the client...because the server will error when schema is invalid. This should be compile time validated instead, in this case via integration tests. Also removed react-icons... I was going to use this in place of material-design-icons, because I thought loading the full font, when I needed only a few icons, would be unnecessarily expensive. It turns out that I cannot find a library where a single icon import (react-icons or MaterialUI) is smaller than Google's entire material design font: a single font (there are several needed to cover all icons) is ~500B. A single react-icons icon is ~2KB on dev (production may be smaller due to tree shaking). Also, am opposed to CSS-in-JS: slower, worse tooling, larger. Benefits are dynamic selectors, which are really no advantage that I can see (without them can still dynamically apply classes, as in the yee ol days of pleb vanilla js). Home page down to <2kb when not logged in, and 3.1KB logged in. This includes header, simple body, and dark mode button.; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 49 59 pm"" src=""https://user-images.githubusercontent.com/5543229/50264482-ed4c3000-03e8-11e9-80d1-81d195a7b37a.png"">; <img width=""2636"" alt=""screen shot 2018-12-19 at 11 50 33 pm"" src=""https://user-images.githubusercontent.com/5543229/50264483-ed4c3000-03e8-11e9-8180-1409ca16573f.png"">. edit: Further .1KB shaved (gzipp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665:750,integrat,integration,750,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-448868665,1,['integrat'],['integration']
Deployability,"GCR is disabled. I can delete the artifacts whenever we're confident they are unneeded. I'll probably delete this week. Dataproc buckets exist, but we're waiting on https://github.com/hail-is/hail/pull/14270 to merge before we delete the multi-regional buckets. US VEP and Datasets API are copied and ready for use. Europe is in process. UK is being moved in order to rename it from -uk- to -europe-west2- to comply with the new regional naming scheme. Once everything is transitioned, we need to merge https://github.com/hail-is/hail/pull/14286 and release. Then loudly inform everyone of the loss of these buckets. Then we delete them before March 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13507#issuecomment-1939789668:550,release,release,550,https://hail.is,https://github.com/hail-is/hail/issues/13507#issuecomment-1939789668,1,['release'],['release']
Deployability,GE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10950,release,release,10950,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,GVS team confirms their pipeline containing interval literals went from >50 GB (crashing at that point) to less than 11GB! 👏,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13748#issuecomment-1791063434:24,pipeline,pipeline,24,https://hail.is,https://github.com/hail-is/hail/issues/13748#issuecomment-1791063434,3,['pipeline'],['pipeline']
Deployability,"Gah, OK, I think I have it now, but there was one more detail:. The gradle configuration `testCompileOnly` [1] *does not* inherit from the `shadow` configuration (as evidence see [this search](https://github.com/search?q=repo%3Ajohnrengelman%2Fshadow%20extendsFrom&type=code) of the shadow repo). We must explicitly request that `shadow` dependencies are included in the compile-time class path of the tests. This is as it should be: the things in `shadow` are things which are provided to us by our runtime environment. That's true of both the *test* runtime environment and the normal runtime environment. The Gradle Shadow plugin takes a different perspective by default, it suggests that `shadow` dependencies shouldn't be used in the tests at all. [1] NB: `testCompile` does not exist but you don't get an error if you try to use it, thanks for nothing gradle.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563:75,configurat,configuration,75,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1710414563,2,['configurat'],['configuration']
Deployability,"Gah, caching. The changes haven't actually been deployed by ci yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5282#issuecomment-463364346:48,deploy,deployed,48,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463364346,1,['deploy'],['deployed']
Deployability,Going to update the elasticsearch version and make a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9686#issuecomment-724956104:9,update,update,9,https://hail.is,https://github.com/hail-is/hail/pull/9686#issuecomment-724956104,1,['update'],['update']
Deployability,"Good catch, updated!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14619#issuecomment-2260855350:12,update,updated,12,https://hail.is,https://github.com/hail-is/hail/pull/14619#issuecomment-2260855350,1,['update'],['updated']
Deployability,"Good idea, I'll check. I feel like I initially found this in deep in a redhat tutorial, but ultimately found it again at the bottom of the [man page](https://man7.org/linux/man-pages/man8/xfs_quota.8.html). I was following this example:; ```; Enabling project quota on an XFS filesystem (restrict files in; log file directories to only using 1 gigabyte of space). # mount -o prjquota /dev/xvm/var /var; # echo 42:/var/log >> /etc/projects; # echo logfiles:42 >> /etc/projid; # xfs_quota -x -c 'project -s logfiles' /var; # xfs_quota -x -c 'limit -p bhard=1g logfiles' /var. Same as above without a need for configuration files. # rm -f /etc/projects /etc/projid; # mount -o prjquota /dev/xvm/var /var; # xfs_quota -x -c 'project -s -p /var/log 42' /var; # xfs_quota -x -c 'limit -p bhard=1g 42' /var; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396:607,configurat,configuration,607,https://hail.is,https://github.com/hail-is/hail/pull/10467#issuecomment-834771396,1,['configurat'],['configuration']
Deployability,Good point. I'm going to first make sure that it would have failed on a sparse matrix with the `.get` and then show that the same test (hopefully!) passes for the updated PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5500#issuecomment-468696410:163,update,updated,163,https://hail.is,https://github.com/hail-is/hail/issues/5500#issuecomment-468696410,1,['update'],['updated']
Deployability,Got blocked on dev deploy not working (500). Will try again in the morning.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539818744:19,deploy,deploy,19,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539818744,1,['deploy'],['deploy']
Deployability,"Got it. In spite of my claim ""I was trying to avoid bulk operations"" I see that close_batch scans over all ready jobs (a bulk operation) to update ready_cores. I'm not quite sure what to do here. I'm not actually sure if a long-running query on close_batch is going to cause problems and it is not trivial to make it incremental.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526:140,update,update,140,https://hail.is,https://github.com/hail-is/hail/pull/7714#issuecomment-565101526,1,['update'],['update']
Deployability,"Got it; for my own future reference I'm going to link to the [Cloud Dataproc Image version list](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-versions) and note that the 1.5 series, currently in [preview](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-1.5), does use Scala 2.12 as of December 2019 according to the [release notes](https://cloud.google.com/dataproc/docs/release-notes). Also, there's an [issue](https://issuetracker.google.com/issues/132603281) on Google Cloud's insanely janky issue tracker for the Scala version upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854:296,release,release-,296,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580692854,4,"['release', 'upgrade']","['release', 'release-', 'release-notes', 'upgrade']"
Deployability,Got rid of the cleanup on deploy. Still fixed the Makefile.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5955#issuecomment-486799272:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/pull/5955#issuecomment-486799272,1,['deploy'],['deploy']
Deployability,Great change. Can you:; - delete chi1; - replace uses of chi1 with your more general version in the few places it appears in the code; - update the docs in HailExpressionLanguage.md to reflect only your version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1393#issuecomment-280140767:137,update,update,137,https://hail.is,https://github.com/hail-is/hail/pull/1393#issuecomment-280140767,1,['update'],['update']
Deployability,"Great question. The need for two clients is we have an asynchronous one that ci uses and a synchronous one that pipeline uses. The client code in `aioclient.py` is the asynchronous one and the code in `client.py` is for the synchronous one. Rather than duplicating the code as was done before, I either had to make the synchronous client use the asynchronous code or vice versa. It was easier to make the asynchronous code synchronous by using the `run_until_complete` function and wrapping calls to the asynchronous classes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673:112,pipeline,pipeline,112,https://hail.is,https://github.com/hail-is/hail/pull/6208#issuecomment-497540673,1,['pipeline'],['pipeline']
Deployability,"Great! I think we are good on the client. I'll modify it later today to use the new scheme. Before we get into the nitty gritty of the actual implementation, can we move on to the UI components and the semantics of using the client in `test_batch.py`? There's also a change to how the batch fields `time_closed` and `time_created` are used. I also added `time_updated`. I think the new semantics are:. - time_created -- time the batch was created; - time_updated -- time the last update was committed; - time_completed -- time the last time n_completed == n_jobs regardless of whether there are outstanding updates that haven't been committed. For old batches:; - time_created => same; - time_closed => time_updated; - time_completed => same. I also changed what the batch state means:; > There are only two batch states in the database: running and complete. A batch starts out as complete until an update is committed at which point if the n_jobs > 0, it will change to running. There are no longer ""open"" batches. . It's possible I didn't actually implement exactly what I described above as I was having a hard time figuring out whether time_updated should be equal to time_completed if the batch has no outstanding jobs to run. That's why I'd like to take a step back and make sure we agree on the interface.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1218125677:480,update,update,480,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1218125677,3,['update'],"['update', 'updates']"
Deployability,Great! I think we should run the integration tests as part of `testAll` and kill the integration tests in the CI.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1827#issuecomment-302116530:33,integrat,integration,33,https://hail.is,https://github.com/hail-is/hail/pull/1827#issuecomment-302116530,2,['integrat'],['integration']
Deployability,"Great! No worries!. On Tue, 21 Apr 2020, 19:12 Patrick Schultz, <notifications@github.com>; wrote:. > @astheeggeggs <https://github.com/astheeggeggs> Thanks for the bug; > report. It lead to finding a rather serious bug. See; > https://discuss.hail.is/t/possible-incorrect-linreg-aggregator-results-in-0-2-29-0-2-37/1375; > for more details on what other regressions could have been affected. A new; > release should go out today with the fix.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/issues/8349#issuecomment-617327917>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABVQA76SRJ64CRAO36BG2GLRNXO2FANCNFSM4LS2ZGUA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8349#issuecomment-617334301:402,release,release,402,https://hail.is,https://github.com/hail-is/hail/issues/8349#issuecomment-617334301,1,['release'],['release']
Deployability,"Great! So here's what the docs look like now:; https://hail.is/docs/devel/methods/genetics.html#hail.methods.nirvana. Here's the Python source:; https://github.com/hail-is/hail/blob/master/python/hail/methods/qc.py. You can see the built docs of this PR by clicking on Details next to the passing 2.2.0 test, and then clicking on Docs, e.g.:; https://ci.hail.is/viewLog.html?buildId=63354&buildTypeId=HailSourceCode_PRsOnly_HailTestJarSpark220&tab=report_project8_Docs. I'd appreciate if you could:; - ensure the docs are still accurate and add information on what version(s) of Nirvana is compatible.; - update the schema in the documentation to match your changes in Scala; - try running the same pipeline with a few block sizes to see whether its reasonable to reduce the default block size so that users will get more parallelism by default. I suspect a user with a 1 million variant VCF would prefer running 100 cores with 10k variants each to 2 cores with 500k variants each. I'd be surprised if the per-block overhead is so high to outweigh the benefit.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-379138339:605,update,update,605,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-379138339,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,Great. Just address the localSize == 0 thing (fix or tell me why I'm wrong) and update to master and I'll merge.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/326#issuecomment-213803970:80,update,update,80,https://hail.is,https://github.com/hail-is/hail/pull/326#issuecomment-213803970,1,['update'],['update']
Deployability,"Great. So what I'm also interested in comparing is, if I just need, say, hail/pipeline/test, what's the download full tar and extract (of just hail/pipeline/test) vs download just hail/pipeline/test tar with full extract?. > There's something to be said for tar'ing everything except for .git, but I didn't carefully check which steps need it and which steps do not. I would have hoped no downstream steps need .git, but some build steps do trivially (e.g. look at the hash). Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090:78,pipeline,pipeline,78,https://hail.is,https://github.com/hail-is/hail/pull/7626#issuecomment-560458090,3,['pipeline'],['pipeline']
Deployability,"Ha, couldn't have come at a better time -- this is the profile trace from a split/densify/sampleqc pipeline:. ![image](https://user-images.githubusercontent.com/10562794/126006170-ed653c9c-ec36-4d29-95b9-732be6313cca.png)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10672#issuecomment-881705887:99,pipeline,pipeline,99,https://hail.is,https://github.com/hail-is/hail/pull/10672#issuecomment-881705887,1,['pipeline'],['pipeline']
Deployability,"Had a quick chat with Konrad and since the other VEP bug is unrelated to these changes, it would be great if we could get this into master (if you're happy with these changes obviously) as at the moment we have to use different jars for different parts of the pipeline.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1531#issuecomment-286852400:260,pipeline,pipeline,260,https://hail.is,https://github.com/hail-is/hail/pull/1531#issuecomment-286852400,1,['pipeline'],['pipeline']
Deployability,"Haha, I had intentionally not assigned you until I sorted what I anticipate is a long tail of issues. Nonetheless, updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12908#issuecomment-1515268289:115,update,updated,115,https://hail.is,https://github.com/hail-is/hail/pull/12908#issuecomment-1515268289,1,['update'],['updated']
Deployability,"Hail 0.1 isn't tested against or believed to work with Spark 2.2. Can you update to Hail 0.2 (devel)? 0.1 will be fully deprecated when 0.2 is released, and is already in its end of life process.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3946#issuecomment-405779642:74,update,update,74,https://hail.is,https://github.com/hail-is/hail/issues/3946#issuecomment-405779642,2,"['release', 'update']","['released', 'update']"
Deployability,"Hail depends on the `decorator` module, and in this case it looks like you've got it but it's out of date. The following should fix it:. ```bash; pip install -U decorator; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968:150,install,install,150,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302064968,1,['install'],['install']
Deployability,"Hail doesn't have a conda package -- the bioconda package there was not uploaded by the Hail Team (could even be malware -- we don't know). It's certainly a very old version. If you install Hail with pip, you should pick up the latest version 0.2.100 and have access to hl.vds, which is somewhat recent functionality.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111:182,install,install,182,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262357111,1,['install'],['install']
Deployability,"Hail has a set of commands that can be strung together by a user on the command line to create an analysis pipeline. We have a few users with development backgrounds who have started to build their own commands. It would be great if they could just throw those in their CLASSPATH and then run them directly from the command line by name. We'd have to pick a shell-friendly syntax, so something like, but not:. `$ hail importvcf ... $com.company.CustomLifeSavingAnalysis ...`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/353#issuecomment-240550514:107,pipeline,pipeline,107,https://hail.is,https://github.com/hail-is/hail/issues/353#issuecomment-240550514,1,['pipeline'],['pipeline']
Deployability,Hail is deployed to the Python package index: https://pypi.org/project/hail/. Adding a conda recipe isn't a high-priority task right now. What are the reasons that a PyPI package is insufficient?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8352#issuecomment-603528845:8,deploy,deployed,8,https://hail.is,https://github.com/hail-is/hail/issues/8352#issuecomment-603528845,1,['deploy'],['deployed']
Deployability,"Hail is no longer distributed as a zip file. You should use `make install-on-cluster` (described further [here](https://hail.is/docs/0.2/install/other-cluster.html)) to install Hail on a cluster. I do not recommend creating a wheel or zip file because the native binaries included in that wheel or zip file might be incompatible with the computer on which you install that file. If you are absolutely certain that the build machine *and the executing machine* can share native binaries, then you can do this:; ```; make wheel HAIL_COMPILE_NATIVES=1; ```; This produces a wheel file at `build/deploy/dist/hail-0.2.XX-py3-none-any.whl` where XX is the current patch version of Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914353777:66,install,install-on-cluster,66,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914353777,6,"['deploy', 'install', 'patch']","['deploy', 'install', 'install-on-cluster', 'patch']"
Deployability,Hail requires Java 8. Please install Java 8: https://hail.is/docs/0.2/getting_started.html#requirements,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6747#issuecomment-515511257:29,install,install,29,https://hail.is,https://github.com/hail-is/hail/issues/6747#issuecomment-515511257,1,['install'],['install']
Deployability,"Hail tries to do a lot of data integrity checks and warn the user about problems. We've found a number of bugs in upstream tools and workflows that were arguably incorrect. But generating warnings when importing a 2TB file is a challenge in Spark. Right now we use Spark's Accumulators to accumulate classes of error messages and write them out at the end of the pipeline run (see the VCFReport object). However, we use them in non-actions and get incorrect reports (due to job restarts or reused stages in the pipeline). I have an idea about how to fix this by accumulating only at the end of a successful mapPartitions operation and recording the stageId and taskAttemptId from the TaskContext. The accumulator should only accumulate one of the reports from a successful mapPartitions. Using this, I wanted to build an abstraction for reporting warnings and other messages reliably on large import steps. If this works, we plan to float it up to the Spark mailing list to see if it can be of use, or at least write a nice blog post explaining how to get reliable accumulators in Spark. See the discussion here for the current situation:. http://stackoverflow.com/questions/29494452/when-are-accumulators-truly-reliable. Closed as won't fix:. https://issues.apache.org/jira/browse/SPARK-732. Of course, I might be missing something obvious and this won't work.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/371#issuecomment-240550289:363,pipeline,pipeline,363,https://hail.is,https://github.com/hail-is/hail/issues/371#issuecomment-240550289,2,['pipeline'],['pipeline']
Deployability,Hand deploy succeeded. CI appears OK now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606704529:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606704529,1,['deploy'],['deploy']
Deployability,Hand deploy successful. Monitoring logs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/8424#issuecomment-607417090,1,['deploy'],['deploy']
Deployability,Has `_prev_nonnull` in python been updated to use the new `Aggregator2`?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5414#issuecomment-466549827:35,update,updated,35,https://hail.is,https://github.com/hail-is/hail/pull/5414#issuecomment-466549827,1,['update'],['updated']
Deployability,"Haven't been able to have pipeline benchmarks finish, but from the looks of things this change does not make things significantly slower or faster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7060#issuecomment-535609382:26,pipeline,pipeline,26,https://hail.is,https://github.com/hail-is/hail/pull/7060#issuecomment-535609382,1,['pipeline'],['pipeline']
Deployability,"Haven't figured it out yet, but reproduced the error with a simpler pipeline that just uses one annotate instead of `sample_qc`:. ```; P = 1; S = 1000; V = 50000; for N in range(350, 400, 1):; try:; mt = hail.balding_nichols_model(P, S, V, N); mt = mt.annotate_cols(n_called = hl.agg.filter(hl.is_defined(mt.GT), hl.agg.count())); mt = mt.filter_cols(mt.n_called > 0); print(""\n[PASS] with"", N, ""partitions:"", mt.count()); except Exception as e:; print(""\n[FAIL] with "", N, ""partitions""); raise e; break; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734:68,pipeline,pipeline,68,https://hail.is,https://github.com/hail-is/hail/issues/8944#issuecomment-652065734,1,['pipeline'],['pipeline']
Deployability,"Heh, so turns out that `test_weird_urls` is missing the `@pytest.mark.asyncio` decorator, and so it was getting skipped with a warning this whole time. The pytest upgrade added auto-detection of async tests and so it ran this broken test for the first time. I'm PR'ing to treat most warnings as errors in #12322.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498:163,upgrade,upgrade,163,https://hail.is,https://github.com/hail-is/hail/pull/11974#issuecomment-1278201498,1,['upgrade'],['upgrade']
Deployability,"Hello developers,; Sorry for resurrecting the issue. I have the same problem with a different Error. ```; conda create -n hail2; conda activate hail2; conda install -c bioconda hail; pip install gnomad; ```. Installation has no issues. But below command throws error. This is the same on two different machines that I have tried so far. Removing conda env, or changing env location, has not helped me so far. ```; import hail; from gnomad.sample_qc.ancestry import assign_population_pcs. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/sample_qc/ancestry.py"", line 9, in <module>; from gnomad.utils.filtering import filter_to_autosomes; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/utils/filtering.py"", line 9, in <module>; from gnomad.resources.resource_utils import DataException; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/__init__.py"", line 3, in <module>; from .resource_utils import *; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 162, in <module>; class VariantDatasetResource(BaseResource):; File ""/datadir1/conda_envs_AMT/hail2/lib/python3.7/site-packages/gnomad/resources/resource_utils.py"", line 173, in VariantDatasetResource; def vds(self, force_import: bool = False) -> hl.vds.VariantDataset:; AttributeError: module 'hail' has no attribute 'vds'; ```. Could anyone please recommend me how to circumvent this?; Thanks for the amazing package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275:157,install,install,157,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262353275,3,"['Install', 'install']","['Installation', 'install']"
Deployability,"Hello, I am having similar problems. I installed using conda according to https://hail.is/docs/0.2/getting_started.html#requirements ; I created the environment, activated it and installed with pip. When I try to load a vcf file, I am getting:; ` hl.import_vcf('/Volumes/Macintosh HD2/data/thousands_genome/hector.Q15d5.vcf.gz')`; `py4j.protocol.Py4JJavaError: An error occurred while calling z:is.hail.HailContext.apply.; : is.hail.utils.HailException: Hail requires Java 8, found 12.0.1`; Any help? Best, Zillur",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374:39,install,installed,39,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-515502374,2,['install'],['installed']
Deployability,"Here is a Hail log.... I will work on getting the YARN container logs next. . more /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; ```; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:513,configurat,configuration,513,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['configurat'],['configuration']
Deployability,Here is a straight-line pipeline that replicates the high memory use. In my experience this can get up to 100GiB of RAM use. https://gist.github.com/danking/3432deabd997ce08515b2088e202a039. The VDS file is privileged. Next steps:. - [ ] replicate on a public VDS like the HGDP/1KG VDS.; - [ ] delete as much code as possible from this file to reduce the possible causes.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13606#issuecomment-1717799683:24,pipeline,pipeline,24,https://hail.is,https://github.com/hail-is/hail/issues/13606#issuecomment-1717799683,1,['pipeline'],['pipeline']
Deployability,"Here's a link with an absolute time window: https://cloudlogging.app.goo.gl/gXAWZpZtUiV8jphXA. This is the assertion's stack trace:; ```; at scala.Predef$.assert(Predef.scala:208); at is.hail.QoBOutputStreamManager.createOutputStream(QoBAppender.scala:38); at org.apache.logging.log4j.core.appender.OutputStreamManager.getOutputStream(OutputStreamManager.java:165); at org.apache.logging.log4j.core.appender.OutputStreamManager.writeToDestination(OutputStreamManager.java:250); at org.apache.logging.log4j.core.appender.OutputStreamManager.flushBuffer(OutputStreamManager.java:283); at org.apache.logging.log4j.core.appender.OutputStreamManager.flush(OutputStreamManager.java:294); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:217); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208); at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199); at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161); ```. And the line of our code that triggers the logger appender:; ```; is.hail.JVMEntryway$2.run(JVMEntryway.java:139); ```. On that line, we should have already evaluated line 97:; ```; QoBOutputStreamManager.changeFileInAllAppenders(logFile);; ```; Which updates the filename for all `QoBOutputStreamManager`s. We should be the only ones allocating `QoBOutputStreamManager` (it has no magic annotations, we don't pass its constructor anywhere). We should only allocate `QoBOutputStreamManager` in its associated object. We always put it into the map in `getInstance`. We don't synchronize the other methods though, so that could be the issue? If we have a stale version of that map?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030:1406,update,updates,1406,https://hail.is,https://github.com/hail-is/hail/issues/13242#issuecomment-1703383030,1,['update'],['updates']
Deployability,"Here's an update to the action table. | Part | Description | Action |; | --- | --- | --- |; | GT | the hard call | _minning_ or _subsetting_ |; | AD | allele depth | the filtered allele's column is eliminated, e.g. filtering allele 1 transforms `[25,5,20]` to `[25,20]` |; | DP | number of informative reads | no change |; | PL | Phred-likelihoods for each allele pair | _minning_ or _subsetting_ |; | GQ | genotype quality | increasing-sort PL and take `PL[1] - PL[0]` |. We choose either _minning_ or _subsetting_ consistently for all parts. I now feel:; - when _minning_ (i.e. _believe real_) we should move AD value to reference.; - when _subsetting_ (i.e. _believe not-real_) we should subtract removed depth from DP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240825234:10,update,update,10,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240825234,1,['update'],['update']
Deployability,"Here's the deadlock that I'm observing now on this branch. I don't have a great understanding of what's happening. I believe Transaction 1 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L1186-L1188) in MJC, and Transaction 2 to be [here](https://github.com/hail-is/hail/blob/40d8882470af71f2d08dd1aa6b723357ca8a1245/batch/sql/estimated-current.sql#L449-L453) in the jobs_after_update trigger. Looking at the second transaction in context now, it looks like that is probably another MJC transaction toward the end of its run after it updated the jobs table. I think it would make sense then that T2 would still hold the lock for `instances_free_cores_mcpu` but I'm not sure where the contention for `batch_inst_coll_cancellable_resources` is coming from, as I don't see how T1 could be holding any form of lock on it. Either way it seems like how we use these tables is similarly a mess. ```; *** (1) TRANSACTION:; TRANSACTION 644409381, ACTIVE 0 sec starting index read; mysql tables in use 1, locked 1; LOCK WAIT 39 lock struct(s), heap size 3520, 50 row lock(s), undo log entries 28; MySQL thread id 1941960, OS thread handle 140297909716736, query id 1869168359 10.32.3.8 dgoldste updating; UPDATE instances_free_cores_mcpu; SET free_cores_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTI",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:609,update,updated,609,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,1,['update'],['updated']
Deployability,"Here's the diff when I replace the current `CreateDatabaseStep` with the new `CreateDatabase2Step`. ```diff; diff --git a/ci/ci/build.py b/ci/ci/build.py; index cbc186ef2d..7ea399713b 100644; --- a/ci/ci/build.py; +++ b/ci/ci/build.py; @@ -240,8 +240,10 @@ class Step(abc.ABC):; return CreateNamespaceStep.from_json(params); if kind == 'deploy':; return DeployStep.from_json(params); - if kind in ('createDatabase', 'createDatabase2'):; + if kind == 'createDatabase':; return CreateDatabaseStep.from_json(params); + if kind == 'createDatabase2':; + return CreateDatabase2Step.from_json(params); raise BuildConfigurationError(f'unknown build step kind: {kind}'); ; def __eq__(self, other):; @@ -967,7 +969,7 @@ date; ); ; ; -class CreateDatabaseStep(Step):; +class CreateDatabase2Step(Step):; def __init__(self, params, database_name, namespace, migrations, shutdowns, inputs, image):; super().__init__(params); ; @@ -989,12 +991,7 @@ class CreateDatabaseStep(Step):; self.create_database_job = None; self.cleanup_job = None; ; - if params.scope == 'dev':; - self.database_server_config_namespace = params.code.namespace; - else:; - self.database_server_config_namespace = DEFAULT_NAMESPACE; -; - self.cant_create_database = is_test_deployment or params.scope == 'dev'; + self.cant_create_database = is_test_deployment; ; # MySQL user name can be up to 16 characters long before MySQL 5.7.8 (32 after); if self.cant_create_database:; @@ -1005,6 +1002,11 @@ class CreateDatabaseStep(Step):; self._name = database_name; self.admin_username = f'{database_name}-admin'; self.user_username = f'{database_name}-user'; + elif params.scope == 'dev':; + dev_username = params.code.config()['user']; + self._name = f'{dev_username}-{database_name}'; + self.admin_username = f'{dev_username}-{database_name}-admin'; + self.user_username = f'{dev_username}-{database_name}-user'; else:; assert params.scope == 'test'; self._name = f'{params.code.short_str()}-{database_name}-{self.token}'; @@ -1030,7 +1032,7 @@ cl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:337,deploy,deploy,337,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,2,"['Deploy', 'deploy']","['DeployStep', 'deploy']"
Deployability,"Here's the terraform configurations in GCP and Azure:. - GCP: Batch has admin storage permissions, as granted here https://github.com/hail-is/hail/blob/1f5e1540c04abfde58ead1084841fec5aa6e0ed3/infra/gcp/main.tf#L415-L424. We also grant it a Viewer role on the query bucket after that which seems redundant. We should really not grant it global storage admin and instead give it admin for just the query bucket and other associated batch buckets. I checked in hail-vdc and batch does not have the global storage admin role, and it has the Viewer role on the query bucket. I've changed that role now to admin on the query bucket. - Azure: Story is simpler. The `query` storage container is part of the `batch` storage account. The batch SP has ownership over the `batch` storage account and by extension all of the containers inside it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011:21,configurat,configurations,21,https://hail.is,https://github.com/hail-is/hail/pull/11870#issuecomment-1138806011,1,['configurat'],['configurations']
Deployability,"Here's what _local_ looks like now. Note that I've already converted to a `vds` this time. ```; dking@wmb16-359 # rm -rf foo && time ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' ; hail: info: running: read -i profile.vds; [Stage 1:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o foo; [Stage 8:=====================================================> (210 + 4) / 214]hail: info: timing:; read: 3.047s; ibd: 4m35.1s; ../hail/build/install/hail/bin/hail read -i profile.vds ibd -o 'foo' 924.50s user 16.11s system 333% cpu 4:42.04 total; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/738#issuecomment-249995538:147,install,install,147,https://hail.is,https://github.com/hail-is/hail/pull/738#issuecomment-249995538,2,['install'],['install']
Deployability,"Hey @JKosmicki,. Your branch has diverged from master a fair bit at this point. I can get this PR moving again if you do two simple things for me:; - rebase your commit on hail-is's master; - apply a patch I created, which fixes some compile errors. If you don't already have a remote (you can list remotes with `git remote -v`) for `hail-is/hail`, let's create one:. ``` bash; git remote add hi https://github.com/hail-is/hail.git; ```. I'll refer to this remote as `hi` from now on. If you already had a remote for `hail-is/hail` then substitute its name below for `hi`. First, we rebase to get the latest code from `hail-is/hail`'s `master` branch. ``` bash; git fetch hi; git rebase hi/master tdt; ```. And now we download [this `.patch` file](https://github.com/danking/hail/commit/6ea3d77684596abf171920e014c2aedd2a209f9c.patch) and apply it to the `tdt` branch:. ``` bash; git am the/path/to/that/file/you/downloaded.patch; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-248645143:200,patch,patch,200,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-248645143,4,['patch'],['patch']
Deployability,"Hey @anh151 !. I'm sorry you're having trouble with Hail. The message ""Container killed on request. Exit code is 137"" comes from Apache Spark, our underlying distributed compute framework. It indicates that the worker machines have insufficient RAM. In general, using worker machines with higher RAM-to-core ratios will help. If you're on GCP, try the n1-highmem family. Some other suggestions:. 1. Hail is a ""lazy"" system. Your entire pipeline is executed, from the beginning, when you run ""export"" or ""write"". That means that Hail has to do all of that work at once. You can ease the memory pressure by performing less operations at once, by writing an intermediate file (and reading back in and proceeding with it).; 2. We recommend against directly exporting from a complex operation (like group-by-aggregate). Instead, grab the cols table and write it to Hail's fast, binary, parallel format: `.cols().select('field_of_interest').write('my-cols.ht')`. Then read that table and export that: `hl.read_table('my-cols.ht').field_of_interest.export(...)`. Exporting to a text file requires more memory because we have to construct ASCII strings.; 3. Always use a compressed export: `.export('foo.tsv.bgz')` or `hl.export_vcf(..., 'foo.vcf.bgz')`. This won't help your memory problem, but you can avoid parsing strings to create loci by constructing an `hl.Locus` which is the Python-side representation of loci (`hl.locus` is the inside-Hail representation):; ```python3; def create_intervals(data):; return [; hl.Locus(chromosome, start, reference_genome=""GRCh38""); for i, (chromosome, start) in data[[""CHROM"", ""POS""]].iterrows(); ]; ```. Please reply here if you're still having problems after incorporating the above suggestions as it may indicate a more fundamental issue with Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485:436,pipeline,pipeline,436,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1674035485,1,['pipeline'],['pipeline']
Deployability,"Hey @daniel-goldstein, just checking what caused this issue. Was it the open-batches PR, or prior to the 100 release?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12235#issuecomment-1261571243:109,release,release,109,https://hail.is,https://github.com/hail-is/hail/pull/12235#issuecomment-1261571243,1,['release'],['release']
Deployability,"Hey @daniel-goldstein, super keen for this fix! Anything we can do on our side to test this, or ease this PR getting merged and triggering a release?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14576#issuecomment-2177245584:141,release,release,141,https://hail.is,https://github.com/hail-is/hail/pull/14576#issuecomment-2177245584,1,['release'],['release']
Deployability,"Hey @danking. I've also got a PR open for this issue (#9250) which includes a check for the minimum required version of `gcloud`. Also, including the `--secondary-worker-type` flag increases the minimum `gcloud` version to [291.0.0](https://cloud.google.com/sdk/docs/release-notes#28500_2020-03-17) (released in May).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9262#issuecomment-673163784:267,release,release-notes,267,https://hail.is,https://github.com/hail-is/hail/pull/9262#issuecomment-673163784,2,['release'],"['release-notes', 'released']"
Deployability,"Hey @dlcotter ! Thanks for the report. I anticipate a fix in the next version of Hail. For now, I think you can fix with `pip3 install 'parsimonious>=0.9'` or downgrading to Python 3.10",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443:127,install,install,127,https://hail.is,https://github.com/hail-is/hail/issues/12759#issuecomment-1458660443,1,['install'],['install']
Deployability,Hey @mhebrard !. I'm really sorry Hail has been such a pain to install. This looks to me like a Scala version incompatibility. In your Makefile you specified this:; ```; ... SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.0; ```; [EMR's docs](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-691-release.html) confirms that Scala 2.12.15 should be installed. I think my next questions are:; 1. Which `spark-shell` is that?; 2. What latent JVMs are around?; 3. What's the class path and what's on it?; 4. What scala executables are around?. ```; which spark-shell; spark-shell --version; which java; java -version; which scala; scala -version; echo $CLASSPATH; ```. That `SettingsOps` is an implicit nested class of the `MutableSettings` object. It is definitely present in [2.13](https://github.com/scala/scala/blob/2.13.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L70-L88) and [2.12](https://github.com/scala/scala/blob/2.12.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L83-L94). It appears to be missing in [2.11](https://github.com/scala/scala/blob/2.11.x/src/reflect/scala/reflect/internal/settings/MutableSettings.scala#L64-L68). It appears to have arrived in [2.12.14](https://github.com/scala/scala/commit/3bd24299fc34e5c3a480206c9798c055ca3a3439).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525:63,install,install,63,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1766477525,4,"['Release', 'install', 'release']","['ReleaseGuide', 'install', 'installed', 'release']"
Deployability,"Hey @tomwhite, sorry for the massive delay. There was some concern about not having instructions generic to any cluster in the docs, so I've restructured your PR a bit more to capture the generic Spark cluster instructions and then have a separate section on getting started with a Cloudera cluster. I also opted for ""Cloudera"" instead of ""CDH"" because I don't think our users will recognize the acronym. Does that seem OK to you?. I made my changes as [a PR into your branch](https://github.com/tomwhite/hail/pull/1/files). Also, don't worry about the failing integration test, that's a CI issue on our end. It should resolve it self after the next new commit to your branch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429:561,integrat,integration,561,https://hail.is,https://github.com/hail-is/hail/pull/1452#issuecomment-290546429,1,['integrat'],['integration']
Deployability,"Hey @williambrandler ! Thanks for your contribution to Hail. We endeavor to keep our docs always accurate and up-to-date. Our continuous deployment system verifies the correctness of our Google Dataproc and Azure HDInsight instructions before releasing a new version of Hail to PyPI. Does Databricks have an open source program that would provide us with free credits to incorporate the Databricks platform into our continuous deployment process? Alternatively, I'm comfortable accepting these new instructions with a disclaimer that clearly identifies these instructions as contributed by Databricks and not maintained by the Hail team. Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11800#issuecomment-1112572464:126,continuous,continuous,126,https://hail.is,https://github.com/hail-is/hail/pull/11800#issuecomment-1112572464,4,"['continuous', 'deploy']","['continuous', 'deployment']"
Deployability,"Hey Nick, wanted to loop you in on an offline discussion I had with Cotton about this. First, thank you for picking up review responsibilities! I'll just do a brief review focusing on interaction of this change with intended directions for hailctl. Here are the conclusions from our discussion:. 1. This is a breaking change to the hailctl interface. We're OK with that.; 2. Although we are OK making breaking changes, we should get Grace's team on board and update their scripts/repos before merging/releasing. For that reason this will sit for a few weeks until their current urgent analysis push is done.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298:459,update,update,459,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171298,1,['update'],['update']
Deployability,Hey Tim! Just curious what the status on this bug is. I also just got this error (I did update to the newest version incase it was fixed).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721:88,update,update,88,https://hail.is,https://github.com/hail-is/hail/issues/7824#issuecomment-597216721,1,['update'],['update']
Deployability,"Hi @Sun-shan,. First, I should note that we do not currently test hail against Spark version 2.2.0, I recommend using Spark 2.1.1 or 2.0.2. Spark versions aside, the error you encountered is unrelated to Spark, as far as I know. What version of the `decorator` package is installed on your machine? `decorator` version 4.0.10 should work correctly. Unfortunately, we are still looking for a python dependency management solution. My apologies that you've run into this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534:272,install,installed,272,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-336903534,1,['install'],['installed']
Deployability,"Hi @Sun-shan,. I am unsure what is wrong. I tried to replicate your environment as follows:; - I downloaded the CentOS 7.2 1511 [""everything ISO""](http://isoredirect.centos.org/centos/7/isos/x86_64/CentOS-7-x86_64-Everything-1511.iso); - On a VM, I installed CentOS using that iso; - I downloaded the Gradle ""Binary distribution"" from the [Gradle website](https://gradle.org/gradle-download/); - I downloaded a zip file of the hail repository from github; - In the hail directory, I issued `gradle installDist`, which succeeded; - In the hail directory, I issued `gradle check`, which succeeded except for the five tests that require PLINK or R. I did not see any undefined symbol errors. Unfortunately, further debugging your environment is outside of the scope of this project. The only remaining recommendation I can give is to use the (slow) reference implementations of BLAS functions. To use the reference implementations, run the following command instead of `gradle check`:. ``` bash; gradle -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.NativeRefBLAS check; ```. ---. The following details about the VM may be helpful if you attempt to modify your system. ```; [dking@cg-router1 hail-master]$ rpm --query centos-release; centos-release-7-2.1511.el7.centos.2.10.x86_64; ```. ```; [dking@cg-router1 hail-master]$ hostnamectl; Static hostname: cg-router1.broadinstitute.org; Icon name: computer-vm; Chassis: vm; Machine ID: 0d856e1616ee4961bfc1b76c6ec420a1; Boot ID: 1fc0d1ffc3d24218a81ea8fc5abd9776; Virtualization: kvm; Operating System: CentOS Linux 7 (Core); CPE OS Name: cpe:/o:centos:centos:7; Kernel: Linux 3.10.0-327.el7.x86_64; Architecture: x86-64; ```. The output of `yum list installed` is in [installed-packages.txt](https://github.com/broadinstitute/hail/files/422887/installed-packages.txt)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-240446097:249,install,installed,249,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-240446097,7,"['install', 'release']","['installDist', 'installed', 'installed-packages', 'release', 'release-']"
Deployability,"Hi @alanmejiamaza ,. Just to be clear, you did `pip install hail` and then you opened a notebook and ran something like:; ```; import hail as hl; hl.init(); from hail.plot import show; from pprint import pprint; hl.plot.output_notebook(); ht = hl.utils.range_table(1000); ht = mt.annotate(DP = hl.rand_unif(0, 100)); p = hl.plot.histogram(ht.DP, range=(0,30), bins=30, title='DP Histogram', legend='DP'); show(p); ```; And the plot didn't appear? Did you get a message saying ""BokehJS 1.4.0 successfully loaded.""? What version of Jupyter are you using? What web browser are you using?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951:52,install,install,52,https://hail.is,https://github.com/hail-is/hail/issues/12717#issuecomment-1452599951,1,['install'],['install']
Deployability,Hi @daniel-goldstein! I'm actually running this through Amazon CodeBuild so these are logs from an actual Amazon Linux 2 Image running on an EC2 instance build... So I don't know if that makes a difference here. I see what you're saying about the `xargs -0` however wouldn't this still be a change to the installation files for Hail or is that something that's likely happening in one of the files that I'm using and just haven't found it yet?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255333669:305,install,installation,305,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255333669,1,['install'],['installation']
Deployability,"Hi @danking, sorry this took me a little to test. I think there's a problem with the latest changes, in my dev-deploy, it failed on the '`create_certs` and `create_accounts`, with the error:. ```; FileNotFoundError: [Errno 2] No such file or directory: '/usr/local/lib/python3.7/dist-packages/hailtop/hail_version'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814:111,deploy,deploy,111,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-791067814,1,['deploy'],['deploy']
Deployability,"Hi @danking, this should be working now! I realises that hailtop isn't installed through `setup.py`, but `setup-hailtop.py`. The setup also requires `include_package_data=True`, so I've added that into both. I've added one more line into the `Dockerfile.service-base` to check whether the version was actually installed correctly, as other services use this, and better to fail early. This is working on my local dev-deploy (that uses CPG infrastructure though). Later edit: this file is re-installing hailtop and breaking this PR: https://github.com/hail-is/hail/blob/main/docker/Dockerfile.service-java-run-base",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-797251759:71,install,installed,71,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-797251759,4,"['deploy', 'install']","['deploy', 'installed', 'installing']"
Deployability,"Hi @nawatts, your comments are very welcome, and I appreciate your perspective as a hailctl user. OK, mulling over your comments, I think I can address collectively by removing all hailctl options that pass through to gcloud. This removes the question of providing them twice, makes all the commands consistent. I think this also addresses the issue `hailctl dataproc submit` not supporting `--`, because you can specify it twice: once to break out of hailctl options, and once to break out of gcloud options to specify options the script being submitted: `hailctl dataproc submit --halictl-option -- --gcloud-options -- --script-options and-parameters`. What do you think?. > it would be nice if the --configuration/--gcloud-configuration argument was consistent across hailctl dataproc commands; > It would also be nice to standardize on kebab case for all arguments. Agree on both accounts, will fix. Thanks again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935:703,configurat,configuration,703,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758080935,2,['configurat'],['configuration']
Deployability,"Hi @pettyalex, thank you for the detailed and thoughtful issue. Hopefully I can shed some light and address all your concerns. I think the assertion on Java 8 and 11 was an overly defensive precaution put in place some time ago, as hail uses some unsafe JVM APIs that have been deprecated for a while. But as you noted, the world goes on in Java 17 and I don't see a reason Hail shouldn't be compatible. Since most of our closest users use Hail on GCP Dataproc, we generally keep in lock-step with their platform which is unfortunately still on Java 11 so that is what we test against and officially support. Nevertheless, we should remove the restriction and add some light validation in CI against Java 17 and advertise it as unofficially supported until such a time that Dataproc moves to Java 17. Hopefully Spark 3.6 will force their hand. The release process for 0.2.129 is already underway but expect this to be resolved in 0.2.130. Thanks for your suggestions regarding bundling the JRE and the GC options, we'll definitely consider them. Regarding the `module-info.class` nonsense, my apologies. That just seems like a bug we should fix. I will create a separate tracking issue for that but I'm not yet sure where that will get prioritized. If it is more than an annoyance for you, please let us know. Regarding conda-forge, I don't think we currently have the bandwidth or demand (that we know of) to add more distribution systems. Again, this is something where hearing from the community is the best way to figure out how to direct our efforts. Hopefully this addresses your concerns. Please do follow up if I've missed anything or open more issues if you encounter new problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704:848,release,release,848,https://hail.is,https://github.com/hail-is/hail/issues/14433#issuecomment-2030358704,1,['release'],['release']
Deployability,"Hi @vladsaveliev, sorry for the delay on this. I'm going to leave this for when @danking comes back next week because I think there's some nuance around our dev cert expiration that I'm not 100% on. In the meantime, I looked and saw you try to delete then try to create repeatedly. Could you instead use a dry-run then apply trick like [this](https://stackoverflow.com/questions/45879498/how-can-i-update-a-secret-on-kubernetes-when-it-is-generated-from-a-file)? We do this for some other keys in `build.yaml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-802861885:398,update,update-a-secret-on-kubernetes-when-it-is-generated-from-a-file,398,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-802861885,1,['update'],['update-a-secret-on-kubernetes-when-it-is-generated-from-a-file']
Deployability,"Hi @williambrandler, it does seem like everyone is getting hit with this issue. We pinned Jinja2 to 3.0.3 once this broke our tests and it should be fixed now. I'm going to close this issue but if you still experience these problems on the latest release please re-open and we'll address it promptly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11705#issuecomment-1099734097:247,release,release,247,https://hail.is,https://github.com/hail-is/hail/issues/11705#issuecomment-1099734097,1,['release'],['release']
Deployability,"Hi @yc000000, unfortunately, I don't know how Amazon Glue works. I don't know the purpose of copying the Hail jar into S3. You might try uploading the wheel file instead of the zip file. . You might also try instead using the HMS DBMI scripts for starting a [Hail Cluster on AWS Spot instances](https://github.com/hms-dbmi/hail-on-AWS-spot-instances). ---. Installing Hail on a cluster should only require a couple steps. Once you have a Spark cluster, you just need to:. 1. ssh to the master node of the Spark Cluster.; 2. clone the hail repository.; 3. run `make -C hail/hail install HAIL_COMPILE_NATIVES=1`. After that you should be able to submit Python files using Hail to the cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914493872:357,Install,Installing,357,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914493872,2,"['Install', 'install']","['Installing', 'install']"
Deployability,"Hi Cotton,. Interesting that this is during tablet creation not while inserting data.; Looks like this is a known issue, but with no fix or workaround yet that I; can see:. https://issues.cloudera.org/plugins/servlet/mobile#issue/KUDU-383. Does it work if you retry, or delete the table and retry? I successfully; imported chr1 from 1k genomes on a 6 node cluster. This would create fewer; tablets though as it only covers one chromosome, so I should try with the; full dataset - I'll do that in the next few days when I'm back from; travelling. Thanks for trying it out. Do you have any more review comments for the PR?. Cheers,; Tom; On 11 Apr 2016 21:29, ""cseed"" notifications@github.com wrote:. Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to; 40m to fix a The requested number of tablets is over the permitted maximum; (100) error. I was able to write a small table. When I tried to write a; larger file (~900 exomes) and I got:. hail: writekudu: caught exception:; org.kududb.client.NonRecoverableException: Too many attempts:; KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6,; DeadlineTracker(timeout=10000, elapsed=7721),; Deferred@1490962783(state=PENDING, result=null, callback=(continuation; of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) ->; (continuation of Deferred@919337785 after; org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) ->; (continuation of Deferred@1962741581 after; org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) ->; (continuation of Deferred@1202081964 after; org.kududb.client.AsyncKuduClient$5@49391441@1228477505),; errback=(continuation of Deferred@813205641 after; org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) ->; (continuation of Deferred@1748842457 after; org.kududb.client.AsyncKuduClient$5@42031f30@11075008; 48) -> (continuation of Deferred@919337",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208722298:719,install,installed,719,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208722298,1,['install'],['installed']
Deployability,"Hi Jerome, yup, the first three require plink 1.9 and the fourth requires qctool. I'm surprised FisherExactSuite didn't fail as well, perhaps you have R installed or pulled Hail before that went into master. Thanks for the feedback, super helpful!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240399174:153,install,installed,153,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240399174,1,['install'],['installed']
Deployability,"Hi Tim,. What's the problem with this implementation? I've tested it and it works... On Wed, Sep 21, 2016 at 11:07 AM, Tim Poterba notifications@github.com; wrote:. > Laurent, I was totally wrong about being able to do this per-command --; > I'm really sorry. I thought that it would be possible to create a new; > configuration just for this command and use that, but this is only possible; > for HadoopConfigurations and not SparkContexts. Can you reopen the old; > PR? That model is our only option.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248641543, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgcPW4xK16W3DlZfdE5U6RTcVmJthks5qsUhMgaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248643185:315,configurat,configuration,315,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248643185,1,['configurat'],['configuration']
Deployability,"Hi Tim,; Sorry for the bother again. I am following this short tutorial as described [here](https://gnomad.broadinstitute.org/news/2021-09-using-the-gnomad-ancestry-principal-components-analysis-loadings-and-random-forest-classifier-on-your-dataset/). The code snippet was working properly with earlier. Now that I have installed hail from pip, I have this error while running RF model. ```; ht, rf_model = assign_population_pcs(; ... ht,; ... pc_cols=ht.scores,; ... fit=fit,; ... ). 2022-09-29 14:55:46 Hail: INFO: Coerced sorted dataset (0 + 1) / 1]; INFO (gnomad.sample_qc.ancestry 224): Found the following sample count after population assignment: sas: 1; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/bioinfoRD/ARCdata/Projects_AMT/conda_envs/hail/lib/python3.10/site-packages/gnomad/sample_qc/ancestry.py"", line 235, in assign_population_pcs; min_assignment_prob=min_prob, error_rate=error_rate; UnboundLocalError: local variable 'error_rate' referenced before assignment; ```. Could you please suggest what might be happening here?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759:320,install,installed,320,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-1262408759,1,['install'],['installed']
Deployability,"Hi Tom,. I got Kudu installed on the cluster. I had to set --rows-per-partition to 40m to fix a `The requested number of tablets is over the permitted maximum (100)` error. I was able to write a small table. When I tried to write a larger file (~900 exomes) and I got:. ```; hail: writekudu: caught exception: org.kududb.client.NonRecoverableException: Too many attempts: KuduRpc(method=IsCreateTableDone, tablet=null, attempt=6, DeadlineTracker(timeout=10000, elapsed=7721), Deferred@1490962783(state=PENDING, result=null, callback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505), errback=(continuation of Deferred@813205641 after org.kududb.client.AsyncKuduClient$4@2c0dff53@739114835) -> (continuation of Deferred@1748842457 after org.kududb.client.AsyncKuduClient$5@42031f30@1107500848) -> (continuation of Deferred@919337785 after org.kududb.client.AsyncKuduClient$5@75ff6dd4@1979674068) -> (continuation of Deferred@1962741581 after org.kududb.client.AsyncKuduClient$5@2edd647d@786261117) -> (continuation of Deferred@1202081964 after org.kududb.client.AsyncKuduClient$5@49391441@1228477505))); ```. In the Kudu logs, I'm seeing tons of:. ```; W0411 15:20:09.832504 129721 catalog_manager.cc:1880] TS a72be89d736f49a799e1b544197675be: Create Tablet RPC failed for tablet 6652d540f73a4ba5a0b9758a3aeeb1e4: Remote error: Service unavailable: CreateTablet request on kudu.tserver.TabletServerAdminService from 69.173.65.227:42904 dropped due to backpressure. The service queue is full; it has 50 items.; ```. Suggestions on how to proceed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-208516279:20,install,installed,20,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-208516279,1,['install'],['installed']
Deployability,"Hi Tpoterba,. That would be a big help. I've attached the patch file I used. [HadoopFS.scala.patch.zip](https://github.com/hail-is/hail/files/6023321/HadoopFS.scala.patch.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10087#issuecomment-783486496:58,patch,patch,58,https://hail.is,https://github.com/hail-is/hail/issues/10087#issuecomment-783486496,3,['patch'],['patch']
Deployability,"Hi Vlad, thanks for the PR! I'm afraid there are some internal migrations we're making that are probably not clear from just looking at the codebase. Are you up to date on our `main`? We've found working with `config.mk` cumbersome because it can be stale if you switch between different instances of Batch (e.g. one deployed in azure and the other in GCP). > DOCKER_ROOT_IMAGE used to build batch workers and benchmark. I've recently updated the scripts for building the batch worker VM image to query kubernetes directly and we should probably do the same for benchmark. > HAIL_TEST_GCS_BUCKET used to build query; KUBERNETES_SERVER_URL used to build amundsen. These services are both currently deleted in our `main`. > PROJECT, ZONE, REGION are probably not need, but might make sense to add for consistency. These will fail in an Azure deployment, and while we want to move away from `config.mk` entirely, we would at least want it to contain configurations that are valid across clouds.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055:317,deploy,deployed,317,https://hail.is,https://github.com/hail-is/hail/pull/11371#issuecomment-1041941055,4,"['configurat', 'deploy', 'update']","['configurations', 'deployed', 'deployment', 'updated']"
Deployability,"Hi all,. Here's the error message that I get when I go to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-eb",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:58,install,install,58,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,7,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Hi! Are you trying to compile hail from source? You can get `lz4` on OS X with Homebrew by doing something like `brew install lz4`, or `apt-get install liblz4-dev` on a Debian-flavored Linux, but we don't (currently) ship with that dependency because the C++ code isn't enabled yet. If you don't need to build from source, but just want a local version to play around with, you can either use `pip` or download the prebuilt distribution following the instructions here: https://hail.is/docs/stable/getting_started.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724:118,install,install,118,https://hail.is,https://github.com/hail-is/hail/issues/4651#issuecomment-433220724,2,['install'],['install']
Deployability,Hi! Found this long closed thread but had a related question - any updates on implementing PC-Air in Hail in the intervening years by any chance?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-1182425769:67,update,updates,67,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-1182425769,1,['update'],['updates']
Deployability,"Hi!; This is an odd error message to get -- is your repository updated to the current master? There was an update to the `importannotations table` module a few weeks ago, before which the `-e` option didn't exist. . We are in the midst of a documentation reorganization, so I apologize if it's difficult to find things at the moment. From the cloned repository, all test files are at `src/test/resources/*`. . This command worked for me just now:. ```; hail importannotations table src/test/resources/variantAnnotations.alternateformat.tsv --impute -e '`Chromosome:Position:Ref:Alt`' write -o tmp.vds; ```. The `-e` argument uses an expression to specify how to construct a `Variant`, which in this case is just the column name since the type of that column is `Variant`. If we don't use the `--impute` argument, we can construct it with . ```; -e 'Variant(`Chromosome:Position:Ref:Alt`)'; ```. More info on that [here](https://github.com/broadinstitute/hail/blob/master/docs/commands/ImportAnnotations.md)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/561#issuecomment-238502640:63,update,updated,63,https://hail.is,https://github.com/hail-is/hail/issues/561#issuecomment-238502640,2,['update'],"['update', 'updated']"
Deployability,"Hi, I think this is outdated given the new annotation database.; cheers,. > On Feb 13, 2017, at 3:47 PM, jbloom22 <notifications@github.com> wrote:; > ; > @andgan <https://github.com/andgan> any update on this issue? should it remain open?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/hail-is/hail/issues/174#issuecomment-279517149>, or mute the thread <https://github.com/notifications/unsubscribe-auth/ADIkAnlW6KE-f6entdPPA6wzrnTBTrz6ks5rcMF1gaJpZM4HLmN9>.; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/174#issuecomment-279574525:195,update,update,195,https://hail.is,https://github.com/hail-is/hail/issues/174#issuecomment-279574525,1,['update'],['update']
Deployability,"Hi, I'm getting the same error trying to build Hail on Amazon Linux on an EMR cluster.; The suggested fix from issue #454 did not work. To reproduce:; - Create EMR cluster (using default Amazon Linux AMI ami-044cb769); - Install git (`sudo yum install git`); - Install gradle . > #!/bin/bash; > cd /root; > gradle_package=`curl -s http://services.gradle.org/distributions --list-only | sed -n 's/.*\(gradle-.*.all.zip\).*/\1/p' | egrep -v ""milestone|rc"" | head -1`; > gradle_version=`ls ${gradle_package} | cut -d ""-"" -f 1,2`; > mkdir /opt/gradle; > wget -N http://services.gradle.org/distributions/${gradle_package}; > unzip -oq ./${gradle_package} -d /opt/gradle; > ln -sfnv ${gradle_version} /opt/gradle/latest; > printf ""export GRADLE_HOME=/opt/gradle/latest\nexport PATH=\$PATH:\$GRADLE_HOME/bin"" > /etc/profile.d/gradle.sh; > . /etc/profile.d/gradle.sh; > hash -r ; sync; > gradle -v; - gradle -v. > [...]; > Gradle 2.6; > [...]; > Build time: 2015-08-10 13:15:06 UTC; > Build number: none; > Revision: 233bbf8e47c82f72cb898b3e0a96b85d0aad166e; > Groovy: 2.3.10; > Ant: Apache Ant(TM) version 1.9.3 compiled on December 23 2013; > JVM: 1.7.0_101 (Oracle Corporation 24.95-b01); > OS: Linux 4.4.11-23.53.amzn1.x86_64 amd64; - Clone hail from commit 6382678846a9c187d448713f26a2c38f21a683db; - `$ gradle installDist`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229750270:221,Install,Install,221,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229750270,4,"['Install', 'install']","['Install', 'install', 'installDist']"
Deployability,"Hi, Is there a plan to merge this branch in soon? We are testing out HAIL for some of our in-house pipelines and an ability to import bgens would be really handy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/243#issuecomment-233953902:99,pipeline,pipelines,99,https://hail.is,https://github.com/hail-is/hail/pull/243#issuecomment-233953902,1,['pipeline'],['pipelines']
Deployability,"Hi, not sure if this is the right avenue, but I'd also like to report a similar `orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0)` bug first reported by https://discuss.hail.is/t/hail-fails-after-installing-it-on-a-single-computer/3653. Hail installed from https://anaconda.org/sfe1ed40/hail; EDIT: the same error occurs after `pip install hail` into a fresh conda env, which produced hail `version 0.2.130-bea04d9c79b5`. Terminal output: ; ```; Python 3.10.13 | packaged by conda-forge | (main, Dec 23 2023, 15:36:39) [GCC 12.3.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import hail as hl; hl.init(); >>> hl.init(); SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; Running on Apache Spark version 3.4.1; SparkUI available at http://xxxx:xxxx; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.127-d18228b9bc5b; LOGGING: writing to xxxx.log; >>> hl.utils.range_table(10).collect(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-1234>"", line 2, in collect; File ""/xxxx/lib/python3.10/site-packages/hail/typecheck/check.py"", line 584, in wrapper; return __original_func(*args_, **kwargs_); File ""/xxxx/lib/python3.10/site-packages/hail/table.py"", line 2213, in collect; return Env.backend().execute(e._ir, timed=_timed); File ""/xxxx/lib/python3.10/site-packages/hail/backend/backend.py"", line 188, in execute; result, timings = self._rpc(ActionTag.EXECUTE, payload); File ""/xxxx/lib/python3.10/site-packages/hail/backend/py4j_backend.py"", line 219, in _rpc; error_json = orjson.loads(resp.content); orjson.JSONDecodeError: unexpected character: line 1 column 1 (char 0); ```. Log file:; ```; 2024-04-25 16:07:16.773 Hail: INFO: SparkUI: http://xxxx:xxxx; 2024-04-25 16:07:21.589 Hail: I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076:218,install,installing-it-on-a-single-computer,218,https://hail.is,https://github.com/hail-is/hail/issues/14049#issuecomment-2077624076,3,['install'],"['install', 'installed', 'installing-it-on-a-single-computer']"
Deployability,"Hi, sorry to leave this hanging - we aren't especially well-equipped to answer this kind of question, since it seems to be a problem with the ES config. We just convert the Hail Table to a Spark DataFrame and call `saveToEs`: ; ```scala; def export(df: spark.sql.DataFrame, host: String = ""localhost"", port: Int = 9200,; index: String, indexType: String, blockSize: Int = 1000,; config: Map[String, String], verbose: Boolean = true) {. // config docs: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/configuration.html. val defaultConfig = Map(; ""es.nodes"" -> host,; ""es.port"" -> port.toString,; ""es.batch.size.entries"" -> blockSize.toString,; ""es.index.auto.create"" -> ""true""). val mergedConfig = if (config == null); defaultConfig; else; defaultConfig ++ config. if (verbose); println(s""Config ${ mergedConfig }""). df.saveToEs(s""${ index }/${ indexType }"", mergedConfig); }; ```. I'd try debugging entirely in Spark to see if you can isolate the issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584:512,configurat,configuration,512,https://hail.is,https://github.com/hail-is/hail/issues/5643#issuecomment-476544584,1,['configurat'],['configuration']
Deployability,"Hi, thanks for the quick response!. I updated the original post with more info to reproduce.; Please let me know if I can get you more information about the Scala environment!. All the best,; Michael",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229753452:38,update,updated,38,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229753452,1,['update'],['updated']
Deployability,"Hmm, actually GKE seems to vary in how quickly it gets new versions out. It [supported k8s 1.11](https://cloud.google.com/kubernetes-engine/release-notes) for Early Access Partners (EAPs) within about a month. 1.12 was released a little less than a month ago. The first generally available GKE k8s 1.10 release was May 15, and k8s 1.10 was released on March 26th, so that's less than two months from k8s to GKE.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145:140,release,release-notes,140,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-432726145,4,['release'],"['release', 'release-notes', 'released']"
Deployability,"Hmm, looks like it's something like `spark.executorEnv.FOO=...` based on this: https://spark.apache.org/docs/latest/configuration.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753:116,configurat,configuration,116,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583434753,1,['configurat'],['configuration']
Deployability,Hmm. I see that `Annotation.copy` has been updated to not rely on `Region`s. I'll reimplement these methods in terms of `Annotation.copy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3353#issuecomment-380450020:43,update,updated,43,https://hail.is,https://github.com/hail-is/hail/pull/3353#issuecomment-380450020,1,['update'],['updated']
Deployability,Hmm. It sounds like you’re saying that you’re using some kind of prepackaged version of Hail on AWS. The Hail team only maintains tooling for Microsoft and Google. Can you direct me at whatever Amazon tools you’re using? It sounds like someone at Amazon needs to fix their installation script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255339494:273,install,installation,273,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255339494,1,['install'],['installation']
Deployability,Hmm. I’ll have to sort this out tomorrow. Not sure what’s going on with that. It seems like the shadowTestJar target is probably not correctly pulling in the testImolemebtation configuration.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126:177,configurat,configuration,177,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1709453126,1,['configurat'],['configuration']
Deployability,Hmm. OrderedRDD and OrderedPartitioner are being phased out in master. OrderedRDD2 and OrderedPartitioner2 are in. We should probably have an offline discussion about how the linear algebra routines are going to interact with the new RegionValue-based stuff. There seem to be two competing goals here: getting something working for UKB and building something that will integrate with the new 0.2 stuff. We should probably have a chat about how to navigate this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2171#issuecomment-326647606:369,integrat,integrate,369,https://hail.is,https://github.com/hail-is/hail/pull/2171#issuecomment-326647606,1,['integrat'],['integrate']
Deployability,"Hmm. This means every dev deploy will generate a new root key. I'm worried about the derived keys and trust lists. After this runs, any service which was not dev deployed needs to know to reload the trust list and start using the new key. For example, if you dev deploy batch, then separately dev deploy query, the new query will get cert errors when talking to batch, I think. I will give some thought this week to the right long-term certificate strategy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/pull/10188#issuecomment-804338199,4,['deploy'],"['deploy', 'deployed']"
Deployability,"Hmm. this works: https://internal.hail.is/dking/site/vendors/vanta/viz.min.js, where is it not getting loaded correctly? I probably need another rule in dev deploy's site's nginx to fix the path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804:157,deploy,deploy,157,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639035804,1,['deploy'],['deploy']
Deployability,"Hmmmm, I still don't totally understand why we're hitting this specific import error. The system pip should still be able to install and run hail, I think -- I'd expect either an import error saying that `hail` cannot be found (if it's installed somewhere not on the Python path), or success. I still want to replicate in a docker, will report back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733:125,install,install,125,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533279733,2,['install'],"['install', 'installed']"
Deployability,"Hmmmm. OK, I think we're getting tripped up by MANIFEST.in and/or setup.py. This bit in setup.py:. ```; package_data={; 'hail': ['hail_pip_version',; 'hail_version',; 'experimental/datasets.json'],; 'hail.backend': ['hail-all-spark.jar'],; 'hailtop.hailctl': ['hail_version', 'deploy.yaml']},; ```. Should be; ```; package_data={; 'hail': ['hail_pip_version',; 'hail_version',; 'experimental/datasets.json'],; 'hail.backend': ['hail-all-spark.jar'],; 'hailtop': ['hail_version'],; 'hailtop.hailctl': ['deploy.yaml']},; ```. A similar change needs to be made to `setup-hailtop.py`. And let's add a MANIFEST.in file in the same directory with this contents:; ```; include hail/hail_pip_version; include hail/hail_version; include hail/experimental/datasets.json; include hail/backend/hail-all-spark.jar; include hailtop/hail_version; include hailtop/hailctl/deploy.yaml; ```. And you'll need to copy that into Dockerfile.service-base:; ```; COPY hail/python/MANIFEST.in /hailtop/MANIFEST.in; ```. My bad on all this, I didn't fully check the Dockerfiles!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-793064791:277,deploy,deploy,277,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-793064791,3,['deploy'],['deploy']
Deployability,Holding on this until #10056 goes in because it removes the need for pypi credentials in deploying site.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10066#issuecomment-783517593:89,deploy,deploying,89,https://hail.is,https://github.com/hail-is/hail/pull/10066#issuecomment-783517593,1,['deploy'],['deploying']
Deployability,"However, a `NameError` is surprising here: I would've thought that this would be an attribute error instead. Let us know any updates!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642:125,update,updates,125,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-301725642,1,['update'],['updates']
Deployability,"Huh, you can't request changes on your own PR. So, right now the hail/apiserver dependency is cyclic. I'll need to fix that to get testing and deploying working right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308:143,deploy,deploying,143,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-473961308,1,['deploy'],['deploying']
Deployability,"Huh; somehow you received a development version of Spark after installing Hail?. ```; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.21; Branch HEAD; Compiled by user liangchi on 2023-02-11T02:24:04Z; Revision 5103e00c4ce5fcc4264ca9c4df12295d42557af6; Url https://github.com/apache/spark; Type --help for more information.; ```. https://github.com/apache/spark/commit/5103e00c4ce5fcc4264ca9c4df12295d42557af6",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772997892:63,install,installing,63,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772997892,1,['install'],['installing']
Deployability,I added a makePyHailDocs gradle task that uses spark.home System property to set up the PYTHONPATH and copyPyHailDocs that moves them to build/www/pyhail. You should be able to build the docs with:. ```; hail $ gradle -Dspark.home=/path/to/spark-1.6.2-bin-hadoop2.6 createDocs; ```. I also installed Spark 1.6.2 on ci.hail.is and added spark.home to the relevant build steps.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1061#issuecomment-258921388:290,install,installed,290,https://hail.is,https://github.com/hail-is/hail/pull/1061#issuecomment-258921388,1,['install'],['installed']
Deployability,I added a new `trait BroadcastSerializable` that tries to verify classes implementing this trait are only serialized when broadcasting. It works by getting the current stack trace and verifying that serialization only happens within a call to a `broadcast` method on the class. `ReferenceGenome` and `RVDPartitioner` implement `BroadcastSerializable`. @chrisvittal This also reduces the size of the RDD broadcast in the VCF combiner pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004:433,pipeline,pipeline,433,https://hail.is,https://github.com/hail-is/hail/pull/5512#issuecomment-469069004,1,['pipeline'],['pipeline']
Deployability,"I added the cli. I kind of winged it looking at how `hailctl dev deploy` was done. It seems to work though:. ```; (base) wmecc-475:hail jigold$ hailctl batch billing; usage: hailctl batch billing [-h] {list,get} ... Manage billing on the service managed by the Hail team. positional arguments:; {list,get}; list List billing projects; get Get a particular billing project's info. optional arguments:; -h, --help show this help message and exit; (base) wmecc-475:hail jigold$ hailctl batch billing fake; usage: hailctl batch billing [-h] {list,get} ...; hailctl batch billing: error: invalid choice: 'fake' (choose from 'list', 'get'); Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x111288208>; (base) wmecc-475:hail jigold$ hailctl batch billing list; - accrued_cost: 0.0; billing_project: ci; cost: null; limit: null; users: [ci]; - accrued_cost: 0.0012024241022130966; billing_project: test; cost: 0.0012024241022130966; limit: null; users: [test]; - accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]; - accrued_cost: 0.0; billing_project: test-zero-limit; cost: null; limit: 0.0; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get; usage: hailctl batch billing get [-h] [-o {yaml,json}] billing_project; hailctl batch billing get: error: the following arguments are required: billing_project; Unclosed client session; client_session: <aiohttp.client.ClientSession object at 0x10a635208>; (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. (base) wmecc-475:hail jigold$ hailctl batch billing get test-tiny-limit; accrued_cost: 9.62974093086927e-05; billing_project: test-tiny-limit; cost: 9.62974093086927e-05; limit: 1.0e-05; users: [test]. Unclosed client session; client_session: <aiohttp.client.ClientSession o",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/pull/9385#issuecomment-684964006,1,['deploy'],['deploy']
Deployability,I added to ci2 but not pipeline. https://github.com/hail-is/hail/pull/6150,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6149#issuecomment-494557340:23,pipeline,pipeline,23,https://hail.is,https://github.com/hail-is/hail/pull/6149#issuecomment-494557340,1,['pipeline'],['pipeline']
Deployability,"I addressed all your comments except making the decision how this should be structured between MySQL and Python. Once we make that decision, then I'll make the changes and double check with dev deploy that the costs are still the same and the resource aggregation is correct. Sound good?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8759#issuecomment-628002023:194,deploy,deploy,194,https://hail.is,https://github.com/hail-is/hail/pull/8759#issuecomment-628002023,1,['deploy'],['deploy']
Deployability,"I addressed the comments, except for the following, which I plan to do as separate PRs:; - ~~put list of domains in a file~~,; - least privileged: gateway-service-account (with certs read) and letsencrypt-sa (with certs update),; - second process in gateway pod to bump nginx on cert change",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4624#issuecomment-433179293:220,update,update,220,https://hail.is,https://github.com/hail-is/hail/pull/4624#issuecomment-433179293,1,['update'],['update']
Deployability,I addressed the grafana situation. I dev deployed into my namespace and I'm able to log in. I cannot test that I've addressed the issue because the default namespace is still using the old version and thus I hit 401s there before even reaching the dev namespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12380#issuecomment-1297584226:41,deploy,deployed,41,https://hail.is,https://github.com/hail-is/hail/pull/12380#issuecomment-1297584226,1,['deploy'],['deployed']
Deployability,"I agree cancel_after_n_failures should be on the group. That lets us match Spark semantics for QoB. 1. I agree, callback per group seems valuable.; 2. I agree attributes seem useful on groups.; 3. I agree, not much value in updates being at the job-group level. . Depends what you mean by prefix search, if that means `LIKE ""X%""`, I think that'll be quite fast on a normal index because you can jump directly to the first record whose prefix is X. I don't see how a fulltext index could do any better in that case. On the other hand, if you mean `LIKE ""%X""` then I agree, a normal index is useless and MySQL will do a table scan. In that case, I expect a fulltext index to be a substantial improvement. > I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1. Ah, that sounds good. So the plan would be to drop, for example, `aggregated_batch_resources_v2` and the other tables which are now replaced with the job group ones? That's exactly what I had in mind.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048:224,update,updates,224,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450945048,1,['update'],['updates']
Deployability,"I agree. I'll study up on testing this stuff. Scorecard isn't tested, either. A few thoughts:; - I don't feel quite so bad having some of this untested (scorecard, etc.) while we get up to speed since they are internal tools (and not too complicated, unlike ci), but at the very least we need to test hl.upload_log() since that's the user facing bit.; - It will get easier to run tests if we can deploy the service in a test namespace to mirror the production namespace. I'll bump up the priority on looking into this.; - We need authentication without oauth2 for the tests. I'm at a total loss about how to automate testing of oauth2 login. The internet has some thoughts: https://stackoverflow.com/questions/39180008/automated-api-testing-of-oauth2-openid-connect-protected-api, including using headless automation: https://medium.com/@vicusbass/api-testing-with-rest-assured-oauth2-flow-with-redirect-uri-ba48b5953823; - Flask has a test fixture, so at least I can write local tests: http://flask.pocoo.org/docs/1.0/testing/; - Created an issue to track these: https://github.com/hail-is/hail/issues/4539",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595:396,deploy,deploy,396,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-429338595,1,['deploy'],['deploy']
Deployability,"I already have these packages installed, and there was no `netcdf` issue with my version of R. @maccum is going to install the latest version of R fresh and try to add all the packages and see if tests pass. Thanks Meredith!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379056688:30,install,installed,30,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379056688,2,['install'],"['install', 'installed']"
Deployability,I also added some debugging logs to try and figure out why you were getting a batch with no jobs when you dev deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6670#issuecomment-512577477:110,deploy,deployed,110,https://hail.is,https://github.com/hail-is/hail/pull/6670#issuecomment-512577477,1,['deploy'],['deployed']
Deployability,"I also prefer the second option in https://github.com/hail-is/hail/pull/9842#issuecomment-758128554. The more I think about this, the more problematic the notion of having an opaque list of ""arguments to pass through to gcloud"" seems. For example, `hailctl dataproc start` may run multiple gcloud commands: one to start the cluster and another to apply tags to the master node. In that case, we'd want to pass through extra args to the cluster start command, but not the apply tags command. Or `hailctl dataproc modify`, where we might want to accept extra args for `gcloud dataproc clusters update`. Those args shouldn't be passed through to the `gcloud compute ssh` commands, but options like `--project` or `--configuration` should. It seems like pass through arguments would be best handled on a case by case basis for each hailctl command. That would probably make any approach that required parsing those pass through options more cumbersome to use.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032:592,update,update,592,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-758143032,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,I also pushed a commit to make it harder to accidentally `make deploy` into the main namespace. `site`'s `make deploy` now requires a `NAMESPACE` argument to be set.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639136314:63,deploy,deploy,63,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639136314,2,['deploy'],['deploy']
Deployability,"I am (temporarily) defeated. Getting a working python 3.7 with pip on a standard distribution turns out to be non-trivial. Since ci doesn't specifically rely on 3.7 yet, I say we start with 3.6 and upgrade when it isn't too painful. It seems everyone is onboard with this. I am happy to move forward if you are, @jigold.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474131577:198,upgrade,upgrade,198,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474131577,1,['upgrade'],['upgrade']
Deployability,I am going to close this pull request and open a new one with more docstring updates.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14241#issuecomment-1922467179:77,update,updates,77,https://hail.is,https://github.com/hail-is/hail/pull/14241#issuecomment-1922467179,1,['update'],['updates']
Deployability,I am still getting the same error when I take the type explicitly from the table I am trying to transform. Updated code is here:; https://github.com/chrisvittal/hail/blob/404cbd2b3255fc58656801febccce6ed98e594b9/hail/python/hail/experimental/vcf_combiner.py#L13-L59,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354:107,Update,Updated,107,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467992354,1,['Update'],['Updated']
Deployability,"I attempted to use the TLS stuff and I've decided against it for this PR. It appears that the batch tests do not work locally at all. The whole essence of this PR is getting the shuffler IR tested and into the mainline. I started making the fixes necessary to support local testing of a local server and when that proved complicated investigated how BatchClientSuite works locally. It seems that using the existing TLS stuff would require fixing all the TLS stuff to allow for at least local->remote testing, if not local->local testing. This PR is already very complex, I'd like to get it merged so we can move forward separately with deploying and eventually harmonizing with the existing TLS infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852:636,deploy,deploying,636,https://hail.is,https://github.com/hail-is/hail/pull/8726#issuecomment-650380852,1,['deploy'],['deploying']
Deployability,"I believe that this patch and the original version both prevent this. They would both lock the parent directory of `path` and thus would prevent concurrent copying. Also, it appears that as long as we don't use the `-d` option, and the filenames are unique, `gsutil rsync` kinda already does what we want. I also feel like the orignal approach was way too aggressive, it seems like it was serializing _all_ locked filesystem operations in the entire filesystem subtree since it would wait for the lock on every parent other than `/` of the requested file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366:20,patch,patch,20,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701598366,1,['patch'],['patch']
Deployability,I believe the fix here is to update the 'entry points' to the Spark and Local backends as we did similarly to the Service backend in #14567.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14650#issuecomment-2266139361:29,update,update,29,https://hail.is,https://github.com/hail-is/hail/issues/14650#issuecomment-2266139361,1,['update'],['update']
Deployability,"I believe the next step should be the minimal changes to move the global variables in `server/globals.py` to a database table(s). We also need to think about sql configuration. For now, I suppose we can just create a table manually, but we need a longer term strategy for this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381:162,configurat,configuration,162,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-456848381,1,['configurat'],['configuration']
Deployability,I believe this is the only thread that needs to be updated: https://hail.zulipchat.com/#narrow/stream/223457-Hail-Batch-support/topic/Duplicate.20Sharded.20VCFs.20when.20exporting.20v3.20Hail.20Table/near/348554844,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12969#issuecomment-1532226084:51,update,updated,51,https://hail.is,https://github.com/hail-is/hail/pull/12969#issuecomment-1532226084,1,['update'],['updated']
Deployability,"I believe you're referencing [Chrome Bug 675308](https://bugs.chromium.org/p/chromium/issues/detail?id=675308) which suggests that lines thicker than 1 are broken in some versions of Chrome. My change uses line width 1. The screenshots you shared seem to suggest that Brave is making the lines somewhat fainter than Safari. I can't explain that. The visuals look the same to me across Chrome and Safari. I don't have Firefox installed to check a non-WebKit renderer. At this pointI feel that I have said my piece. It feels brittle to fiddle with device pixel ratio and line width to try and simulate line widths smaller than a pixel. If you think the only acceptable solution is the proposed changes in this PR, then let's do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-646218723:425,install,installed,425,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-646218723,1,['install'],['installed']
Deployability,I can dev deploy it as well,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8920#issuecomment-638971214:10,deploy,deploy,10,https://hail.is,https://github.com/hail-is/hail/pull/8920#issuecomment-638971214,1,['deploy'],['deploy']
Deployability,"I can move some of it over to a python diagnose, but I'm also targeting the ""I installed hail, but import hail fails"".",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5910#issuecomment-484684741:79,install,installed,79,https://hail.is,https://github.com/hail-is/hail/pull/5910#issuecomment-484684741,1,['install'],['installed']
Deployability,I can reproduce the error in the current master with:. ```; ./build/install/hail/bin/hail importvcf ~/sample2.vcf splitmulti annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]' count; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-241806708:68,install,install,68,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-241806708,1,['install'],['install']
Deployability,"I can stack this change with the change that defines the function, I do test that the `vcf_combiner` pipeline runs in `test_impex.py::VCFTests::test_combiner_works`. That may be sufficient, since it would fail without this change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5471#issuecomment-469340908:101,pipeline,pipeline,101,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469340908,1,['pipeline'],['pipeline']
Deployability,"I can write an RFC for how to do this with regards to billing updates and the database. I don't think it's too difficult, but it will take a bit of work to add some new metadata that says whether a resources is `by_time` or `by_unit` and compute usage accordingly per billing update. If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. If we have to track by IP address, I don't know how to do that and would have to look into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482:62,update,updates,62,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692021482,2,['update'],"['update', 'updates']"
Deployability,"I can't update that until this goes in, though. Otherwise everything else will fail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287901053:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287901053,1,['update'],['update']
Deployability,I can’t help you without an error message or description of what didn’t work. I recommend waiting for the next release which should come out today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992:111,release,release,111,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1501909992,1,['release'],['release']
Deployability,"I changed that the `_update_token` is no longer cached. I thought it was the source of a bug, but it was something else that was an issue. However, I felt it was confusing and I didn't see what value it provided as we have retries on all of our client operations that would need the token and the token is not used in future operations to submit the update.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255509466:350,update,update,350,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255509466,1,['update'],['update']
Deployability,"I checked out your branch, ran `make install-hailctl`, started a cluster, connected to a notebook, and ran `hl.utils.range_table(1_000_000, 10000)._force_count()`. Did not see any monitor UI show up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246:37,install,install-hailctl,37,https://hail.is,https://github.com/hail-is/hail/pull/7087#issuecomment-532861246,1,['install'],['install-hailctl']
Deployability,I checked the database and was surprised to see the SKUs weren't necessarily unique to a specific region. But it makes sense when I looked at their API here: https://cloud.google.com/billing/docs/reference/rest/v1/services.skus/list#sku. I think we should put this in and address what happens if they change the SKU of a particular region if that occurs in the future. We'll just get a bunch of error messages with no price updates and it shouldn't impact users. ~~I will also manually check this in Azure.~~ I checked in both GCP and Azure and the updates looked fine with no errors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597:424,update,updates,424,https://hail.is,https://github.com/hail-is/hail/pull/13607#issuecomment-1726229597,2,['update'],['updates']
Deployability,"I couldn't get rid of gcloud, sadly. `kubectl` is still to pervasively integrated into what build.yaml does. Although removing it would save us 100s of MB of unnecessary transfer and extraction, using the ci_utils_image for this purpose delays deployment of most services by an unacceptable amount of time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10502#issuecomment-845459536:71,integrat,integrated,71,https://hail.is,https://github.com/hail-is/hail/pull/10502#issuecomment-845459536,2,"['deploy', 'integrat']","['deployment', 'integrated']"
Deployability,"I created a new multi-branch configuration that should be better for what we are trying to accomplish. This should fix issues 2 and 3. . For the reproducibility of errors, that will probably take both setting the random seed parameter in Hail for all random tests and getting Jenkins to give better error messages.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/335#issuecomment-214377125:29,configurat,configuration,29,https://hail.is,https://github.com/hail-is/hail/issues/335#issuecomment-214377125,1,['configurat'],['configuration']
Deployability,"I decided to try to do this in two passes since making changes to deploy logic always finnicky on its own. I think this does the right thing though. . Does build.yaml support a way to say ""depend on this step x if we are doing x at all""? Redeploying the website will have to happen after the `deploy` step runs in the future and publishes the latest version of the docs to hail-common.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11294#issuecomment-1024606310:66,deploy,deploy,66,https://hail.is,https://github.com/hail-is/hail/pull/11294#issuecomment-1024606310,2,['deploy'],['deploy']
Deployability,"I deleted the pod; ```; # k delete pod batch-2554-job-4-main-cc8d4 -n batch-pods; ```; Batch logs when batch discovered 2554 task 4 ""failed"":; ```; INFO | 2019-06-25 12:37:07,611 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-cc8d4; INFO | 2019-06-25 12:37:07,671 | batch.py | update_job_with_pod:989 | job (2554, 4) mark complete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipelin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:218,update,update,218,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['update'],['update']
Deployability,"I deployed this to the cluster. There was a permissions bootstrap issue that batch could never deploy something that could deploy stuff, because it itself didn't have deploy privileges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221:2,deploy,deployed,2,https://hail.is,https://github.com/hail-is/hail/pull/4596#issuecomment-431613221,4,['deploy'],"['deploy', 'deployed']"
Deployability,I dev deployed it [here](https://internal.hail.is/dgoldste/website/docs/batch/tutorial.html). I clicked around a fair amount but didn't check all the links by hand. The only reason I can think of rendering at runtime is rewriting links but I grepped for '{{' and '{%' in the batch docs on a prod pod and found nothing. Also running jinja over post-sphinx rendered files concerns me in general.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10190#issuecomment-799456662:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/10190#issuecomment-799456662,1,['deploy'],['deployed']
Deployability,I dev deployed it and this looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8018#issuecomment-580526141:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/8018#issuecomment-580526141,1,['deploy'],['deployed']
Deployability,I dev deployed this and it still is working fine. It's still pretty slow and I was getting rate limit exceeded errors still trying to attch/detach 64 disks. Average operation time was still 15 seconds. I think part of the problem might be the delay starts at 0.1 for retry_transient_errors. We can consider making this a parameter and setting it to a higher number for this use case.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838:6,deploy,deployed,6,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872539838,1,['deploy'],['deployed']
Deployability,"I did update all the random tests in the randomness PR, if I disabled any it was definitely unintentional.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397385080:6,update,update,6,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397385080,1,['update'],['update']
Deployability,I didnt quite fix all of the select for updates when getting the instance state. Don’t merge until I get a chance to fix it (computer ran out of battery),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759:40,update,updates,40,https://hail.is,https://github.com/hail-is/hail/pull/7833#issuecomment-573028759,1,['update'],['updates']
Deployability,"I discovered this when I tried to run a vcf combiner pipeline. To me, this signals that we need better knowledge of where integration tests live and how to add to them.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618:53,pipeline,pipeline,53,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449468618,2,"['integrat', 'pipeline']","['integration', 'pipeline']"
Deployability,I do not recall but I think it was after the upgrade to major version 8,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14366#issuecomment-2253343407:45,upgrade,upgrade,45,https://hail.is,https://github.com/hail-is/hail/pull/14366#issuecomment-2253343407,1,['upgrade'],['upgrade']
Deployability,I don't know how to write this operation other than to do an insert on duplicate key update or to do a for loop and update each attempt individually. @danking Do you know of a better way to update multiple values in a single query?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11998#issuecomment-1222867456:85,update,update,85,https://hail.is,https://github.com/hail-is/hail/pull/11998#issuecomment-1222867456,3,['update'],['update']
Deployability,"I don't know if your deployment does this, but this is the sequence that I see with the edit page. <img width=""1242"" alt=""Screen Shot 2020-11-12 at 2 21 18 PM"" src=""https://user-images.githubusercontent.com/1693348/98986116-78110900-24f2-11eb-91f1-d9e35b826b16.png"">; <img width=""1263"" alt=""Screen Shot 2020-11-12 at 2 21 29 PM"" src=""https://user-images.githubusercontent.com/1693348/98986126-7b0bf980-24f2-11eb-8ebd-97e1319068df.png"">; <img width=""1262"" alt=""Screen Shot 2020-11-12 at 2 21 37 PM"" src=""https://user-images.githubusercontent.com/1693348/98986135-7d6e5380-24f2-11eb-9f77-9161800e0a59.png"">; <img width=""1256"" alt=""Screen Shot 2020-11-12 at 2 21 44 PM"" src=""https://user-images.githubusercontent.com/1693348/98986141-7fd0ad80-24f2-11eb-8b38-23b9ea5dd32c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-726287286:21,deploy,deployment,21,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726287286,1,['deploy'],['deployment']
Deployability,I don't know why I'm getting pylint errors for Pipeline. The only thing I can think of is I changed the PR build environment with the new docker image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362:47,Pipeline,Pipeline,47,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471732362,1,['Pipeline'],['Pipeline']
Deployability,"I don't like `inNonParX`. It will confuse people (it confused me). What about `inHemiX`? You're already using that in CopyState. So `(v.contig == ""X"" || v.contig == ""23"") == (inParX || inHemiX)`. Check the plink chromosome numbers in Variant until the reference fix goes in. The Pos versions are fine. When you change the expression language, you have to update the docs, too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/499#issuecomment-235660516:355,update,update,355,https://hail.is,https://github.com/hail-is/hail/pull/499#issuecomment-235660516,1,['update'],['update']
Deployability,"I don't love what I've had to do with the deploy config stuff. That's in my opinion the most finicky part of this (has already broken multiple times) and it's mostly our fault, because we overload the `namespace` parameter with both identifying the namespace in Kubernetes and signifying whether the environment is prod or not. All I want really is to change the `domain` to a domain and path prefix, and not have the namespace have such an impact on routing. Like what if `namespace` didn't affect routing, but if the deploy config only gave a domain with no path e.g. `hail.is`, we use subdomains so `batch.hail.is`, but if we provided a domain with a path prefix like `internal.hail.is/dgoldste`, we make the batch root `internal.hail.is/dgoldste/batch`?. Alternative: Actually have and use a `base_path` in the deploy config. This would be used in dev and terra environments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/13944#issuecomment-1785575616,3,['deploy'],['deploy']
Deployability,"I don't think I actually understand how artifact and snapshot dependencies work in TeamCity. I thought a build by the main build configuration (the regular CI) would trigger a build of the docs build configuration. This was not the case and I'm not sure why. I've set up the docs build to trigger on any change to master. Unfortunately, we have to `compileScala` twice because these are separate builds. I'll add an issue to clean this up and make it more sensible. There's got to be a right way to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/733#issuecomment-244475645:129,configurat,configuration,129,https://hail.is,https://github.com/hail-is/hail/issues/733#issuecomment-244475645,2,['configurat'],['configuration']
Deployability,"I don't think it is bad to have both. They have two different use cases. I envisioned `head` as being a mechanism to test pipelines on small amounts of data. `take` seems to be useful if someone actually wants to look at each object in the first n rows of data. However, it does add extra methods to VariantDataset when `take` is equivalent to `head().collect()`. Thinking back to the group/ungroup discussion, we decided to add those methods even though they could be implemented by the user in expr. However, I think those operations were more complicated than `take`. I don't have strong feelings either way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2204#issuecomment-328148402:122,pipeline,pipelines,122,https://hail.is,https://github.com/hail-is/hail/pull/2204#issuecomment-328148402,1,['pipeline'],['pipelines']
Deployability,"I don't think so - as long as the same version of Spark and Hail are installed, it should work with 2.0.2 or 2.1.0 or CDH deployments of those versions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321255115:69,install,installed,69,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321255115,2,"['deploy', 'install']","['deployments', 'installed']"
Deployability,"I don't think there's any good reason why the other pip packages are in a separate RUN step. In fact, using one pip invocation should ensure we get compatible versions whereas what we have now doesn't ensure that. I have no objections to using the hail/python/pinned-requirements.txt. If we install that and all these extra dependencies in one layer then install the hail wheel after (without -U), I *think* it should see all the dependencies are met and just install the hail package.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12301#issuecomment-1275353805:291,install,install,291,https://hail.is,https://github.com/hail-is/hail/pull/12301#issuecomment-1275353805,3,['install'],['install']
Deployability,"I don't think this is actually a bug -- I think this line doesn't support valid requirements.txt files with comments:; ```; sed '/^pyspark/d' python/requirements.txt \| xargs python3 -m pip install -U; ```. Instead, let's try:. ```; cat python/requirements.txt | sed '/^pyspark/d' | grep -v ""^#""; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-826851749:190,install,install,190,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-826851749,1,['install'],['install']
Deployability,"I don't think this keeps too much garbage in memory. Your next method extracts exactly the data it needs from its producer. No garbage there, you asked for only data you absolutely need. You stated (via `addReferenceTo`) that your region references these child regions, so that memory must be accessible at least as long as your region is accessible. Whoever is consuming your data can release all this memory by clearing the region you're using. The only nodes which should be clearing are folks who call `next` multiple times *and don't need that data to have the same lifetime*. This is true for filter, only surviving values must live, other values' lifetimes may end when we discover they fail the filter condition. It's also true for `write` because after one value is dumped into a file, it is no longer needed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366:386,release,release,386,https://hail.is,https://github.com/hail-is/hail/pull/7952#issuecomment-578798366,1,['release'],['release']
Deployability,I don't understand why Kubernetes wants to put `batch-deployment` and the service `batch` in the `batch-pods` namespace.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432381399:54,deploy,deployment,54,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432381399,1,['deploy'],['deployment']
Deployability,I duplicated globals.py so I can get the tests going. I'll think about how to organize sharing between the client in the server. Might just be to install the client on the server image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6423#issuecomment-504153212:146,install,install,146,https://hail.is,https://github.com/hail-is/hail/pull/6423#issuecomment-504153212,1,['install'],['install']
Deployability,I ended up posting on the forum. I did update to the newest version. It did not generate an error this time. The job ran much further but hung with 4 tasks left. . John,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896:39,update,update,39,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-599765896,1,['update'],['update']
Deployability,"I eventually found the command line below that worked. It would be helpful to update the Getting Started page to include any necessary command line --conf parameters. ` spark-submit --jars build/libs/hail-all-spark.jar --conf ""spark.driver.extraClassPath=file:///restricted/projectnb/genpro/github/hail/build/libs/hail-all-spark.jar"" --conf ""spark.executor.extraClassPath=file:////restricted/projectnb/genpro/github/hail/build/libs/hail-all-spark.jar"" --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator --py-files build/distributions/hail-python.zip --num-executors 6 test.py; `",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342#issuecomment-380210064:78,update,update,78,https://hail.is,https://github.com/hail-is/hail/issues/3342#issuecomment-380210064,1,['update'],['update']
Deployability,"I feel a bit like a cheat here since there's been a fair bit of work since you last reviewed. Most of it was fixes of tiny bugs that the CI revealed. There was [one, kind of notable, change](https://github.com/hail-is/hail/pull/5194/commits/f95e4e0ff1cdd2865cf703aa27f780c7f162316c). I removed Spark from the Dockerfile. It is no longer necessary because the pip install will pull the correct version of Spark. To avoid pulling Spark on each PR build, I cache 2.2.0 (and all our other pip dependencies) in the hail conda env in the docker image. Doing this required that I move our requirements into a requirements file which is parameterized by spark version. A good follow up PR would be to either a) entirely remove dependency on conda or b) generate the conda `environment.yml` from `requirements.txt.in`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333:363,install,install,363,https://hail.is,https://github.com/hail-is/hail/pull/5194#issuecomment-460305333,1,['install'],['install']
Deployability,I feel like this will only be useful once it updates all requirements files at once. It's missing one here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11696#issuecomment-1081106041:45,update,updates,45,https://hail.is,https://github.com/hail-is/hail/pull/11696#issuecomment-1081106041,1,['update'],['updates']
Deployability,"I find the SHOUTing is part of what makes the file feel so visually overwhelming, but there's much less SHOUTing now because there's just fewer keywords. Updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13372#issuecomment-1673746693:154,Update,Updated,154,https://hail.is,https://github.com/hail-is/hail/pull/13372#issuecomment-1673746693,1,['Update'],['Updated']
Deployability,"I fixed scorecard deploy stuff and now it is working with `dev deploy`. I also pushed some CSS changes:; - body { margin: 0; } that removes the extra header spacing; - but added an 0 8px 8px 8px margin to #content; - simplified the header layout CSS; - fixed the header item clickable area, should be bigger and uniform across header items; - fixed the misalignment on Safari. I'm pretty happy with this for this iteration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835:18,deploy,deploy,18,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-531991835,2,['deploy'],['deploy']
Deployability,"I fixed up refreshing from batch on deploy jobs recently, but now we might find a job from a previous CI instance (which was killed by an update say, or a node loss) and that job might have a different job id than the one we expected (if we eagerly launched a deploy before we refreshed from batch). That's totally OK, as long as the target SHAs match, because if the target SHAs match, then the job *must* be deploying the same thing we intended to deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500:36,deploy,deploy,36,https://hail.is,https://github.com/hail-is/hail/pull/4723#issuecomment-435512500,5,"['deploy', 'update']","['deploy', 'deploying', 'update']"
Deployability,"I forgot that we still had cron jobs running gcr-cleaner daily. This could have been conflicting with the new cleanup policy deletion settings. Let's reopen if this occurs again. Posting the job configurations here before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:195,configurat,configurations,195,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['configurat'],['configurations']
Deployability,"I found a linux that doesn't have `<execinfo.h>` as part of `libc`, thus requiring an external library. This is mostly so I can work on my desktop again, which had gotten to a pretty screwed up state regarding updates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8387#issuecomment-605674200:210,update,updates,210,https://hail.is,https://github.com/hail-is/hail/pull/8387#issuecomment-605674200,1,['update'],['updates']
Deployability,"I get the following version: ; ``; $ java -version; java version ""1.8.0_111""; Java(TM) SE Runtime Environment (build 1.8.0_111-b14); Java HotSpot(TM) 64-Bit Server VM (build 25.111-b14, mixed mode); ``; I'm using virtualenv to run python 2.7 and I think I installed all the dependencies and python libraries that were required. Any further idea on what I can do to get this to work?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319675148:256,install,installed,256,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319675148,1,['install'],['installed']
Deployability,I get this error trying to install GWASTools,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080:27,install,install,27,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701080,1,['install'],['install']
Deployability,"I gotta go to bed but it looks like maybe we need more role bindings all for `""get""` for all the objects we want to deploy in `batch-pods`? Or maybe we're applying that `deployment.yaml` to the wrong namespace? That one should be deployed in the default namespace. So maybe the answer is to ignore this until that PR gets merged? At this point I think we should just force merge the PR @cseed. NB: this is still without the `deploy-svc` PR merged due to some other yet undiagnosed weirdness, see #4608 for a taste.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571:116,deploy,deploy,116,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432084571,4,['deploy'],"['deploy', 'deploy-svc', 'deployed', 'deployment']"
Deployability,"I guess I was afraid of somebody copying and pasting from the README. But if they're going to do that, they're not going to get right version anyway. You don't know the release hash until it goes in, which is annoying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538:169,release,release,169,https://hail.is,https://github.com/hail-is/hail/pull/5915#issuecomment-484732538,1,['release'],['release']
Deployability,"I guess it depends whether you want up to date or just compatible, the maintainers seem to be of the opinion that you should either always update the lock file immediately or set upper bounds if you're not ok with a certain upgrade, seen [here](https://github.com/jazzband/pip-tools/issues/882). Continuous work, but maybe the right way to go honestly. In that case trivially make a build.yaml step that asserts the lock file is valid and up to date.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131859471:139,update,update,139,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131859471,3,"['Continuous', 'update', 'upgrade']","['Continuous', 'update', 'upgrade']"
Deployability,I had the same issue - I did `pip install pypandoc` and it worked fine after that :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9742#issuecomment-816041422:34,install,install,34,https://hail.is,https://github.com/hail-is/hail/issues/9742#issuecomment-816041422,1,['install'],['install']
Deployability,"I had to make a few changes to CI to make it test against a local version of the latest batch. Long term, we'll actually deploy batch and CI into a fresh namespace to do testing. For now, we explicitly start batch in the test container for CI to test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184:121,deploy,deploy,121,https://hail.is,https://github.com/hail-is/hail/pull/4930#issuecomment-446008184,1,['deploy'],['deploy']
Deployability,"I have a branch where I've upgraded the dependency to libsimdpp-2.1 and resolved issues around depreciation warnings, this does solve the issue. We would need to discuss if we want to upgrade the dependency, and benchmark against the new version to see if it causes any performance regression. Branch is [here](https://github.com/chrisvittal/hail/tree/libsimdpp-2.1)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780:27,upgrade,upgraded,27,https://hail.is,https://github.com/hail-is/hail/issues/3955#issuecomment-406297780,2,['upgrade'],"['upgrade', 'upgraded']"
Deployability,"I have a mild preference to get https://github.com/hail-is/hail/pull/12769/files into this release so I don't have to release again tomorrow, but I will release again tomorrow if necessary.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12770#issuecomment-1460486196:91,release,release,91,https://hail.is,https://github.com/hail-is/hail/pull/12770#issuecomment-1460486196,3,['release'],['release']
Deployability,"I have no explanation for the behavior of `pip`, it simply refuses to upgrade to the latest cloud tools. ```; + pip search cloudtools; cloudtools (1.1.16) - Collection of utilities for working on the Google Cloud Platform.; datawire-cloudtools (0.2.6) - Datawire Cloud Tools; cloudseed (0.0.1) - Cloudtools. real	0m0.867s; user	0m0.649s; sys	0m0.084s; + pip install -U cloudtools; Collecting cloudtools; Downloading https://files.pythonhosted.org/packages/46/78/966c9af5b88a01af73bb56486e853c00ff4865de0bf380282aa54fdec43a/cloudtools-1.1.15-py3-none-any.whl; Installing collected packages: cloudtools; Successfully installed cloudtools-1.1.15. real	0m1.718s; user	0m1.378s; sys	0m0.158s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700:70,upgrade,upgrade,70,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776700,4,"['Install', 'install', 'upgrade']","['Installing', 'install', 'installed', 'upgrade']"
Deployability,I have now installed the pre-commit hooks on my new machine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12959#issuecomment-1531617322:11,install,installed,11,https://hail.is,https://github.com/hail-is/hail/pull/12959#issuecomment-1531617322,1,['install'],['installed']
Deployability,"I have to go, but what happens when you update the rows to 0. Does the trigger run? I presume the n_cancelled_creating_jobs stay at 0 because now the additions / subtractions are in sync?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11278#issuecomment-1023511163:40,update,update,40,https://hail.is,https://github.com/hail-is/hail/pull/11278#issuecomment-1023511163,1,['update'],['update']
Deployability,"I haven't run very many Hail pipelines since ASHG, so there hasn't been much opportunity to see this bug. Sorry I can't help more!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4418#issuecomment-454131636:29,pipeline,pipelines,29,https://hail.is,https://github.com/hail-is/hail/issues/4418#issuecomment-454131636,1,['pipeline'],['pipelines']
Deployability,"I haven't yet Chris, but `astroid` also hasn't released in ~4 months, so I still have to build and check if it's fixed yet. Planning on doing so when I have some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9804#issuecomment-742033412:47,release,released,47,https://hail.is,https://github.com/hail-is/hail/pull/9804#issuecomment-742033412,1,['release'],['released']
Deployability,I implemented all pods and pvcs for a job have the same name. That way we let k8s be responsible for keeping track of the atomicity of pod and pvc updates rather than our database.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6519#issuecomment-507063867:147,update,updates,147,https://hail.is,https://github.com/hail-is/hail/pull/6519#issuecomment-507063867,1,['update'],['updates']
Deployability,"I installed latest hail (0.2.120-f00f916faf78), gnomad (e6f042a74c91e462b77fca24d070c815e02f6f5b), and gnomad_qc (0c52cf47e48fa5b503d874e96482ea4286474c71). I cloned the repo in question; ```bash; pip3 uninstall hail gnomad gnomad_qc. pip3 install -U \; hail \; git+https://github.com/broadinstitute/gnomad_methods.git \; git+https://github.com/broadinstitute/gnomad_qc.git. git clone git@github.com:broadinstitute/gnomad-readviz.git; ```. I applied this patch:; ```diff; diff --git a/step1__select_samples.py b/step1__select_samples.py; index c159207..9ba1812 100644; --- a/step1__select_samples.py; +++ b/step1__select_samples.py; @@ -38,14 +38,7 @@ def hemi_expr(mt):; ; def main(args):; ; - hl.init(log=""/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); - meta_ht = hl.import_table(args.sample_metadata_tsv, force_bgz=True); - meta_ht = meta_ht.key_by(""s""); - meta_ht = meta_ht.filter(hl.is_defined(meta_ht.cram_path) & hl.is_defined(meta_ht.crai_path), keep=True); - meta_ht = meta_ht.repartition(1000); - meta_ht = meta_ht.checkpoint(; - re.sub("".tsv(.b?gz)?"", """", args.sample_metadata_tsv) + "".ht"", overwrite=True, _read_if_exists=True); -; + hl.init(log=""/tmp/select_samples"", default_reference=""GRCh38"", idempotent=True, tmp_dir=args.temp_bucket); vds = gnomad_v4_genotypes.vds(); ; # see https://github.com/broadinstitute/ukbb_qc/pull/227/files; @@ -55,19 +48,8 @@ def main(args):; ; v4_qc_meta_ht = meta.ht(); ; - mt = vds.variant_data; - #mt = vds.variant_data._filter_partitions([41229]); -; - mt = mt.filter_cols(v4_qc_meta_ht[mt.s].release); -; - meta_join = meta_ht[mt.s]; - mt = mt.annotate_cols(; - meta=hl.struct(; - sex_karyotype=meta_join.sex_karyotype,; - cram=meta_join.cram_path,; - crai=meta_join.crai_path,; - ); - ); + #mt = vds.variant_data; + mt = vds.variant_data._filter_partitions([41229]); ; logger.info(""Adjusting samples' sex ploidy""); lgt_expr = hl.if_else(; @@ -88,9 +70,9 @@ def main(args):; logger.info(""Filter variants wi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664:2,install,installed,2,https://hail.is,https://github.com/hail-is/hail/issues/13248#issuecomment-1703383664,3,"['install', 'patch']","['install', 'installed', 'patch']"
Deployability,"I installed pandas 1.5.2 and ran every test with the word pandas in the name:; ```; =============================================== short test summary info ===============================================; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_mismatched_object_rows; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_missing_and_nans; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_objects; PASSED test/hail/table/test_table.py::Tests::test_from_pandas_works; PASSED test/hail/table/test_table.py::test_to_pandas; PASSED test/hail/table/test_table.py::test_to_pandas_flatten; PASSED test/hail/table/test_table.py::test_to_pandas_null_ints; PASSED test/hail/table/test_table.py::test_to_pandas_nd_array; PASSED test/hail/table/test_table.py::test_literal_of_pandas_NA_and_numpy_int64; PASSED test/hail/table/test_table.py::test_literal_of_pandas_NA_and_numpy_int32; ======================================== 10 passed, 1357 deselected in 40.40s =========================================; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12580#issuecomment-1396345811:2,install,installed,2,https://hail.is,https://github.com/hail-is/hail/pull/12580#issuecomment-1396345811,1,['install'],['installed']
Deployability,"I integrated these changes into https://github.com/broadinstitute/hail/pull/652 except I used block instead of block-inline. I also fixed the line height. (Problem was not using display: block, padding was per-line.) With the line-height fixed, font-size: 0.8em feels too small. How do you think it looks now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/644#issuecomment-241179566:2,integrat,integrated,2,https://hail.is,https://github.com/hail-is/hail/pull/644#issuecomment-241179566,1,['integrat'],['integrated']
Deployability,"I investigated this and it will ~half cluster start up time from 4m30s to 2m30s. Should we do this @cseed? We can use the hail deployment to generate a new image each time master changes (that ensures its always younger than 30 days, given our pace of development).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4038#issuecomment-422929159:127,deploy,deployment,127,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-422929159,1,['deploy'],['deployment']
Deployability,I just built the image using dev deploy of this branch to check its size and it's only grown by 8MB. What else is causing disk usage growth?. ```; # docker image ls gcr.io/hail-vdc/ci-intermediate:anrd6xyjsrnd; REPOSITORY TAG IMAGE ID CREATED SIZE; gcr.io/hail-vdc/ci-intermediate anrd6xyjsrnd 2a479e5a34c4 53 seconds ago 363MB; # docker image ls gcr.io/hail-vdc/batch-worker:g76daybmi5g1 ; REPOSITORY TAG IMAGE ID CREATED SIZE; gcr.io/hail-vdc/batch-worker g76daybmi5g1 6782b39e4d31 45 hours ago 355MB; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-609899181:33,deploy,deploy,33,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609899181,1,['deploy'],['deploy']
Deployability,I just noticed that we didn't add a foreign key constraint on the jobs table for the batch update. I think unfortunately we should add it... thoughts?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12154#issuecomment-1248619288:91,update,update,91,https://hail.is,https://github.com/hail-is/hail/pull/12154#issuecomment-1248619288,1,['update'],['update']
Deployability,"I just tried cloned locally and `gradle installDist` worked on the first try. We've gotten this compiler error sporadically for a few months, and every time it's been resolved by rebuilding after `gradle clean`. I'll continue to investigate, and see if I can find the source of the problem (could be a compiler bug).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/453#issuecomment-229752430:40,install,installDist,40,https://hail.is,https://github.com/hail-is/hail/issues/453#issuecomment-229752430,1,['install'],['installDist']
Deployability,I kinda feel like we can't ship this in 0.2 release in its current state of brokenness,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4193#issuecomment-422375767:44,release,release,44,https://hail.is,https://github.com/hail-is/hail/issues/4193#issuecomment-422375767,1,['release'],['release']
Deployability,"I like this plan overall. Some comments to consider:. - I don't think batch should persist job intermediates which I think more as a pipe (|) or a temporary file in a traditional script. This means the downstream batch clients (CI, Pipeline) should be copying files they want to persist to something like gs. One thing we should be sure here is that the solution doesn't involve excessive copies, e.g. we don't want to generate batch steps that just copy from a temporary persisted location in gs to a permanent location in gs.; - We need a way to refer to individual inputs/outputs. A bioinformatics command might output a massive data file and a report file, and we want to run a command to process or format the report file, but we don't want to copy the (unused) massive data file unnecessarily. Copying everything is a fine start.; - This might be covered by ""parse and exec series of commands"", but we want to be able to specify a series of stacked containers to execute, e.g. in the case of Pipeline, a user command to execute followed by a pipeline-controlled container with Pipeline gs credentials to copy the output to Pipeline (or user) controlled bucket. Same for CI.; - In the long run, we're going to want to be able to control the size of the local disk for user jobs (e.g. a bioinformatics command that needs 1TB of scratch space) separate from the host node's local file system. This will go into the job configuration. We probably don't need this for CI.; - We should separate the database management from the job execution. Probably easiest to make this a monolithic service, but we could separate them. One will interact with batch users to update the database and watch the database, and the other to reconcile the database and k8s by running jobs and updating results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739:232,Pipeline,Pipeline,232,https://hail.is,https://github.com/hail-is/hail/issues/5193#issuecomment-457709739,7,"['Pipeline', 'configurat', 'pipeline', 'update']","['Pipeline', 'configuration', 'pipeline-controlled', 'update']"
Deployability,"I like this, `uvloop.install()` has always made me a bit unsettled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7208#issuecomment-539151226:21,install,install,21,https://hail.is,https://github.com/hail-is/hail/pull/7208#issuecomment-539151226,1,['install'],['install']
Deployability,"I like this. That + depending on the `release` step ensures that we only submit the benchmarks on the exact sha that we release. When we eventually split these steps out into their own release pipeline, we can just delete the file and use the normal `depends_on: release` behavior to achieve the same result.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14398#issuecomment-2027251314:38,release,release,38,https://hail.is,https://github.com/hail-is/hail/pull/14398#issuecomment-2027251314,5,"['pipeline', 'release']","['pipeline', 'release']"
Deployability,"I looked at the yarn logs. It looks like it is not finding the GLIBCXX_3.4.18 lib. This is how the hail script is being submitted... ```; module load anaconda3/5.2.0; source activate hail2; module load gcc/7.2.0; module load lz4/1.8.3; module load spark/2.2.1; echo ""Export env vars""; export HAIL_HOME=/restricted/projectnb/genpro/github/hail/hail; export PYTHONPATH=""${PYTHONPATH:+$PYTHONPATH:}$HAIL_HOME/build/distributions/hail-python.zip""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python""; export PYTHONPATH=""$PYTHONPATH:$SPARK_HOME/python/lib/py4j-*-src.zip""; echo ""Submitting Spark job""; spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.k",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:953,deploy,deploy-mode,953,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,1,['deploy'],['deploy-mode']
Deployability,"I looked at this again and I think we can do this with `online: true`. It's a quick enough migration where it shouldn't impact the driver for too long that it's trying to query the long tables. If there's a problem and the driver can't make forward progress once the database migration is done, we can just shut down the deployment to 0 replicas. Thoughts?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153:321,deploy,deployment,321,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1810311153,1,['deploy'],['deployment']
Deployability,"I looked over the code and it looks fine, but I'm having trouble understanding the bigger picture of what you're trying to accomplish. I see that you have a new step that creates a test database in the default namespace in the test scope. Then you create the database config secret from this new database. And then deploy_ci depends on it, which makes sense because it needs the secret to be able to create new databases. And this is all only in the test scope. It looks like you cleaned up the build database in the case of dev deploy, which is fine too. > we also create a ""test_instance"" database that will be used as the database instance inside the tests. I don't understand what you wrote here because test_instance database doesn't seem to be used at all. Aren't we still creating the same batch and ci databases? I don't see what the test_instance database is buying you except to be able to make the database config secret that doesn't have the root username and password. I also don't quite understand what's going on in the build_cant_create_database build step. Shouldn't those secrets already exist? Won't this fail?. I'm sorry if I'm missing something obvious.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906:529,deploy,deploy,529,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562887906,1,['deploy'],['deploy']
Deployability,I made the docs build in docs/<hailVersion> so the header links work in either local build or the web site. I will update the master branch docs deploy script when this is ready to go in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2923#issuecomment-367442418:115,update,update,115,https://hail.is,https://github.com/hail-is/hail/pull/2923#issuecomment-367442418,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,I made the timeout for the standing workers to be 5 minutes for the test scope and 2 hours for the deploy scope.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001:99,deploy,deploy,99,https://hail.is,https://github.com/hail-is/hail/pull/8850#issuecomment-632344001,1,['deploy'],['deploy']
Deployability,"I made these changes and a few others in LinearRegressionCommand (flatMap instead of Array.concat), and updates the docs to reflect removal of --output. Remerged with master and pushed. Back to you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/292#issuecomment-212196570:104,update,updates,104,https://hail.is,https://github.com/hail-is/hail/pull/292#issuecomment-212196570,1,['update'],['updates']
Deployability,I mean I can close it since we re-updated the same function. It had just been a previous task of the day.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11529#issuecomment-1065511689:34,update,updated,34,https://hail.is,https://github.com/hail-is/hail/pull/11529#issuecomment-1065511689,1,['update'],['updated']
Deployability,I need to fix the pipeline pylint errors in this branch as well...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743:18,pipeline,pipeline,18,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-473565743,1,['pipeline'],['pipeline']
Deployability,I need to rethink the test-tiny-limit and test-zero-limit. This is going to fail every time after the first merge on the deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148:121,deploy,deployment,121,https://hail.is,https://github.com/hail-is/hail/pull/9354#issuecomment-705066148,1,['deploy'],['deployment']
Deployability,I need to split this up and think about how we're going to release the change.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9598#issuecomment-718269756:59,release,release,59,https://hail.is,https://github.com/hail-is/hail/pull/9598#issuecomment-718269756,1,['release'],['release']
Deployability,"I needed this to deploy 0.2.21. `make deploy` relies on the docs having been deployed (by CI) but there's no guarantee that happens -- we went a week without any successful deploys of master commits, for instance.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237:17,deploy,deploy,17,https://hail.is,https://github.com/hail-is/hail/pull/6991#issuecomment-528461237,4,['deploy'],"['deploy', 'deployed', 'deploys']"
Deployability,I now unconditionally update. The update system will either find another PR that was started in the meantime or it will start a new batch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8398#issuecomment-609908738:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/8398#issuecomment-609908738,2,['update'],['update']
Deployability,I only use install-benchmark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9567#issuecomment-704535423:11,install,install-benchmark,11,https://hail.is,https://github.com/hail-is/hail/pull/9567#issuecomment-704535423,1,['install'],['install-benchmark']
Deployability,"I originally added a `build.yaml` step to this that ran the script on every deploy, but I think there are some subtleties in there around releases that are best discussed in follow-up PR and may distract from some immediate goals around getting AR up and working. This will help me easily get the images we need to move over into AR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12204#issuecomment-1251646501:76,deploy,deploy,76,https://hail.is,https://github.com/hail-is/hail/pull/12204#issuecomment-1251646501,2,"['deploy', 'release']","['deploy', 'releases']"
Deployability,"I ported pipeline to new batch, and in doing so I ripped out `copy_service_account_name` and just replaced its uses with `user`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5605#issuecomment-473422952:9,pipeline,pipeline,9,https://hail.is,https://github.com/hail-is/hail/pull/5605#issuecomment-473422952,1,['pipeline'],['pipeline']
Deployability,I propose saving the configuration question to a different PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194:21,configurat,configuration,21,https://hail.is,https://github.com/hail-is/hail/pull/9769#issuecomment-738084194,1,['configurat'],['configuration']
Deployability,I pushed another commit with Sphinx docs for HailContext. They can be generated by running:. ```; /path/to/hail/python/pyhail/docs $ PYTHONPATH=/path/to/spark-1.6.2-bin-hadoop2.6/python:/path/to/spark-1.6.2-bin-hadoop2.6/python/lib/py4j-0.9-src.zip make html; ```. Then the docs will be found in `_build/html`. I don't yet know how to install pyspark through gradle.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1061#issuecomment-258503049:335,install,install,335,https://hail.is,https://github.com/hail-is/hail/pull/1061#issuecomment-258503049,1,['install'],['install']
Deployability,"I pushed another commit. Toplevel hail-ci-build.sh handles all known projects (and skips them if they aren't there yet or don't have a build script). Also, deploy batch only when changed. I will also make this generic by adding a get-deployed-sha.sh to retrieve the current deployed sha to compare against when considering redeploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229:156,deploy,deploy,156,https://hail.is,https://github.com/hail-is/hail/pull/4414#issuecomment-424099229,3,['deploy'],"['deploy', 'deployed', 'deployed-sha']"
Deployability,I put the WIP tag back on until I've fully debugged and fixed the azure deployment.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12724#issuecomment-1454085562:72,deploy,deployment,72,https://hail.is,https://github.com/hail-is/hail/pull/12724#issuecomment-1454085562,1,['deploy'],['deployment']
Deployability,I put the time_updated field in. I'll test it with dev deploy once everything else is ready to go.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1252827587:55,deploy,deploy,55,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1252827587,1,['deploy'],['deploy']
Deployability,I ran into this today...spent a bit of time debugging and was able to ssh to one of the workers and poke around the `docker` logs. The issue appears to be some kind of race between the `docker` install that the `VEP` initialization script does and the limited number of retries by `systemd` to get the `docker` daemon up and running. . Adding `sudo service docker restart` at the end of the the `VEP` initialization bash script worked as a short term fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412:194,install,install,194,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412,1,['install'],['install']
Deployability,"I realize I forgot to update the latest-hash lines, I'll fix that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/4220#issuecomment-416617372,1,['update'],['update']
Deployability,"I reassigned you to #6121 -- it shouldn't need much review because it's a copy of cloudtools. Then we can PR this onto that, and I'll get to hacking on the deployment strategy discussed over email yesterday.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6118#issuecomment-493507016:156,deploy,deployment,156,https://hail.is,https://github.com/hail-is/hail/pull/6118#issuecomment-493507016,1,['deploy'],['deployment']
Deployability,"I removed the extra ``bin`` - when I run hc=HailContext(), I get the following: . ``>>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-422>"", line 2, in __init__; File ""/Users/ih/languages/hail.is/hail/python/hail/typecheck/check.py"", line 226, in _typecheck; return f(*args, **kwargs); File ""/Users/ih/languages/hail.is/hail/python/hail/context.py"", line 83, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable``. I am realizing that pip still installs pyspark 2.2.0 - is this the issue?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319706791:586,install,installs,586,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319706791,1,['install'],['installs']
Deployability,I renamed a Makefile target but didn't update the deploy script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4745#issuecomment-436859081:39,update,update,39,https://hail.is,https://github.com/hail-is/hail/pull/4745#issuecomment-436859081,2,"['deploy', 'update']","['deploy', 'update']"
Deployability,"I replaced the maven source ; maven { url ""https://repo.hortonworks.com/content/repositories/releases/"" }. by; maven { url ""https://nexus-private.hortonworks.com/nexus/repository/maven-central/"" }; maven { url ""https://nexus-private.hortonworks.com/nexus/service/rest/repository/browse/maven-central/"" }. then I can compile again:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9419#issuecomment-688740276:93,release,releases,93,https://hail.is,https://github.com/hail-is/hail/issues/9419#issuecomment-688740276,1,['release'],['releases']
Deployability,"I reran the benchmarks with nothing else running on my laptop. I think this should work right?. ```; git checkout pc-relate && \; git status && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big && \; git checkout master && \; (cd ../../hail && make install) && \; git status && \; git checkout pc-relate && \; python3 -m benchmark_hail run -t pc_relate,pc_relate_big; ```; Benchmark should import the installed hail. pc-relate branch:; ```; 2020-01-24 18:38:08,147: INFO: burn in: 30.09s; 2020-01-24 18:38:35,904: INFO: run 1: 27.75s; 2020-01-24 18:39:03,001: INFO: run 2: 27.09s; 2020-01-24 18:39:29,144: INFO: run 3: 26.14s; ```; master:; ```; 2020-01-24 18:41:08,254: INFO: burn in: 32.71s; 2020-01-24 18:41:37,239: INFO: run 1: 28.98s; 2020-01-24 18:42:05,393: INFO: run 2: 28.15s; 2020-01-24 18:42:33,411: INFO: run 3: 28.01s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916:259,install,install,259,https://hail.is,https://github.com/hail-is/hail/pull/7962#issuecomment-578344916,2,['install'],"['install', 'installed']"
Deployability,I second this one. Had to install the `parsimonious` module which was missing from my Anaconda Python.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2978#issuecomment-370497953:26,install,install,26,https://hail.is,https://github.com/hail-is/hail/issues/2978#issuecomment-370497953,1,['install'],['install']
Deployability,"I see what is happening. . The Hail cluster install instructions specify the following for a spark cluster:. export PYSPARK_SUBMIT_ARGS=""\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; pyspark-shell"". On our cluster, this will run as a local job. It needs a ""--master yarn"" for an argument. Running it locally probably is related to the out of memory error and the limited cores. I will rerun this with the --master yarn argument. . Regarding the bgen file versus matrix table, are you suggesting, it would be faster to run an analysis such as a logistic regression starting with the bgen file instead of the imported bgen mt file. The phenotypes would need to annotated the imported bgen mt every time. Just trying to understand the trade offs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395:44,install,install,44,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439414395,1,['install'],['install']
Deployability,"I see, so our proposed fix is this? https://github.com/src-d/jgscm/compare/master...danking:patch-1",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5788#issuecomment-480383944:92,patch,patch-,92,https://hail.is,https://github.com/hail-is/hail/pull/5788#issuecomment-480383944,1,['patch'],['patch-']
Deployability,I semi-tested this as follows: deployed in auth and created a user. The logic then failed with 403 Forbidden because the dev namespace gsa can't create service accounts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104:31,deploy,deployed,31,https://hail.is,https://github.com/hail-is/hail/pull/8798#issuecomment-628891104,1,['deploy'],['deployed']
Deployability,"I separated setting the email from enabling pipeline uploads, so people can set the email once and use hl.upload_log() without arguments independent of pipeline upload.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4543#issuecomment-429567973:44,pipeline,pipeline,44,https://hail.is,https://github.com/hail-is/hail/pull/4543#issuecomment-429567973,2,['pipeline'],['pipeline']
Deployability,"I still dislike job_group_tree because trees are usually represented in terms of their ancestor or parent-child relationships, neither of which are what this is. That said, I don't think we should block this PR on a naming quibble. We can do renames separately. We should update the RFC to be clear about the unique identifiers of the three things groups, jobs, and batches. (In particular, jobs are uniquely identifier by batch id and job id, group id is not part of it).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475#issuecomment-1760376350:272,update,update,272,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1760376350,1,['update'],['update']
Deployability,I still haven't managed to set it up to work from that. I hit errors while installing the Genesis stuff,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312:75,install,installing,75,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-379045312,1,['install'],['installing']
Deployability,"I still need to test this with dev deploy, but at least it's on the radar again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12848#issuecomment-1680688341:35,deploy,deploy,35,https://hail.is,https://github.com/hail-is/hail/pull/12848#issuecomment-1680688341,1,['deploy'],['deploy']
Deployability,"I suppose we have lost to stand up a working CI (b/c it must talk to batcH) without a working router. Or, really, we've lost the ability to do that without copying the service definition from the router's list. I'm OK with this. I agree `make deploy` not also setting up the service definition is out of sync with our previous behavior. I could engineer some system to have one definition but two uses of the `Service` definition but it just doesn't feel important enough to justify it. We rarely if ever deploy the services by hand without deploying the router first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990:243,deploy,deploy,243,https://hail.is,https://github.com/hail-is/hail/pull/8504#issuecomment-611711990,3,['deploy'],"['deploy', 'deploying']"
Deployability,"I swapped out the read/write for a serialize/deserialize thing; got some times for a pipeline that was just read -> densify -> write on the same matrix table and got the following times:. old: [19.249044491007226, 20.367718594003236, 18.515784285002155, 18.116745114995865, 18.104985954996664]; new: [15.649361574003706, 16.205813756998396, 17.644299295003293, 16.90835837100167, 17.12396009400254]. I'd be interested in trying this on a wider matrix table, since I think that's when we start running into issues.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6580#issuecomment-509490837:85,pipeline,pipeline,85,https://hail.is,https://github.com/hail-is/hail/pull/6580#issuecomment-509490837,1,['pipeline'],['pipeline']
Deployability,"I tested `wget http://batch/jobs` from within the cluster...that works fine. However, I do see something potentially relevant in the output, pertaining to it seems a CI deployment attempt of batch (I don't see a timestamp, but this is the last job run). ```; deployment.yaml\nkubectl delete persistentvolumeclaim --all --namespace test\nError from server (Forbidden): persistentvolumeclaims is forbidden: User \""system:serviceaccount:batch-pods:deploy-svc\"" cannot list persistentvolumeclaims in the namespace \""test\""\nMakefile:70: recipe for target 'deploy' failed\nmake: *** [deploy] Error 1\n""},""state"":""Complete""}]; ```. Namely `system:serviceaccount:batch-pods:deploy-svc cannot list persistentvolumeclaims in the namespace \""test\`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751:169,deploy,deployment,169,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468935751,6,['deploy'],"['deploy', 'deploy-svc', 'deployment']"
Deployability,"I tested deploy locally, and things worked great.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6136#issuecomment-494081311:9,deploy,deploy,9,https://hail.is,https://github.com/hail-is/hail/pull/6136#issuecomment-494081311,1,['deploy'],['deploy']
Deployability,I tested this by applying it to the cluster directly (I ran make deploy in the `gateway/` project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/pull/4645#issuecomment-433180843,2,['deploy'],['deploy']
Deployability,I tested this by applying it to the cluster directly (I ran make deploy in the gateway/ project). It will continue to run until a master deploy blows it away.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315:65,deploy,deploy,65,https://hail.is,https://github.com/hail-is/hail/pull/4648#issuecomment-433191315,2,['deploy'],['deploy']
Deployability,"I tested this by hand with the following schenario:. 1. Deployed this branch and set the standard pool to use 4-core workers. Then submitted two jobs, with a total of 3 cores requested between them. The state looked as follows after a worker spun up:; <img width=""643"" alt=""Screenshot 2023-05-15 at 5 16 30 PM"" src=""https://github.com/hail-is/hail/assets/24440116/28d94a41-ff37-4331-8f4f-c2e649fd1c62"">. 2. I then bumped the instance version and redeployed. Then submitted a 1 core job. This should be able to fit on the existing worker, but batch instead spins up a new worker because the instance version has changed to 25 and the existing worker is version 24.; <img width=""639"" alt=""Screenshot 2023-05-15 at 5 19 41 PM"" src=""https://github.com/hail-is/hail/assets/24440116/92460dae-6aee-40b6-b6f3-46363dbdb12a"">. <img width=""643"" alt=""Screenshot 2023-05-15 at 5 20 59 PM"" src=""https://github.com/hail-is/hail/assets/24440116/5256762f-b361-40a7-a6d2-1aaa61f7b4ae"">. 3. The following steps I consider additional verification. I submitted another job, and observed it falling on the new worker instead of the most utilized worker. <img width=""641"" alt=""Screenshot 2023-05-15 at 5 21 36 PM"" src=""https://github.com/hail-is/hail/assets/24440116/176a9055-6ba7-49f1-88dc-c8496147db07"">. 4. I then cancelled the first batch and submitted another job and observed it fell on the new worker instead of the least utilized worker. So this step and the previous step confirm that regardless of utilization the old worker is not considered. We also see that the old worker receives no more work and successfully deactivates.; <img width=""644"" alt=""Screenshot 2023-05-15 at 5 23 03 PM"" src=""https://github.com/hail-is/hail/assets/24440116/d671a947-da44-4f05-bcdb-2fe6df28a655"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13055#issuecomment-1548633947:56,Deploy,Deployed,56,https://hail.is,https://github.com/hail-is/hail/pull/13055#issuecomment-1548633947,1,['Deploy'],['Deployed']
Deployability,"I tested this by running `make deploy` in the `image-fetcher/` directory. You can take a look at the pods with `kubectl`, they are in the default namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4647#issuecomment-433188993:31,deploy,deploy,31,https://hail.is,https://github.com/hail-is/hail/pull/4647#issuecomment-433188993,1,['deploy'],['deploy']
Deployability,I tested this works with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8760#issuecomment-626929606:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-626929606,1,['deploy'],['deploy']
Deployability,"I tested with dev deploy, this looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8002#issuecomment-580248649:18,deploy,deploy,18,https://hail.is,https://github.com/hail-is/hail/pull/8002#issuecomment-580248649,1,['deploy'],['deploy']
Deployability,I think I addressed all the comments. I also noticed some get-deployed-sha.sh scripts were broken (name of deployment was wrong) and I fixed them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-428393236:62,deploy,deployed-sha,62,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428393236,2,['deploy'],"['deployed-sha', 'deployment']"
Deployability,I think I addressed most of the comments. I haven't tested the new code -- I don't want to do that until we're happy with it. I don't know that I like how this is turning out. I think we're conflating what `hailctl config init` should be which is intitializing an environment configuration file versus a quick start to using Hail Batch and QoB. My intention for this feature was to idiot proof the latter especially for the ATGU workshop and make it as few commands as possible. I worry that needing to run `hailctl auth login` before this is not a just run this single command and then you can get going with little effort. I don't think `hailctl config init` is the right place for what I have written. Maybe it should be `hailctl batch quick-start`???,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382:276,configurat,configuration,276,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1662716382,1,['configurat'],['configuration']
Deployability,"I think I addressed the changes in the client, but I'm sure it's not 100% correct yet. Will deal with any bugs later. Can we move on to the implementation? I'm thinking the best place to start is `_create_jobs` and then the SQL implementation of what it means to create and utilize the new updates infrastructure, but happy to do what's easiest for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219720049:290,update,updates,290,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219720049,1,['update'],['updates']
Deployability,"I think I got everything, but I still want to test it again by hand in the morning once we're sure there's no other changes to make. I was testing the before migration by running dev deploy from master and cancelling a batch while it was submitting along with some completed ones and then tried to deploy the new version and made sure the ready cores etc. were now correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792:183,deploy,deploy,183,https://hail.is,https://github.com/hail-is/hail/pull/7851#issuecomment-574442792,2,['deploy'],['deploy']
Deployability,I think I have some quibbles on dropping the granularity of tracking cpus by zone and the terminology but I'm going to try deploying this in my project and see that we get the behavior we want.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12078#issuecomment-1206702152:123,deploy,deploying,123,https://hail.is,https://github.com/hail-is/hail/pull/12078#issuecomment-1206702152,1,['deploy'],['deploying']
Deployability,"I think I know the problem! We are testing against Plink 1.9, but you have the old version 1.07 (which is my fault for linking the plink base page). Install it from the link below and please try again:. [https://www.cog-genomics.org/plink2](https://www.cog-genomics.org/plink2)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230288836:149,Install,Install,149,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230288836,1,['Install'],['Install']
Deployability,"I think I should return the google service account email rather than the name. I propose that I return instead of:. ```; 'gsa_name': 'projects/hail-vdc/serviceAccounts/user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; ```. this:. ```; 'gsa_email': 'user-f2khk67pq8a9pc38wnbjigarg@hail-vdc.iam.gserviceaccount.com',; ```. This is because, although google service account management occurs by the `name`, acl operations appear to use the `email` or `uniqueId<int>`. Also, it appears that we don't need to specify the projectId to take operations on the [google service account](https://cloud.google.com/iam/docs/creating-managing-service-accounts#deleting_a_service_account). For reference, the google service account creation response. ```; {'name': 'projects/hail-vdc/serviceAccounts/user-<some_ascii>@hail-vdc.iam.gserviceaccount.com', 'projectId': 'hail-vdc', 'uniqueId': '<some_int_id>', 'email': 'user-<some_ascii>@hail-vdc.iam.gserviceaccount.com', 'displayName': 'user', 'etag': 'MDEwMjE5MjA=', 'oauth2ClientId': '<some_int_id>'}; ```. I will update this PR to return gsa_email (and optionally gsa_projectId, although omitting for now because we have only 1, and may always have only 1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5633#issuecomment-474443321:1066,update,update,1066,https://hail.is,https://github.com/hail-is/hail/pull/5633#issuecomment-474443321,1,['update'],['update']
Deployability,"I think I'm seeing more where this approach is coming from, specifically we put batches as they exist today in a special category of having no updates and avoid the new code path in that case. An alternative which pairs with my above suggestion of not adding new staging tables is that all batches have at least 1 update. I feel like if we can force all batches down the new code path we'll be incentivized to make it really low overhead for batches that only submit jobs once, and that will benefit all batches, as well as simplifying the mental model. I may be wrong that we can do this with minimal performance tradeoff, but I'd like to try it first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488:143,update,updates,143,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1219807488,2,['update'],"['update', 'updates']"
Deployability,"I think it could be enough to `make deploy` prometheus in your own namespace, kick off a couple PR builds if there aren't any, and check that your prometheus doesn't contain any of those entries. You can run queries through the prometheus UI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10732#issuecomment-892827072:36,deploy,deploy,36,https://hail.is,https://github.com/hail-is/hail/pull/10732#issuecomment-892827072,1,['deploy'],['deploy']
Deployability,"I think it will actually just work out of the box. In particular, the updated file paths that specify the full destination also work with gsutil. The build.yaml changes are being run by the production batch, so we already know those are working with gsutil. I'm running some benchmarks vs gsutil now and will have numbers in a while. If that all looks good, I vote to merge this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10131#issuecomment-790877506:70,update,updated,70,https://hail.is,https://github.com/hail-is/hail/pull/10131#issuecomment-790877506,1,['update'],['updated']
Deployability,I think it's safe. I was more worried about what the namespace should be and whether for developers it should have the service_namespace. I think the deploy config only tells you how to get the correct URL. There's nothing special about it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075:150,deploy,deploy,150,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691241075,1,['deploy'],['deploy']
Deployability,"I think one of the following needs to happen:; 1. we document the pc relate setup sufficiently; 2. we precompute results somewhere that PC-Relate runs and test against that. I feel strongly that any PRs that introduce new testing dependencies must also include the relevant information to install those dependencies, probably in the ""getting started developing"" doc or somewhere like that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3274#issuecomment-377932960:289,install,install,289,https://hail.is,https://github.com/hail-is/hail/pull/3274#issuecomment-377932960,1,['install'],['install']
Deployability,"I think that's the fix, actually. Just hasn't been deployed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9628#issuecomment-714496429:51,deploy,deployed,51,https://hail.is,https://github.com/hail-is/hail/issues/9628#issuecomment-714496429,1,['deploy'],['deployed']
Deployability,I think the gnomAD group would probably prefer Hail point to the `gcp-public-data--gnomad` version of this dataset once that is updated. @gtiao,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-795555847:128,update,updated,128,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-795555847,1,['update'],['updated']
Deployability,"I think the issue is that I used `pip3` to install `hail`, following the docs. Unfortunately, `conda` only installs `pip` into the `hail` environment and doesn't create the `pip3` alias, so I was using my system-wide `pip3` to install `hail`. When I use `pip` to install `hail` within the `hail` conda environment, everything works fine. Maybe just update the docs to use `pip` instead of `pip3`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332:43,install,install,43,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533266332,5,"['install', 'update']","['install', 'installs', 'update']"
Deployability,"I think the proposed new default and the option to change it is much more intuitive than the current behavior and worth a change. Though, I think it would be most polite to announce it on zulip/email list a week or two in advance of the release (which you may already planned to do).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11884#issuecomment-1145290949:237,release,release,237,https://hail.is,https://github.com/hail-is/hail/pull/11884#issuecomment-1145290949,1,['release'],['release']
Deployability,I think this PR still has good changes. We should avoid nesting when possible; I fear it leads to confusing situations. I'm gonna take it off the release 0.2.125 checklist though,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13677#issuecomment-1779897341:146,release,release,146,https://hail.is,https://github.com/hail-is/hail/pull/13677#issuecomment-1779897341,1,['release'],['release']
Deployability,"I think this covers most spots. I updated the python_requires, left the docker images, tried to update batch docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11219#issuecomment-1011493435:34,update,updated,34,https://hail.is,https://github.com/hail-is/hail/pull/11219#issuecomment-1011493435,2,['update'],"['update', 'updated']"
Deployability,"I think this is a known scheduler bug in Spark 1.5, where cancelled executors are incorrectly counted as failed. This will be fixed by an upgrade that will be installed this week. As a temporary fix, I increased the failed job retry count to 30. You hit this, although I don't see any genuine errors in your job. This is exasperated by jobs where each partition takes a long time to run. You can make the partition size smaller by increasing the number of partitions. I suggest you try it again with `-n 1000`. I increased the retry count in `hail-new-vep` to 50.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/302#issuecomment-210903100:138,upgrade,upgrade,138,https://hail.is,https://github.com/hail-is/hail/issues/302#issuecomment-210903100,2,"['install', 'upgrade']","['installed', 'upgrade']"
Deployability,"I think this is happening from the below line in ci/ci.py (~674). When ci finds a `wait` step of `kind: Service`, its rollout command checks a `deployment` resource of `name` (name here is blog), which we of course don't have. I think we need to not hardcode `deployment` here, either through a config property in the wait step, or by checking whether the deploy `config` for the build step has a `StatefulSet`, or `Deployment`. ```python; if self.wait:; for w in self.wait:; # ... redacted ...; elif w['kind'] == 'Service':; assert w['for'] == 'alive', w['for']; port = w.get('port', 80); timeout = w.get('timeout', 60); script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h deployment {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available deployment {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. edit: A possible config/ci change:. build.yaml; ```yaml; wait:; - kind: Service; name: blog; for: alive; resource_type: statefulset; ```. ci/ci.py:. ```python; if self.wait:; for w in self.wait:; name = w['name']; resource_type = w.get(""resource_type"", ""deployment""); # ... redacted ...; elif w['kind'] == 'Service':; # ... redacted; script += f'''; set +e; kubectl -n {self.namespace} rollout status --timeout=1h {resource_type} {name} && \; kubectl -n {self.namespace} wait --timeout=1h --for=condition=available {resource_type} {name} && \; python3 wait-for.py {timeout} {self.namespace} Service -p {port} {name}; EC=$?; kubectl -n {self.namespace} logs --tail=999999 -l app={name} | {pretty_print_log}; set -e; (exit $EC); '''; ```. @cseed does this seem a reasonable change?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626:118,rollout,rollout,118,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546407626,10,"['Deploy', 'deploy', 'rollout']","['Deployment', 'deploy', 'deployment', 'rollout']"
Deployability,I think this is ready for another look. I'll cleanup the database once we're happy with everything and I no longer need to test with dev deploy (I don't want to have to nuke the database).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1276247643:137,deploy,deploy,137,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1276247643,1,['deploy'],['deploy']
Deployability,"I think this is ready. Adds a createDatabase2 step that adds database migrations. What's a database migration? The idea is that a database starts as an empty database and is built up or modified over time by a series of patches or migrations. The database has a version which is the number of migrations applied (starting from 1). This is stored in the table `{database_name}_migration_version`. Each migration involves running a `.sql` or `.py` script. The logic that applies migrations computes a checksum of these scripts and stores them in the database in table `{database_name}_migrations`. When applying migrations again in the future, these checksums are verified. A create database step now looks like (from the CI tests):. ```; - kind: createDatabase2; name: hello2_database; databaseName: hello2; migrations:; - name: create-tables; script: /io/sql/create-hello2-tables.sql; - name: insert; script: /io/sql/insert.py; inputs:; - from: /repo/ci/test/resources/sql; to: /io/; namespace:; valueFrom: default_ns.name; dependsOn:; - default_ns; - copy_files; ```. migrations is a the list of migrations that need to be applied to get the current version. So the idea is, if you want to change the schema of the database, you just add another migration at the end to make the changes you want. After this goes in, I'll make a separate PR to switch everything to this new createDatabase2 step.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346:220,patch,patches,220,https://hail.is,https://github.com/hail-is/hail/pull/7674#issuecomment-562891346,1,['patch'],['patches']
Deployability,"I think this is the TSV from the public release, so I'll put it on a bucket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2089#issuecomment-321580612:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/2089#issuecomment-321580612,1,['release'],['release']
Deployability,"I think this may fix it:. ```yaml; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: list-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list""]; ---; apiVersion: v1; kind: RoleBinding; apiVersion: rbac.authorization.k8s.io/v1; metadata:; name: deploy-svc-list-test-pvc; namespace: test; subjects:; - kind: ServiceAccount; name: deploy-svc; namespace: batch-pods; roleRef:; kind: Role; name: list-test-pvc; apiGroup: ""rbac.authorization.k8s.io""; ```. I don't have permissions to create the role however. Another solution would be to modify the existing role to include ""list"" permissions. ```yaml; ---; apiVersion: v1; kind: Role; apiVersion: rbac.authorization.k8s.io/v1; metadata:; namespace: test; name: delete-test-pvc; rules:; - apiGroups: [""""]; resources: [""persistentvolumeclaims""]; verbs: [""list"", ""delete""]; ---; ```. `""get""` may also be needed",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060:336,deploy,deploy-svc-list-test-pvc,336,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-468958060,2,['deploy'],"['deploy-svc', 'deploy-svc-list-test-pvc']"
Deployability,"I think this will fix the problem @xiaolicbs is having on dataflow. Some (somewhat) unrelated remarks on why `-b 16` didn't work for him. It's more subtle than I made out on gitter. `-b` sets the min block size, yes. `sc.defaultMinPartitions` is `sc.defaultParallelism`. On Cray that is 260 or however many cores we're asking for. On dataflow under YARN, that is dynamically updated based on the number of cores we have. If we're still building the DAG and haven't requested any executors yet, it is 2. We can set the default parallelism (although it won't update dynamically anymore) with `--conf spark.default.parallelism=250` for example. We should probably do this on dataflow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/824#issuecomment-248511352:375,update,updated,375,https://hail.is,https://github.com/hail-is/hail/pull/824#issuecomment-248511352,2,['update'],"['update', 'updated']"
Deployability,"I think tying the reset to the iterator is a mistake. First, iterator is the wrong abstraction here. Whole-stage code generation should use the aggregator/array strategy we're using in Emit to generate nothing, conditionals and loops for map, filter and flatMap, respectively. Ideally read ... do stuff ... write will generate an RDD with no per-element iterators at all. I want to make sure this picture is clear. Second, we want to vectorize in the database sense: we want to process multiple rows together in batches. Then overall structure of a stage is a loop over the batches, and and a loop within batches. Thus, the common case should not be we reset after every element, so I think it's the wrong direction to bake it in. The place where we do this should be interface points with the Spark stack which should be looked at with scorn and derision and as the organizing model. Finally, this points to an ongoing difference in our views about the meaning of context. I see context as serving two purposes (neither of which involve reset):. - First, context is a set of resources needed to process a partition that should be released when the partition is complete. For example, I'm working on GenomicsDB which needs to localize a GenomicsDB shard to a local file that needs to be cleaned up when the partition is complete. - Second, it is a way to tell an iterator where to return its value. (This is the ""current"" region business.). I'd be happy to separate these, but I don't see clean way. In no case do I see generic logic to manage the lifetime of regions (e.g. knowing when to call reset) inside the Context.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739:1131,release,released,1131,https://hail.is,https://github.com/hail-is/hail/pull/3365#issuecomment-381180739,1,['release'],['released']
Deployability,"I think we also need to be clear when installing something that will break everyone's local tests (email, dev post, something).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023:38,install,installing,38,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377700023,1,['install'],['installing']
Deployability,I think we need it to be offline unless we're willing to tolerate up to 5-10 mins of not being able to cancel a batch and some alerts. The only parts that would be referencing the wrong tables are in the `Canceller` and `notify_batch_complete`. I think scheduling and MJC would just work because we update those stored procedures and don't change the child code. We can shut batch down though for the migration. Seems safest although more of a pain.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477:299,update,update,299,https://hail.is,https://github.com/hail-is/hail/pull/13810#issuecomment-1812841477,1,['update'],['update']
Deployability,"I think we need this fix:; ```; commit 4cb998d1c7cbc9954d66c6e39d7fd48b0e936f51 (HEAD -> add-version-endpoint); Author: Daniel King <dking@broadinstitute.org>; Date: Mon Mar 22 17:47:22 2021 -0400. fix. diff --git a/build.yaml b/build.yaml; index 7a100adec8..256ca99c91 100644; --- a/build.yaml; +++ b/build.yaml; @@ -86,7 +86,7 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; - make -C hail python/hail/hail_version python/hail/hail_pip_version; + make -C hail python-version-info; git rev-parse HEAD > git_version; outputs:; - from: /io/repo/auth/sql; diff --git a/ci/test/resources/build.yaml b/ci/test/resources/build.yaml; index 3b1df5214c..b994d2787c 100644; --- a/ci/test/resources/build.yaml; +++ b/ci/test/resources/build.yaml; @@ -27,10 +27,13 @@ steps:; mkdir repo; cd repo; {{ code.checkout_script }}; + make -C hail python-version-info; timeout: 300; outputs:; - from: /io/repo; to: /; + - from: /io/repo/hail/python/hail/hail_version; + to: /hail_version; dependsOn:; - inline_image; - kind: buildImage; @@ -52,6 +55,10 @@ steps:; publishAs: service-base; dependsOn:; - base_image; + - copy_files; + inputs:; + - from: /hail_version; + to: /hail_version; - kind: buildImage; name: hello_image; dockerFile: ci/test/resources/Dockerfile; ```; EDIT: updated with more changes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107:214,a/b,a/build,214,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-804418107,3,"['a/b', 'update']","['a/build', 'updated']"
Deployability,"I think we should find a time to discuss this in person if the following explanation doesn't make sense. . Right now, for small batches, we send one REST request to the server to both create the batch and create the jobs. However, if we want one REST request for an update (ideal for the query service and low latency jobs?), we have to use relative job ids because (1) we don't know the absolute start index of the jobs until we've gotten the start id of the update back from the server and (2) the job dependencies can be a mix of known job ids that have already been previously submitted in a previous creation/update. The negative job IDs are a way to deal with a mix of relative ids within an update and known, submitted job ids. We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd have to make sure this will actually simplify things because I also ran into a bifurcation in how the job IDs are handled in `BatchBuilder.create_job()`. We currently populate the spec with a job id before we've made any requests to the server. We need to know how many total jobs there are before we can figure out the job ids because the API for creating a new update requires reserving a block of job IDs which then returns the start id. This complexity is because we allow multiple updates to occur simultaneously to a batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856:266,update,update,266,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215919856,8,['update'],"['update', 'updates']"
Deployability,"I think we should have the following design that runs the benchmarks in k8s because then we are using google's internal network to transmit data (compared to running on my local computer via a cloud proxy):. - Have a `db-benchmark` namespace in k8s specifically for this. 1. create_db.py; a. This will take the parameters needed for `gcloud sql instances create` including database flags, disk space, cores, etc. and create an instance; b. Get the IP address of the instance (hopefully the REST API works for this); c. Create a database; d. Create user and password for the database; e. Create config file; f. Create secret in the db-benchmark namespace from the config file; ; 2. run.py; a. Build the docker image with the benchmark.py code and installs aiomysql, etc.; b. Create pod which mounts the correct secret with the sql config for the instance to use. Environment variables specify the n_replicates, etc. Print out the pod name.; c. Wait for the pod to complete (you have code in CI that does this); d. Download logs; e. Delete the pod. 3. cleanup.py; a. Delete mysql instance; b. Delete kubernetes secret in db-benchmark namespace. Thoughts? . I tried to think about how to use the current build system and what I would do is add a new CreateSQLInstance step, CreateDatabase takes the instance name and IP address as a parameter, and have CI take a path to the build.yaml file to build from. But this wasn't straightforward with how to do this, so I thought the above was simpler to reason about.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887:746,install,installs,746,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538453887,1,['install'],['installs']
Deployability,"I think we should just always mount the deploy config. That might trigger some latent bugs where we don't do quite the right thing, but we can fix those.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767891514:40,deploy,deploy,40,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767891514,1,['deploy'],['deploy']
Deployability,"I think we should remove tutorials from devel for now, and add them back in before 0.2 release",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2067#issuecomment-320243639:87,release,release,87,https://hail.is,https://github.com/hail-is/hail/issues/2067#issuecomment-320243639,1,['release'],['release']
Deployability,"I think your error is happening because you're requesting a resource that doesn't exist. `kubectl -n pr-7381-default-vo6ftnzp8ta2 rollout status --timeout=1h deployment blog`. You need ""statefulset"" here:. Example with a similar kind of rollout:. ```sh; $ k -n monitoring rollout status --timeout=1h deployment prometheus; Error from server (NotFound): deployments.extensions ""prometheus"" not found. $ k -n monitoring rollout status --timeout=1h statefulset prometheus; partitioned roll out complete: 1 new pods have been updated...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522:130,rollout,rollout,130,https://hail.is,https://github.com/hail-is/hail/pull/7381#issuecomment-546404522,8,"['deploy', 'rollout', 'update']","['deployment', 'deployments', 'rollout', 'updated']"
Deployability,"I thought about this some more. I think the correct solution is to have two values for `n_jobs` in the `job_groups` table. There should be `n_jobs` (direct child jobs) and `n_jobs_recursive` (all descendent jobs). This way the UI and the status will make sense for whichever use case is most applicable. To do this, we'll need to write a migration that adds the new column and backfills the column. Right now, those two values are the same so it's just a copy of one column to another column. We might actually be able to do this migration in one update command. The other tables had hundreds of millions of rows and would have taken days and crashed the db if there was not enough memory. I can test this locally on my laptop by creating a test database and inserting 10 million records and then seeing how long it takes to do the update. ```; mysql> select count(*) from job_groups limit 10;; +----------+; | count(*) |; +----------+; | 8122788 |; +----------+; 1 row in set (16.75 sec); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1934138382:547,update,update,547,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1934138382,2,['update'],['update']
Deployability,"I thought self.steps was still used. For example in `build`:. ```; def build(self, batch, code, scope):; assert scope in ('deploy', 'test', 'dev'). for step in self.steps:; if step.scopes is None or scope in step.scopes:; step.build(batch, code, scope). if scope == 'dev':; return. step_to_parent_steps = defaultdict(set); for step in self.steps:; for dep in step.all_deps():; step_to_parent_steps[dep].add(step). for step in self.steps:; parent_jobs = flatten([parent_step.wrapped_job() for parent_step in step_to_parent_steps[step]]). log.info(f""Cleanup {step.name} after running {[parent_step.name for parent_step in step_to_parent_steps[step]]}""). if step.scopes is None or scope in step.scopes:; step.cleanup(batch, scope, parent_jobs); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375:123,deploy,deploy,123,https://hail.is,https://github.com/hail-is/hail/pull/7722#issuecomment-568536375,1,['deploy'],['deploy']
Deployability,"I thought the config file was better for QoB where versioning and backwards compatibility is important and there could be substantially more configuration in the future. I don't think we mount that config file in the worker regardless for DockerJobs, but I could be wrong.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12662#issuecomment-1420895778:141,configurat,configuration,141,https://hail.is,https://github.com/hail-is/hail/pull/12662#issuecomment-1420895778,1,['configurat'],['configuration']
Deployability,"I thought the purpose of the cache was to cache the latest version in production. Let's take service-base as an example. There's the deployment in production that we care about. But every PR is now going to change the cache each time to what it thinks service-base is. This means that the last 4 layers for service-base will change for every time we run a test PR and it changes hailtop, gear, or web-common. If you don't like this change, then feel free to close it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213:133,deploy,deployment,133,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152568213,1,['deploy'],['deployment']
Deployability,"I tried it in both raw python and pyspark and I got a new error. Seem to be a problem with the profile having too small a starting maxPartition size and openCost size. I'm uncertain how to change these parameters even after extensive googling. Any Ideas? Thank you!. Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/scratch/PI/dpwall/computeEnvironments/hail/python/hail/context.py"", line 64, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/share/sw/free/spark.2.1.0/spark-2.1.0-bin-hadoop2.7/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py"", line 319, in get_return_value; py4j.protocol.Py4JJavaError: An error occurred while calling o18.apply.; : is.hail.utils.package$FatalException: Found problems with SparkContext configuration:; Invalid config parameter 'spark.sql.files.openCostInBytes=': too small. Found 0, require at least 50G; Invalid config parameter 'spark.sql.files.maxPartitionBytes=': too small. Found 0, require at least 50G; 	at is.hail.utils.ErrorHandling$class.fatal(ErrorHandling.scala:5); 	at is.hail.utils.package$.fatal(package.scala:20); 	at is.hail.HailContext$.checkSparkConfiguration(HailContext.scala:104); 	at is.hail.HailContext$.apply(HailContext.scala:162); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978:925,configurat,configuration,925,https://hail.is,https://github.com/hail-is/hail/pull/1507#issuecomment-285792978,1,['configurat'],['configuration']
Deployability,"I tried to look at this, but libsimdpp is completely spamming the diff visualizer. I'm back to thinking we shouldn't include this in the repo. We should either assume it is installed or download it during the build process. I'm inclined to do the former for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579:173,install,installed,173,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261979579,1,['install'],['installed']
Deployability,"I updated the ""before attempts"" trigger because there was a bug where the start and end time on error (i.e. create fails) are both None and then when the instance gets deactivated, the reason is overwritten to deactivated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579919520,1,['update'],['updated']
Deployability,"I updated the HTTP to HTTPS redirect in `/etc/apache2/sites-enabled/000-default.conf` to preserve the sub-domains:. ``` apache; RewriteEngine on; RewriteCond %{HTTPS} off; RewriteRule (.*) https://%{HTTP_HOST}%{REQUEST_URI} [END,QSA,R=permanent]; ```. and I modified the `VirtualHost` set up to use [name-based VirtualHost discrimination](https://httpd.apache.org/docs/2.4/vhosts/name-based.html) based on information from the [TeamCity wiki](https://confluence.jetbrains.com/pages/viewpage.action?pageId=74845225#HowTo...-SetUpTeamCitybehindaProxyServer):. ``` apache; <IfModule mod_ssl.c>; <VirtualHost *:443>; # The ServerName directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsenc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['update'],['updated']
Deployability,I updated the dev deploy too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639040701:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639040701,2,"['deploy', 'update']","['deploy', 'updated']"
Deployability,I updated this one too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5622#issuecomment-477363453:2,update,updated,2,https://hail.is,https://github.com/hail-is/hail/pull/5622#issuecomment-477363453,1,['update'],['updated']
Deployability,"I used filters for the following images when I've run the Azure cleanup script, but we should double check these make sense still in light of changing how we use ""cache"" and there aren't any additional images or ones that we don't want to delete that are in this list:. ```; --filter 'auth:.*' \; --filter 'base:.*' \; --filter 'base_spark_3_2:.*' \; --filter 'batch:.*' \; --filter 'batch-driver-nginx:.*' \; --filter 'batch-worker:.*' \; --filter 'benchmark:.*' \; --filter 'blog_nginx:.*' \; --filter 'ci:.*' \; --filter 'ci-intermediate:.*' \; --filter 'ci-utils:.*' \; --filter 'create_certs_image:.*' \; --filter 'echo:.*' \; --filter 'grafana:.*' \; --filter 'hail-base:.*' \; --filter 'hail-build:.*' \; --filter 'hail-buildkit:.*' \; --filter 'hail-run:.*' \; --filter 'hail-run-tests:.*' \; --filter 'hail-pip-installed-python37:.*' \; --filter 'hail-pip-installed-python38:.*' \; --filter 'hail-ubuntu:.*' \; --filter 'memory:.*' \; --filter 'monitoring:.*' \; --filter 'notebook:.*' \; --filter 'notebook_nginx:.*' \; --filter 'prometheus:.*' \; --filter 'service-base:.*' \; --filter 'service-java-run-base:.*' \; --filter 'test-ci:.*' \; --filter 'test-monitoring:.*' \; --filter 'test-benchmark:.*' \; --filter 'website:.*' \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349:820,install,installed-,820,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1255120349,2,['install'],['installed-']
Deployability,I used pip install - currently struggling to install the pyspark 2.0.2 version after downgrading to spark 2.0.2 . ``$SPARK_HOME is /Users/ih/languages/spark-2.0.2-bin-hadoop2.7/bin``,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678:11,install,install,11,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319702678,2,['install'],['install']
Deployability,I vaguely remember things working on Java 9 when someone joined the group and installed that. Am I imagining that?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896#issuecomment-444558390:78,install,installed,78,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444558390,1,['install'],['installed']
Deployability,I verified that the right foreign key constraints were deleted in my namespace with dev deploy and I didn't touch the foreign key constraints in the other databases.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11938#issuecomment-1162407420:88,deploy,deploy,88,https://hail.is,https://github.com/hail-is/hail/pull/11938#issuecomment-1162407420,1,['deploy'],['deploy']
Deployability,"I verified this works on 1.5.2, 1.6.3 and 2.0.2, but fails if we remove the relevant configuration settings (maxPartitionBytes, parquet.blocks.size).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157:85,configurat,configuration,85,https://hail.is,https://github.com/hail-is/hail/pull/1170#issuecomment-266622157,1,['configurat'],['configuration']
Deployability,"I want cancel_after_n_failures to be on a job group. The things a job group doesn't have which maybe it should is:; - callback; - attributes; - updates. I think updates should be on a batch and not part of a job group. An update can add jobs to multiple job groups. Otherwise, the batches table should only have static fields that apply to the entire batch. I think we can do callbacks and attributes on a job group. I added a PATCH endpoint to be able to update a job group's cancel_after_n_attributes as the hailtop.batch interface was going to automatically generate job groups without any configuration settings. As for the full text search, I think prefix searches are faster with full text search than with a regular index, but I could be wrong. We'd have to benchmark it. > If we made batches simpler, does that ease complexity and decrease code duplication? In particular, what if batches didn't contain jobs at all? Instead, a batch contains exactly one job group. That job group contains zero or more job groups. Job groups manage: resource aggregation, cancellation, etc. I believe my plan is basically already doing this. It might not be clear because I didn't put the migrations in. But basically all of the current batches tables are now indexed by batch_id, job_group_id where the current ""batch"" has job_group_id = 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163:144,update,updates,144,https://hail.is,https://github.com/hail-is/hail/pull/12697#issuecomment-1450603163,6,"['PATCH', 'configurat', 'update']","['PATCH', 'configuration', 'update', 'updates']"
Deployability,I want to get this out for Grace's team to run VEP. No reason we can't cut another release tomorrow.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9984#issuecomment-773391418:83,release,release,83,https://hail.is,https://github.com/hail-is/hail/pull/9984#issuecomment-773391418,1,['release'],['release']
Deployability,"I want to respond to some of your objections to unique_ptr. > If you buy into using std::unique_ptr, then everyone who writes or reads the code has to get; their head around the massively confusing and counter-intuitive concept of move semantics (a; form of assignment which modifies the source) and the somewhat bizarre terminology and syntax; used to express that in C++. I agree that move semantics takes getting used to, but I think it is much too integrated into modern C++ to ignore, going far beyond unique_ptr. Writing interfaces that take advantage of move semantics requires understanding rvalue-references in more detail, but for users of those move-enabled interfaces I think the guidelines are easy to teach: a variable will only be modified by moving if it is explicitly tagged with a `std::move`, so all you have to remember is ""after a `std::move(foo)`, the variable `foo` may only be assigned to or deleted."". > And then you get into a whole host of associated design decisions (I'm holding this as a unique_ptr,; but I want to pass it to a function - should I pass it as a raw pointer ? a raw reference ? a reference; to the unique_ptr ?). Keeping in mind the model that letting a function/class `foo` hold a `unique_ptr<Widget>` means explicitly ""`foo` owns this Widget, and is responsible for deleting it or passing ownership somewhere else"", these questions have pretty clear answers. * If a function `bar` takes a `Widget` but isn't concerned with its lifetime management, it should take its argument as a `Widget*` or `Widget&`, with the usual reasoning to choose between them. The caller owns the widget, and the lifetime of `bar` is nested inside that of its caller, so lifetime management isn't an issue.; * If `bar` takes a `Widget` and needs to take ownership, it should take its argument as a `unique_ptr<Widget>`. This serves as documentation that the function is taking over responsibility for deleting the Widget, in a way enforced by the compiler.; * A `unique_ptr<Wid",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638:452,integrat,integrated,452,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396669638,1,['integrat'],['integrated']
Deployability,"I was able to narrow this down a bit further. The issue appears due to this statement: https://github.com/broadinstitute/gnomad-browser/blob/80430090645ce087aa54d67688a4f0920ad1c8fd/data-pipeline/src/data_pipeline/datasets/gnomad_v3/gnomad_v3_variants.py#L127-L143. `subsets` contains 8 elements: `{'non_cancer', 'tgp', 'controls_and_biobanks', 'non_neuro', None, 'non_topmed', 'hgdp', 'non_v2'}`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609:187,pipeline,pipeline,187,https://hail.is,https://github.com/hail-is/hail/issues/12533#issuecomment-1341807609,1,['pipeline'],['pipeline']
Deployability,"I was having trouble figuring out how to handle the token and the attributes in hailtop.batch_client.aioclient.Batch. When we create an update from a Batch that already existed perhaps in a different process, we don't have the attributes and token. I made a contract where `commit_update` always returns the token and attributes regardless of whether the BatchBuilder already has that infromation. However, we could also get that information available lazily and cache the result. In addition, the `n_jobs` returned to the client are the number of jobs that are committed and not the same as the `n_jobs` in the batches table. Things to do before merging:; 1. Get rid of the batch updates additions to the UI2. ; 2. Double check the GCP LogsExplorer to make sure there are no silent error messages especially with regards to cancellation.; 3. Have @danking look over the SQL stored procedure for `commit_batch_update` to make sure that query is going to perform as good as what is possible given the complexity of the check.; 4. Run a test batch with the old client (I just checked out the current version of main). You need to make sure both create and create-fast are accounted for and succeed. I've been using the following script to make sure we're using the slow path in addition to the fast path with a regular small test job:. ```python3; from hailtop.batch import ServiceBackend, Batch; import secrets. backend = ServiceBackend(billing_project='hail'); b = Batch(backend=backend); # 8 * 256 * 1024 = 2 MiB > 1 MiB max bunch size; for i in range(8):; j1 = b.new_job(); long_str = secrets.token_urlsafe(256 * 1024); j1.command(f'echo ""{long_str}"" > /dev/null'); batch = b.run(); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347:136,update,update,136,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1226043347,2,['update'],"['update', 'updates']"
Deployability,I was just concerned that I hadn't tested dataproc after the changes and didn't want the release to fail. There wasn't anything about the actual release I changed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1885047981:89,release,release,89,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1885047981,2,['release'],['release']
Deployability,I was looking for a PR to be the first deployed ;),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4235#issuecomment-417332020:39,deploy,deployed,39,https://hail.is,https://github.com/hail-is/hail/pull/4235#issuecomment-417332020,1,['deploy'],['deployed']
Deployability,"I was thinking we would get rid of that update completely in MJC. If you want to know the state or the time completed, then you query the tokenized table. We won't store these values explicitly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039648911:40,update,update,40,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039648911,1,['update'],['update']
Deployability,"I went with version 0.2 based on the entry in the [changelog](https://pan.ukbb.broadinstitute.org/docs/changelog):. ```; Version 0.2. Added LD scores and matrices (released Oct 29, 2020).; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10186#issuecomment-804358709:164,release,released,164,https://hail.is,https://github.com/hail-is/hail/pull/10186#issuecomment-804358709,1,['release'],['released']
Deployability,"I went with your suggestions, changing `until` to `to` and using `foldLeft` rather than `flatMap` since it's cleaner on memory (even if slightly slower still). We can always speed these back up in any use case where they become a bottleneck, but right now they won't be. In @maccum 's pruning case there is a rectangle (window) per variant, but also a more appropriate single-pass algorithm. Definitely a performance hit on a rather heinous example using my original code versus your versions, but it's nothing compared to the distributed block matrix computations that follow in pipelines:; ```; val gp = GridPartitioner(512, 100000, 100000); val rnd = new scala.util.Random; def rects = Array.fill(100000){; val i = rnd.nextInt(90000); val j = rnd.nextInt(20000); Array[Long](i, i + 10000, i, i + 10000); }. outer: keep; inner: array. time: 66.642ms; time: 66.549ms; time: 64.039ms; time: 74.439ms. outer: for; inner: array. time: 1.251s; time: 1.389s; time: 1.439s; time: 1.353s. outer: keep; inner: for. time: 723.906ms; time: 715.612ms; time: 721.161ms; time: 707.852ms. outer: for; inner: for. time: 1.820s; time: 1.842s; time: 2.011s; time: 1.718s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3094#issuecomment-372717222:580,pipeline,pipelines,580,https://hail.is,https://github.com/hail-is/hail/pull/3094#issuecomment-372717222,1,['pipeline'],['pipelines']
Deployability,I will make the sparsifying rename/doc update a separate PR. @konradjk I also added another test along the same vein but for a matrix that has been filtered for a subset of rows and columns.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5511#issuecomment-469262861:39,update,update,39,https://hail.is,https://github.com/hail-is/hail/pull/5511#issuecomment-469262861,1,['update'],['update']
Deployability,I will need to manually deploy this when it lands.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8583#issuecomment-616612347:24,deploy,deploy,24,https://hail.is,https://github.com/hail-is/hail/pull/8583#issuecomment-616612347,1,['deploy'],['deploy']
Deployability,I will not wait for #14113. It's a really important improvement but there's also many bug fixes in here that need to be released.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14086#issuecomment-1887439034:120,release,released,120,https://hail.is,https://github.com/hail-is/hail/pull/14086#issuecomment-1887439034,1,['release'],['released']
Deployability,"I would also suggest the following as necessary for an upcoming release:. * #13728. Google's gcsfuse APT repository currently produces 502 Bad Gateway errors when accessed via http, which shows no sign of being resolved any time soon. I've commented on #13728 noting how this PR can work around the problem. At present (since early October), the `batch_worker_image` job always fails with 502 during a hail deployment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602:64,release,release,64,https://hail.is,https://github.com/hail-is/hail/issues/13806#issuecomment-1763650602,2,"['deploy', 'release']","['deployment', 'release']"
Deployability,"I'd argue this is a nicer UX - Having an ""invalid"" or ""unknown"" type lets people with weird alleles (and people do have weird alleles) actually run their pipelines instead of erroring out.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195:154,pipeline,pipelines,154,https://hail.is,https://github.com/hail-is/hail/pull/3491#issuecomment-386425195,1,['pipeline'],['pipelines']
Deployability,"I'd prefer a solution that addresses https://github.com/hail-is/hail/issues/4875. I'm happy to make the required change, but I feel like this answer will look weird to people who normally use `pip install x.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4877#issuecomment-443822085:197,install,install,197,https://hail.is,https://github.com/hail-is/hail/pull/4877#issuecomment-443822085,1,['install'],['install']
Deployability,I'll approve once the deploy-svc one goes in,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4601#issuecomment-431987841:22,deploy,deploy-svc,22,https://hail.is,https://github.com/hail-is/hail/pull/4601#issuecomment-431987841,1,['deploy'],['deploy-svc']
Deployability,I'll deploy to cluster by hand when this is merged,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4446#issuecomment-424727330:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424727330,1,['deploy'],['deploy']
Deployability,I'll do the deploy now!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8760#issuecomment-649874088:12,deploy,deploy,12,https://hail.is,https://github.com/hail-is/hail/pull/8760#issuecomment-649874088,1,['deploy'],['deploy']
Deployability,I'll pick this one up in medium term since NDArray stuff has involved a lot of fleshing out of numpy integration,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3699#issuecomment-550356741:101,integrat,integration,101,https://hail.is,https://github.com/hail-is/hail/issues/3699#issuecomment-550356741,1,['integrat'],['integration']
Deployability,"I'll stew on this a little further and I have yet to look closely at the queries themselves, but my first two questions are:. 1. I'm not opposed to adding tokens to the `batches_n_jobs_in_complete_states` table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. > (C) The new server code deploys with the new mark_batch_complete code that runs periodically. Eventually the newly completed batches since the migration will get set to ""complete"". It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, *then* remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412:554,deploy,deploys,554,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701536412,2,"['deploy', 'update']","['deploys', 'update']"
Deployability,"I'll think about this more, but making the CI version explicit rather than implicit would at least provide a clear progression of necessary deploys. We'd want to tag the CI versions in git so that folks know which commits are necessary to achieve the step-wise transition. Let's find some time to chat next next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985759470:140,deploy,deploys,140,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985759470,1,['deploy'],['deploys']
Deployability,I'll update them to run `python3 -m pip`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-533269724:5,update,update,5,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-533269724,1,['update'],['update']
Deployability,I'm adding an additional migration PR that will put in the backwards compatibility for the updates table etc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1220817788:91,update,updates,91,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1220817788,1,['update'],['updates']
Deployability,I'm beginning to think the configuration stuff is confusing because it ignores the distinction between clients and servers.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252:27,configurat,configuration,27,https://hail.is,https://github.com/hail-is/hail/pull/8513#issuecomment-611765252,1,['configurat'],['configuration']
Deployability,I'm closing this because the pipeline causing these issues had a number of other complicating issues. We can re-open if/when we find a pipeline whose issues we're confident are due to a large alleles array rather than something else.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13584#issuecomment-1738201066:29,pipeline,pipeline,29,https://hail.is,https://github.com/hail-is/hail/issues/13584#issuecomment-1738201066,2,['pipeline'],['pipeline']
Deployability,I'm closing this while debugging with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-579013590:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-579013590,1,['deploy'],['deploy']
Deployability,"I'm convinced. @johnc1231, also change ""TruncatedBeta"" to ""TruncatedBetaDist"" global annotation. Be sure to update the documentation as well.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1332#issuecomment-276719598:108,update,update,108,https://hail.is,https://github.com/hail-is/hail/issues/1332#issuecomment-276719598,1,['update'],['update']
Deployability,"I'm currently expanding BlockMatrix binary ops to work naturally between BlockMatrix and 2d ndarray, with the useful ones for LMM being broadcasting ndarray over rows or columns of BlockMatrix. That'll round out the linear algebra, so that next week I'll begin refactoring the pipeline, with the interesting bits being the form of Python form of the small ""global model"" (initially using Scala side as black box) and then applying the local model to the pair or RowMatrices `X` and `PX` (read from BlockMatrices) to get per variant results.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3114#issuecomment-373451854:277,pipeline,pipeline,277,https://hail.is,https://github.com/hail-is/hail/pull/3114#issuecomment-373451854,1,['pipeline'],['pipeline']
Deployability,I'm dev deploying now to take a look!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9982#issuecomment-774142818:8,deploy,deploying,8,https://hail.is,https://github.com/hail-is/hail/pull/9982#issuecomment-774142818,1,['deploy'],['deploying']
Deployability,"I'm fairly certain I know understand this and the AoU VDS creation issue. In Dataproc versions 1.5.74, 2.0.48, and 2.1.0, Dataproc introduced ""memory protection"" which is a euphemism for a newly aggressive OOMKiller. When the OOMKiller kills the JVM driver process, there is no hs_err_pid...log file, no exceptional log statements, and no clean shutdown of any sockets. The process is simply SIGTERM'ed and then SIGKILL'ed. From Hail 0.2.83 through Hail 0.2.109 (released February 2023), Hail was pinned to Dataproc 2.0.44. From Hail 0.2.15 onwards, `hailctl dataproc`, by default, reserves 80% of the advertised memory of the driver node for the use of the Hail Query Driver JVM process. For example, Google advertises that an n1-highmem-8 has 52 GiB of RAM, so Hail sets the `spark:spark.driver.memory` property to `41g` (we always round down). Before aggressive memory protection, this setting was sufficient to protect the driver from starving itself of memory. Unfortunately, Hail 0.2.110 upgraded to Dataproc 2.1.2 which enabled ""memory protection"". Moreover, in the years since Hail 0.2.15, the memory in use by system processes on Dataproc driver nodes appears to have increased. Due to these two circumstances, the driver VM's memory usage can grow high enough to trigger the OOMKiller before the JVM triggers a GC. Consider, for example, these slices of the syslog of the n1-highmem-8 driver VM of a Dataproc cluster:. ```; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: earlyoom v1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:463,release,released,463,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,1,['release'],['released']
Deployability,"I'm fine removing copy-paste-tokens. They were for a prototype with Terra. We obviously are pursuing a different approach now. Hmm. I suppose old versions of hailctl have no way to know that the fix is to upgrade to a newer version of hailctl? Like, the server can't send a message in the auth failure? We can just ask our local users to upgrade. As long as there's a stable & robust version of query that they can rely on, I think they're happy to upgrade. Which version of hailctl is compatible with new auth?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024:205,upgrade,upgrade,205,https://hail.is,https://github.com/hail-is/hail/issues/13531#issuecomment-1767018024,3,['upgrade'],['upgrade']
Deployability,"I'm going to benchmark this today and if nothing has changed since Cotton's benchmarks, I think we're good for a merge. I had hoped to begin working with Grace's team to use this tool on Thursday. I can build a wheel, since I don't anticipate a release happening so fast.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10752#issuecomment-896959032:245,release,release,245,https://hail.is,https://github.com/hail-is/hail/pull/10752#issuecomment-896959032,1,['release'],['release']
Deployability,I'm going to change this branch to instead add `pre-commit` with `black` as one of its commit hooks. Developers can choose to `pre-commit install` which will set up useful hooks and on commit will run `black` over any modified files for directories that opt into it (which I'll start as just CI).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9931#issuecomment-769157671:138,install,install,138,https://hail.is,https://github.com/hail-is/hail/pull/9931#issuecomment-769157671,1,['install'],['install']
Deployability,I'm going to temporarily close this while I debug with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8961#issuecomment-644384619:59,deploy,deploy,59,https://hail.is,https://github.com/hail-is/hail/pull/8961#issuecomment-644384619,1,['deploy'],['deploy']
Deployability,I'm going to test this with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7909#issuecomment-575790241:32,deploy,deploy,32,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575790241,1,['deploy'],['deploy']
Deployability,I'm gonna close on account of this is the dev requirements. We don't mandate developer python installation systems and they might be using a package manager for this. Developers should update using whatever system they're using to manage python.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12896#issuecomment-1518317989:94,install,installation,94,https://hail.is,https://github.com/hail-is/hail/pull/12896#issuecomment-1518317989,2,"['install', 'update']","['installation', 'update']"
Deployability,I'm gonna force merge this since it only changes the deploy script and I'm not sure the PR build job will finish before the deploy job re-deploys.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229:53,deploy,deploy,53,https://hail.is,https://github.com/hail-is/hail/pull/4449#issuecomment-424797229,3,['deploy'],"['deploy', 'deploys']"
Deployability,I'm good with this if @tpoterba and @johnc1231 are especially with the installing Hail script.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11187#issuecomment-1006773562:71,install,installing,71,https://hail.is,https://github.com/hail-is/hail/pull/11187#issuecomment-1006773562,1,['install'],['installing']
Deployability,"I'm good with this, I'll approve whenever you're ready to handle CI maybe breaking. Don't want to mess up dev deploys somehow.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6973#issuecomment-526692251:110,deploy,deploys,110,https://hail.is,https://github.com/hail-is/hail/pull/6973#issuecomment-526692251,1,['deploy'],['deploys']
Deployability,I'm having trouble replicating this. Do you happen to remember the pipeline you were executing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13064#issuecomment-1559836307:67,pipeline,pipeline,67,https://hail.is,https://github.com/hail-is/hail/issues/13064#issuecomment-1559836307,1,['pipeline'],['pipeline']
Deployability,I'm intending to debug/fix this today. I'll update this evening about progress.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10722#issuecomment-903834761:44,update,update,44,https://hail.is,https://github.com/hail-is/hail/issues/10722#issuecomment-903834761,1,['update'],['update']
Deployability,"I'm losing track of all the threads. Trying to summarize:; - Leave the CORS stuff, I don't understand it well enough to have an opinion anyway.; - Global gzip settings with gzip_min_length.; - Back out Docker changes, separately PR upgrade to nginx on Debian (would be my preference).; - Leave auth notebook commented out (although it makes me uncomfortable) and let's keep discussing how to solve it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5244#issuecomment-460333195:232,upgrade,upgrade,232,https://hail.is,https://github.com/hail-is/hail/pull/5244#issuecomment-460333195,1,['upgrade'],['upgrade']
Deployability,"I'm merging into https://github.com/hail-is/hail/pull/13325, but @daniel-goldstein , I'd still like your review of the updated code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13326#issuecomment-1656118980:119,update,updated,119,https://hail.is,https://github.com/hail-is/hail/pull/13326#issuecomment-1656118980,1,['update'],['updated']
Deployability,I'm missing something. Why shouldn't test deployments benefit from and contribute to the cache? Why isolate them somewhere else?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11907#issuecomment-1152542361:42,deploy,deployments,42,https://hail.is,https://github.com/hail-is/hail/pull/11907#issuecomment-1152542361,1,['deploy'],['deployments']
Deployability,"I'm not sure I follow. Can you elaborate more on ""We aren't including the test repo on deploy""? I thought we were always testing everything on deploy (ie running Hail, batch, pipeline tests etc).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872:87,deploy,deploy,87,https://hail.is,https://github.com/hail-is/hail/pull/7814#issuecomment-571721872,3,"['deploy', 'pipeline']","['deploy', 'pipeline']"
Deployability,"I'm not sure I understand. `hail/python/hail/docs/_templates/layout.html` references `/navbar.css` which should be present on the deployed site. I might misunderstand `conf.py`, but, AFAICT, this makes unused copies of navbar.css and the PNG: https://hail.is/docs/0.2/navbar.css and https://hail.is/docs/0.2/hail-logo-cropped.png. The page at https://hail.is/docs/0.2/ loads `hail.is/navbar.css`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411:130,deploy,deployed,130,https://hail.is,https://github.com/hail-is/hail/pull/8956#issuecomment-644213411,1,['deploy'],['deployed']
Deployability,"I'm not sure what the best/easiest thing to do is. I think we can accomplish the same thing by copying and pasting the delete tables step (runImage) into a dev branch while testing. . The other easiest thing I can think of is to add a `run_if_requested=True` option to each build step config and modify ci to skip over steps that aren't specifically requested in dev deploy. I don't think a new step is a good idea because what if I want an optional runImage step or an optional Deploy step, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061:367,deploy,deploy,367,https://hail.is,https://github.com/hail-is/hail/pull/7705#issuecomment-565009061,2,"['Deploy', 'deploy']","['Deploy', 'deploy']"
Deployability,"I'm not sure what to do about this. I don't think I can update this to use Python semantics instead of java semantics (n = number of times to split, instead of number of split results) without breaking pipelines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7893#issuecomment-613679220:56,update,update,56,https://hail.is,https://github.com/hail-is/hail/issues/7893#issuecomment-613679220,2,"['pipeline', 'update']","['pipelines', 'update']"
Deployability,"I'm not sure whether we should add this proactively or not, and to be clear I don't intend users to generally use this, but this is the best solution I can think of so far for @illusional's question about what to do when we have removed support for the old hail access tokens but users still are forced to run a pipeline on an old hail version. Old hail access tokens are stored in JSON in `~/.hail/tokens.json`, so I believe (though have not yet tested, that the following should allow a user to use an old version of hail against a version of batch that only supports cloud access tokens:. On the *new* version of hail, run. ```; hailctl auth login; hailctl auth print-access-token | jq -R -c '{ default: . }' > ~/.hail/tokens.json; ```. Then switch to an old version and proceed as usual (but don't run `hailctl auth login`!).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528:312,pipeline,pipeline,312,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1783136528,1,['pipeline'],['pipeline']
Deployability,"I'm now happy with the updates, ready for another look",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-233967833:23,update,updates,23,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-233967833,1,['update'],['updates']
Deployability,"I'm personally OK not supporting a long lineage of Hail versions on QoB. If we don't have to make people update for a few versions, great, but we should be OK making people run a pip upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12941#issuecomment-1527997196:105,update,update,105,https://hail.is,https://github.com/hail-is/hail/pull/12941#issuecomment-1527997196,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,"I'm pretty sure that Hail isn't being installed correctly. But since you're not the one installing it, we can't really help. Let's continue discussion on the forum: https://discuss.hail.is/t/using-hail0-2-on-azure-databrick/1128",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7298#issuecomment-542178553:38,install,installed,38,https://hail.is,https://github.com/hail-is/hail/issues/7298#issuecomment-542178553,2,['install'],"['installed', 'installing']"
Deployability,I'm putting a wip tag on this so we make sure we dev deploy and test the behavior one last time before merging.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10630#issuecomment-872355294:53,deploy,deploy,53,https://hail.is,https://github.com/hail-is/hail/pull/10630#issuecomment-872355294,1,['deploy'],['deploy']
Deployability,I'm putting the WIP tag on until we get the batch database migration deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13980#issuecomment-1798843967:69,deploy,deployed,69,https://hail.is,https://github.com/hail-is/hail/pull/13980#issuecomment-1798843967,1,['deploy'],['deployed']
Deployability,"I'm rather inclined to wait for @cseed's commentary on this before merging it. I'm not sure if this is the best solution, but it seems like a viable way to hold necessary configuration parameters.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728:171,configurat,configuration,171,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290197728,1,['configurat'],['configuration']
Deployability,"I'm testing out the dev deploy now, but realized this is super slow because Dan's PR changing how the base image is done with a new hail_ubuntu image is in now. Just FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-723127410:24,deploy,deploy,24,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723127410,1,['deploy'],['deploy']
Deployability,"I'm thinking that I want to have this:. Basic setup for the domain, remote_tmpdir, billing_project, and only select one region; ```; hailctl init; ```. Which will prompt for:; 1. GCP project; 2. Hail domain; 3. region; 4. remote tmpdir location. And set:; - domain; - batch/billing_project; - batch/regions; - batch/remote_tmpdir; - batch/backend; - query/backend. And then print out a message with the default settings with instructions on how to change any of the settings and warnings about the configuration by using `hailctl config set ...`. I'll have the autocomplete PR merged (hopefully) by then so we can make that nice. The trial billing project is automatically used and we don't prompt for that. Then we'll have optional flags for components to configure; ```; hailctl init --container-registry --requester-pays-buckets --extra-query-settings --extra-batch-settings; ```; This would prompt for the same as above as well as advanced regions settings, query settings, and create the container registry. And lastly; ```; hailctl config check; ```; which prints warnings about misspecified regions and tmpdirs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451:498,configurat,configuration,498,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1650549451,1,['configurat'],['configuration']
Deployability,"I'm using it locally (installed using `./gradlew installDist`) but working on our cluster, rather than with `spark-submit`. I have not loaded the spark module on the cluster. Is Hail installing its own spark libraries? Is there a way to configure the tmp dir for these?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251732459:22,install,installed,22,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251732459,3,['install'],"['installDist', 'installed', 'installing']"
Deployability,"I'm using java 1.8,; `java version ""1.8.0_71""; Java(TM) SE Runtime Environment (build 1.8.0_71-b15); Java HotSpot(TM) 64-Bit Server VM (build 25.71-b15, mixed mode); `; Although I realized Spark was the requirement, however, I'm unsure how to install spark2.1.1. I have downloaded and unzipped the file spark-2.1.1-bin-hadoop2.7. UPDATE: I reinstalled JDK8 and now the :compileScala error has gone away. Build was successful.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246:243,install,install,243,https://hail.is,https://github.com/hail-is/hail/issues/1327#issuecomment-302834246,2,"['UPDATE', 'install']","['UPDATE', 'install']"
Deployability,"I've added some more information. I haven't quite figured out a good way to present all this. There seems to be three distinct things:; - the mounting of secrets to paths in the pods (documented as code in `deployment.yaml`s); - the name of k8s secrets, their contents, and the meaning of the contents (specifically what the applications expect of it). The latter would be best documented with scripts that regenerate the secrets from some root secret. We can [programmatically generate oauth tokens](https://developer.github.com/v3/oauth_authorizations/) (which are different from personal access tokens) with username and password authentication. A recreation script could use one privileged key that has access to username/password for each hail test user. That is used to generate auth-tokens (we might need to adapt our code to use oauth tokens instead of personal access tokens). GCP service account keys can be generated programmatically. Unfortunately, there seems to be a little bit of work involved in using OAuth instead of personal access tokens. We have to register our ""app"". I can look into this sometime soon. I'll create an issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141:207,deploy,deployment,207,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430432141,1,['deploy'],['deployment']
Deployability,"I've addressed the two comments: now using an `entry_fields` parameter and throwing an error if 'dosage' is requested and any variant is multi-allelic. Docs updated accordingly. I considered setting dosage on multi-allelics to missing rather than throwing an error, but I think error is safest since I could imagine the missingness leading to QC confusion, and if users want dosage in the presence of multi-allelics than they should either use a custom expression or split and then use `hl.gp_dosage`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913:157,update,updated,157,https://hail.is,https://github.com/hail-is/hail/pull/2930#issuecomment-366829913,1,['update'],['updated']
Deployability,I've asked Wenhan to run this pipeline with a JAR that has extra debugging information enabled https://github.com/hail-is/hail/compare/main...danking:hail:debug-13979.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1834228219:30,pipeline,pipeline,30,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1834228219,1,['pipeline'],['pipeline']
Deployability,"I've manually deployed site to fix the broken links, but site will continue to point at 05a792599, while hail itself will successfully deploy (because it is deployed before batch is).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790:14,deploy,deployed,14,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594790,3,['deploy'],"['deploy', 'deployed']"
Deployability,"I've now addressed the sequence dictionary point made by @laserson, as well as rebasing on master. . Persisting annotations is a challenge as it looks like the schema is dynamic - is that right? I'm not sure how that will work with Kudu, which expects a fixed schema at table-creation time. . BTW here's an updated link to the Kudu issue I mentioned earlier: https://issues.apache.org/jira/browse/KUDU-383. Note that it's not a blocker for this PR as there's a workaround.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-220610690:307,update,updated,307,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220610690,1,['update'],['updated']
Deployability,"I've rebased John's branch, added the default block_size change by @shulik7 at Nirvana, and made a few additional small changes including adding the @typecheck_method and @record_method decorators (the latter is now required, else history goes histrionic). The configuration file, init script, and resource files are in`gs://hail-common/nirvana`. The init script `gs://hail-common/nirvana/nirvana-init-GRCh37.sh` makes local copies of the resource files and .net: ; ```; #!/bin/bash. mkdir -p /nirvana/Data/Cache; mkdir -p /nirvana/Data/References; mkdir -p /nirvana/Data/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs:/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:261,configurat,configuration,261,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['configurat'],['configuration']
Deployability,I've removed the `make test-deploy` stuff to simplify this PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110:28,deploy,deploy,28,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-481837110,1,['deploy'],['deploy']
Deployability,"I've seen this before, the PyPI databases are out of sync. You can see the latest with list but not install",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418776942:100,install,install,100,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418776942,1,['install'],['install']
Deployability,I've updated docs and code to ensure that NaN and infinite values are caught.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2893#issuecomment-365318621:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/2893#issuecomment-365318621,1,['update'],['updated']
Deployability,I've updated the PR accordingly — feel free to wordsmith the warning message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14700#issuecomment-2384183254:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/14700#issuecomment-2384183254,1,['update'],['updated']
Deployability,I've updated the code to address the easy comments. The annotated.globalSignature and hadoop_writer ones I don't know how and whether to fix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1991#issuecomment-316414396:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/1991#issuecomment-316414396,1,['update'],['updated']
Deployability,I've updated the docs. ; One remaining issue is how to allow multiple `--plugin` options (http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html) ?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/1712#issuecomment-298847712,1,['update'],['updated']
Deployability,"I've updated this to allow stream consumers to choose an allocation strategy independently of the producer: the producer is written to allocate temporary regions that it owns, as needed, but the consumer can override that behavior to actually put all temporary allocations in a single region. This is implemented via the two abstract interfaces `StagedRegion` and `StagedOwnedRegion`, which have two implementations, one of which can create temporary regions, and one which cannot. The producer is written using the interface, and the consumer passes in one of the concrete implementations, determining the allocation strategy. Currently, only the non-allocating instance is ever used, which generates the same code we did before, so this should have zero run-time effect, even if we continue to update more stream nodes to use smarter region management. We can figure out separately how to introduce the other strategy (use less memory, but with more allocator overhead) in a controlled way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-664591165,2,['update'],"['update', 'updated']"
Deployability,"I've updated this with the code from Hadoop-BAM (in 7.6.0), so it's ready for review. Can you take a look please @cseed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/426#issuecomment-229917065:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/426#issuecomment-229917065,1,['update'],['updated']
Deployability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/gi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3778,deploy,deploy-,3778,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,IL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c H,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9254,deploy,deploy-,9254,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + ec,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:11978,deploy,deploy,11978,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy']
Deployability,IP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'REMOTE is unset or empty'; REMOTE is unset or empty; + exit 1; make: *** [release] Error 1. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:15427,deploy,deploy-,15427,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,8,"['deploy', 'release']","['deploy-', 'release']"
Deployability,ITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6508,deploy,deploy-,6508,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"If this passes, I'll update the setup.py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6214#issuecomment-497073906:21,update,update,21,https://hail.is,https://github.com/hail-is/hail/pull/6214#issuecomment-497073906,1,['update'],['update']
Deployability,"If we always mount a deploy config (which sounds like a great idea), I think it would be nice to also always mount the user tokens too (similar to how the GSA key is always mounted). That way, nested batches ""just work"". But happy to make it a configurable option, if you prefer that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-768605979:21,deploy,deploy,21,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-768605979,1,['deploy'],['deploy']
Deployability,"In addition to the main change:; - I made all the global configuration options editable now that we have flexible billing,; - We now check the worker cores and standing worker cores against the worker type to make sure they are valid (there is no highmem/highcpu-1).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850:57,configurat,configuration,57,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674977850,1,['configurat'],['configuration']
Deployability,"In anticipation of changes to the CI system, I've moved the local tutorial files to `/usr/local/hail-tutorial-files` and updated the CI configuration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114:121,update,updated,121,https://hail.is,https://github.com/hail-is/hail/pull/1374#issuecomment-279443114,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,"In fact I had Firth mixed into this branch but ripped it out when it was making the update too complicated. Whereas Wald, LRT, and score only require fitting the null model once, the Firth LRT requires fitting the null and full models per variant. So plan is to add Firth, support for subsetting samples per variant (rather than imputing missing genotypes), and better tests by comparing Hail and R results for randomly generated datasets. I'd also like to add more [optional] user control on convergence criteria and on what's returned in annotations (for example, statistics for the other covariates...these are computed anyway...also on the null fit in globals). And there are ways to speed up the numerical linear algebra, this is a first pass. Do you have thoughts on Firth LRT versus Wald? My understanding is that LRT is better calibrated for p-value, but would the Wald standard error for Firth be a useful annotation as well? Also, check out v1 of doc: ; https://github.com/jbloom22/hail/blob/jb_logreg3/docs/LogisticRegression.md",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-239686959:84,update,update,84,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239686959,1,['update'],['update']
Deployability,"In fact, changing `px.t * dpa` to `(dpa.t * px).t` also resolves the bug, without the need to copy `px`. Updated accordingly.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4229#issuecomment-416813716:105,Update,Updated,105,https://hail.is,https://github.com/hail-is/hail/pull/4229#issuecomment-416813716,1,['Update'],['Updated']
Deployability,"In particular, isn't it possible that you have the n-1th and nth job racing to complete. Everyone else is already done. Call the n-1th job's transaction T1 and the nth job's transaction T2. Both race down to this statement in MJC:; ```; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id AND n_completed = batches.n_jobs;; ```. That will now need to have a sum(n_completed) over all tokens. The isolation level is repeatable read. Assume T1 and T2 generate non equal tokens. T1 and T2 may both snapshot the state of the database before either T1 or T2 executes. T1 and T2 will necessarily see the changes they've made (which affect distinct rows because they have distinct tokens), but neither is required to see the changes the other has made. I think the only way to guarantee that at least one of T1 or T2 sees the database with sum(n_completed) == n_jobs is for both of them to LOCK IN SHARE MODE when doing the sum(n_completed). That will cause lock contention. Maybe that's OK? In the worst case you could have this happen:. 1. Job 1 executes all the way to just before the sum(n_completed).; 2. Job 2 executes all the way to modifying the volatile state.; 3. Job 1 blocks waiting for Job 2 to modify the volatile state.; 4. Job 3 executes all the way to modifying the volatile state.; 5. Job 1 and 2 now wait for Job 3 to modifying the volatile state.; 6. ...; 7. Job 1, 2, 3, n-1 now all wait for Job n to modify the volatile state.; 8. Job 1...n finally execute the sum, all in parallel. I guess that's not terrible, it just means that the latency of Job 1 is extended as long as other jobs can race in before it grabs a shared lock on all the rows.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916:237,UPDATE,UPDATE,237,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039642916,1,['UPDATE'],['UPDATE']
Deployability,"In particular, it should link to the index.html for the deploy,e.g. `gs://hail-ci-0-1/deploy/f69da9402030173e68b33608f38d94c45a1d7928/index.html`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4437#issuecomment-424558979:56,deploy,deploy,56,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-424558979,2,['deploy'],['deploy']
Deployability,"In trying to test this (from your branch, ran pip install on /hail/python just in case). ```sh; (hail) alex:~/projects/hail/hail/python:$ hailctl dev deploy -b cseed:batch-web -s deploy_auth,deploy_router,deploy_notebook2; Traceback (most recent call last):; File ""/miniconda3/envs/hail/bin/hailctl"", line 11, in <module>; sys.exit(main()); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/__main__.py"", line 100, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/cli.py"", line 52, in main; cli.main(args); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 66, in main; loop.run_until_complete(submit(args)); File ""/miniconda3/envs/hail/lib/python3.6/asyncio/base_events.py"", line 484, in run_until_complete; return future.result(); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 57, in submit; batch_id = await ci_client.dev_deploy_branch(args.branch, steps); File ""/Users/alex/projects/hail/hail/python/hailtop/hailctl/dev/deploy/cli.py"", line 46, in dev_deploy_branch; self._deploy_config.url('ci', '/api/v1alpha/dev_deploy_branch'), json=data) as resp:; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 1005, in __aenter__; self._resp = await self._coro; File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client.py"", line 581, in _request; resp.raise_for_status(); File ""/miniconda3/envs/hail/lib/python3.6/site-packages/aiohttp/client_reqrep.py"", line 942, in raise_for_status; headers=self.headers); aiohttp.client_exceptions.ClientResponseError: 500, message='Internal Server Error'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733:50,install,install,50,https://hail.is,https://github.com/hail-is/hail/pull/7064#issuecomment-532037733,5,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Increasing the executor memory per core to 20G/core seemed to help get by this memory error. . It would be useful to have some rule of thumbs for estimating memory requirements based on number of samples and variants. spark-submit --verbose --master yarn --deploy-mode client \; --num-executors 12\; --executor-cores 4\; --jars $JAR \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator \; --conf ""spark.driver.extraClassPath=$JAR"" \; --conf ""spark.executor.extraClassPath=$JAR"" \; --executor-memory 80G\; --driver-memory 60g\; --driver-cores 1\; --name ""$1"" \; --conf spark.yarn.executor.memoryOverhead=8000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120\",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303:257,deploy,deploy-mode,257,https://hail.is,https://github.com/hail-is/hail/issues/3463#issuecomment-385433303,1,['deploy'],['deploy-mode']
Deployability,Installation of 0.1 works; `git clone -b 0.1 https://github.com/broadinstitute/hail.git`; The only thing to add is that you have to do `chmod u+x gradlew` in order to `$ ./gradlew -Dspark.version=2.0.2 shadowJar`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2067#issuecomment-320256546:0,Install,Installation,0,https://hail.is,https://github.com/hail-is/hail/issues/2067#issuecomment-320256546,1,['Install'],['Installation']
Deployability,"Interesting: I tried in a new session (after checking out master, installing-editable), and this time I could break things with out-of-bounds slices, but `hl.eval(a[0:a.shape[0],0:1]) ` and `hl.eval(a[0:a.shape[0],0:2])` worked ok. So what's going on?. ```python; In [22]: hl.eval(a[0:a.shape[0],0:1]) ; Out[22]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [23]: hl.eval(a[0:a.shape[0],0:1]) ; Out[23]: ; array([[1],; [2],; [3],; [4],; [5]], dtype=int32). In [24]: a = a.T . In [25]: hl.eval(a) ; Out[25]: ; array([[ 1, 2, 3, 4, 5],; [ 7, 6, 8, 9, 10]], dtype=int32). In [26]: a = a.T . In [27]: hl.eval(a) ; Out[27]: ; array([[ 1, 7],; [ 2, 6],; [ 3, 8],; [ 4, 9],; [ 5, 10]], dtype=int32). In [28]: hl.eval(a[0:a.shape[0],0:1]) ; Out[28]: ; array([[1],; [7],; [0],; [2],; [0]], dtype=int32). In [32]: hl.eval(a[0:a.shape[0],0:1]) ; Out[32]: ; array([[ 1],; [ 7],; [ 0],; [ 4],; [32749]], dtype=int32); ``` . totally broken. Seems like 2 problems: 1) out of bounds checks not being done on inner dimension. 2) strides get scrambled between transposes, or something isn't being cleared after transposition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587:66,install,installing-editable,66,https://hail.is,https://github.com/hail-is/hail/issues/9144#issuecomment-663267587,1,['install'],['installing-editable']
Deployability,Is it possible to check this with dev deploy?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10190#issuecomment-799425185:38,deploy,deploy,38,https://hail.is,https://github.com/hail-is/hail/pull/10190#issuecomment-799425185,1,['deploy'],['deploy']
Deployability,"Is there a possibility of adding an exception for users who need to pass a SparkContext to `hl.init`? . In our use case, our notebooks use the sparkmagic kernel to communicate with livy running on multiple clusters. Sparkmagic automatically creates a SparkContext when connecting to livy on the cluster master node (AWS EMR). And in releases prior to 0.2.20 we were passing that SparkContext to `hl.init`. . In our case, the JAR is in our class path.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7080#issuecomment-536709685:333,release,releases,333,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536709685,1,['release'],['releases']
Deployability,Is there an easy way to specify at the end once migrations have completed successfully to restart the deployments you cancelled once the migrations have succeeded? Maybe have k8s get the deployment yaml configuration before deleting each deployment and then reapply the yamls for each shutdown step in order. I'm worried this PR will break all dev deploys with migrations in them as you can't just do `make -c batch deploy`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956:102,deploy,deployments,102,https://hail.is,https://github.com/hail-is/hail/pull/7855#issuecomment-573683956,6,"['configurat', 'deploy']","['configuration', 'deploy', 'deployment', 'deployments', 'deploys']"
Deployability,Is this high prio enough to get in before we release 0.2.26?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7368#issuecomment-545942557:45,release,release,45,https://hail.is,https://github.com/hail-is/hail/pull/7368#issuecomment-545942557,1,['release'],['release']
Deployability,"Is this still an issue without command line? If so, can we update the issue to python terms?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/322#issuecomment-291943984:59,update,update,59,https://hail.is,https://github.com/hail-is/hail/issues/322#issuecomment-291943984,1,['update'],['update']
Deployability,"Issue I ran into: I need to pass each child IR's ptype to the join point loop body. There isn't a very easy way to get one PType out of an IndexedSeq[PType] of ptypes. For instance, even though srvb.advance() runs inside the body of the JoinPoint loop, its staticIdx does not update (since the loop only iterates at runtime). I want to pass Code[IndexedSeq[PType]] but that isn't possible. Will work on tomorrow. . edit:. I think I need to do something like this to access individual ptypes (but within toEmitTriplet loop body): . ```scala; case x@MakeStream(elements, t) =>; val e = coerce[PStreamable](x.pType).elementType; implicit val eP = TypedTriplet.pack(e); sequence(elements.map { ir => TypedTriplet(e, emitIR(ir, env)) }); .map(_.untyped); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692:276,update,update,276,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-583963692,1,['update'],['update']
Deployability,"It adds the last applied configuration annotation which is used to know if a subsequent configuration is different. ```; (base) # kubectl -n default create secret generic foo \ ; --save-config --dry-run=client -o yaml; apiVersion: v1; kind: Secret; metadata:; annotations:; kubectl.kubernetes.io/last-applied-configuration: |; {""kind"":""Secret"",""apiVersion"":""v1"",""metadata"":{""name"":""foo"",""creationTimestamp"":null}}; creationTimestamp: null; name: foo; namespace: default; (base) # kubectl -n default create secret generic foo \; --dry-run=client -o yaml ; apiVersion: v1; kind: Secret; metadata:; creationTimestamp: null; name: foo; namespace: default. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10217#issuecomment-809406141:25,configurat,configuration,25,https://hail.is,https://github.com/hail-is/hail/pull/10217#issuecomment-809406141,3,['configurat'],['configuration']
Deployability,"It doesn't build out of the box, but it should. That just needs py4j and breeze version numbers. Separately, we should probably upgrade cloudtools to Dataproc 1.3-deb9 which has Spark 2.3.1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4578#issuecomment-431608757:128,upgrade,upgrade,128,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431608757,1,['upgrade'],['upgrade']
Deployability,"It doesn't seem like headless mode is in effect, at least in the most recent published image. Will grab this and play around with it. Tested Dan's image in app.hail.is, seems to work, except for all of the .js/.css resources; first guess is SSL, but it's clearly a diff issue. I can't connect to your workers, can to his. Will update in a bit. Yours:; (notebook) alexkotlar:~/projects/hail-clone/notebook-api:$ k logs notebook-worker-5xq2w -f; [I 21:29:01.483 NotebookApp] Writing notebook server cookie secret to /home/jovian/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:29:03.742 NotebookApp] Serving notebooks from local directory: /home/jovian; [I 21:29:03.743 NotebookApp] The Jupyter Notebook is running at:; [I 21:29:03.743 NotebookApp] http://localhost:8888/instance/notebook-worker-service-qzppk/?token=...; [I 21:29:03.743 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [W 21:29:03.750 NotebookApp] No web browser found: could not locate runnable browser. Dan’s; [I 21:44:38.439 NotebookApp] Writing notebook server cookie secret to /home/jovyan/.local/share/jupyter/runtime/notebook_cookie_secret; [I 21:44:38.808 NotebookApp] [jupyter_nbextensions_configurator] enabled 0.4.1; [I 21:44:38.898 NotebookApp] Jupyter-Spark enabled!; [I 21:44:38.942 NotebookApp] JupyterLab extension loaded from /opt/conda/lib/python3.6/site-packages/jupyterlab; [I 21:44:38.942 NotebookApp] JupyterLab application directory is /opt/conda/share/jupyter/lab; [I 21:44:38.945 NotebookApp] Serving notebooks from local directory: /home/jovyan; [I 21:44:38.945 NotebookApp] The Jupyter Notebook is running at:; [I 21:44:38.946 NotebookApp] http://(notebook-worker-v7fr4 or 127.0.0.1):8888/instance/notebook-worker-service-sv5jl/?token=...; [I 21:44:38.946 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).; [I 21:44:55.324 NotebookApp] 302 GET /instance/notebook-worker-service-sv5jl/?acce",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942:327,update,update,327,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460092942,1,['update'],['update']
Deployability,"It is mighty fishy that both azure and google failed the callback test. What are we missing? If MJC returns, then the database was clearly updated. Subsequent DB queries should see those changes. total_jobs_in_batch won't change during the lifetime of the batch, so that should be correct (though we should probably LOCK IN SHARE MODE anyway). Assuming I'm reading the [reference manual](https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html) correctly, that select should see the result of the UPDATE *or a later state*. The updates to a single row are serial. So there must exist a transaction that takes it from n_jobs-1 to n_jobs. That transaction thus must see n_jobs for new_n_completed. That transaction thus ought to update batches. Once that transaction is committed the subsequent query for notification should see the changes...",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121:139,update,updated,139,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1040809121,4,"['UPDATE', 'update']","['UPDATE', 'update', 'updated', 'updates']"
Deployability,"It looks good except for the behavior I was seeing. I can try redeploying my version if you think that's what the issue is (the edit page ""update"" doesn't return a page with the description I just added even though it's there on the resources page).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-726289411:139,update,update,139,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-726289411,1,['update'],['update']
Deployability,It looks like `gs://gcp-public-data--gnomad/papers/2019-tx-annotation/pre_computed/all.possible.snvs.tx_annotated.021520.ht` has been updated now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-820648738:134,update,updated,134,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-820648738,1,['update'],['updated']
Deployability,"It looks like it is failing when trying to start Java. Do you have Java installed? What version? This is what I get on my mac:. ```; $ java -version; java version ""1.8.0_31""; Java(TM) SE Runtime Environment (build 1.8.0_31-b13); Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319673723:72,install,installed,72,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319673723,1,['install'],['installed']
Deployability,"It looks like it's called `netcdf` on `brew`. ```; dking@wmb16-359 # brew info netcdf; netcdf: stable 4.6.0 (bottled); Libraries and data formats for array-oriented scientific data; https://www.unidata.ucar.edu/software/netcdf; Not installed; From: https://github.com/Homebrew/homebrew-core/blob/master/Formula/netcdf.rb; ==> Dependencies; Build: cmake ✘; Required: hdf5 ✘, gcc ✘; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377930479:232,install,installed,232,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377930479,1,['install'],['installed']
Deployability,"It looks like permissions for deleting disks and VMs are broken for the `delete_batch_instances` CI step. This job also hung for a long time and then got restarted. There's some other wonky things about this PR, but it just seems like the main issue was the Batch deployment was cancelled mid-run and the driver didn't have time to cleanup those 2 VMs that weren't responding before being shut off. Then the cleanup step isn't actually working so they didn't get cleaned up. The only remaining question I have is why these VMs weren't starting up correctly. There were at least 5 in this one PR that didn't start up in time before the driver was shut down. https://batch.hail.is/batches/7908998/jobs/207",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569:264,deploy,deployment,264,https://hail.is,https://github.com/hail-is/hail/issues/13554#issuecomment-1737433569,1,['deploy'],['deployment']
Deployability,"It looks like the Parquet schema in the file stores the `nullable = false` (as a `required` field), but it's ignored when being read back. I ran the following on a Spark 2 shell (updated from the Stack Overflow question, note that the Parquet handling has been rewritten since then, see https://issues.apache.org/jira/browse/SPARK-9095):. ```scala; scala> import org.apache.spark.sql._; import org.apache.spark.sql._. scala> import org.apache.spark.sql.types._; import org.apache.spark.sql.types._. scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc); warning: there was one deprecation warning; re-run with -deprecation for details; sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@625f5712. scala> val schema = StructType(Seq(StructField(""foo"", IntegerType, false))); schema: org.apache.spark.sql.types.StructType = StructType(StructField(foo,IntegerType,false)). scala> val df1 = sqlContext.createDataFrame(sc.parallelize(Array(Row(1))), schema); df1: org.apache.spark.sql.DataFrame = [foo: int]. scala> df1.printSchema; root; |-- foo: integer (nullable = false). scala> df1.write.parquet(""temp.df1""); ; scala> val df2 = sqlContext.read.parquet(""temp.df1""); df2: org.apache.spark.sql.DataFrame = [foo: int]. scala> df2.printSchema; root; |-- foo: integer (nullable = true); ```. Then. ```bash; parquet-tools schema part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; message spark_schema {; required int32 foo;; }; ```. Which shows that the field is `required`, not `optional`. Also. ```bash; parquet-tools meta part-r-00000-94aa6aa3-4799-4b78-9717-5397c8e983f9.snappy.parquet; creator: parquet-mr version 1.5.0-cdh5.7.0 (build ${buildNumber}) ; extra: org.apache.spark.sql.parquet.row.metadata = {""type"":""struct"",""fields"":[{""name"":""foo"",""type"":""integer"",""nullable"":false,""metadata"":{}}]} . file schema: spark_schema ; ----------------------------------------------------------------------------------------------------------------------------",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861:179,update,updated,179,https://hail.is,https://github.com/hail-is/hail/pull/1421#issuecomment-281967861,1,['update'],['updated']
Deployability,"It looks like you have two options:; 1. Install the Gradle ppa: https://launchpad.net/~cwchien/+archive/ubuntu/gradle; ; In a nutshell, uninstall the previous version of Gradle and then run:; ; ```; sudo add-apt-repository ppa:cwchien/gradle; sudo apt-get update; sudo apt-get install gradle-2.14.1; ```; 2. Download the the latest complete distribution of Gradle 2:; ; https://gradle.org/gradle-download/; ; Go to Previous Release and select 2.14.1 and download the complete distribution. Gradle is written in Java and it is pre-compiled. No need to build it. Run `gradle-2.14.1/bin/gradle` and you should be good to go.; ; Gradle 3 was just released a few days ago. We haven't tested against it, so I would recommend Gradle 2 for now.; ; I'll update the documentation to warn about Gradle 2.10. Let me know if either of these work for you.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240306249:40,Install,Install,40,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240306249,6,"['Install', 'Release', 'install', 'release', 'update']","['Install', 'Release', 'install', 'released', 'update']"
Deployability,"It seems the biggest remaining issue is that CI doesn't have access to hail imports when testing hailtop. Could we address this? Do you want a PR?. edit: nvm, patched here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799:159,patch,patched,159,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-670044799,1,['patch'],['patched']
Deployability,"It sounds like this is the kind of configuration change that needs a separate tracker issue, so I've created https://github.com/hail-is/hail/issues/14718 to encompass all the various actions we'll need to take. That issue can also cover the impact assessment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14717#issuecomment-2400754176:35,configurat,configuration,35,https://hail.is,https://github.com/hail-is/hail/pull/14717#issuecomment-2400754176,1,['configurat'],['configuration']
Deployability,It was an old worker that didn't have the idempotent updates. The error was in create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922:53,update,updates,53,https://hail.is,https://github.com/hail-is/hail/pull/8193#issuecomment-592582922,1,['update'],['updates']
Deployability,"It's a successful build for a previous version of master. I'm working on a PR to note when ci2 builds are out of date. We only update on approved PR at a time, so it is in a queue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361:127,update,update,127,https://hail.is,https://github.com/hail-is/hail/pull/6028#issuecomment-491016361,1,['update'],['update']
Deployability,"It's already deployed, so, I guess that's how I tested it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9391#issuecomment-685116503:13,deploy,deployed,13,https://hail.is,https://github.com/hail-is/hail/pull/9391#issuecomment-685116503,1,['deploy'],['deployed']
Deployability,"It's interesting, this reading problem is basically a little configuration language, but somehow it's really hard to write the performance we want without lots of code duplication.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255:61,configurat,configuration,61,https://hail.is,https://github.com/hail-is/hail/pull/1778#issuecomment-300169255,1,['configurat'],['configuration']
Deployability,"It's not so much so that **we** can check out a tagged release, as we have already worked around the problem. But I would expect that you and any other installations will also run into the same `batch_worker_image` failure. We have been running our production instance using gcsfuse 1.2.0 for about a week now, and I think @illusional will agree with me that we haven't seen any problems from it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622:55,release,release,55,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1769212622,2,"['install', 'release']","['installations', 'release']"
Deployability,"It's not the data, it's a complicated pipeline to replicate. I can try harder.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7455#issuecomment-549889843:38,pipeline,pipeline,38,https://hail.is,https://github.com/hail-is/hail/pull/7455#issuecomment-549889843,1,['pipeline'],['pipeline']
Deployability,"It's quite likely CI was restarted, the job ids are fairly low. There's a current bug wherein CI will spin up a bunch of jobs, kill them all then start them again when it first starts (there's a subtle issue WRT to whether we update state from GH or batch first).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5550#issuecomment-471026187:226,update,update,226,https://hail.is,https://github.com/hail-is/hail/issues/5550#issuecomment-471026187,1,['update'],['update']
Deployability,"John, one improvement to this that I think we should consider: add a build step that changes the url to a versioned link. This will be a bit involved since you won't be able to link inside of a GitHub release (which is a zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7570#issuecomment-557265229:201,release,release,201,https://hail.is,https://github.com/hail-is/hail/pull/7570#issuecomment-557265229,1,['release'],['release']
Deployability,Jupyter-server is already updated and we're not adding setup tools to requirements.txt,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13516#issuecomment-1708540616:26,update,updated,26,https://hail.is,https://github.com/hail-is/hail/pull/13516#issuecomment-1708540616,1,['update'],['updated']
Deployability,Just double checking -- I'm thinking I can dev deploy only the step `upload_query_jar` for those specific missing commits and then copy the resulting JAR to the appropriate place. Or can I build the JAR locally?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1559883168:47,deploy,deploy,47,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1559883168,1,['deploy'],['deploy']
Deployability,"Just for reference, I had to make the following changes:; 1. There's a new table that tracks attempts that have been already aggregated. The reason I can't put this variable into the attempts table is due to a circular update that's not allowed.; 2. I added a dummy variable to the attempts table that is just a way to get the update trigger to run. We find all complete attempts from the batches state and update the dummy variable for the attempts table in chunks. Within the new trigger, if the attempt has not been aggregated yet, then we use `NEW.end - NEW.start` as the time to aggregate with. If the attempt has been aggregated, then we use the original way which is just to take the difference between New and Old. At the end, we find all attempts that have not been aggregated (not in the new table that keeps track of whether an attempt has been aggregated) and update the dummy variable to run the trigger. I don't think we need to take any locks on the attempts table because any future writes that occur while we do the last processing will already be aggregated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11996#issuecomment-1178118437:219,update,update,219,https://hail.is,https://github.com/hail-is/hail/pull/11996#issuecomment-1178118437,4,['update'],['update']
Deployability,"Just for the record, I have another patch coming that will ignore the partitioner on `InvalidClassException`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/728#issuecomment-244372653:36,patch,patch,36,https://hail.is,https://github.com/hail-is/hail/pull/728#issuecomment-244372653,1,['patch'],['patch']
Deployability,Just got something like this on a completely separate pipeline. This appears to occur after grouping on something and not including a previous key. FWIW works with a4f79a3b3269. cc @jbloom22,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4799#issuecomment-440070333:54,pipeline,pipeline,54,https://hail.is,https://github.com/hail-is/hail/issues/4799#issuecomment-440070333,1,['pipeline'],['pipeline']
Deployability,Just hit it again on another pipeline that has some `.key_by`s - this is now a showstopper ☹️,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4314#issuecomment-420343076:29,pipeline,pipeline,29,https://hail.is,https://github.com/hail-is/hail/issues/4314#issuecomment-420343076,1,['pipeline'],['pipeline']
Deployability,"Just to be clear, this pipeline was what I wrote when trying to replicate the bug Duncan was seeing, but it hit a different assertion error than the one he was hitting. He was hitting ""local in the wrong method builder"" problems.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150:23,pipeline,pipeline,23,https://hail.is,https://github.com/hail-is/hail/issues/8325#issuecomment-603567150,1,['pipeline'],['pipeline']
Deployability,Just to make sure I understand -- the variable rename is to make sure it is clear that `HAIL_PRODUCTION_DOMAIN` means something different than `HAIL_DOMAIN` and is only applicable for CI? This is because the other services will have the correct deploy config?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580:245,deploy,deploy,245,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898843580,1,['deploy'],['deploy']
Deployability,"Just to update, when updating to 0.2.125 I discovered a bug :( https://hail.zulipchat.com/#narrow/stream/123010-Hail-Query-0.2E2-support/topic/array.20index.20out.20of.20bounds.20error.20on.20dict.2Eget",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1783285545:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1783285545,1,['update'],['update']
Deployability,"Just updated the path, should be good now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-820680874:5,update,updated,5,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-820680874,1,['update'],['updated']
Deployability,Keeping the most recent 10 `deploy-`m `pr-` and `dev-` images seems reasonable. I think we also use the `cache-` prefix. We maybe should keep 10 each of those?. Anything that's untagged is absolutely good to delete.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13441#issuecomment-1679605182:28,deploy,deploy,28,https://hail.is,https://github.com/hail-is/hail/issues/13441#issuecomment-1679605182,1,['deploy'],['deploy']
Deployability,"Known facts:; - the last CI k8s deployment that started the deploy=1 batch on April 27th was active since at least April 25th.; - For a small window of time that I looked at on April 25th, it kept getting errors when trying to get the Github status for possible merge candidates: 12848, 12849, 12547. There might be other PRs at later dates. I saw at least the same errors for 12848 on April 27th. I'm going to throw out a hypothesis. I merged the dedup attempt resources PR on April 19th. The PRs that were stacked on previous commits of that PR now have merge conflicts with the set of commits that actually got merged. This caused problems because the next merge candidates CI was selecting was causing bad GitHub rate limit requests for exceeding the number of statuses. So it kept retrying that same merge candidate. CI didn't get restarted at least from the 25th to the 27th so the merge candidate never would have been refreshed. We know that there's less GKE node turnover in Azure, so not unexpected that the ci pod wouldn't get redeployed on its own. I'm thinking it's possible that I merged the database trigger fix on April 27th in response to the excessive deadlocks we noticed and then rebased the subsequent stacked PRs that had merge conflicts, thus unblocking CI, but I'm not sure (it's really hard to get what I want from the Azure log analytics system). I think the ""bug fix"" here is to reassess the code in CI and possibly harden it where we select the merge candidate and try to get the status so it doesn't block deployments. I have a screenshot from April 25th below in case it's helpful. The log analytics query that is helpful is:. ```; ContainerLog; | where ContainerID == ""273584134970cdae08cf0d412461862e2a0e558888a52c91870ca46a146cbb8a""; | order by TimeGenerated; ```. <img width=""1085"" alt=""Screen Shot 2023-05-24 at 12 58 18 PM"" src=""https://github.com/hail-is/hail/assets/1693348/e2da08b6-5982-46cb-9e2c-2178a19f2f86"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011:32,deploy,deployment,32,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1561641011,3,['deploy'],"['deploy', 'deployment', 'deployments']"
Deployability,"Konrad got his relatedness estimates, so I'm tabling this PR until I have more time to clean it up for a proper release into 0.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2821#issuecomment-362299166:112,release,release,112,https://hail.is,https://github.com/hail-is/hail/pull/2821#issuecomment-362299166,1,['release'],['release']
Deployability,"Kudu 0.9.0 was released a few days ago, and it has a re-written Spark library so we don't need the `org.kududb.spark` package any more. It also fixes bugs, like the one @cseed saw with the context not being shut down. Annotations still don't work though - is there a way to get their schema early on so we can create a database table for them?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-226535225:15,release,released,15,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-226535225,1,['release'],['released']
Deployability,Labelling as a bug because this can adversely affect CI pipelines.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13722#issuecomment-1737675608:56,pipeline,pipelines,56,https://hail.is,https://github.com/hail-is/hail/issues/13722#issuecomment-1737675608,1,['pipeline'],['pipelines']
Deployability,"Lastly, a bunch of LOC come from the lock files. Those can be ignored for review purposes; they just maintain versioning information, ensure installs are consistent.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454272823:141,install,installs,141,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272823,1,['install'],['installs']
Deployability,"Laurent, I was totally wrong about being able to do this per-command -- I'm really sorry. I thought that it would be possible to create a new configuration just for this command and use that, but this is only possible for `HadoopConfiguration`s and not `SparkContext`s. Can you reopen the old PR? That model is our only option.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248641543:142,configurat,configuration,142,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248641543,1,['configurat'],['configuration']
Deployability,Let me know if you want me to dev deploy this one last time and test the changes. I haven't done so recently.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11750#issuecomment-1167512297:34,deploy,deploy,34,https://hail.is,https://github.com/hail-is/hail/pull/11750#issuecomment-1167512297,1,['deploy'],['deploy']
Deployability,"Let's enumerate the use-cases. In all cases, the point is to run the client script in the cloud.; 1. Submit a QoB pipeline.; 2. Submit a Batch pipeline.; 3. Submit a single-Python-job batch to execute some python code remotely.; 4. Same as (3) but using local-mode Hail (as opposed to (1)). I think use-case (4) is rare, though useful. If we're pitching `hailctl batch submit` as the QoB replacement for `hailctl dataproc submit`, then I think people will be very confused if (4) happens. I am somewhat frustrated that we have these different deployment strategies. It seems like unnecessary intellectual burden for a user to think about when they're just trying to run some Hail code. One option is to have different commands. Submitting a QoB job is done like this:; ```; hailctl qob submit ...; ```; And submitting a batch job is done like this:; ```; hailctl batch submit ...; ```. It pains me to think about trying to explain the difference between (1) and (4) to a scientist. At least with two distinct commands we can sort of ignore (4) and say: if you want to run Hail Query on Hail Batch use `hailctl qob` and if you want to run a normal batch pipeline use `hailctl batch`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12471#issuecomment-1324113118:114,pipeline,pipeline,114,https://hail.is,https://github.com/hail-is/hail/pull/12471#issuecomment-1324113118,4,"['deploy', 'pipeline']","['deployment', 'pipeline']"
Deployability,Let's get https://github.com/hail-is/hail/pull/12854 into this release. I slapped high:prio on it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12853#issuecomment-1500424252:63,release,release,63,https://hail.is,https://github.com/hail-is/hail/pull/12853#issuecomment-1500424252,1,['release'],['release']
Deployability,"Let's not approve this right now. I don't think deploying will make a difference, but I'd rather not risk it while the BroadE is happening.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023:48,deploy,deploying,48,https://hail.is,https://github.com/hail-is/hail/pull/4863#issuecomment-443234023,1,['deploy'],['deploying']
Deployability,"Let's say we have the following structure. gs://bucket/a/b/foo.txt; gs://bucket/a/b/bar.txt; gs://bucket/a/baz.txt. In the old design with rsync, we could be doing in parallel the following:. ```; gsutil rsync gs://bucket/a; ```. and. ```; gsutil rsync gs://bucket/a/b/; ```. I was worried we would delete files in the rsync in the middle of one job doing the copying. Basically some kind of race condition. However, with the new design of the cache, this might not be a problem where we're not using gsutil rsync but instead writing our own version of rsync. Cotton's fleshing out the design of that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816:55,a/b,a/b,55,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701580816,4,['a/b'],"['a/b', 'a/baz']"
Deployability,"Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevqS.haps; __RESOURCE__17=GLx",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1207,pipeline,pipeline,1207,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,1,['pipeline'],['pipeline']
Deployability,"Like a release? I think the terminology is a little overloaded. In services code we consider ""deploy"" as the CI pipeline that runs whenever main is updated.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1329692943:7,release,release,7,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1329692943,4,"['deploy', 'pipeline', 'release', 'update']","['deploy', 'pipeline', 'release', 'updated']"
Deployability,"Looking at fonts in the network panel of Chrome dev tools:. On https://hail.is/, I see https://ka-f.fontawesome.com/releases/v5.15.3/webfonts/free-fa-brands-400.woff2, which looks like it matches the font family specified by the `fab` class (""Font Awesome 5 Brands""). On https://hail.is/docs/0.2/index.html however, the only Font Awesome font I see is https://hail.is/docs/0.2/_static/fonts/fontawesome-webfont.woff2?v=4.7.0.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10277#issuecomment-813532470:116,release,releases,116,https://hail.is,https://github.com/hail-is/hail/pull/10277#issuecomment-813532470,1,['release'],['releases']
Deployability,"Looking at the `…/Packages` URL in the previous comment, 1.2.0 is now available (and 1.1.0 does not appear to be there). In our recent local hail update deployment, the `batch_worker_image` job failed repeatedly due to GoogleCloudPlatform/gcsfuse#1424. We worked around this as initially suggested on that issue with populationgenomics/hail@607408bee752dabca48d9a2732b14d32813ace9f, but later comments on the issue suggest that the better approach would be this PR with an additional change to access the apt repo via https:. ```diff; - echo ""deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; + echo ""deb https://packages.cloud.google.com/apt $GCSFUSE_REPO main"" | tee /etc/apt/sources.list.d/gcsfuse.list && \; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502:146,update,update,146,https://hail.is,https://github.com/hail-is/hail/pull/13728#issuecomment-1763644502,2,"['deploy', 'update']","['deployment', 'update']"
Deployability,"Looking at the auth image, it is down from 2.76GB in main to 674MB. The hail-ubuntu image underneath it has stayed basically the same at about half the new auth image. Nearly all of the 674MB is split evenly between the layer that installs python in hail-ubuntu and the layer that installs the pip dependencies in the auth image. I've not yet inspected the hail-ubuntu layer, but for the pip dependencies the main offenders are:. ```; 77M	/usr/local/lib/python3.7/dist-packages/googleapiclient; 76M	/usr/local/lib/python3.7/dist-packages/botocore; 33M	/usr/local/lib/python3.7/dist-packages/_sass.abi3.so; 29M	/usr/local/lib/python3.7/dist-packages/kubernetes_asyncio; 20M	/usr/local/lib/python3.7/dist-packages/uvloop; 14M	/usr/local/lib/python3.7/dist-packages/pip; 14M	/usr/local/lib/python3.7/dist-packages/cryptography; 8.9M	/usr/local/lib/python3.7/dist-packages/google; 7.9M	/usr/local/lib/python3.7/dist-packages/pygments; 7.0M	/usr/local/lib/python3.7/dist-packages/azure; 5.0M	/usr/local/lib/python3.7/dist-packages/setuptools; 4.2M	/usr/local/lib/python3.7/dist-packages/aiohttp; 2.5M	/usr/local/lib/python3.7/dist-packages/googlecloudprofiler; 2.2M	/usr/local/lib/python3.7/dist-packages/yaml; 2.2M	/usr/local/lib/python3.7/dist-packages/hailtop; 2.0M	/usr/local/lib/python3.7/dist-packages/rich; 1.6M	/usr/local/lib/python3.7/dist-packages/pyasn1_modules; 1.5M	/usr/local/lib/python3.7/dist-packages/boto3; 1.4M	/usr/local/lib/python3.7/dist-packages/pkg_resources; 1.4M	/usr/local/lib/python3.7/dist-packages/oauthlib; 1.1M	/usr/local/lib/python3.7/dist-packages/pycparser; ```. Most of a gigabyte still feels annoyingly bloated but might just have to do for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308:231,install,installs,231,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1459376308,2,['install'],['installs']
Deployability,Looks like this failed due to the wrong init script. Probably just need to update cloudtools on the CI server?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422132532:75,update,update,75,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422132532,1,['update'],['update']
Deployability,"Looks like this mainly drops 3.6 support, but I'll rope it into a dev deployed branch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11520#issuecomment-1061046995:70,deploy,deployed,70,https://hail.is,https://github.com/hail-is/hail/pull/11520#issuecomment-1061046995,1,['deploy'],['deployed']
Deployability,Looks like we need to update the tests too.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7361#issuecomment-545585187:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/7361#issuecomment-545585187,1,['update'],['update']
Deployability,Looks like you need to [update the Google Artifact Registry cleanup policies](https://batch.hail.is/batches/8076011/jobs/210) to account for your new image. Instructions to do so are in the error message.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/13936#issuecomment-1783512175,1,['update'],['update']
Deployability,"Looks ready. It'll break many a pipeline, so can you whip up a discuss post on the change that includes last commit before it goes in?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-430342019:32,pipeline,pipeline,32,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342019,1,['pipeline'],['pipeline']
Deployability,"Love it! Though email, not slack integration? 😛",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468:33,integrat,integration,33,https://hail.is,https://github.com/hail-is/hail/pull/4509#issuecomment-428052468,1,['integrat'],['integration']
Deployability,"Made this change backwards compatible. Note that I have not made any changes to worker.py in this PR anymore, so there's no danger of incompatibility. I tested the JAR from this PR against default and ran a simple hail query to see that it behaved as usual. Separately, I made #12246, dev deployed it, then ran this same JAR against my dev namespace to see that it added all worker jobs to the same batch as the driver job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715:289,deploy,deployed,289,https://hail.is,https://github.com/hail-is/hail/pull/12222#issuecomment-1262670715,1,['deploy'],['deployed']
Deployability,Making way for the release fix,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13832#issuecomment-1781334414:19,release,release,19,https://hail.is,https://github.com/hail-is/hail/pull/13832#issuecomment-1781334414,1,['release'],['release']
Deployability,"Matt S fortuitously asked a question that lead me to https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md , so I'm trying that now. That might be the last necessary fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961689398:126,INSTALL,INSTALL,126,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961689398,1,['INSTALL'],['INSTALL']
Deployability,"Maybe we should make an issue tag for things to fix before 0.2? We can't fix this without making a breaking interface change, but it's a really easy change that we shouldn't forget to include once we do 0.2. More generally, it seems like it would be good if we knew what we were planning on including before next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1887#issuecomment-305268107:313,release,release,313,https://hail.is,https://github.com/hail-is/hail/issues/1887#issuecomment-305268107,1,['release'],['release']
Deployability,Maybe we should update the WARNING message to be clear that this is a transient error and we've automatically retried it and there's nothing to be worried about?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877:16,update,update,16,https://hail.is,https://github.com/hail-is/hail/pull/11817#issuecomment-1117649877,1,['update'],['update']
Deployability,Mitigation: `pip3 install 'ipython<8.17'`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14099#issuecomment-1858099553:18,install,install,18,https://hail.is,https://github.com/hail-is/hail/issues/14099#issuecomment-1858099553,1,['install'],['install']
Deployability,Mmm. We should upgrade AKS to 1.21.x. I'll address this after March 28th though.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11602#issuecomment-1069373611:15,upgrade,upgrade,15,https://hail.is,https://github.com/hail-is/hail/pull/11602#issuecomment-1069373611,1,['upgrade'],['upgrade']
Deployability,More concerning is why we just didn't deploy for a week,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1559928342:38,deploy,deploy,38,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1559928342,1,['deploy'],['deploy']
Deployability,"More extensive cloud tests are showing speedups in the combiner pipeline compared to master, about 15-20 seconds per partition, but that adds up quickly at scale.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5828#issuecomment-481367173:64,pipeline,pipeline,64,https://hail.is,https://github.com/hail-is/hail/pull/5828#issuecomment-481367173,1,['pipeline'],['pipeline']
Deployability,"Moreover, the deploy jobs keep trying to access things in the batch-pods namespace, but the things they need to deploy are in the regular batch namespace.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049:14,deploy,deploy,14,https://hail.is,https://github.com/hail-is/hail/issues/4592#issuecomment-431594049,2,['deploy'],['deploy']
Deployability,"Most of the changes here are totally fine, but yes, I think we should hold off on changing paths. I have a to-do item to look into configuring / monkey patching the Sphinx function that's trying to generate links for the signatures. Dan is also OOO this week so not high prio.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000:152,patch,patching,152,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-688855000,1,['patch'],['patching']
Deployability,"My assumption has been that each fresh pipeline should get a new `EmitModuleBuilder` and `ExecuteContext`, if this is the case, I believe what I have done here should be alright.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9044#issuecomment-652737719:39,pipeline,pipeline,39,https://hail.is,https://github.com/hail-is/hail/pull/9044#issuecomment-652737719,1,['pipeline'],['pipeline']
Deployability,"My concern is if we release the code as is now, then we cannot change the buckets where the VEP data is stored without breaking backwards compatibility. Therefore, one idea I had was to keep a single up-to-date hail manifest file in GCS with the current information on what configurations are supported, where the data lives, what VEP version etc as a way of versioning the supported configurations. I did not try and implement this yet. I wanted to run the idea by you first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1514997203:20,release,release,20,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1514997203,3,"['configurat', 'release']","['configurations', 'release']"
Deployability,My dev deploy now has the light up graph 🥳,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8928#issuecomment-639218266:7,deploy,deploy,7,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218266,1,['deploy'],['deploy']
Deployability,"My issue with both of those names is the same as with `_tree`. It's not clear if you're storing the non-transitive or the transitive relation & its not clear if self-edges are included. I want a name that unambiguously says ""the self-edge and all the ancestor edges are in here"" or a name that is more domain-specific like ""job groups that need to be updated when a job in this job group is changed"". . `job_group_self_and_ancestor` feels like the shortest name so far that satisfies my concerns, but I'm open to other ones that are clear about self-edge-ness and transitive-ness.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13475#issuecomment-1761867398:351,update,updated,351,https://hail.is,https://github.com/hail-is/hail/pull/13475#issuecomment-1761867398,1,['update'],['updated']
Deployability,"My main motivation is that I can use up-to-date versions of NumPy, SciPy and Pandas. > I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**. Wow, that's bad. I did not know conda removes old packages. Are you using some continuous integration?; We are solving these issues by some policy that pull requests have to pass CI building. ; If there is a new version of e.g. Pandas, CI will install it when somebody pushes a commit.; In case the new package version breaks something, CI fails and we get a notification to fix it. This way, we can in general keep up to date with the latest package versions and only rise the minimum version number of dependencies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340:413,continuous,continuous,413,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542194340,3,"['continuous', 'install', 'integrat']","['continuous', 'install', 'integration']"
Deployability,"My team is pretty excited about hail being released with support for Spark 3.5. One thing I noticed is that it looks like the plan is to [restrict to Spark 3.5.0](https://github.com/hail-is/hail/pull/14158/files#diff-7e9fff5f09cc109665f7fe9baa107affaac24f5dc5a0aa8bc3769221a4c6c328R53) - would it be possible to allow some wiggle room for minor releases? Spark has been beginning to release upgrades much more often than in the past, so restricting to 3.5.0 will prevent access to bug fixes, feature enhancements, etc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582:43,release,released,43,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1900997582,4,"['release', 'upgrade']","['release', 'released', 'releases', 'upgrades']"
Deployability,"NFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 Security",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8286,install,install,8286,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"NFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 Secu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1488,install,install,1488,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"Namely, you can solve this. > I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control. without making your new configuration the default, right? The 2nd change you made restored the default behavior for the case of tokens.json, which, it seems to me, gives you inconsistent behavior for 'deploy-config.json', or if we ever changes tokens.json to some other name.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700:204,configurat,configuration,204,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535660700,2,"['configurat', 'deploy']","['configuration', 'deploy-config']"
Deployability,Need to update list of annotations in sampleqc doc.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/851#issuecomment-250204932:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/851#issuecomment-250204932,1,['update'],['update']
Deployability,"Need to update the version and regenerate backcompat files, should this pass.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1533991561:8,update,update,8,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1533991561,1,['update'],['update']
Deployability,Needs a Sphinx update. We should build the docs locally and spot check the results before making this change. In particular try `make -C website run` after updating Sphinx and make sure it looks fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11503#issuecomment-1059658439:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/11503#issuecomment-1059658439,1,['update'],['update']
Deployability,"Needs a `python3 -m black batch --line-length=120 --skip-string-normalization`; ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/driver/main.py	2023-04-05 14:40:12.638902 +0000; +++ batch/driver/main.py	2023-04-05 14:44:25.172615 +0000; @@ -1226,11 +1226,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_v3 (billing_project, `user`, resource_id, token, `usage`); SELECT billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; async def compact_agg_billing_project_users_by_date_table(app):; @@ -1254,11 +1255,12 @@; ; INSERT INTO aggregated_billing_project_user_resources_by_date_v3 (billing_date, billing_project, `user`, resource_id, token, `usage`); SELECT billing_date, billing_project, `user`, resource_id, 0, `usage`; FROM scratch; ON DUPLICATE KEY UPDATE `usage` = `usage` + scratch.`usage`;; -'''); +'''; + ); ; await compact() # pylint: disable=no-value-for-parameter; ; ; USER_CORES = pc.Gauge('batch_user_cores', 'Batch user cores (i.e. total in-use cores)', ['state', 'user', 'inst_coll']); would reformat batch/driver/main.py. Oh no! 💥 💔 💥; 1 file would be reformatted, 93 files would be left unchanged.; make[1]: *** [Makefile:18: check] Error 1; make[1]: Leaving directory '/io/repo/batch'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925:589,UPDATE,UPDATE,589,https://hail.is,https://github.com/hail-is/hail/pull/12850#issuecomment-1518033925,2,['UPDATE'],['UPDATE']
Deployability,New dev deploy after switching from `gs://b` to `b` because that's apparently the format dataproc wants. https://ci.hail.is/batches/8125093,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14270#issuecomment-1938912707:8,deploy,deploy,8,https://hail.is,https://github.com/hail-is/hail/pull/14270#issuecomment-1938912707,1,['deploy'],['deploy']
Deployability,Nice update!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4113#issuecomment-413067829:5,update,update,5,https://hail.is,https://github.com/hail-is/hail/pull/4113#issuecomment-413067829,1,['update'],['update']
Deployability,"Nice work. Let's block release on this, clearly it was bugged before.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494:23,release,release,23,https://hail.is,https://github.com/hail-is/hail/pull/8436#issuecomment-609932494,1,['release'],['release']
Deployability,"Nice!!! This appears to have skipped all the extraneous stages, and my pipeline goes straight to the write stage!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4293#issuecomment-419658619:71,pipeline,pipeline,71,https://hail.is,https://github.com/hail-is/hail/pull/4293#issuecomment-419658619,1,['pipeline'],['pipeline']
Deployability,"No no, I reset the codecs afterwards. I tested and it works as intended; (loading a .gz annotation file with the Gzip codec). I'm trying to fix the; small letter / capital issue (thanks Daniel), but it Git seems to be; case-insensitive when it comes to files... On Wed, Sep 21, 2016 at 11:19 AM, Tim Poterba notifications@github.com; wrote:. > This sets the configuration permanently -- any following commands will use; > the overridden codecs. Setting a global option is almost certainly better; > than getting this kind of leakage, I think; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/826#issuecomment-248645129, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgYRNZnsCXFQnDx9z5wRR1WD4rr0cks5qsUr_gaJpZM4KC1O-; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248646084:358,configurat,configuration,358,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248646084,1,['configurat'],['configuration']
Deployability,No sorry I'm asking if [this](https://github.com/hail-is/hail/blob/3a8be2b37ec387cbf0664354148a66e5930f2f88/infra/azure/modules/batch/main.tf#L27-L40) is unused and can be deleted or whether we are using it somewhere (and need to update it).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13440#issuecomment-1688974347:230,update,update,230,https://hail.is,https://github.com/hail-is/hail/pull/13440#issuecomment-1688974347,1,['update'],['update']
Deployability,"No state. I printed the session in the logs (I was deploying into prod to test while I had a broken cookie). My session had nothing set except that it was marked as created on January 7th. This despite that I saw a log statement from when I hit /login,m that clearly showed me session with all the right values. My guess is that there was some old signing key or somehow the session got corrupted so you can decode it to get an empty session and add new fields but they fail to be written back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388:51,deploy,deploying,51,https://hail.is,https://github.com/hail-is/hail/pull/8052#issuecomment-583226388,1,['deploy'],['deploying']
Deployability,"No update yet, sorry. We've just found a pipeline that may replicate it more easily, so hoping for a fix soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-683888038:3,update,update,3,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-683888038,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"No, but it will be included in the next release, which should be in the next week or two.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1967694813:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1967694813,1,['release'],['release']
Deployability,"No, but its the same string I used to install hail dependencies for Terra's notebooks. I can run a test now",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283:38,install,install,38,https://hail.is,https://github.com/hail-is/hail/pull/9502#issuecomment-698497283,1,['install'],['install']
Deployability,"No, it will be in 0.2.129 release date currently unknown, but we think it will be early March.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14297#issuecomment-1967979634:26,release,release,26,https://hail.is,https://github.com/hail-is/hail/pull/14297#issuecomment-1967979634,1,['release'],['release']
Deployability,"No, the framework has changed a lot since then so it just works now. My guess for why this happens would be memory issues, but not sure. What's your pipeline?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-419657089:149,pipeline,pipeline,149,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-419657089,1,['pipeline'],['pipeline']
Deployability,"No, the input strings are all on `gs://` but in the error I get:. ```; subprocess.CalledProcessError: Command '#!/bin/bash; # change cd to tmp directory; cd /tmp//pipeline-dc5b53d50f45/. cp /Users/konradk/Dropbox (Partners HealthCare)/src/python/gnomad_hail/gs:/phenotype...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017:163,pipeline,pipeline-,163,https://hail.is,https://github.com/hail-is/hail/issues/5759#issuecomment-479599017,1,['pipeline'],['pipeline-']
Deployability,"No, you're totally right, good catch! Fixed. How did this ever work? Because `get-deployed-sha.sh` was universally broken? I also added the (recently added) spark project to the list of projects.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4573#issuecomment-431244990:82,deploy,deployed-sha,82,https://hail.is,https://github.com/hail-is/hail/pull/4573#issuecomment-431244990,1,['deploy'],['deployed-sha']
Deployability,"Nobody has ever asked for this. I'm tabling it for now. If it does come up, we should integrate with Hadoop-BAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/35#issuecomment-208692497:86,integrat,integrate,86,https://hail.is,https://github.com/hail-is/hail/issues/35#issuecomment-208692497,1,['integrat'],['integrate']
Deployability,"Not in our batch jobs, but we do use tokens to submit on behalf of different users, eg: https://github.com/populationgenomics/analysis-runner/blob/d7f6d8fa5b61ac20f9952c50ee9ce27bd0ba5974/server/main.py#L105. So outside yes, but inside - we updated hail batch and I'm pretty confident it's using Oauth.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14059#issuecomment-1841779614:241,update,updated,241,https://hail.is,https://github.com/hail-is/hail/pull/14059#issuecomment-1841779614,1,['update'],['updated']
Deployability,Not sure. It doesn't throw an error on my version of anaconda. Which I've been meaning to update for awhile because all of the make files fail with my version of conda...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999:90,update,update,90,https://hail.is,https://github.com/hail-is/hail/pull/5563#issuecomment-471737999,1,['update'],['update']
Deployability,"Not that it helps in the commit history, but at least it's somewhere... this PR fixed that the secrets for batch deployment needed the deploy flag on which secret to use depending on whether we're testing or in production.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303:113,deploy,deployment,113,https://hail.is,https://github.com/hail-is/hail/pull/6168#issuecomment-495361303,2,['deploy'],"['deploy', 'deployment']"
Deployability,Not using anything prepackaged specifically for amazon. Just running the `make install-on-cluster HAIL_COMPILE_NATIVES=1 SPARK_VERSION=3.1.2` command on my instance which kicks off the install and runs through the requirements wheel through the Makefile. So wouldn't line 238 of the [Makefile](https://github.com/hail-is/hail/blob/57537fea08d4dcb1548a4ab1f55f093eae9bd016/hail/Makefile#L238) need to be adjusted with the changes?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255358708:79,install,install-on-cluster,79,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255358708,2,['install'],"['install', 'install-on-cluster']"
Deployability,"Note some highlights from the log:; ```; #12 42.27 ./Bio/tmp/Bio-DB-HTS-2.9 - moving files to ./biodbhts; #12 42.27 - making Bio::DB:HTS; #12 42.40 Checking prerequisites...; #12 42.40 requires:; #12 42.40 ! Bio::Root::Version is not installed; #12 42.40 ; #12 42.40 ERRORS/WARNINGS FOUND IN PREREQUISITES. You may wish to install the versions; #12 42.40 of the modules indicated above before proceeding with this installation; #12 42.40 ; #12 42.40 Run 'Build installdeps' to install missing prerequisites.; ```; ```; #13 138.3 Building and testing Test2-Suite-0.000152 ... ! Installing Test2::V0 failed. See /root/.cpanm/work/1682614674.13506/build.log for details. Retry with --force to force install it.; #13 150.9 FAIL; #13 150.9 --> Working on FFI::CheckLib; #13 150.9 Fetching http://www.cpan.org/authors/id/P/PL/PLICEASE/FFI-CheckLib-0.31.tar.gz ... OK; #13 150.9 Configuring FFI-CheckLib-0.31 ... OK; #13 151.1 ==> Found dependencies: Test2::V0, Test2::Require::EnvVar, Test2::Require::Module; #13 151.1 ! Installing the dependencies failed: Module 'Test2::Require::EnvVar' is not installed, Module 'Test2::V0' is not installed, Module 'Test2::Require::Module' is not installed; #13 151.1 ! Bailing out the installation for FFI-CheckLib-0.31. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230:234,install,installed,234,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1526412230,12,"['Install', 'install']","['Installing', 'install', 'installation', 'installdeps', 'installed']"
Deployability,"Note that pandas 2.0.0 [removes the deprecated `DataFrame.iteritems()`](https://pandas.pydata.org/docs/whatsnew/v2.0.0.html#removal-of-prior-version-deprecations-changes), which is used by bokeh-1.4.0. That particular old version of bokeh is listed in _hail/python/requirements.txt_ but it is thus incompatible with pandas 2; so one or the other of these pinnings probably needs to be revisited. (This incompatibility has caused the [large_cohort unit test failure](https://github.com/populationgenomics/production-pipelines/actions/runs/4782280056/jobs/8501466504?pr=354#step:5:134) in populationgenomics/production-pipelines#354.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581:515,pipeline,pipelines,515,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1519857581,2,['pipeline'],['pipelines']
Deployability,"Note: I have not updated the batch page for CI pipelines or the batch page for non-CI batch because I wanted to first run this change by the rest of the team, hence the WIP status on the PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13148#issuecomment-1579334984:17,update,updated,17,https://hail.is,https://github.com/hail-is/hail/pull/13148#issuecomment-1579334984,2,"['pipeline', 'update']","['pipelines', 'updated']"
Deployability,Nothing suspicious there. Something is going wrong in the executors. I think the only way we're gonna solve this is by running a pipeline and looking at the executor logs. I'm at a complete loss for how Jupyter could affect what happens on the executors.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1731963811:129,pipeline,pipeline,129,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1731963811,1,['pipeline'],['pipeline']
Deployability,"Now passing by killing safety on readPartitions, I've updated the comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2367#issuecomment-340074758:54,update,updated,54,https://hail.is,https://github.com/hail-is/hail/pull/2367#issuecomment-340074758,1,['update'],['updated']
Deployability,"OK, I cleaned this up a bit. Now stacked on: https://github.com/hail-is/hail/pull/5826. Summary of changes:; - added heal; - pipeline is now batch rather than job centeric; - batch logs page shows logs for all batch jobs; - GET /batches/{id} endpoint now returns entire array of jobs, instead of the state counter and exit_codes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454:125,pipeline,pipeline,125,https://hail.is,https://github.com/hail-is/hail/pull/5814#issuecomment-481079454,1,['pipeline'],['pipeline']
Deployability,"OK, I made a suite of additional changes:; - create docker/requirements.txt,; - batch doesn't use conda,; - pr-builder (rebuilt) installs docker/requirements.txt (same requirements as base image),; - put Spark in base image, removed spark-base image,; - set ENV IN_HAIL_CI=1 in hail-ci scripts,; - pull remote images and use --cache-from, allow push and deploy in CI, build locally only otherwise",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687:129,install,installs,129,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475123687,2,"['deploy', 'install']","['deploy', 'installs']"
Deployability,"OK, I reverted the retry on apt-get install, but kept the refactoring that makes apt-get update and pip use the same retry script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9906#issuecomment-767084684:36,install,install,36,https://hail.is,https://github.com/hail-is/hail/pull/9906#issuecomment-767084684,2,"['install', 'update']","['install', 'update']"
Deployability,"OK, I think I addressed all the comments. Here is a summary of the code changes I made:; - use *-tokens instead of *-jwt for the session tokens; - db event to clean up sessions,; - there was a bunch of legacy garbage in batch/ and batch/Makefile that I nuked,; - added hailtop test for deploy_config,; - fixed up hail ServiceBackend (which isn't actually tested now); - create_user => insert_user. I think there is still some legacy garbage in apiserver/, but that's not being deployed right now so I just left it. Let me know if I missed anything.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572:477,deploy,deployed,477,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-528993572,1,['deploy'],['deployed']
Deployability,"OK, I think I addressed the comments and it is ready for another look. I tested the batch2 UI witih deploy dev and it seems good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7530#issuecomment-555241754:100,deploy,deploy,100,https://hail.is,https://github.com/hail-is/hail/pull/7530#issuecomment-555241754,1,['deploy'],['deploy']
Deployability,"OK, I think I've addressed all the comments. I made some additional changes:. - pipe input file directly into tar instead of writing to disk (writing to SSDs, I get ~100MB/s/core download saturating gs://),; - report records read,; - directly create OrderedRVD instead of coercing,; - updated GenomicsDB to latest: 0.9.2-proto-3.0.0-beta-1+ed318f7e815 which involved revising GenomicsDBFeatureReader ctor call",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861:285,update,updated,285,https://hail.is,https://github.com/hail-is/hail/pull/3537#issuecomment-388564861,1,['update'],['updated']
Deployability,"OK, I think this PR is OK wrt that issue. I introduce one SQL command and it uses FOR UPDATE because we later insert that record.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7875#issuecomment-575754308:86,UPDATE,UPDATE,86,https://hail.is,https://github.com/hail-is/hail/pull/7875#issuecomment-575754308,1,['UPDATE'],['UPDATE']
Deployability,"OK, I think this is ready to go.; - Migration tested with passing colors (!); - I disabled the check incremental loop (but left the code in for future debugging); - I updated estimated-current.txt with the latest from improve-cancel.sql",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7933#issuecomment-577704626:167,update,updated,167,https://hail.is,https://github.com/hail-is/hail/pull/7933#issuecomment-577704626,1,['update'],['updated']
Deployability,"OK, I won't be able to fix this. @ehigham @patrick-schultz @daniel-goldstein some combo of you three can probably figure it out. The local backend tests that hit requester pays buckets are failing with new Spark. New Spark needs new GCS hadoop connector (see the Dockerfiles). New GCS hadoop connector has [brand new configuration parameters](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/v3.0.0/gcs/INSTALL.md). Somehow I managed to make the normal Spark backend work correctly but the Local backend (which still, afaik, uses Spark & Hadoop for filesystems) is still trying to pick up CI's credentials instead of the test account's credentials. ```; E hail.utils.java.FatalError: GoogleJsonResponseException: 403 Forbidden; E GET https://storage.googleapis.com/storage/v1/b/hail-test-requester-pays-fds32/o/zero-to-nine?fields=bucket,name,timeCreated,updated,generation,metageneration,size,contentType,contentEncoding,md5Hash,crc32c,metadata&userProject=hail-vdc; E {; E ""code"": 403,; E ""errors"": [; E {; E ""domain"": ""global"",; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist)."",; E ""reason"": ""forbidden""; E }; E ],; E ""message"": ""ci-910@hail-vdc.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project. Permission 'serviceusage.services.use' denied on resource (or it may not exist).""; E }; E ; E Java stack trace:; E java.io.IOException: Error accessing gs://hail-test-requester-pays-fds32/zero-to-nine; E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getObject(GoogleCloudStorageImpl.java:1986); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloudStorageImpl.getItemInfo(GoogleCloudStorageImpl.java:1882); E 	at com.google.cloud.hadoop.repackaged.gcs.com.google.cloud.hadoop.gcsio.GoogleCloud",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236:317,configurat,configuration,317,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1969609236,3,"['INSTALL', 'configurat', 'update']","['INSTALL', 'configuration', 'updated']"
Deployability,"OK, I won't notify you about importlib-metadata again, unless you re-open this PR or update it yourself. 😢",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11596#issuecomment-1069575150:85,update,update,85,https://hail.is,https://github.com/hail-is/hail/pull/11596#issuecomment-1069575150,1,['update'],['update']
Deployability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/14507#issuecomment-2206596777,16,"['release', 'update']","['release', 'updates']"
Deployability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. You can also ignore all major, minor, or patch releases for a dependency by adding an [`ignore` condition](https://docs.github.com/en/code-security/supply-chain-security/configuration-options-for-dependency-updates#ignore) with the desired `update_types` to your config file. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/13576#issuecomment-1709450787,1275,"['configurat', 'patch', 'release', 'update']","['configuration-options-for-dependency-updates', 'patch', 'release', 'releases', 'updates']"
Deployability,"OK, I'll upgrade to 3.7. There is a Python ppa with the latest version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474007645:9,upgrade,upgrade,9,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474007645,1,['upgrade'],['upgrade']
Deployability,"OK, I'm not sure how to fix this but the work is to explain to the GCS Hadoop Connector which credentials we want it to use. See the failure here: https://batch.hail.is/batches/8136069/jobs/49 . It uses CI's credentials instead of the test credentials. We use core-site.xml to do this in Spark <3.5, but the GCS connector is different in Spark 3.5 and it uses different configuration parameters. My most recent change did not successfully configure it. Daniel G can help you a bit with credentials in Batch if that's necessary but the real work is to figure out how to tell the GCS Hadoop Connector to use the /gsa-key/key.json file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844:370,configurat,configuration,370,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1961672844,1,['configurat'],['configuration']
Deployability,"OK, I've made most of the changes and I'd appreciate some feedback before I finalize the PR. Notable changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:168,configurat,configuration,168,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['configurat'],['configuration']
Deployability,"OK, a third option:. Gradle has support for something called a Gradle wrapper, a set of distribution scripts that download and run a specific version of Gradle. I just added a Gradle wrapper for 2.14.1 to the master branch. You should now be able to build the local version of Hail with `gradlew installDist` or `./gradlew shadowJar` to build the shadow (fat, uber) jar to run against a Spark cluster.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240309173:296,install,installDist,296,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240309173,1,['install'],['installDist']
Deployability,"OK, cool. Thanks for adding the dataproc tests! I'll approve this once the current release goes in, then we'll try this out for the next release, 0.2.38.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462:83,release,release,83,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603462,2,['release'],['release']
Deployability,"OK, everything checks out in dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7996#issuecomment-580044196:33,deploy,deploy,33,https://hail.is,https://github.com/hail-is/hail/pull/7996#issuecomment-580044196,1,['deploy'],['deploy']
Deployability,"OK, fixed and deployed. It works now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536246943:14,deploy,deployed,14,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246943,1,['deploy'],['deployed']
Deployability,"OK, for the record, I changed ci-agent to:; - only exist in batch-pods,; - have deploy permissions in both default and batch-pods. In the future, I think we should:; - have a service account just for creating and destroying namespace (creator-and-destroyer-of-namespaces) used by create namespace step,; - the deploy step should use the admin account for the target namespace,; - and similarly for create database. This means we'll eventually need to include the namespace with the service account in batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7470#issuecomment-551005475:80,deploy,deploy,80,https://hail.is,https://github.com/hail-is/hail/pull/7470#issuecomment-551005475,2,['deploy'],['deploy']
Deployability,"OK, so that was way more pain than I expected. Apparently when you're importing a JS module which needs authentication, you must specify the `crossorigin` attribute to the `script` tag. If you lack that attribute, no headers are sent. Since dev deploys are limited to developer access only (even though there is no in-dev-namespace authentication), this obviously doesn't work. Why does the word `crossorigin` mean send headers to the same origin? Who knows! Anyway. I don't think that bug should block this PR. That bug is an underlying issue with dev deploy. The fix for the module import bug is here: https://github.com/hail-is/hail/pull/8928",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078:245,deploy,deploys,245,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639135078,2,['deploy'],"['deploy', 'deploys']"
Deployability,"OK, so, this is apparently an issue where browsers have not yet really implemented the standard correctly. From my reading of [HTML 2.6.4](https://html.spec.whatwg.org/#cors-settings-attributes), `crossorigin=""anonymous""` ought to be sufficient for requests to the same origin as the page containing the script tag. Jake Archibald has an informative [blog post](https://jakearchibald.com/2017/es-modules-in-browsers/) about modules. He links to a [demo](https://module-script-tests-sreyfhwvpq.now.sh/cookie-page) of three cross origin configurations. The three options are:; ```; <script type=""module"" src=""cookie-script""></script>; <script type=""module"" crossorigin="""" src=""cookie-script?1""></script>; <script type=""module"" crossorigin=""use-credentials"" src=""cookie-script?2""></script>; ```; I'm using Safari Version 13.1 (14609.1.20.111.8). I usually only see the very last script working. However, inexplicably, I have seen the first one very rarely work. All I've been doing is refreshing here and there as I try to understand this. The Safari inspector confirms that the cookie is only sent with thee last option. So, anyway, I'm adding `crossorigin=""use-credentials""`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007:535,configurat,configurations,535,https://hail.is,https://github.com/hail-is/hail/pull/8928#issuecomment-639218007,1,['configurat'],['configurations']
Deployability,"OK, so, this really feels like bad data. We just merged https://github.com/hail-is/hail/commit/98adcce1d07001995b0819fd6afe161bf34ba840 which fixed https://github.com/hail-is/hail/issues/13979 . Google Cloud Storage's Java library was very rarely returning just flat-out bad data. The frequency of occurrence on one particularly large pipeline appears to be 1/30000 tasks (0.003% or 3 in 100,000). The tasks were reading two files, the larger of which was 131MiB. The Java library reads in 8MiB chunks so that's at least 17 network requests per partition. That puts the frequency of this closer to 1 in 1,000,000 requests or 1 in 10TiB of data read. Before we had Zstandard, it seems that this data corruption either (a) was unnoticed (b) caused a rare decoding error or (c) caused segfaults. After we added Zstandard (0.2.119), decompression often failed due to corrupt data. It seems to me that Zstandard more aggressively verifies integrity than LZ4 does. OK, so, when was this bug introduced in Hail? As far as I can tell, this new code path was added in google-cloud-storage 2.17.0 almost one year ago: https://github.com/googleapis/java-storage/commit/94cd2887f22f6d1bb82f9929b388c27c63353d77 . We upgraded to 2.17.1 (😭 ) in Hail 0.2.109 https://github.com/hail-is/hail/commit/fec0cc2263c04c00e02cef5dda8ec46916717152 . All of the attempts above could have been plagued by this rare transient data corruption error. OK, action items:. - [ ] Ask Cal and Lindo to try their pipelines again with the next release of Hail 0.2.127.; - [x] Hail must introduce large-scale testing before releases. We, sadly, cannot assume our underlying storage libraries are reliable. https://github.com/hail-is/hail/issues/14082. Once the first action item is successfully completed, I will close this issue. For the second action item, I have created a separate ticket.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114:335,pipeline,pipeline,335,https://hail.is,https://github.com/hail-is/hail/issues/13688#issuecomment-1845732114,5,"['pipeline', 'release', 'upgrade']","['pipeline', 'pipelines', 'release', 'releases', 'upgraded']"
Deployability,"OK, so. Dev deployed `site` uses the same docs and www as non-dev-deployed site. Unfortunately, this means we have a root-URL issue. I use [an nginx `sub_filter`](https://github.com/hail-is/hail/blob/master/site/site.sh) rule to fix links and anchors. I didn't add any rules for javascript modules. The right fix is to move to a system that can build for different environments and set the root URL properly. Until we switch to that hypothetical new system, I'll add a rule that properly rewrites JS module imports. We can't fix this with symlinks or any kind of redirection. The root issue is that we, the users, are outside of the system and the way we specify to whom we're talking is `internal.hail.is/NAMESPACE/SERVICE/`. Cotton wanted to do `service.namespace.internal.hail.is` but there were some TLS wildcard issues. It is perhaps worth revisiting this at some point because it is a perennial issue for us.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704:12,deploy,deployed,12,https://hail.is,https://github.com/hail-is/hail/pull/8923#issuecomment-639077704,2,['deploy'],['deployed']
Deployability,"OK, so. I added test_dataproc as a separate build step and pulled that code out of the makefile into a script. I'm running a dev-deploy test of it here: ~~https://ci.hail.is/batches/32232~~. I added a commit to this branch (not present in the aforementioned dev deploy) that exits 0 if the git tag already exists. This prevents running the Dataproc tests on every master commit. I broadened the scope to include `dev`. This means that developers can run a deploy with `hailctl dev deploy hail-is/hail:master deploy`. I haven't tested the deploy step. I volunteer to do the next deploy and deal with whatever is broken. EDIT: Copying data between jobs loses the chmod settings so I had to switch away from `./`, that was why the last batch failed. https://ci.hail.is/batches/32237",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506:129,deploy,deploy,129,https://hail.is,https://github.com/hail-is/hail/pull/8533#issuecomment-613147506,7,['deploy'],['deploy']
Deployability,"OK, switched to no pip installs. the hailjwt error was due to using python instead of python3. Makefile now defines PYTHON variable that sets the path correctly before invoking python3. Addressed other comments as well. ---. Don't approve yet, I discovered a race condition wrt pod creation and updates from k8s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312:23,install,installs,23,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-484144312,2,"['install', 'update']","['installs', 'updates']"
Deployability,"OK, thanks, will update the tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5509#issuecomment-468830620:17,update,update,17,https://hail.is,https://github.com/hail-is/hail/pull/5509#issuecomment-468830620,1,['update'],['update']
Deployability,"OK, the pip installed version uses a local copy of the annotation db JSON file. I cleaned up the lens thing a bit put it in a separate file and used ABCs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7178#issuecomment-542408914:12,install,installed,12,https://hail.is,https://github.com/hail-is/hail/pull/7178#issuecomment-542408914,1,['install'],['installed']
Deployability,"OK, there was a `justSpark` ""configuration"". I dunno what that means, it wasn't referenced by the other configurations. I initially made it `implementation` which includes it in the shadow JAR. I switched it to `shadow` b/c mllib (the only thing in the justSpark) should be provided by Spark at runtime anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13551#issuecomment-1708500255:29,configurat,configuration,29,https://hail.is,https://github.com/hail-is/hail/pull/13551#issuecomment-1708500255,2,['configurat'],"['configuration', 'configurations']"
Deployability,"OK, this is now higher priority for me. The Query-on-Batch tests are absolutely hammering the database with huge spikes in deadlock errors during working hours (when deploys trigger tests).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265:166,deploy,deploys,166,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1039618265,1,['deploy'],['deploys']
Deployability,"OK, this is passing. Sorry for another big PR, I think I'm getting close to converging and I'll start spreading things around. Summary of changes:; - break batch into two services: batch2 and batch2-driver; - batch2 handles connections to the client, but has no driver; - driver has the driver and all the control loops; - workers now connect to batch2-driver, not batch2; - batch2 hits batch2-driver after close, cancel and delete to actually handle the jobs; - also removed abort and jsonify from utils and prefer the native aiohttp interfaces. I'll change the batch2 deployment in a later PR to run in triplicate, tolerate preemptibles and have a horizontal autoscaler.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072:570,deploy,deployment,570,https://hail.is,https://github.com/hail-is/hail/pull/7226#issuecomment-539972072,1,['deploy'],['deployment']
Deployability,"OK, this passes all the tests except for `test_vcf_parser_golden_master__gvcf_GRCh37` which inexplicably hangs. I've marked that as skip. I've attached the WIP tag because the longest tests now take 47 minutes. I'll leave this PR up as a canary for when a `main` change fails service tests. However, I won't merge it until we improve test latency. cc: @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11444#issuecomment-1069516954:238,canary,canary,238,https://hail.is,https://github.com/hail-is/hail/pull/11444#issuecomment-1069516954,1,['canary'],['canary']
Deployability,"OK, update from Google: they suggest we check if the preemptible quota is non-zero and assume that if it is non-zero preemptible is in use and if it is zero normal quota is in use. In very rare cases, people can increase and then reduce their preemptible quota and Hail will not work properly. Google wasn't interested in providing an API to detect this case. I'll change this PR accordingly sometime next week.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354:4,update,update,4,https://hail.is,https://github.com/hail-is/hail/pull/10588#issuecomment-868865354,1,['update'],['update']
Deployability,"OK, will change to exec/wait and fix the merge conflict (there was a conflict due to the upgrade to; libsimdpp-2.1, I tried to fix that but may need to do more, or there may be a new issue). My preference is to leave the build-command execution on the C++ side because it's (arguably); easier to read/understand the combined Scala + C++ functionality if the Scala NativeModule is a ; trivial wrapper and all the substance is on one side, in this case in C++. Since that's also acceptable to; you, I'll keep it that way.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339:89,upgrade,upgrade,89,https://hail.is,https://github.com/hail-is/hail/pull/4211#issuecomment-416093339,1,['upgrade'],['upgrade']
Deployability,"OK, yeah, I need a coherent strategy for someone deploying a fresh Hail install. If folks are not continuously deploying every commit, they might miss a tagged commit. @daniel-goldstein , thoughts on how we might support things that should be run on a tagged commit for users who are not deploying every commit?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985723595:49,deploy,deploying,49,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985723595,5,"['continuous', 'deploy', 'install']","['continuously', 'deploying', 'install']"
Deployability,"ONFIG_DIR, 'tokens.json'); return '/user-tokens/tokens.json'; ; def __init__(self):; diff --git a/hail/python/hailtop/config/__init__.py b/hail/python/hailtop/config/__init__.py; index aeb00dd76..414f0a1d5 100644; --- a/hail/python/hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:2938,Deploy,DeployConfig,2938,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,2,['Deploy'],['DeployConfig']
Deployability,"Of course. Cotton also had a few comments, I will integrate too. On Tuesday, September 20, 2016, Tim Poterba notifications@github.com; wrote:. > Let's get this in. Can you rebase?; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/hail-is/hail/pull/541#issuecomment-248438901, or mute; > the thread; > https://github.com/notifications/unsubscribe-auth/ADVxgSLomjBSxoQFMUqrmHl1RdXl_mD3ks5qsE7igaJpZM4Jb_3Y; > .",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/541#issuecomment-248439671:50,integrat,integrate,50,https://hail.is,https://github.com/hail-is/hail/pull/541#issuecomment-248439671,1,['integrat'],['integrate']
Deployability,"Oh and the pipeline is a series of:; ```; next_vds = hl.read_matrix_table('{}/parts/part_{}.vds'.format(root, base)); next_vds = next_vds.select_rows(next_vds.row_id); vds = vds.union_cols(next_vds); ```; (about 30 of them and then a write)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-369956421:11,pipeline,pipeline,11,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-369956421,1,['pipeline'],['pipeline']
Deployability,"Oh grrr, the current build is extra borked because a minor version bump in the azure blob storage library broke. I'll back out that change. Oh I think I totally misunderstood the ""pip-installed images"". [here](https://ci.hail.is/batches/774665/jobs/63) is the job I was initially confused about. Not clear why in that batch only the pip-installed hail failed the lint and not the `check_hail` step",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1062078939:184,install,installed,184,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1062078939,2,['install'],['installed']
Deployability,Oh nice I had seen that but didn’t know how well it worked. Would be nice for notebook (to watch fine-grained state updates),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-483868483:116,update,updates,116,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-483868483,1,['update'],['updates']
Deployability,"Oh right sorry that doesn't make the Linux prebuilt, which you need to build locally on Linux and check in. CI should build those before testing but doesn't yet. The workaround @catoverdrive showed me when I had to remake the prebuilt was to build it in a docker container with the CI image on my machine. Would be happy to help you get them built tomorrow and/or make the appropriate updates to CI and deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668:385,update,updates,385,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-492911668,2,"['deploy', 'update']","['deploy', 'updates']"
Deployability,"Oh yikes, looks like something in the LLVM dependencies requires/installs python3.8. I wonder whether that is necessary",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12526#issuecomment-1352514979:65,install,installs,65,https://hail.is,https://github.com/hail-is/hail/pull/12526#issuecomment-1352514979,1,['install'],['installs']
Deployability,"Oh, and the app is meant to operate behind HTTPS; when deployed, running the web app with ; `npm run start` instead of `npm run prod-test` will enable secureOnly cookies.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121:55,deploy,deployed,55,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454272121,1,['deploy'],['deployed']
Deployability,Oh. I remember why now. When we were using rsync I was worried someone would simultaneous try to do this:. cp gs://bucket/a/b/; cp gs://bucket/a/. Then they would clobber each other.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9523#issuecomment-701545652:122,a/b,a/b,122,https://hail.is,https://github.com/hail-is/hail/pull/9523#issuecomment-701545652,1,['a/b'],['a/b']
Deployability,Ok @tpoterba I added the environment variables where I think you originally wanted them in the pipeline task.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445:95,pipeline,pipeline,95,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583557445,1,['pipeline'],['pipeline']
Deployability,"Ok I just looked at the scala code, and this must have happened around the sex chromosomes when my dataset shifted to haploid (or more specifically, a mix of haploid and diploid calls for male and female). I'll write in the workaround for my own pipeline, but you might want to have a more explicit error message.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263:246,pipeline,pipeline,246,https://hail.is,https://github.com/hail-is/hail/issues/3465#issuecomment-385281263,1,['pipeline'],['pipeline']
Deployability,Ok I will for the new release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502350952:22,release,release,22,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502350952,1,['release'],['release']
Deployability,Ok clearly I need to install g++.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941:21,install,install,21,https://hail.is,https://github.com/hail-is/hail/pull/5354#issuecomment-464188941,1,['install'],['install']
Deployability,Ok thank you for the update let me try 2.2.0 . Highly appreciate a quick reply.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-368912193:21,update,update,21,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-368912193,1,['update'],['update']
Deployability,"Ok, I think I sorted out the make->mill dependency propagation. Any real target which invokes mill to build it now depends on a target `FORCE` which is always out-of-date, so mill is always invoked. But mill will not change the modification time if it doesn't need to, so downstream targets aren't forced to be run. For example, we have targets; ```; FORCE:. SHADOW_JAR := out/assembly.dest/out.jar; $(SHADOW_JAR): FORCE; 	$(mill) assembly. PYTHON_JAR := python/hail/backend/hail-all-spark.jar; $(PYTHON_JAR): $(SHADOW_JAR); 	cp -f $(SHADOW_JAR) $@. .PHONY: python-jar; python-jar: $(PYTHON_JAR); ```. If I remove the python jar and invoke make, it runs mill then copies:; ```; ❯ rm python/hail/backend/hail-all-spark.jar. ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If run again, mill is invoked to check for changes, but as the jar doesn't change it isn't copied again:; ```; ❯ make python-jar; bash millw assembly; [105/110] memory.resources; ```. If I change some scala sources, the jar is updated and copied:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 10 Scala sources to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [110/110] assembly; cp -f out/assembly.dest/out.jar python/hail/backend/hail-all-spark.jar; ```. If I change some scala sources in a way that doesn't actually affect the jar, such as modifying comments, mill is smart enough to not change the jar, so it won't be copied again:; ```; ❯ make python-jar; bash millw assembly; [95/110] compile; [info] compiling 1 Scala source to /Users/pschulz/hail/mill-worktree/hail/out/compile.dest/classes ...; [info] done compiling; [105/110] memory.resources; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793:1234,update,updated,1234,https://hail.is,https://github.com/hail-is/hail/pull/14147#issuecomment-1930325793,1,['update'],['updated']
Deployability,"Ok, I think I'm getting a much better understanding of the situation, thank you! I think this is a lot easier for at least me to understand and if I'm not mistaken, it's mostly just moving around of the same code into `JobSpec`, right? I think `submit` is a lot easier to follow now, though I do sympathize with the pain of the three round-trips for small updates. What do you think about the following proposal?. 1. Jobs are always submitted to the server with job_ids relative to the update that is being submitted (sorry for the complete reversal, this is just an idea!). This shouldn't have any affect on the current create/create-fast since only 1 update means relative and absolute job ids are the same. This also means that the client doesn't need to know what the `start_job_id` is ahead of submitting a bunch. Submitting the update could return the `start_job_id` such that the client can rectify its local `Job` objects with absolute IDs like you do in `_get_job_specs_as_json` after the fact. The server will take care of doing `absolute_job_id = update_start_job_id + in_update_job_id`.; 2. parent_ids are represented as positive integers that are tagged with a type, either `in-update` or `absolute`. This is very similar to what you were previously doing with negative numbers, but I think baking it into the schema is going to be less foot-gunny than negative numbers, and the client doesn't have to do any special logic of counting backwards. Sorry if it's similar to what you were doing before but I think it has taken me a while to fully understand the limitations here. I also think it wouldn't take much changes to this current client implementation to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1217244185:356,update,updates,356,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1217244185,5,['update'],"['update', 'updates']"
Deployability,"Ok, I've downloaded JSON for the batch, default, and CI dashboards (I don't think any of the other ones are really used?), and recorded the configurations for the GCP and prometheus datasources, so I think I should be able to quickly reconfigure grafana if everything is lost.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923:140,configurat,configurations,140,https://hail.is,https://github.com/hail-is/hail/pull/10772#issuecomment-906539923,1,['configurat'],['configurations']
Deployability,"Ok, getting back up to speed on this. There've been a number of changes on either side of this project, so going to give new timings and profiling results for the two queries. Here are mean timings for the two queries, run 5 times, and taking the mean of all but the first. It seems there's been a considerable regression since November on the second query, highlighting our need to get automated benchmark runs in per release (https://github.com/hail-is/hail/issues/14221). | query	| spark |; |-------|-------|; | 0	| 7s |; | 1	| 87s |. Attached are YourKit profiler results of the two queries. 'fast' refers to query 0 and 'slow' to the longer-running query 1.; [seqr-profile-data.zip](https://github.com/hail-is/hail/files/14103185/seqr-profile-data.zip); [seqr-logs.zip](https://github.com/hail-is/hail/files/14104795/seqr-logs.zip)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1917732829:419,release,release,419,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1917732829,1,['release'],['release']
Deployability,"Ok, just used this script to release 0.2.28 without a problem so I think we should merge this",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7536#issuecomment-557703477:29,release,release,29,https://hail.is,https://github.com/hail-is/hail/pull/7536#issuecomment-557703477,1,['release'],['release']
Deployability,"Ok, so I thought I left a comment on here but I guess I didn't: when I tested this with dev deploy, I didn't see any plots show up, got JS console errors. So I'm not sure this was quite ready to be merged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394:92,deploy,deploy,92,https://hail.is,https://github.com/hail-is/hail/pull/11712#issuecomment-1097208394,1,['deploy'],['deploy']
Deployability,"Ok, so, llvm-dev requires python3-yaml. Attempting to pip install version 6 (our dep) onto the system fails. So, that's fun.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12410#issuecomment-1299393355:58,install,install,58,https://hail.is,https://github.com/hail-is/hail/pull/12410#issuecomment-1299393355,1,['install'],['install']
Deployability,"Ok, this is seeming not related to this closed github issue. Can you make a post on our support forum: https://discuss.hail.is/ explaining what step went wrong (I'm not sure where you're trying to deploy to AWS or what your installation procedure is here)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-908575361:197,deploy,deploy,197,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-908575361,2,"['deploy', 'install']","['deploy', 'installation']"
Deployability,"Ok, this should pass. I'll deploy 0.2.1 after master is merged. I'll get self-deployment working soon.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4498#issuecomment-430451063:27,deploy,deploy,27,https://hail.is,https://github.com/hail-is/hail/pull/4498#issuecomment-430451063,2,['deploy'],"['deploy', 'deployment']"
Deployability,"Ok. I thought about this some more. What you've implemented is essentially a ""taint"" in Kubernetes. I ultimately want both taints and something more complicated that will have to be integrated into the scheduler. I think for now your change is self contained and it will be easy to transform later on without too much complexity or breaking changes for users. I think if you want to rename ""label"" to ""taint"" then that might make it clearer what's going on. cc: @daniel-goldstein",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746:182,integrat,integrated,182,https://hail.is,https://github.com/hail-is/hail/pull/11879#issuecomment-1145343746,1,['integrat'],['integrated']
Deployability,"Ok. Still working on getting the tests to pass and cleaning things up. However, I ran into a small snag. The code below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?. cc: @daniel-goldstein . ```mysql; UPDATE batches SET; `state` = 'running',; time_completed = NULL,; n_jobs = n_jobs + expected_n_jobs; WHERE id = in_batch_id;. ### FIXME FIXME what should the state be of nested job groups?; UPDATE job_groups; INNER JOIN (; SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; FROM job_groups_inst_coll_staging; WHERE batch_id = in_batch_id AND update_id = in_update_id; GROUP BY batch_id, job_group_id; ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1932547516:521,UPDATE,UPDATE,521,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1932547516,2,['UPDATE'],['UPDATE']
Deployability,"On April 2016, this exact issue was reported: https://issues.jenkins-ci.org/browse/JENKINS-34177. This issue results in Jenkins using arbitrary amounts of disk space (until builds fail due to a full disk). JENKINS-34177 was closed as a duplicate of: https://issues.jenkins-ci.org/browse/JENKINS-2111 which has been open since 2008. The thread of conversation seems to be about how difficult it would be to ensure that all files are deleted across all slaves. There have been no updates to that thread since October of 2015.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/655#issuecomment-241418469:478,update,updates,478,https://hail.is,https://github.com/hail-is/hail/issues/655#issuecomment-241418469,1,['update'],['updates']
Deployability,"On Oct 21, 2017, at 1:27 PM, Sun-shan <notifications@github.com> wrote:. > hail: info: SparkUI: http://10.131.101.159:4040; > Welcome to; > __ __ <>__; > / // /__ __/ /; > / __ / _ `/ / /; > // //_,/// version 0.1-f69b497; > ; > print sc; > ; > >>> print hc >>> hc.import_vcf() Traceback (most recent call last): File """", line 1, in TypeError: import_vcf() takes at least 2 arguments (1 given) >>> hc.import_vcf('/hail/sample.vcf') [Stage 0:> (0 + 1) / 2]Traceback (most recent call last): File """", line 1, in File """", line 2, in import_vcf File ""/opt/Software/hail/python/hail/java.py"", line 112, in handle_py4j 'Error summary: %s' % (deepest, full, Env.hc().version, deepest)) hail.java.FatalError: SparkException: Failed to get broadcast_4_piece0 of broadcast_4. This type of Spark exception seems to be related to the configuration option spark.cleaner.ttl. Have you set that to a value other than the default?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661:822,configurat,configuration,822,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338442661,1,['configurat'],['configuration']
Deployability,"On the issue of how to give people a standardized environment with Hail +; compiler + everything else, one; approach is to build a Bitnami package, which then installs itself into a; directory tree with zero/minimal; dependencies or interactions with anything outside its tree. I've used; that for web services like Jenkins.; It's just one step short of using containers - but since it doesn't require; a containerized OS, I think it works; for laptops etc. I believe the package could have all the stuff we currently manage my; manual install, viz JDK, Spark, Python-3.6,; R, R packages, as well as Hail and a friendly-C++17-capable compiler. All; without perturbing anything else; on the system. See https://bitnami.com. I took a similar approach at PhysicsSpeed, though without using any bitnami; tools because we had less than; zero dollars :-(. I don't know if this adds any value in the containerized/cloud environment,; where custom machine images; are presumably the way to go. But it makes setup easy for standalone use. Regards; Richard. On Thu, Aug 2, 2018 at 10:44 PM Richard Cownie <rcownie@broadinstitute.org>; wrote:. > We have a difference of opinion about the risks involved in using whatever; > compiler happens to show up as $(CXX); > to try to compile arbitrarily large auto-generated C++ files, and maybe; > about what happens when that fails; > and gives an error message about something in the middle of 12000 lines of; > code that bears no obvious relationship; > to what the user is doing. Or when that compiler takes 15 minutes to; > compile it. It's the C++ equivalent of; > the JVM ""no, that's just too much bytecode"". Or worst of all, it compiles; > it but the code gives the wrong answers; > because that particular compiler has a bug, and we never tested the; > combination of our codegen with *that*; > compiler/version.; >; > A couple of years ago I was seeing g++ take 40-60 seconds to compile; > something that clang did in 2 seconds; > (fairly heavily templated cod",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287:159,install,installs,159,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410235287,2,['install'],"['install', 'installs']"
Deployability,"Once upon a time, we did option 1. We took that out because there was no way to know in general that the within-key sort would be small enough to do locally. In fact, `RVD.makeCoercer` still supports this. It takes a `partitionKey` argument, and if the first scan over the data determines that it's already sorted by the partitionKey, but not the full key, it does a local sort. But it looks like all calls will have set `partitionKey` to be the full key. I like option 2. `key_by` could take an optional `local_sort_by` parameter, which is a prefix of the key, and set that as the partition key. We'd need to update the TableKeyBy lowering to support this, but that shouldn't be hard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13455#issuecomment-1683930595:610,update,update,610,https://hail.is,https://github.com/hail-is/hail/issues/13455#issuecomment-1683930595,1,['update'],['update']
Deployability,"Once we can draw from Cassandra (with the likes of `annotatevariants cass`, we can write/read/compare in the usual way. We need a Cassandra cluster for testing. There are three options: an embedded server as part of Hail, assume the user has installed Cassandra locally, or run against a fixed server. I set up a single-node Cassandra install on hail-ci. In the spirit of small commits, I want to be able to test in experimental/untested/in progress work so we don't get so much divergence. We need a way to mark it. I will investigate the options.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/282#issuecomment-208368921:242,install,installed,242,https://hail.is,https://github.com/hail-is/hail/pull/282#issuecomment-208368921,2,['install'],"['install', 'installed']"
Deployability,"One last thing, we now have a file called `CanLowerEfficiently.scala` (rebase if you don't have it). In that file, you have to update the `TableTail` case to match `TableHead`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10798#issuecomment-903821822:127,update,update,127,https://hail.is,https://github.com/hail-is/hail/pull/10798#issuecomment-903821822,1,['update'],['update']
Deployability,"One of the problems with the new model is that tests on the deployed version are going to leave billing project poop everywhere unless we actually go in and delete them, hmmm....",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9553#issuecomment-705227991:60,deploy,deployed,60,https://hail.is,https://github.com/hail-is/hail/pull/9553#issuecomment-705227991,1,['deploy'],['deployed']
Deployability,"Oops, I had a bug where the readiness check was hitting the notebook service, not the actual notebook. Here is an updated scale test:. ```; successes: 10 / 10 = 1.0; mean time: 6.920137214660644; max time: 14.664504528045654; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-534707944:114,update,updated,114,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-534707944,1,['update'],['updated']
Deployability,"Oops, I messed up the last PR, I forget the ""3"" in ""miniconda3"" in the build image Docker. I presume deploys are broken now, but I haven't checked. Rebuilding a new docker image now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4404#issuecomment-423779185:101,deploy,deploys,101,https://hail.is,https://github.com/hail-is/hail/pull/4404#issuecomment-423779185,1,['deploy'],['deploys']
Deployability,"Oops, I still have to update the `vep` docs.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3872#issuecomment-401411707:22,update,update,22,https://hail.is,https://github.com/hail-is/hail/pull/3872#issuecomment-401411707,1,['update'],['update']
Deployability,"Oops, quite right. I pulled this diff from the other install, I just fixed the pieces I deployed there. I didn't intend this to be a common include for all Makefiles (should it be?), but just pull together the relevant defines used in services Makefiles. I grep'ed to make sure the variables defined config.mk are now only defined there.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861:53,install,install,53,https://hail.is,https://github.com/hail-is/hail/pull/9339#issuecomment-679385861,2,"['deploy', 'install']","['deployed', 'install']"
Deployability,"Our cluster is running Centos 6 which uses GLIBC 2.12 while the most recent version of Hail 0.2 requires GLIBC 2.14. It look like Broad must have moved to Centos 7 recently and new releases of their software now require GLIBC 2.14. . The following allowed us to run Hail 0.2 locally but not on the Hadoop Cluster. At least, we did not get an error and the tutorials would run. . LD_PRELOAD=/share/pkg/glibc/2.14/install/lib/libc.so.6 ipython . So I am also interested in a workaround for this that would allow Hail to run on a Centos 6 hadoop cluster. ; Is there a way to compile Hail with GLIBC 2.12? Or set a spark setting to use LD_PRELOAD for the hail jobs that run on the yarn master. Otherwise it looks like our IT staff will need to upgrade our 20 Hadoop nodes to Centos 7 which will require some planning and cluster downtime.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754:181,release,releases,181,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-439969754,3,"['install', 'release', 'upgrade']","['install', 'releases', 'upgrade']"
Deployability,Our production images are mostly tagged with a `deploy-` prefix but there are the `third-party/images.txt` which we need to handle differently. ```; (base) dking@wm28c-761 gar-cleaner % k get pods -o json | jq -r '.items[].spec.containers[].image' | sort -u; ghost:3.0-alpine; prom/prometheus:v2.34.0; us-docker.pkg.dev/hail-vdc/hail/admin-pod:deploy-qd833uw7kcyn; us-docker.pkg.dev/hail-vdc/hail/auth:deploy-crsithjyoxfg; us-docker.pkg.dev/hail-vdc/hail/batch:deploy-kpd6nqk4t25o; us-docker.pkg.dev/hail-vdc/hail/batch:deploy-v1yv8cgd1003; us-docker.pkg.dev/hail-vdc/hail/blog_nginx:deploy-wnrqjf4h6qto; us-docker.pkg.dev/hail-vdc/hail/ci:deploy-du68h4bouvp9; us-docker.pkg.dev/hail-vdc/hail/envoyproxy/envoy:v1.22.3; us-docker.pkg.dev/hail-vdc/hail/grafana/grafana:9.1.4; us-docker.pkg.dev/hail-vdc/hail/monitoring:deploy-ljz4mgjf132m; us-docker.pkg.dev/hail-vdc/hail/notebook:deploy-gmftvyf0op87; us-docker.pkg.dev/hail-vdc/hail/notebook_nginx:deploy-n9uipfhjn3jg; us-docker.pkg.dev/hail-vdc/hail/website:deploy-gb1372nuge4g; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13441#issuecomment-1679603478:48,deploy,deploy,48,https://hail.is,https://github.com/hail-is/hail/issues/13441#issuecomment-1679603478,11,['deploy'],"['deploy', 'deploy-', 'deploy-crsithjyoxfg']"
Deployability,Override merging this because it only touches deploy things (which aren't tested) and it was waiting on other approved PRs.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4616#issuecomment-432424552:46,deploy,deploy,46,https://hail.is,https://github.com/hail-is/hail/pull/4616#issuecomment-432424552,1,['deploy'],['deploy']
Deployability,Overriding and merging directly so it definitely gets included in next deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4583#issuecomment-431457123:71,deploy,deploy,71,https://hail.is,https://github.com/hail-is/hail/pull/4583#issuecomment-431457123,1,['deploy'],['deploy']
Deployability,Overriding review for hotfix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9406#issuecomment-686086418:22,hotfix,hotfix,22,https://hail.is,https://github.com/hail-is/hail/pull/9406#issuecomment-686086418,1,['hotfix'],['hotfix']
Deployability,P_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10438,release,release,10438,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,P_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:dep,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2639,deploy,deploy-,2639,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,Parsy is transitive from curlylint. We can't update until they do.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12934#issuecomment-1708534298:45,update,update,45,https://hail.is,https://github.com/hail-is/hail/pull/12934#issuecomment-1708534298,1,['update'],['update']
Deployability,Patch was merged. Closing issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9833#issuecomment-747571669:0,Patch,Patch,0,https://hail.is,https://github.com/hail-is/hail/issues/9833#issuecomment-747571669,1,['Patch'],['Patch']
Deployability,"Performance note:; to do an aggregation - export sites pipeline, master took 7m, this branch took 14s.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/954#issuecomment-253405048:55,pipeline,pipeline,55,https://hail.is,https://github.com/hail-is/hail/pull/954#issuecomment-253405048,1,['pipeline'],['pipeline']
Deployability,"Pipeline failure, needs a bump :-/",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414:0,Pipeline,Pipeline,0,https://hail.is,https://github.com/hail-is/hail/pull/6237#issuecomment-498882414,1,['Pipeline'],['Pipeline']
Deployability,"Pipeline is a little involved but mostly annotate_rows(some_aggregators), followed by a group_cols_by().aggregate() -> annotate_rows(take())",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4263#issuecomment-418797402:0,Pipeline,Pipeline,0,https://hail.is,https://github.com/hail-is/hail/issues/4263#issuecomment-418797402,1,['Pipeline'],['Pipeline']
Deployability,"Please be picky! You can see what the new UI looks like by checking out this branch and running `make devserver SERVICE=batch`. If you can, I'd also appreciate a sanity check dev deploy to make sure the links to other apps work. I dev deployed it myself but it's hard to make sure I covered all the bases. I struggled a bit with making it mobile friendly but I hope this general approach is a good enough improvement over the current markup. I also don't have much of an opinion on colors if you have thoughts there. I was trying to go for cool and neutral and might have accidentally ended up with ""dentist office""",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14562#issuecomment-2127828741:179,deploy,deploy,179,https://hail.is,https://github.com/hail-is/hail/pull/14562#issuecomment-2127828741,2,['deploy'],"['deploy', 'deployed']"
Deployability,Please update to be consistent with python/pyhail/docs/style-guide.txt and resubmit.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270:7,update,update,7,https://hail.is,https://github.com/hail-is/hail/pull/1114#issuecomment-266934270,3,['update'],['update']
Deployability,Previous dev deploy passed. Rebased on main and submitted a new dev deploy: https://ci.hail.is/batches/8120683,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1927618210:13,deploy,deploy,13,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1927618210,2,['deploy'],['deploy']
Deployability,"Prior to this we add an entry to /etc/hosts so that jobs can contact the batch front end to submit batches. This entry in default is `batch.hail`, and in a dev/pr namespace is `internal.hail`. The batch tests are jobs that run in default but submit test batches to the dev deployment and therefore need an entry for `internal.hail`. This essentially changes it to ""everything knows about internal but only default knows about default. We haven't seen this before because cloud dns in google provides these entries as a fallback, but I never set up in azure because it didn't seem necessary. If this succeeds, we can delete cloud dns in google",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11071#issuecomment-973003982:273,deploy,deployment,273,https://hail.is,https://github.com/hail-is/hail/pull/11071#issuecomment-973003982,1,['deploy'],['deployment']
Deployability,"Prometheus storage was only 10Gb, so it filled up after 14 days. By default, Prometheus deletes logs after 15 days. I increased the storage size to 50Gb accordingly. I also decided to use this opportunity to switch Prometheus from a Deployment to a StatefulSet. This meant turning the PersistentVolume for PrometheusStorage in the monitoring.yaml file to a PersistentVolumeClaim within the Prometheus StatefulSet spec. However, the claim was configured to mount at the same location as the previous PersistentVolume, and I did not first delete the old PersistentVolume. As a result, the new 50Gb disk was not initially allocated. I resolved this by deleting the StatefulSet and the old PersistentVolume, then redeploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681:233,Deploy,Deployment,233,https://hail.is,https://github.com/hail-is/hail/issues/6483#issuecomment-505884681,1,['Deploy'],['Deployment']
Deployability,Protection re-enabled. Hand deploying CI now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8402#issuecomment-606678817:28,deploy,deploying,28,https://hail.is,https://github.com/hail-is/hail/pull/8402#issuecomment-606678817,1,['deploy'],['deploying']
Deployability,"Pushed an additional commit:; - try to deploy all projects from the toplevel file; - at toplevel, only deploy if project has changed,; - added new build convention `<project>/get-deployed-sha.sh` which should output the current deployed sha for the project (if there is one)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841:39,deploy,deploy,39,https://hail.is,https://github.com/hail-is/hail/pull/4408#issuecomment-424187841,4,['deploy'],"['deploy', 'deployed', 'deployed-sha']"
Deployability,"Pushed some more changes:; - first foray into RBAC; - created service account for batch; - batch run jobs in batch-pods namespace; - authorize with role binding; - hand-tested, batch is working. batch will now be found at `batch.default` instead of `batch` when running from batch-jobs namespace. I updated the batch Client to reflect this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232:299,update,updated,299,https://hail.is,https://github.com/hail-is/hail/pull/4545#issuecomment-429681232,1,['update'],['updated']
Deployability,"Python interface changes:; - filter_variants_all -> drop_variants; - filter_samples_all -> drop_samples; - renamed ""condition"" to ""expr"" in parameter names where appropriate. Removed gradle installDist",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318:190,install,installDist,190,https://hail.is,https://github.com/hail-is/hail/pull/1397#issuecomment-280419318,1,['install'],['installDist']
Deployability,"Python versioning is a huge problem. Basically every time we have used unbounded dependency versions, we've gotten burned (some package updates and now Hail is broken for anyone who tries to install it). John could find out that 0.24 is supported, but then we'd have to pin at `<0.25`, so this doesn't solve the problem generally. I think we're also feeling quite sour on conda at the moment as well. In particular, I had to fix the [environment.yml for LDSC](https://github.com/bulik/ldsc/pull/168) because **recent versions of conda removed scipy==0.18 from their registry**.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134:136,update,updates,136,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542183134,2,"['install', 'update']","['install', 'updates']"
Deployability,RE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$argu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6634,release,release,6634,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,REMINDER TO SELF: Change the database configuration before merging in both GCP and Azure!!!!!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12813#issuecomment-1509030448:38,configurat,configuration,38,https://hail.is,https://github.com/hail-is/hail/pull/12813#issuecomment-1509030448,1,['configurat'],['configuration']
Deployability,Re-tested UI with dev deploy: looks good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7606#issuecomment-557911795:22,deploy,deploy,22,https://hail.is,https://github.com/hail-is/hail/pull/7606#issuecomment-557911795,1,['deploy'],['deploy']
Deployability,"Re: your review @danking . We can make the HailContext available on the workers. As far as I can tell, we don't right now because we would need to serialize all the values of HailContext that aren't serializable, broadcast it, and change get to grab the broadcasted value. I could do that. It probably wouldn't take me that long, but this change reverts TabixReader to a behavior that it had during development due to Tim's concern that the hadoop configuration is not serializable. We thought the original version would be okay because TabixReader was only ever constructed on the driver. We were wrong, and considering that we intend to use this to read hundreds of thousands of files at a time, the parallelization is probably a good thing. This change fixes the bug I had in a way consistent with much of our codebase, without making larger changes to how we handle HailContext.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579:448,configurat,configuration,448,https://hail.is,https://github.com/hail-is/hail/pull/5033#issuecomment-449490579,1,['configurat'],['configuration']
Deployability,Rebased and dev deploy kicked off: https://ci.hail.is/batches/8122588,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1932835714:16,deploy,deploy,16,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1932835714,1,['deploy'],['deploy']
Deployability,"Regarding history and fake pages: I’m confused as to why fake pages would be used, since upon refresh that fake page wouldn’t correspond to a real page, but this shouldn’t interfere. The behavior without this solution should be the same: the url is updated with a hash. If you’ve noticed a concrete issue, please share it, because I may not understand the specific use (e.g. RTD). Haven’t seen any issues in testing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789:249,update,updated,249,https://hail.is,https://github.com/hail-is/hail/pull/7334#issuecomment-544675789,1,['update'],['updated']
Deployability,"Regarding the `pyspark` issue, it looks like you have to use `--properties-file` and then put that comma-sepearted list as a newline-separated list in a file. That error message is pyspark's rather terrible way of telling you that it doesn't support a `--properties` option. Regarding the old version of VDS, the `master` branch of hail is now an unstable development branch. If you want a consistent user experience with backwards compatible interfaces, please check out and exclusively use the `0.1` branch. Tim discusses the wider change [here](http://discuss.hail.is/t/deployment-changes-branching-off-for-faster-development/261/1). The VDS format will likely change on the scale of days on the `master` branch. Regarding the `hail/scripts` folder, that is a repository of scripts that our build system uses as templates to create a pre-compiled, ready-to-go distribution that only requires a Spark installation. These distributions are available from the Google Storage API at gs://hail-common/distributions. If you're building from source, I recommend following exactly the steps listed [here](https://hail.is/docs/stable/getting_started.html#building-hail-from-source) so as to avoid any future hiccups. NB: the steps for [Running Hail Locally](https://hail.is/docs/stable/getting_started.html#running-hail-locally) are for using the pre-compiled distribution, not for the result of building hail directly from source.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551:573,deploy,deployment-changes-branching-off-for-faster-development,573,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-320242551,2,"['deploy', 'install']","['deployment-changes-branching-off-for-faster-development', 'installation']"
Deployability,"Related (but for bash scripts): https://github.com/conda/conda/issues/7980. I haven't tried to install anaconda for some months, but when I first tried, ~6mo ago, definitely had this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6738#issuecomment-515163173:95,install,install,95,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515163173,1,['install'],['install']
Deployability,"Reopening this after some changes, mostly to see whether it works with g++-4.8.3 as installed; on the CI machines. The src/main/c/Makefile now builds a libboot.so with -fabi-version=2, which should work against; systems with g++-3.4.0 or later, and both libhail_abi_v2.so and libhail_abi_v9.so. The NativeCode; initialization then figures out which one to load. In theory this should work on MacOS systems back to MacOS 10.9 (Mavericks), which was the first; to use libc++ instead of libstdc++, and on Linux systems with g++3.4.0 or later. By default these libraries are built with ""-march=sandybridge"", which would work on all MacBook Pro's; released since 2011 (and is also the first cpu with AVX). In the medium term I favor the idea of packaging a known good tested compiler into the release, but ; believe that probably won't become critical until we're attempting whole-stage compilation, since the; generated PackDecoder's so far are relatively straightforward code and max out at about 2K lines.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-412736583:84,install,installed,84,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-412736583,3,"['install', 'release']","['installed', 'release', 'released']"
Deployability,Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts\n2022-11-15 20:30:18.318 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts response 200\n2022-11-15 20:30:18.331 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff response 200\n2022-11-15 20:30:18.332 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: running job\n2022-11-15 20:30:18.333 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast\n2022-11-15 20:30:18.697 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast response 200\n2022-11-15 20:30:18.697 BatchClient: INFO: run: created update 2 for batch 6627669\n2022-11-15 20:30:18.697 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.802 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.852 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.866 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.917 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.934 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.985 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.999 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:19.049 Requester: INFO: requ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:24522,update,update-fast,24522,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['update'],['update-fast']
Deployability,Resolved by upgrade and mitigations. Created https://github.com/hail-is/hail/issues/6693 to track the more general issue of containers (non-buggily) running as root in our cluster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6679#issuecomment-513230208:12,upgrade,upgrade,12,https://hail.is,https://github.com/hail-is/hail/issues/6679#issuecomment-513230208,1,['upgrade'],['upgrade']
Deployability,Resolves:. > The provided deployment name 'batch-worker-pr-11104-default-fqwcdahrz1nj-highmem-k29sk-deployment' has a length of '67' which exceeds the maximum length of '64'. Please see https://aka.ms/arm-deploy for usage details.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11108#issuecomment-984122659:26,deploy,deployment,26,https://hail.is,https://github.com/hail-is/hail/pull/11108#issuecomment-984122659,3,['deploy'],"['deploy', 'deployment']"
Deployability,"Revised: You can switch the map and collect order to get more parallelism: groupBy, mapValues with computeUpperIndexBounds, collect, shift relative upper bound indices to absolute upper bound indices, zipWithIndex, feed into computeRectangles. Once we have durable partitionStarts on table, the whole pipeline can be done on the workers, with a final reduce to concatenate the blocksToKeep.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3185#issuecomment-379135882:301,pipeline,pipeline,301,https://hail.is,https://github.com/hail-is/hail/pull/3185#issuecomment-379135882,1,['pipeline'],['pipeline']
Deployability,"Right now we run dataproc tests only on release, not on every commit, because they're too expensive/slow. That way we never release a version that can't pass. I wonder if that's also the right strategy here -- adding QoB release tests for things that only go wrong at scale. That said, I don't want to block on that. Awesome change, thank you!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1499155038:40,release,release,40,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1499155038,3,['release'],['release']
Deployability,"Right, we had to manually re-deploy CI for our setup as well. Pinging @daniel-goldstein just in case :)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-805305318,1,['deploy'],['deploy']
Deployability,Rolling into #13276,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13295#issuecomment-1652368804:0,Rolling,Rolling,0,https://hail.is,https://github.com/hail-is/hail/pull/13295#issuecomment-1652368804,1,['Rolling'],['Rolling']
Deployability,Rolling into #14471,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14477#issuecomment-2064381634:0,Rolling,Rolling,0,https://hail.is,https://github.com/hail-is/hail/pull/14477#issuecomment-2064381634,6,['Rolling'],['Rolling']
Deployability,Rolling into https://github.com/hail-is/hail/pull/13276,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13244#issuecomment-1652373979:0,Rolling,Rolling,0,https://hail.is,https://github.com/hail-is/hail/pull/13244#issuecomment-1652373979,14,['Rolling'],['Rolling']
Deployability,"SClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-spark.jar; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/pyspark.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/pyspark.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:15984,install,install,15984,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"STER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as configured by max-age or --expiration-time flags. At most one of the following may be set:. --max-idle=MAX_IDLE: The duration before cluster is auto-deleted; after last job finished, such as '60m' or '1d'. --no-max-idle: Cancel the cluster auto-deletion by cluster idle; duration (configured by --max-idle flag). See 'gcloud dataproc clusters update --help' for more information. Options:; --update-hail-version Update the version of hail running on; cluster to match the currently installed; version. --wheel TEXT New Hail installation.; --extra-gcloud-update-args TEXT; Extra arguments to pass to 'gcloud dataproc; clusters update'. The 'update' command is; only run if this option is specified. --help Show this message and exit.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:3527,update,update,3527,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,8,"['Update', 'install', 'update']","['Update', 'installation', 'installed', 'update', 'update-args', 'update-hail-version']"
Deployability,Sadly we don't test the release step of `build.yaml` - adding `do-not-test` to not waste resources.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045925528:24,release,release,24,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045925528,1,['release'],['release']
Deployability,"Script for above output:; ```; #load hail; from hail import *. #set minimum partition size and log location; hc = HailContext(min_block_size=50, log=""/home/09mh/kt_troubleshooting_issue_042617.hail.log""). #import bgen and convert to vds; vds = hc.import_bgen(""gs://pipeline/testGWAS/chr1.bgen"",sample_file=""gs://pipeline/testGWAS/inds_info.sample""). kt1 = hc.import_keytable('gs://pipeline/testGWAS/var_anno.tsv', config=TextTableConfig(impute=True,delimiter=' ')).rename(['varid','rsid','C1','C2']).select(['varid','C1','C2']).key_by(['varid']); #check import of var_anno & conversion; print(kt1.schema); print(kt1.key_names); kt1.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['v']); #check keytable made from vds; print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). vds_kt = vds.variants_keytable().flatten().select(['v','va.varid']).key_by(['va.varid']); print(vds_kt.schema); print(vds_kt.key_names); vds_kt.to_dataframe().show(10). kt2 = vds_kt.join(kt1,how='left'); #check join; print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10); kt2 = kt2.key_by(['v']). print('After rekeying:'); print(kt2.schema); print(kt2.key_names); kt2.to_dataframe().show(10). kt2.write('gs://pipeline/testGWAS/chr1_var_anno.kt'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527:265,pipeline,pipeline,265,https://hail.is,https://github.com/hail-is/hail/issues/1725#issuecomment-298355527,4,['pipeline'],['pipeline']
Deployability,Seeing 500 errors on create (maybe latest not deployed to your namespace),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649:46,deploy,deployed,46,https://hail.is,https://github.com/hail-is/hail/pull/7112#issuecomment-535220649,1,['deploy'],['deployed']
Deployability,Seems like monkey patching with event can somehow override the timeout. We're not using event though. https://github.com/kennethreitz/requests/issues/3924#issuecomment-307502871,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575:18,patch,patching,18,https://hail.is,https://github.com/hail-is/hail/issues/5566#issuecomment-471054575,1,['patch'],['patching']
Deployability,Seems to be issue at end:. pip2 install ./; hail-ci-build.sh: line 27: pip2: command not found,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5323#issuecomment-463207883:32,install,install,32,https://hail.is,https://github.com/hail-is/hail/pull/5323#issuecomment-463207883,1,['install'],['install']
Deployability,"Seen in a deploy:; ```; Warning: policy/v1beta1 PodDisruptionBudget is deprecated in v1.21+, unavailable in v1.25+; use policy/v1 PodDisruptionBudget. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11602#issuecomment-1068671635:10,deploy,deploy,10,https://hail.is,https://github.com/hail-is/hail/pull/11602#issuecomment-1068671635,1,['deploy'],['deploy']
Deployability,"ServiceBackend/apiserver is known to not be working right now. It isn't being deployed or maintained. @johnc1231 and @catoverdrive were working on some tasks related to this. Few tasks:; - the global reference state in the JVM backend has to go, and needs to be stored in the Python client. This means reference information needs to be including along with queries.; - Table => CollectDArray lowering needs to be finished so apiserver can use the new `scheduler` to execute pipelines.; - Need to implement GoogleFS on the JVM side. I think someone just needs to take on ""get service backend working again"". As per our quarterly planning discussion, it might make sense to focus on upstream tasks for now (lowering, batch).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477:78,deploy,deployed,78,https://hail.is,https://github.com/hail-is/hail/issues/7068#issuecomment-531915477,2,"['deploy', 'pipeline']","['deployed', 'pipelines']"
Deployability,"Should be fixed there. Also, the organization of site vs docs is super confusing imo. We have a site folder, which contains the Nginx configuration of site, and also the kube definition of the site deployment. Which makes a lot of sense. However, it also needs files in ../hail/build/www. Those files are built using a script in /hail/python/docs, which grabs www files from the working directory, which in our case should be /hail and not /site, copies those to its ./build/www, merges them with a bunch of files from /hail/python/docs/... , but not only that, it also compiles all of the templates for our cwd ./www. Oh and we also test hail import during doc build, which seems outside of what a documentation / static html build process should do.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913:134,configurat,configuration,134,https://hail.is,https://github.com/hail-is/hail/issues/5282#issuecomment-463307913,2,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,Should probably be stacked on #11636. I'll send an email notifying others that they need to upgrade their node pools by the time this goes in.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11638#issuecomment-1076345988:92,upgrade,upgrade,92,https://hail.is,https://github.com/hail-is/hail/pull/11638#issuecomment-1076345988,1,['upgrade'],['upgrade']
Deployability,Shuffling reviewers since I'd like to get this merged soonish so I can turn on deployment for 0.1,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4225#issuecomment-417308800:79,deploy,deployment,79,https://hail.is,https://github.com/hail-is/hail/pull/4225#issuecomment-417308800,1,['deploy'],['deployment']
Deployability,"Since everyone is asking about hardcalls:. ```; # (cd ../hail && gradle installDist) && ../hail/build/install/hail/bin/hail read -i profile225-splitmulti-hardcalls.vds ibd -o hail.genome ; :nativeLib UP-TO-DATE; :compileJava UP-TO-DATE; :compileScala UP-TO-DATE; :processResources UP-TO-DATE; :classes UP-TO-DATE; :jar UP-TO-DATE; :startScripts UP-TO-DATE; :installDist UP-TO-DATE. BUILD SUCCESSFUL. Total time: 2.728 secs; hail: info: running: read -i profile225-splitmulti-hardcalls.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.619s; hail: info: timing:; read: 3.824s; ibd: 3m19.2s; total: 3m23.1s. # dc; 5 k; 3 60 * 23 + ; 23 / p; 8.82608; ```. about 9x slower now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639:72,install,installDist,72,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260651639,3,['install'],"['install', 'installDist']"
Deployability,"Since my interface package isn't ready for release yet, here's a reproducible example using just R and sparklyr, with a Hail jar somewhere. Again, this is happening on the Mac, sparklyr version 0.8.4.9004 (there is probably a newer one on CRAN, I doubt that it matters). ```; data(mtcars); hail_jar <- ""/path/to/your/hail-all-spark.jar""; classpath_vars <-; c(spark.driver.extraClassPath=paste(hail_jar, collapse=.Platform$path.sep),; spark.executor.extraClassPath=paste(basename(hail_jar),; collapse=.Platform$path.sep)); config <- list(sparklyr.jars.default=hail_jar,; sparklyr.shell.conf=paste0(names(classpath_vars), ""='"",; classpath_vars, ""'""),; spark.serializer=""org.apache.spark.serializer.KryoSerializer"",; spark.kryo.registrator=""is.hail.kryo.HailKryoRegistrator""); sc <- sparklyr::spark_connect(""local"", version=""2.2.0"", config=config); sdf <- sparklyr::spark_dataframe(dplyr::copy_to(sc, mtcars)); hc <- sparklyr::invoke_static(sc, ""is.hail.HailContext"", ""apply"",; sparklyr::spark_context(sc), ""Hail"", NULL,; ""local[*]"", ""hail.log"", TRUE, FALSE, 1L, 50L,; tempdir()); keys <- sparklyr:::invoke_static(sc, ""is.hail.utils"", ""arrayToArrayList"",; array(character(0L))); ht <- sparklyr::invoke_static(sc, ""is.hail.table.Table"", ""fromDF"", hc, sdf,; keys); sparklyr::invoke(ht, ""count""); ```. Thanks a lot for your continued attention to this issue.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190:43,release,release,43,https://hail.is,https://github.com/hail-is/hail/issues/4513#issuecomment-429475190,1,['release'],['release']
Deployability,"Since netcdf broke my R installation, I upgraded R. Now to revert to 3.3.1, I'm trying to install from the downloadable tarball and running into a bunch of errors. Is this worth it? Why don't we just test against a static results file?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235:24,install,installation,24,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-377952235,3,"['install', 'upgrade']","['install', 'installation', 'upgraded']"
Deployability,"Since the dataproc tests only run on main commits (not on every PR commit, due to cost), I submitted a dev deploy to test the latest commit to this branch against dataproc: https://ci.hail.is/batches/8119055. ```; hailctl dev deploy -b danking/hail:dataproc-2.2 -s test_dataproc-37 -s test_dataproc-38; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1924204910:107,deploy,deploy,107,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1924204910,2,['deploy'],['deploy']
Deployability,"Since this doesn't get tested by CI normally, running a dev deploy here: https://ci.hail.is/batches/403111. If it succeeds, we should feel good I think.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11273#issuecomment-1022499548:60,deploy,deploy,60,https://hail.is,https://github.com/hail-is/hail/pull/11273#issuecomment-1022499548,1,['deploy'],['deploy']
Deployability,"So I did some simple formatting on the ""Filter loci by a list of locus intervals"" example. . The cloud sphinx theme you mentioned on zulip has toggleable sections that look a bit nicer. I could emulate that formatting by writing a sphinx extension if we wanted to get fancier, but what do you think of this layout?. IMAGE 1. <img width=""720"" alt=""screen shot 2018-08-22 at 11 23 34 am"" src=""https://user-images.githubusercontent.com/35241112/44473344-1eb11c80-a5fe-11e8-954d-41440a031d24.png"">. IMAGE 2; clicking on `show` would expose more content:. <img width=""699"" alt=""screen shot 2018-08-22 at 11 23 46 am"" src=""https://user-images.githubusercontent.com/35241112/44473350-2375d080-a5fe-11e8-98e9-31f1c3bb825c.png"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240:143,toggle,toggleable,143,https://hail.is,https://github.com/hail-is/hail/pull/4089#issuecomment-415074240,1,['toggle'],['toggleable']
Deployability,"So I think we should fix this is in a slightly different way. First, I want to unify the log and status page. Second, I want the status to actually include the whole job configuration, of which status is just a sub-field, so you can look at detail to what you submitted. Then it would make sense for these links to always be present. That said, they obviously shouldn't be broken. Instead of 404, we should just say ""job is pending, no logs"" or wahtever. Let me make pass on the batch2 UI and then revisit this, OK?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632:170,configurat,configuration,170,https://hail.is,https://github.com/hail-is/hail/pull/7449#issuecomment-549809632,1,['configurat'],['configuration']
Deployability,"So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. FYI, you can't dev deploy monitoring. It's part of the infrastructure.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584:134,deploy,deploy,134,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-539927584,2,['deploy'],['deploy']
Deployability,So it seems like the problem is with ; `gs://future-variant-calling/future-pipeline/future.vds`. trying to run `sample_qc` on it also fails in exactly the same way...,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743#issuecomment-359552294:75,pipeline,pipeline,75,https://hail.is,https://github.com/hail-is/hail/issues/2743#issuecomment-359552294,1,['pipeline'],['pipeline']
Deployability,"So overall plan is:. - get 2.2.0 build support in (this patch); - stop testing 2.1.0 once it isn't being deployed,; - start testing 2.2.0 (I will need to update the CI image to install Spark 2.2.0),; - add 2.2.0 to the list of deployed versions,; - make Spark 2.2.0/Dataproc 1.2 the version in cloudtools,; - drop testing/deploy support for 2.0.2. Did I miss anything?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2656#issuecomment-355617537:56,patch,patch,56,https://hail.is,https://github.com/hail-is/hail/pull/2656#issuecomment-355617537,6,"['deploy', 'install', 'patch', 'update']","['deploy', 'deployed', 'install', 'patch', 'update']"
Deployability,So the master merge commit status points to the master tests logs but not the deploy logs. I was hoping for the deploy logs (although the test logs are good if we have both).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286:78,deploy,deploy,78,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438075286,2,['deploy'],['deploy']
Deployability,"So the problem is probably that CI is running on master, and it creates the global config in the PR namespace and it won't have docker_root_image. We'll also need to update our production global-config since we're not applying Terraform updates to our cluster. You should be able to do this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10107#issuecomment-799714090:166,update,update,166,https://hail.is,https://github.com/hail-is/hail/pull/10107#issuecomment-799714090,2,['update'],"['update', 'updates']"
Deployability,"So the reason for that error is this rule is insufficient:. ```scala; case x if x.typ == TVoid =>; x.children.foreach(c => infer(c.asInstanceOf[IR])); PVoid; ```. We need to update the environment as well, in some cases, such as ArrayFor",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828:174,update,update,174,https://hail.is,https://github.com/hail-is/hail/pull/8063#issuecomment-584451828,1,['update'],['update']
Deployability,"So this is the same problem you discovered last year. It's supposed to get fixed in 4.0.0, but that hasn't been released yet. https://github.com/hail-is/hail/commit/337383674697f51dd6e04a3be3acedf4ed7a59a9",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10432#issuecomment-841481516:112,release,released,112,https://hail.is,https://github.com/hail-is/hail/pull/10432#issuecomment-841481516,1,['release'],['released']
Deployability,"So we already support building with Spark 3.2 if you build your own jar. We just use 3.1 for our pypi release because it's what Google, AWS, and Azure have their respective Spark images set to last time I checked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11707#issuecomment-1085063134:102,release,release,102,https://hail.is,https://github.com/hail-is/hail/issues/11707#issuecomment-1085063134,1,['release'],['release']
Deployability,"So, certain versions of bokeh require certain versions of pandas. I don't think we can simultaneously support bokeh 1.4 and pandas 2 in the Hail code base (because old bokeh is broken on new pandas). I think the fix is to just forcibly upgrade everyone to latest bokeh (3.x) and update Hail to support latest bokeh. I have a PR coming for this. I haven't checked if new bokeh supports old pandas. Nor do I know if we have old pandas usage lurking in the codebase. Can we make our `pinned-requirements.txt` use pandas 2.0, fix whatever issues arise, but leave `requirements.txt` flexible for folks?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520463643:236,upgrade,upgrade,236,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520463643,2,"['update', 'upgrade']","['update', 'upgrade']"
Deployability,Some notes from discussion:; 1. Maybe add a pricing page with up to date pricing for resources.; 2. It is difficult to determine _all_ the work that will run just from a hail pipeline.; 3. Teach users how to inspect the work that hail actually does?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14711#issuecomment-2397522782:175,pipeline,pipeline,175,https://hail.is,https://github.com/hail-is/hail/issues/14711#issuecomment-2397522782,1,['pipeline'],['pipeline']
Deployability,"Some progress and new blocker on this topic. I moved to emr-6.11.1 that come with spark 3.3.2 & scala 2.12.15.; I upgraded the environment to get python 3.9 and java 11. * `emr-6.11.1`; * Java: java -version `11.0.20` (/usr/bin/java); * Python: python --version `3.9.18` (/usr/bin/python3); * Hadoop: hadoop version `3.3.3` (/usr/bin/hadoop); * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.15` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. But then once I build hail on this environment, the spark version is downgraded to 2.12.13 and the Java error above come back. ```sh; cd /tmp; git clone --branch 0.2.124 --depth 1 https://github.com/broadinstitute/hail.git; cd hail/hail/; make install-on-cluster HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.15 SPARK_VERSION=3.3.2; ```. * Spark: spark-shell --version `3.3.2` (usr/bin/spark-shell); * Scala: spark-shell --version `2.12.13` (usr/bin/spark-shell). ```sh; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2; /_/; ; Using Scala version 2.12.13, OpenJDK 64-Bit Server VM, 11.0.20.1; ```. If I purposly build Hail for scala 2.12.13, the Java error above come back.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000:114,upgrade,upgraded,114,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1767910000,2,"['install', 'upgrade']","['install-on-cluster', 'upgraded']"
Deployability,"Something still isn't right with my configuration. The service and deployment are all up. I can curl to the internal gateway and it shows up in the logs. However, I'm getting 404 with this query: `http://hail.internal/jigold/batch2/healthcheck`. I set up cloud dns to route hail.internal to the internal gateway. I verified no traffic is recorded in the jigold router. I tried adding and removing the `gateway` service account but it made no difference.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114:36,configurat,configuration,36,https://hail.is,https://github.com/hail-is/hail/pull/6918#issuecomment-524103114,2,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,"Sometimes new versions of packages introduce breaking changes, which is why we have version pins. I'll look at pandas shortly and see if there's any reason we can't update the upper bound on that range though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7299#issuecomment-542180948:165,update,update,165,https://hail.is,https://github.com/hail-is/hail/issues/7299#issuecomment-542180948,1,['update'],['update']
Deployability,"Somewhat related to this is the deployment configuration. That's a little easier to handle on the user's side by including it in the job's image directly, but it would be pretty convenient if it could be mounted automatically as well (as `/deploy-config/deploy-config.json`, with `""location"": ""gce""` and the `domain` set accordingly). Should that be another method on `Job` / `Batch` or should all of this functionality be hidden behind something more abstract to enable nested batches (that's really the main use case, I suppose).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9907#issuecomment-767883386:32,deploy,deployment,32,https://hail.is,https://github.com/hail-is/hail/pull/9907#issuecomment-767883386,4,"['configurat', 'deploy']","['configuration', 'deploy-config', 'deployment']"
Deployability,"Sorry @tpoterba , there's still more work to do to integrate this throughout Hail without tests failing. That's the next step!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359:51,integrat,integrate,51,https://hail.is,https://github.com/hail-is/hail/pull/1559#issuecomment-287180359,1,['integrat'],['integrate']
Deployability,Sorry about that! Didn't realize it needed to be updated in 2 places.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3179#issuecomment-374385002:49,update,updated,49,https://hail.is,https://github.com/hail-is/hail/pull/3179#issuecomment-374385002,1,['update'],['updated']
Deployability,"Sorry for a fairly late comment on this PR, but I was wondering about the default configuration:. > CHANGELOG: Added a new method Job.regions() as well as a configurable parameter to the ServiceBackend to specify which cloud regions a job can run in. The default value is a job can run in any available region. We're looking forward to the functionality in this PR particularly because we're hoping that it'll allow us to schedule workers in the US, while our Batch deployment is in Australia. However, by default we really need to make sure that workers won't be scheduled in the US, to avoid accidental egress charges, as all our datasets are located in Australia. For processing gnomAD data (which is located in the US), spinning up workers colocated with the data would be fantastic though. Hence we'd really need a configurable default value on the deployment level, I believe:. - Generally allow scheduling in AU + US regions (specifically `australia-southeast1` and `us-central1`).; - By default, pick any region in AU only (in practice `australia-southeast1`).; - Allow jobs to explicitly specify to run in the US (in practice `us-central1`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218:82,configurat,configuration,82,https://hail.is,https://github.com/hail-is/hail/pull/12221#issuecomment-1275427218,3,"['configurat', 'deploy']","['configuration', 'deployment']"
Deployability,Sorry for not getting this done quicker. There's two new soon to be PRs in the stack that you can see as commits here:; - [Add infrastructure for updates](https://github.com/hail-is/hail/pull/12010/commits/72ff68e628b97bae439d04d4cb45e8508941e8bb); - [Cleanup adding update id infrastructure](https://github.com/hail-is/hail/pull/12010/commits/6364402e965a4f33248eba21639642e14a6f82be). I'll make PRs for them on Monday once you give me the green light that no other major database changes are needed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109305:146,update,updates,146,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1221109305,2,['update'],"['update', 'updates']"
Deployability,"Sorry no one answered this earlier (consider bugging us on hail.zulipchat.com or discuss.hail.is). The first thing I'd say is update to a newer version of hail and see if this still happens, as you're 15 versions behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8106#issuecomment-597321135:126,update,update,126,https://hail.is,https://github.com/hail-is/hail/issues/8106#issuecomment-597321135,1,['update'],['update']
Deployability,"Sorry one more place I remembered this needs to be changed in benchmark-service/deployment.yaml. ```; {% if deploy %}; - name: HAIL_BENCHMARK_BUCKET_NAME; value: hail-test; - name: START_POINT; value: ""2020-11-01T00:00:00Z""; - name: INSTANCE_ID; value: ""WetqnMQMoqq2""; {% else %}; - name: HAIL_BENCHMARK_BUCKET_NAME; value: hail-test-dmk9z; {% endif %}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10807#issuecomment-905864074:80,deploy,deployment,80,https://hail.is,https://github.com/hail-is/hail/pull/10807#issuecomment-905864074,2,['deploy'],"['deploy', 'deployment']"
Deployability,Sorry this was a lot more broken than I thought. I didn't remember everything I stripped down for the previous PRs and didn't add back in. The commit update and update-fast endpoints need to return the `start_job_id`. Hopefully that's the last of these issues,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276:150,update,update,150,https://hail.is,https://github.com/hail-is/hail/pull/12199#issuecomment-1255533276,2,['update'],"['update', 'update-fast']"
Deployability,Sorry to step on your toes here a little bit @illusional! I updated aiohttp to 0.7.4 so you don't have to bump it here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10106#issuecomment-786920511:60,update,updated,60,https://hail.is,https://github.com/hail-is/hail/pull/10106#issuecomment-786920511,1,['update'],['updated']
Deployability,"Sorry you hit this -- I think I understand what happened. Since we don't include jupyter as a Hail package dependency (it's a large dependency and pulls in a host of transitive dependencies as well), when you ran `jupyter` you picked up a different `jupyter` (probably the conda base environment one, which uses an entirely different Python installation). . I don't want to add jupyter as a dependency, but we can certainly add a note in the tutorials landing page.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377:341,install,installation,341,https://hail.is,https://github.com/hail-is/hail/issues/7136#issuecomment-535590377,1,['install'],['installation']
Deployability,"Sorry, I wasn't clear before. The Batch LD Clumping example does not require Hail Query (and, more importantly, a JVM) to be installed on *the computer that submits the batch*. Hail is imported and used inside of the Batch task that performs GWAS. That task runs inside a Docker container that has Hail installed (its derived from `hailgenetics/hail`). I'm hesitant to make the *submission* of a batch dependent on the Hail Query library. Particularly when we have relatively low-effort alternative approaches. I'm delighted any time I see batch tasks use Hail Query! Konrad's Pan UKB work also does this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400:125,install,installed,125,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-671357400,2,['install'],['installed']
Deployability,"Sorry, didn't see this earlier -- . this is intentional. We do this so that we don't require users to recompile the C libraries when the install the Python library. Our native library distribution story is definitely a work in progress and this will change (hopefully improve) in the future. Feel free to ping us if this answer isn't sufficient!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152:137,install,install,137,https://hail.is,https://github.com/hail-is/hail/issues/10170#issuecomment-827847152,1,['install'],['install']
Deployability,"Sorry, should have clarified - the above code ""monkey-patches"" the subprocess Popen function so it prints before it runs. Just pop the code above your `hc = HailContext(...)`, everything else can remain unchanged.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2062#issuecomment-319716779:54,patch,patches,54,https://hail.is,https://github.com/hail-is/hail/issues/2062#issuecomment-319716779,1,['patch'],['patches']
Deployability,"Sorry, the title is wrong, this fixes hail batch deployment",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4442#issuecomment-424543053:49,deploy,deployment,49,https://hail.is,https://github.com/hail-is/hail/pull/4442#issuecomment-424543053,1,['deploy'],['deployment']
Deployability,Sounds good. Updated this PR to add `first` and `last` methods.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9474#issuecomment-694910861:13,Update,Updated,13,https://hail.is,https://github.com/hail-is/hail/pull/9474#issuecomment-694910861,1,['Update'],['Updated']
Deployability,"Spark 3.1.1 is out, dataproc image should be updated from the release candidate dependency within a week or so I think. The only remaining issue I think is a weird one, a particular blockmatrix test is failing because json4s can't find a constructor for `BlockMatrixSparsity` objects. Looking into it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10054#issuecomment-790702427:45,update,updated,45,https://hail.is,https://github.com/hail-is/hail/pull/10054#issuecomment-790702427,2,"['release', 'update']","['release', 'updated']"
Deployability,"Spark doesn't have per-type requiredness but a nullable flag on array elements and struct fields. You just needed to set our required status based on their struct fields. Here is a patch that fixes it:. ```; diff --git a/src/main/scala/is/hail/expr/AnnotationImpex.scala b/src/main/scala/is/hail/expr/AnnotationImpex.scala; index 5039471..4cedfd8 100644; --- a/src/main/scala/is/hail/expr/AnnotationImpex.scala; +++ b/src/main/scala/is/hail/expr/AnnotationImpex.scala; @@ -45,11 +45,11 @@ object SparkAnnotationImpex extends AnnotationImpex[DataType, Any] {; case DoubleType => TFloat64(); case StringType => TString(); case BinaryType => TBinary(); - case ArrayType(elementType, _) => TArray(importType(elementType)); + case ArrayType(elementType, containsNull) => TArray(importType(elementType).setRequired(!containsNull)); case StructType(fields) =>; TStruct(fields.zipWithIndex; .map { case (f, i) =>; - (f.name, importType(f.dataType)); + (f.name, importType(f.dataType).setRequired(!f.nullable)); }: _*); }; ; diff --git a/src/test/scala/is/hail/methods/KeyTableSuite.scala b/src/test/scala/is/hail/methods/KeyTableSuite.scala; index 8a46826..dbb3485 100644; --- a/src/test/scala/is/hail/methods/KeyTableSuite.scala; +++ b/src/test/scala/is/hail/methods/KeyTableSuite.scala; @@ -380,9 +380,9 @@ class KeyTableSuite extends SparkSuite {; .flatten(); ; val df = kt.toDF(sqlContext); -// df.printSchema(); -// df.show(); - val kt2 = KeyTable.fromDF(hc, df); + df.printSchema(); + df.show(); + val kt2 = KeyTable.fromDF(hc, df, key = Array(""v"")); assert(kt2.same(kt)); }; ; ```. We should require the KeyTable row type to be required. We've always worked internally with the invariant that rows themselves can't be missing. (If they were, they wouldn't be in the table.) I commented in the print statements to see what was going on. You should probably delete them. I also had to set the key in `fromDF` the key isn't represented in DataFrames.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2417#issuecomment-343800938:181,patch,patch,181,https://hail.is,https://github.com/hail-is/hail/pull/2417#issuecomment-343800938,1,['patch'],['patch']
Deployability,Specifically : https://github.com/akotlar/hail/blob/3b639cf77e2ad44c3422b619a36cf33523032953/notebook2/hail-ci-deploy.sh,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5451#issuecomment-467633773:111,deploy,deploy,111,https://hail.is,https://github.com/hail-is/hail/pull/5451#issuecomment-467633773,1,['deploy'],['deploy']
Deployability,Still need to update a use case or two.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8365#issuecomment-604286431:14,update,update,14,https://hail.is,https://github.com/hail-is/hail/pull/8365#issuecomment-604286431,1,['update'],['update']
Deployability,Still need to update ci.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8405#issuecomment-606756521:14,update,update,14,https://hail.is,https://github.com/hail-is/hail/pull/8405#issuecomment-606756521,1,['update'],['update']
Deployability,"Still needs a black reformat. You can run `make -C batch/ check` ahead of time to catch these errors or add pre-commit hooks. ```; PYTHONPATH=${PYTHONPATH:+${PYTHONPATH}:}../hail/python:../gear:../web_common python3 -m black . --line-length=120 --skip-string-normalization --check --diff; --- batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:28:49.199357 +0000; +++ batch/cloud/gcp/driver/create_instance.py	2022-06-02 12:31:14.836500 +0000; @@ -78,15 +78,17 @@; 'automaticRestart': False,; 'onHostMaintenance': 'TERMINATE',; }; ; if preemptible:; - result.update({; - 'provisioningModel': 'SPOT',; - 'instanceTerminationAction': 'DELETE',; - 'preemptible': True,; - }); + result.update(; + {; + 'provisioningModel': 'SPOT',; + 'instanceTerminationAction': 'DELETE',; + 'preemptible': True,; + }; + ); ; return result; ; return {; 'name': machine_name,; would reformat batch/cloud/gcp/driver/create_instance.py; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521:564,update,update,564,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144893521,2,['update'],['update']
Deployability,"Strange, I can't reply to directly to your last comment. > We have a difference of opinion about the risks. I think I'd say we have a difference of opinion about the importance of the risks. I'm well aware of the potential pitfalls you list there, and more. I just don't think they're a very big deal. I'm also aware of a shit ton of things that are vastly more important than what we're arguing about and we're not talking about those. Let's talk about goals for the project and the landscape of technical risk in our next 1:1. This is assuming we're controlling the compiler in the packaged distribution and on the cloud, we're testing representative user pipelines against gcc and clang, so the scenario you're imagining is either a Hail developer or someone who is sophisticated enough to maintain a Spark cluster (1000x worse configuration nonsense than we're arguing about here, I promise) who is either (1) running old or obscure compiler, or (2) ran into a bug that had test coverage. You're worrying about (1)? What's the worst that will happen, seriously? We'll get a bug report? Let's make sure the compiler version is in the log. > A couple of years ago; > g++ take 40-60 seconds to compile; > fairly heavily templated cod. Can we avoid heavily (or even moderately) templated code? I'm already nervous long-term about the latency of the C++ compiler overhead and if I'm being honest would prefer to generate LLVM IR directly into memory. We should ship whatever compiler is best on the cloud and in the download package. That already covers a vast majority of our users. If clang is the clear winner, we can make that clear in the documentation and maybe warn about gcc it on startup. > But that becomes a problem in itself if we want the shipped compiler to work on a variety of OS'es. Variety isn't a requirement. We don't need to make this hard for ourselves. Let's have two versions: OSX and a recent linux. If we're getting a lot of requests/questions/issues about older versions of l",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414:658,pipeline,pipelines,658,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-410134414,2,"['configurat', 'pipeline']","['configuration', 'pipelines']"
Deployability,"Strange, I couldn't get a similar example to fail, either through `hl.eval` or in a pipeline:. ```; def test_define_function_locus(self):; contig2 = hl.experimental.define_function(; lambda l: l.contig, hl.tlocus(hl.get_reference('GRCh38'))); t = hl.utils.range_table(1); t = t.annotate(locus = hl.locus('chr22', 123, 'GRCh38')); t = t.annotate(contig = contig2(t.locus)); self.assertEqual(t.collect()[0]['contig'], 'chr22'); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5471#issuecomment-469341849:84,pipeline,pipeline,84,https://hail.is,https://github.com/hail-is/hail/pull/5471#issuecomment-469341849,1,['pipeline'],['pipeline']
Deployability,"Strange, the bad point seems to be:; ```; ++ mktemp -d; + REPO_DIR=/tmp/tmp.2v53NEHcHZ; + cp test-repo/hail-ci-build-image test-repo/hail-ci-build.sh test-repo/hail-ci-deploy.sh /tmp/tmp.2v53NEHcHZ; /tmp/tmp.2v53NEHcHZ /hail/repo/ci; + pushd /tmp/tmp.2v53NEHcHZ; + git init; Initialized empty Git repository in /tmp/tmp.2v53NEHcHZ/.git/; + git config user.email ci-automated-tests@broadinstitute.org; + git config user.name ci-automated-tests; + set +x; + git add hail-ci-build-image hail-ci-build.sh hail-ci-deploy.sh; + git commit -m 'inital commit'; [master (root-commit) da0ddab] inital commit; 3 files changed, 26 insertions(+); create mode 100644 hail-ci-build-image; create mode 100644 hail-ci-build.sh; create mode 100644 hail-ci-deploy.sh; + git push origin master:master; error: RPC failed; HTTP 404 curl 22 The requested URL returned error: 404 Not Found; fatal: The remote end hung up unexpectedly; fatal: The remote end hung up unexpectedly; Everything up-to-date; + cleanup; ```; the `git puts origin master:master`. Hidden from the logs is the URL because it contains a token.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662:168,deploy,deploy,168,https://hail.is,https://github.com/hail-is/hail/issues/4517#issuecomment-428988662,3,['deploy'],['deploy']
Deployability,Successfully ran pipeline in #2377 and found identical output.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358385663:17,pipeline,pipeline,17,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358385663,1,['pipeline'],['pipeline']
Deployability,"Suggested fix:. ```scala; case class UpdatedRow(orig: Row, i: Int, update: Any) extends Row {; ...; }. ```; This gets you the update with one allocation and no copy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410:37,Update,UpdatedRow,37,https://hail.is,https://github.com/hail-is/hail/issues/1629#issuecomment-290846410,3,"['Update', 'update']","['UpdatedRow', 'update']"
Deployability,"Sure! Now that we have docker working, CI should be deployable.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4446#issuecomment-424729339:52,deploy,deployable,52,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424729339,1,['deploy'],['deployable']
Deployability,"Sure, I'd like to know how the pip deploy works.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509644515:35,deploy,deploy,35,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509644515,1,['deploy'],['deploy']
Deployability,"TE 2].; > ; > I don't think changing these will have the desired effect and may make it impossible for someone to reproduce the database. The only changes to _existing_ sql you'll need to make are in the sql strings in python code.; > ; > 2. This needs to be written as a migration and maybe could be simplified?; > ; > I think this needs to be done as a database migration. We'll have no need for a stored procedure once complete. You can assume current columns and constraints exist, dispense with the error checking and simplify. Can you convert this to a sql script and add it to the end of the list of migrations in `build.yaml`? You'll probably want `online: false` too. I fear you'll have to take inspiration from `rename-job-groups-tables.sql` by applying one `ALTER TABLE` command then drop and recreate EVERYTHING that references that name (constraints, triggers, procedures etc). This will likely involve copy+paste and rename. Alternatively, create, execute then drop the procedure within `rename-job-groups-cancelled`.; > ; > [NOTE 1] migration applied in `build.yaml`; > ; > The relevant build step in `build.yaml` can be found by searching for the entry starting with the yaml below. This controls which migrations are applied and in what order.; > ; > ```yaml; > kind: createDatabase2; > name: batch_database; > databaseName: batch; > ```; > ; > [NOTE 2] estimated-current.yaml; > ; > I don't agree with why we have this. It would be nice to generate this automatically. Anyway, please keep your changes to this file as it's meant for documentation purposes only. None of it is applied and who knows how much of it works. Got it! I wasn't sure how Hail usually does schema update. Based on your above description the process becomes clearer ro me. Here's my second try:. - Updated `build.yaml` in the `batch` database migrations section.; - Simplified the sql in `rename-job-groups-cancelled-column.sql`. Do you mean `estimated-current.sql` rather than `estimated-current.yaml` above?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045:2105,update,update,2105,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2334778045,2,"['Update', 'update']","['Updated', 'update']"
Deployability,TE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/h,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2978,deploy,deploy-,2978,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,"Tested as follows from a clean environment.; 1. Build the jars and wheel in release mode:; ```bash; HAIL_RELEASE_MODE=1 make -C hail wheel; ```. 2. Dry-run the upload-artifacts target and inspect output; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:76,release,release,76,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,3,"['deploy', 'release']","['deploy', 'release']"
Deployability,"Tested by deploying it, I know it works. No one else is awake.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8710#issuecomment-624411877:10,deploy,deploying,10,https://hail.is,https://github.com/hail-is/hail/pull/8710#issuecomment-624411877,1,['deploy'],['deploying']
Deployability,"Tested by installing wheel in clean venv, importing and running basic hail commands",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14631#issuecomment-2243375019:10,install,installing,10,https://hail.is,https://github.com/hail-is/hail/pull/14631#issuecomment-2243375019,1,['install'],['installing']
Deployability,Tested by running `hailctl dev deploy -b hail-is/hail:main -s git_make_bash_image` against a CI in my namespace and seeing that it succeeded and pushed an image [here](https://console.cloud.google.com/artifacts/docker/hail-vdc/us/hail/ci-intermediate?project=hail-vdc),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1253928151:31,deploy,deploy,31,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1253928151,1,['deploy'],['deploy']
Deployability,"Tested on https://internal.hail.is/chrisl/auth/user and comparing with previous, non-updated behavior",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14636#issuecomment-2248848361:85,update,updated,85,https://hail.is,https://github.com/hail-is/hail/pull/14636#issuecomment-2248848361,1,['update'],['updated']
Deployability,"Tested this out by running it locally and changing `aiohttp-session` in `docker/requirements.txt` from `2.70` to `2.8.0` and got the following:. ```; + pip-compile --quiet docker/requirements.txt docker/pinned-requirements.txt --output-file=new-pinned.txt; Could not find a version that matches aiohttp-session==2.7.0,==2.8.0 (from -r docker/requirements.txt (line 4)); Tried: 0.0.1, 0.0.1, 0.1.0, 0.1.0, 0.1.1, 0.1.1, 0.1.2, 0.1.2, 0.2.0, 0.2.0, 0.3.0, 0.3.0, 0.4.0, 0.4.0, 0.5.0, 0.5.0, 0.7.0, 0.7.0, 0.7.1, 0.7.1, 0.8.0, 0.8.0, 1.0.0, 1.0.0, 1.0.1, 1.0.1, 1.1.0, 1.1.0, 1.2.0, 1.2.0, 1.2.1, 1.2.1, 2.0.0, 2.0.0, 2.0.1, 2.0.1, 2.1.0, 2.1.0, 2.2.0, 2.2.0, 2.3.0, 2.3.0, 2.4.0, 2.4.0, 2.5.1, 2.5.1, 2.6.0, 2.6.0, 2.7.0, 2.7.0, 2.8.0, 2.8.0, 2.9.0, 2.9.0, 2.10.0, 2.10.0, 2.11.0, 2.11.0; Skipped pre-versions: 2.10.0a0, 2.10.0a0; There are incompatible versions in the resolved dependencies:; aiohttp-session==2.7.0 (from -r docker/pinned-requirements.txt (line 20)); aiohttp-session==2.8.0 (from -r docker/requirements.txt (line 4)); ```. and another example where I added an unrelated pip dependency in the requirements but didn't update the lock file",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11842#issuecomment-1131987651:1132,update,update,1132,https://hail.is,https://github.com/hail-is/hail/pull/11842#issuecomment-1131987651,1,['update'],['update']
Deployability,"Tested using modified code from Lindsay Liang on zulip:. ```python; import hail as hl; hl.init(log='hail.log'); rg37 = hl.get_reference('GRCh37'). rg38 = hl.get_reference('GRCh38'); rg37.add_liftover('gs://hail-common/references/grch37_to_grch38.over.chain.gz', rg38); gnomad_ht = hl.read_table('gs://gcp-public-data--gnomad/release/2.1.1/ht/exomes/gnomad.exomes.r2.1.1.sites.ht'). gnomad_ht = gnomad_ht.annotate(new_locus=hl.liftover(gnomad_ht.locus, 'GRCh38')); gnomad_ht = gnomad_ht.key_by(locus=gnomad_ht.new_locus, alleles=gnomad_ht.alleles); mt = hl.balding_nichols_model(3, 100, 10_000, reference_genome='GRCh38'); mt = mt.annotate_entries(AD=hl.zeros(hl.len(mt.alleles))); mt = mt.annotate_rows(gnomad_non_neuro_AF =; gnomad_ht.index(mt.row_key).freq[hl.eval(gnomad_ht.freq_index_dict[""non_neuro""])].AF); mt = mt.annotate_entries(pAB = hl.or_missing(mt.GT.is_het(),; hl.binom_test(mt.AD[1], hl.sum(mt.AD), 0.5, 'two-sided'))); mt._force_count_rows(); ```. This faithfully replicated the issue and went from pretty much every task failing at least once as they read bad state to no tasks failing.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10888#issuecomment-924123558:325,release,release,325,https://hail.is,https://github.com/hail-is/hail/pull/10888#issuecomment-924123558,1,['release'],['release']
Deployability,Tested with dev deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8194#issuecomment-592560456:16,deploy,deploy,16,https://hail.is,https://github.com/hail-is/hail/pull/8194#issuecomment-592560456,1,['deploy'],['deploy']
Deployability,Tests are all passing. One question that remains is should we put a manifest file with the current Hail configurations somewhere in hail-common?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1514682815:104,configurat,configurations,104,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1514682815,1,['configurat'],['configurations']
Deployability,Tests passed rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9852#issuecomment-754182568:13,rolling,rolling,13,https://hail.is,https://github.com/hail-is/hail/pull/9852#issuecomment-754182568,1,['rolling'],['rolling']
Deployability,Tests with dev deploy worked.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7909#issuecomment-575794175:15,deploy,deploy,15,https://hail.is,https://github.com/hail-is/hail/pull/7909#issuecomment-575794175,1,['deploy'],['deploy']
Deployability,"Thank you :). Could you also make a release soon after this PR is merged, if reasonable? Would be handy to be able to try the package in our setup!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11435#issuecomment-1055996916:36,release,release,36,https://hail.is,https://github.com/hail-is/hail/pull/11435#issuecomment-1055996916,1,['release'],['release']
Deployability,"Thank you all for another round of detailed critique!. OK, I think the only remaining critical fix is to hard-code a mainclass. This is a wee bit complicated because I need to multiplex the ServiceBackendSocketAPI2 and the Worker. I hope to do this tomorrow AM. I'll then dismiss reviews. I also have a list of todos generated by this process which will feedback into some master QoB doc that integrates the two teams necessary todos.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934:393,integrat,integrates,393,https://hail.is,https://github.com/hail-is/hail/pull/11194#issuecomment-1033173934,1,['integrat'],['integrates']
Deployability,"Thank you for getting back to me. I was using the solution provided by aws (https://github.com/awslabs/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/blob/master/source/GenomicsAnalysisCode/buildhail_buildspec.yml) also the main page for reference (https://docs.aws.amazon.com/solutions/latest/genomics-tertiary-analysis-and-data-lakes-using-aws-glue-and-amazon-athena/welcome.html). In order to use the latest Hail (because we have vcf format 4.3 which is not supported in Hail 0.1), we changed it to ; ```. echo 'Installing pre-reqs'; yum install -y g++ cmake git; yum install -y lz4; yum install -y lz4-devel; git clone $HAIL_REPO; cd hail/hail && git fetch && git checkout main; ./gradlew clean; make install HAIL_COMPILE_NATIVES=1; make install HAIL_COMPILE_NATIVES=1 SCALA_VERSION=2.12.13 SPARK_VERSION=3.1.1; cd build; pip download decorator==4.2.1; aws s3 cp decorator-4.2.1-py2.py3-none-any.whl s3://${RESOURCES_BUCKET}/artifacts/decorator.zip; aws s3 cp distributions/hail-python.zip s3://${RESOURCES_BUCKET}/artifacts/; aws s3 cp libs/hail-all-spark.jar s3://${RESOURCES_BUCKET}/artifacts/; ``` . What else I should change in order to deploy this solution successfully? . Thank you for the help!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181:543,Install,Installing,543,https://hail.is,https://github.com/hail-is/hail/issues/10844#issuecomment-914469181,7,"['Install', 'deploy', 'install']","['Installing', 'deploy', 'install']"
Deployability,"Thank you for taking a look!. > 1. I'm not opposed to adding tokens to the batches_n_jobs_in_complete_states table, but I'm not sure why this is related to the other pieces of this PR / job groups. Aren't tokens purely a performance optimization?. The issue is that I started with #13475 and after your insightful comment about keeping the batches and job groups tables in sync, I realized that rather than using the batch_after_update trigger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:553,update,update,553,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,3,"['UPDATE', 'update']","['UPDATE', 'update']"
Deployability,Thank you for the clear and easy-to-follow instructions! I've rebased my commit and applied your patch. What do I need to do now?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/753#issuecomment-251409770:97,patch,patch,97,https://hail.is,https://github.com/hail-is/hail/pull/753#issuecomment-251409770,1,['patch'],['patch']
Deployability,Thanks ! ; I saw that the fix is merged on #13806 v0.2.125 !; I am able to install hail and run it using command line. I do have an issue with jupyter through... working on it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1782294182:75,install,install,75,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1782294182,1,['install'],['install']
Deployability,Thanks @danking. A feature like this would let us clean up some parts of our pipelines significantly. Our current workarounds leave a bad taste in the mouth whenever I see them.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14354#issuecomment-1965395083:77,pipeline,pipelines,77,https://hail.is,https://github.com/hail-is/hail/issues/14354#issuecomment-1965395083,1,['pipeline'],['pipelines']
Deployability,"Thanks @jmarshall for bringing this to our attention. It looks like while we updated the upper bound here, we did not update our fully-pinned requirements which we use to test in CI, so it did not catch this incompatibility. That being said, I don't think that was necessarily a mistake, because by testing our minimum-compatible-version we make sure not to introduce incompatibilities on that end of the spectrum either.. I think I don't see a good way in which we can confidently support more than one major version of a dependency at a given point in time. Even without the bokeh issue, there could easily be places in our codebase where we use pandas 1.x functionality that has been removed in 2.0. @danking thoughts on moving the pandas pin to >= the 2.x.x version that we test with and <3?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12906#issuecomment-1520406276:77,update,updated,77,https://hail.is,https://github.com/hail-is/hail/pull/12906#issuecomment-1520406276,2,['update'],"['update', 'updated']"
Deployability,"Thanks @tomwhite, that's great! Unfortunately, we have a second local cluster with another distribution on Spark 1.5 that won't get upgrade until early 2017 at the earliest.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1124#issuecomment-263332983:132,upgrade,upgrade,132,https://hail.is,https://github.com/hail-is/hail/pull/1124#issuecomment-263332983,1,['upgrade'],['upgrade']
Deployability,"Thanks Shuli, and apologies for the delay! I've taken your changes to the parameters and added some additional fixes in #2377 directly on @johnc1231 branch, which should be reviewed and merged to master this week. I'll be in touch once that's in, and from there, you can make future PRs against master for our review to improve/update Nirvana in Hail.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2300#issuecomment-340895757:328,update,update,328,https://hail.is,https://github.com/hail-is/hail/pull/2300#issuecomment-340895757,1,['update'],['update']
Deployability,Thanks for doing this! Should make things easier when dataproc actually releases a Spark 3 version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-702190052:72,release,releases,72,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-702190052,1,['release'],['releases']
Deployability,"Thanks for figuring this out! For dataproc, the startup script runs the code below the first time the data is used to generate the index for GRCh37 only. I ran the same dummy VEP command when I generated the QoB data for GRCh37. But we don't do this in GRCh38 on dataproc, so I didn't run this command for QoB as well. The fix is to add something similar as below to the GRCh38 dataproc script and then independently fix the QoB data for GRCh38. ```; # Run VEP on the 1-variant VCF to create fasta.index file -- caution do not make fasta.index file writeable afterwards!; cat /vep_data/loftee_data/1var.vcf | docker run -i -v /vep_data:/root/.vep \; ${VEP_DOCKER_IMAGE} \; perl /vep/ensembl-tools-release-85/scripts/variant_effect_predictor/variant_effect_predictor.pl \; --format vcf \; --json \; --everything \; --allele_number \; --no_stats \; --cache --offline \; --minimal \; --assembly ${ASSEMBLY} \; -o STDOUT; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13989#issuecomment-1832737456:697,release,release-,697,https://hail.is,https://github.com/hail-is/hail/issues/13989#issuecomment-1832737456,1,['release'],['release-']
Deployability,"Thanks for mentioning me, very interesting reading. You know, framework comparison is the very biased matter.; Sanic has 11,300 GitHub stars, aiohttp has only 6,900. Monthly download count is different: [4,7M for aiohttp](https://pypistats.org/packages/aiohttp) vs [60K for Sanic](https://pypistats.org/packages/sanic). ; Precise download count is a very hard thing (it misses PyPI caches, installing from Linux packages and Docker images etc. etc.) -- but you see the difference anyway. Sanic team is a champion in the library promotion, guys do their job perfectly well. Performance comparison is even harder.; Libraries have different defaults: sanic worker installs *uvloop* by default, aiohttp doesn't do it but utilizes uvloop if `uvloop.install()` was called.; Moreover, the aiohttp performance depends on how the library was installed.; It has C optimizations with Pure Python fallbacks. If Cython/GCC was not available on target machine at installation time the slow pure python code is executed.; In fact, aiohttp in optimized mode runs the same C written HTTP parser as Sanic. Sanic used to run multiple workers by default, aiohttp uses only one. On a real server it doesn't matter because usually the server is explicitly configured to run multiple web workers by gunicorn and (or) nginx config. Now Sanic switched to the single worker by default IIRC.; Anyway, looking on outdated performance comparisons in different blog posts doesn't show any useful numbers unless you re-run and check the numbers on your environment against latest (or used by you) versions. aiohttp uses standard `json` module by default, Sanic `ujson`. `ujson` is faster but it is not 100% compatible with the standard and can fall into memory dumps IIRC. You can configure aiohttp to run `ujson`, `orjson` or `rapidjson` if needed -- all speedups have own drawbacks. Very famous [Tech Empower Benchmark](https://www.techempower.com/benchmarks/#section=data-r17&hw=ph&test=fortune) demonstrates that sanic is faster",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040:390,install,installing,390,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461299040,5,['install'],"['install', 'installation', 'installed', 'installing', 'installs']"
Deployability,"Thanks for merging. The wild goose chase was amusing in retrospect!. I was considering adding another commit with something like. ```patch; +++ b/hail/python/hailtop/batch/resource.py; @@ -49,7 +49,7 @@ class ResourceFile(Resource, str):; ; def __init__(self, value: Optional[str]):; super().__init__(); - assert value is None or isinstance(value, str); + assert value is None or isinstance(value, str), f'{type(value).__name__} ({value!r}) is not str'; ```. However this iterating is probably the cause of most of the unexpected values/types here, so with the `__iter__` definitions added it doesn't matter too much. (Most of the other asserts are things like `assert value is not None` so if you see it you can tell what's happened without needing to print out `value`.)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14390#issuecomment-1977643581:133,patch,patch,133,https://hail.is,https://github.com/hail-is/hail/pull/14390#issuecomment-1977643581,1,['patch'],['patch']
Deployability,Thanks for the comments and the rewording in the docs @danking ! I just pushed the updated scala files.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/886#issuecomment-251185250:83,update,updated,83,https://hail.is,https://github.com/hail-is/hail/pull/886#issuecomment-251185250,1,['update'],['updated']
Deployability,"Thanks for the report, @JacobBayer! I don't believe anyone on our team uses Spyder unfortunately, but I don't think this is a Hail issue. According to [this thread](https://community.developers.refinitiv.com/questions/88895/spyder-515-erroreikon-data-api.html?childToView=89408#answer-89408) a Spyder upgrade might resolve the issue if you're on an old version?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309:301,upgrade,upgrade,301,https://hail.is,https://github.com/hail-is/hail/issues/11758#issuecomment-1099731309,1,['upgrade'],['upgrade']
Deployability,"Thanks for the review -- that's a much better approach! I've made the change. Happy to say that the patched version has just ingested a 46 million x 1200 VCF without a hitch and in just over an hour, and I'm very much looking forward to seeing what hail can do with the data tomorrow -- thanks for creating such a powerful system!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833:100,patch,patched,100,https://hail.is,https://github.com/hail-is/hail/pull/1066#issuecomment-258820833,1,['patch'],['patched']
Deployability,Thanks for the update. Let me know if there is anything I can do to help with the review.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14326#issuecomment-1964827601:15,update,update,15,https://hail.is,https://github.com/hail-is/hail/pull/14326#issuecomment-1964827601,1,['update'],['update']
Deployability,Thanks! Fix worked (I just rolled a custom jar) so don't worry about integrating immediately on my account.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2456#issuecomment-345872854:69,integrat,integrating,69,https://hail.is,https://github.com/hail-is/hail/pull/2456#issuecomment-345872854,1,['integrat'],['integrating']
Deployability,Thanks! Moving over to the Xcode cc worked. **cc --version**; Apple LLVM version 8.0.0 (clang-800.0.42.1); Target: x86_64-apple-darwin16.3.0; Thread model: posix; InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274296144:163,Install,InstalledDir,163,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274296144,1,['Install'],['InstalledDir']
Deployability,"Thanks, @danking! Looks like I broke auth deploy, investigating.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9713#issuecomment-783498605:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/9713#issuecomment-783498605,1,['deploy'],['deploy']
Deployability,"Thanks,. The `module-info.class` thing is incredibly unlikely to break any user. I only discovered it because it broke `jdeps` when I was testing generating a bundled JRE. None of this directly impacts me or established users of Hail in my group, but I have seen Java version be the single biggest pain point for new users wanting to install and try Hail for the first time, which is why I posted this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14433#issuecomment-2030374619:334,install,install,334,https://hail.is,https://github.com/hail-is/hail/issues/14433#issuecomment-2030374619,1,['install'],['install']
Deployability,That is indeed concerning. Can you share the full pipeline back to the read of `mt` and the creation of `sample_list`? Can you also confirm this bad behavior exists in the latest 0.2.113?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12912#issuecomment-1517847031:50,pipeline,pipeline,50,https://hail.is,https://github.com/hail-is/hail/issues/12912#issuecomment-1517847031,1,['pipeline'],['pipeline']
Deployability,"That said, fixing dataproc with minimal changes seems best to me. If/when we upgrade to a newer VEP version we can change to a more sensible structure then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1846195135:77,upgrade,upgrade,77,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1846195135,1,['upgrade'],['upgrade']
Deployability,"That will get updated once this PR goes in, while the docs and the client users use (batch not pipeline anymore) will be dependent on upgrading to the next PIP release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215:14,update,updated,14,https://hail.is,https://github.com/hail-is/hail/pull/8453#issuecomment-612087215,3,"['pipeline', 'release', 'update']","['pipeline', 'release', 'updated']"
Deployability,"That's a legitimate concern. We don't currently do this for `datasets.json` / the annotation database. For now let's leave it as it is. If the files move the user always has the option of fixing their configuration by explicitly specifying a config. Having some sort of remote configuration seems valuable, but this PR is large already. Whatever solution we come up with for remote configuration should also support the annotation database case. Let's not worry about it for now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12428#issuecomment-1515016056:201,configurat,configuration,201,https://hail.is,https://github.com/hail-is/hail/pull/12428#issuecomment-1515016056,3,['configurat'],['configuration']
Deployability,"That's what the literal was doing. The problem is the result is being used in two completely separate pipelines---the variants and reference. The right thing is probably to collect with `localize=False`, and use the result to annotate globals on both sides, but we still don't support referencing the local environment in MapGlobals.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13749#issuecomment-1743389871:102,pipeline,pipelines---the,102,https://hail.is,https://github.com/hail-is/hail/pull/13749#issuecomment-1743389871,1,['pipeline'],['pipelines---the']
Deployability,"The ATGU intranet service is home for some tools we're going to build help support ATGU operations. After talking to the administrators, we started a simple tool for the admins to curate resources for members of the group. We've also talked about things related to personnel and financial and grant management. Although this will likely be a slightly different resource, we're talking to the PMs about tools we can build to help pulling together large-scale datasets like gnomAD. The complexity of doing this is becoming a blocker for producing such datasets. We're Hail. Whatever we build is part of Hail. Things that we own and operate and deploy together live in our monorepo.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642:642,deploy,deploy,642,https://hail.is,https://github.com/hail-is/hail/pull/9684#issuecomment-723166642,1,['deploy'],['deploy']
Deployability,"The CI tests were failing due to not enough disk space on the workers. I increased the size to 20 GB, but this requires a migration. I'll need to setup a VM with the ability to deploy to have this new image pre-built. The last time I did a migration from the VM, it took a long time to deploy despite pre-caching the image, so I'll want to build a new VM with more cores to see if that helps. We should wait until after Konrad's jobs are done to do this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317:177,deploy,deploy,177,https://hail.is,https://github.com/hail-is/hail/pull/8440#issuecomment-609845317,2,['deploy'],['deploy']
Deployability,"The Hail python package is not a pure-Python package. Installing it from source requires building a JAR file. We have instructions [here](https://hail.is/docs/0.2/getting_started_developing.html), but I recommend waiting for the next PyPI release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502218484:54,Install,Installing,54,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502218484,2,"['Install', 'release']","['Installing', 'release']"
Deployability,"The Makefiles grab the `docker_prefix` from the global-config each time they are run so once we make the change in the global-config nothing else needs to change. Actually, I think this entire PR is not even needed. When CI does a deploy, the changes it applies to Kubernetes include fully qualified image names, e.g. `gcr.io/hail-vdc/batch:asdf1234`. If we were to swap out the `docker_prefix` global-config variable, CI would start to create new images that are pushed to the new repository (it would kill the cache for a single build but whatever), but the existing images would still exist and be undisrupted. The only images that need to exist in the new container registry when the switch is made are the images that we push on bootstrap which I am going to do manually anyway.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486:231,deploy,deploy,231,https://hail.is,https://github.com/hail-is/hail/pull/12211#issuecomment-1259517486,1,['deploy'],['deploy']
Deployability,"The Red Hat folks appear to have restored the package, so we could remove this workaround. However, they're not going to maintain it so we will either have to build from source or ultimately upgrade.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11859#issuecomment-1138065476:191,upgrade,upgrade,191,https://hail.is,https://github.com/hail-is/hail/pull/11859#issuecomment-1138065476,1,['upgrade'],['upgrade']
Deployability,"The SHA's are changing a bit quicker than I want to update them. I'll update when it stabilizes, but the titles are still correct.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10072#issuecomment-782556470:52,update,update,52,https://hail.is,https://github.com/hail-is/hail/pull/10072#issuecomment-782556470,2,['update'],['update']
Deployability,"The [Hail CI Build Configuration](https://ci.hail.is/admin/editBuildRunners.html?id=buildType:HailSourceCode_HailCi) (admin login required) now runs `gradle clean compileTestScala` against three spark versions: `1.6.2`, `1.5.2`, and `1.6.0-cdh5.7.2`. If all of those succeed, it runs `gradle clean test createDocs` against the default spark version in the gradle script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/742#issuecomment-245033280:19,Configurat,Configuration,19,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245033280,1,['Configurat'],['Configuration']
Deployability,"The [TextInputFormat](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/TextInputFormat.html) class clearly comes from hadoop. It's no longer in the location from which we import it. We must get it from some other dependency. OK. So, before my simplification of build.gradle, we used a configuration called `compile` and another one called `testCompile`. [Neither of those exist in modern gradle, apparently](https://docs.gradle.org/current/userguide/java_library_plugin.html#sec:java_library_configurations_graph). I found a side-note about the `compile` configuration [here](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:java_dependency_management_overview) (search for ""compile""):. > **Why no compile configuration?**; > The Java Library Plugin has historically used the compile configuration for dependencies that are required to both compile and run a project’s production code. It is now deprecated, and will issue warnings when used, because it doesn’t distinguish between dependencies that impact the public API of a Java library project and those that don’t. You can learn more about the importance of this distinction in [Building Java libraries](https://docs.gradle.org/current/userguide/building_java_projects.html#sec:building_java_libraries). OK, so, we used to just dump everything into our runtime dependencies. I changed it so that we have three kinds of dependencies:; 1. `shadow`: these are provided by Dataproc/QoB at run-time. They are not in any JAR. They are not on the `testRuntimeClasspath` or `runtimeClasspath`. They are on the `testCompileClasspath` because I [explicitly requested](https://github.com/hail-is/hail/blob/main/hail/build.gradle#L98) that `testCompileOnly` bring in our `shadow` dependencies.; 2. `implementation`: these are included in all class paths and in shadow JARs (but not ""thin"" jars generated by `./gradlew jar`).; 3. `testImplementation`: these are included in test class paths and in shadow JARs. Our t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741:303,configurat,configuration,303,https://hail.is,https://github.com/hail-is/hail/issues/13706#issuecomment-1738232741,4,['configurat'],['configuration']
Deployability,"The `pre-commit` hook is a little sticky because `pre-commit` installs each tool in its own isolated virtual env, which won't have the dependencies unless we tell `pre-commit` to also install all of our pinned dependencies into the pyright virtualenv. We can configure pyright to use a different virtualenv for all its dependencies, but that would require each developer specifying the name of their virtual environment in `pyproject.toml`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432:62,install,installs,62,https://hail.is,https://github.com/hail-is/hail/pull/13437#issuecomment-1681248432,2,['install'],"['install', 'installs']"
Deployability,"The breeze version packaged with Spark [was changed to 0.12](https://issues.apache.org/jira/browse/SPARK-16494) when Spark upgraded from 2.0.2 to 2.1.0. . According to [the PR](https://github.com/apache/spark/pull/14150/files#diff-06b6ad3483185a20d3095743faa5e4f0L15) linked from that JIRA issue, the breeze packaged with Spark 2.0.2 was version 0.11.2.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281757910:123,upgrade,upgraded,123,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281757910,1,['upgrade'],['upgraded']
Deployability,The cancel endpoint must also be updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6556#issuecomment-509397642:33,update,updated,33,https://hail.is,https://github.com/hail-is/hail/issues/6556#issuecomment-509397642,1,['update'],['updated']
Deployability,"The currently running CI is the old one, it chooses the index file. If you want to test that the index file looks the way you want, you'd need to add a `ci/` test. It's not super straightforward, but `ci/test-in-cluster.sh` starts a CI pointing at a test repo. This eventually calls `ci/test/test-ci.py` which triggers builds and deploys on the test repo. Those builds and deploys should have the expected index file.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932:330,deploy,deploys,330,https://hail.is,https://github.com/hail-is/hail/pull/4551#issuecomment-431458932,2,['deploy'],['deploys']
Deployability,"The deploy service account was unnecessarily privileged and not actively used. AFAICT, it's used only by this make file to stand up a new hail vdc from scratch. I removed the deploy service account and modified this makefile to create it, use it, and then destroy it when finished. If this is all bitrot, then I suppose it doesn't matter. I tested that these new and modified targets work as expected. What did you use to set up Konrad's project?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846:4,deploy,deploy,4,https://hail.is,https://github.com/hail-is/hail/pull/8178#issuecomment-599784846,2,['deploy'],['deploy']
Deployability,"The failure doesn't appear to be related to my changes. Installing the docker requirements, which has `setuptools>=38.6.0`, is trying to upgrade to the latest setuptools (56.0.0). Another dependency might be forcing the upgrade. However, setuptools was installed via apt, not pip, and that is causing this:. ```; Attempting uninstall: setuptools; Found existing installation: setuptools 45.2.0; Not uninstalling setuptools at /usr/lib/python3/dist-packages, outside environment /usr; Can't uninstall 'setuptools'. No files were found to uninstall.; ```. So there's two things I don't understand. I'll keep investigating. I glad I PRed this separately!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733:56,Install,Installing,56,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-824092733,5,"['Install', 'install', 'upgrade']","['Installing', 'installation', 'installed', 'upgrade']"
Deployability,The fix landed after the tagged 0.2.65 release. Can you share the git commit hash that's failing?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-829188311:39,release,release,39,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-829188311,1,['release'],['release']
Deployability,"The formatting does become a bit much. This is black's preferred rendering:; ```; @app.command(); def deploy(; branch: Annotated[str, typer.Option(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Annotated[; List[str],; typer.Option(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Annotated[; List[str],; typer.Option(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Annotated[; List[str],; typer.Option(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Annotated[; bool,; typer.Option(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass. ```. We can reduce the noise a bit with aliases:; ```; from typing import Annotated as Ann, List; from typer import Opt. @app.command(); def deploy(; branch: Ann[str, Opt(""--branch"", ""-b"", help=""Fully-qualified branch, e.g., hail-is/hail:feature"")],; steps: Ann[; List[str],; Opt(""--steps"", ""-s"", help=""Comma or space-separated list of steps to run.""),; ],; excluded_steps: Ann[; List[str],; Opt(; ""--excluded_steps"",; ""-e"",; help=""Comma or space-separated list of steps to forcibly exclude. Use with caution!"",; ),; ],; extra_config: Ann[; List[str],; Opt(; ""--extra-config"",; ""-e"",; help=""Comma or space-separated list of key=value pairs to add as extra config parameters."",; ),; ],; open: Ann[; bool,; Opt(""--open"", ""-o"", help=""Open the deploy batch page in a web browser.""),; ],; ):; pass; ```. It seems to me that the benefits of real sub-commands and better dead-option linting is worth the extra noise in the function definition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400:102,deploy,deploy,102,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1570514400,4,['deploy'],['deploy']
Deployability,"The getting started, tutorial, and command documentation are all coming to python in the next week or two. We can certainly look into registering Hail on PyPI, but I'm not sure how versioning works there -- with the current pace of development, we may want to hold off on that until a stable release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-270455160:292,release,release,292,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-270455160,1,['release'],['release']
Deployability,"The handling for covariates in the top of linreg/logreg was broken. I updated, can you double-check?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3477#issuecomment-386075467:70,update,updated,70,https://hail.is,https://github.com/hail-is/hail/pull/3477#issuecomment-386075467,1,['update'],['updated']
Deployability,"The immutable approach is fine, but we should implement it properly so the user doesn't get surprised when an old binding of the object is mutated through a new binding of the object. And there's the issue of update not really being annotate.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2869#issuecomment-364199693:209,update,update,209,https://hail.is,https://github.com/hail-is/hail/pull/2869#issuecomment-364199693,1,['update'],['update']
Deployability,"The integration test is failing. Otherwise, looks good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602:4,integrat,integration,4,https://hail.is,https://github.com/hail-is/hail/pull/1572#issuecomment-287900602,1,['integrat'],['integration']
Deployability,"The issue appears to be something in the way spark is configured in this branch. I cannot broadcast successfully new SerilaizableHadoopConfiguration(sc.hadoopConfiguration, inside of LoadVCF. Meaning it works, but the configuration is null. Manually serializing in a test works fine. No issues on master. Minimal example:. ```scala; // LoadVCF, using master's SerializableHadoopConfiguration class ; private val fileInfo: Array[Array[String]] = externalSampleIds.getOrElse {; val shConf = new SerializableHadoopConfiguration(sc.hadoopConfiguration); val localBcFsConf = sc.broadcast(shConf); var results: Array[Array[String]] = Array(); var stuff = sc.parallelize(files, files.length).map { file =>; sc.hadoopConfiguration; }.collect(). results; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496:218,configurat,configuration,218,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-496946496,1,['configurat'],['configuration']
Deployability,"The layers of wtf really seem to have no end here. Hadoop at least *appears* to [include the configuration in the cache key](https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java#L3833-L3841) for its FileSystem cache, but it is actually just ignored by the constructor. Ergo, even if you stop the Hail context and try to start a new hail context with a new Hadoop Configuration, you'll get a filesystem configured by the first configuration. I'm looking for a way around this now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443:93,configurat,configuration,93,https://hail.is,https://github.com/hail-is/hail/pull/12133#issuecomment-1241322443,3,"['Configurat', 'configurat']","['Configuration', 'configuration']"
Deployability,"The new tar file is now in all VEP replicates for dataproc. The only change is it uses the indexed cache files and the tar file has the word ""_indexed"" in it. Otherwise, it should have the same contents / file structure as the non-indexed tar file that is there currently. I tested this as best as I could, but it would be prudent to give ourselves time when releasing this in case there is a problem in the release script.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14071#issuecomment-1881444531:408,release,release,408,https://hail.is,https://github.com/hail-is/hail/pull/14071#issuecomment-1881444531,1,['release'],['release']
Deployability,The only use of the `_to_json` method I have is in a forthcoming combiner update. Theoretically I can use what I put in the `_to_json` method for tmatrix directly where I need it.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12369#issuecomment-1287161147:74,update,update,74,https://hail.is,https://github.com/hail-is/hail/pull/12369#issuecomment-1287161147,1,['update'],['update']
Deployability,"The original report was about `gnomad.exomes.r2.1.1.sites.liftover_grch38.vcf.bgz`. That's the gnomad v2.1.1 GRCh38 liftover sites table. See [this section of the gnomAD downloads](https://gnomad.broadinstitute.org/downloads#v2-liftover). In particular it is the ""All chromosomes VCF"". That's 85GiB, so I don't want to download it. I believe the chr21 VCF should have just as many row, column, and entry fields, so I downloaded that and tested Hail's ability to import and write it. ```bash; gsutil -m cp \; gs://gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; .; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. With Hail 0.2.108-fc03e9d5dc08 it worked fine. It also worked fine on a recent 0.2.120 development version I had installed. Next I tried running on the first few thousand lines of the full sites table:. ```bash; curl \; https://storage.googleapis.com/gcp-public-data--gnomad/release/2.1.1/liftover_grch38/vcf/exomes/gnomad.exomes.r2.1.1.sites.21.liftover_grch38.vcf.bgz \; | bgzip -d -c\; | head -n 10000 \; | bgzip -c \; > /tmp/head-sites.vcf.bgz; ```; ```python3; import hail as hl; recode = {f""{i}"":f""chr{i}"" for i in (list(range(1, 23)) + ['X', 'Y'])}; mt = hl.import_vcf('/tmp/head-sites.vcf.bgz', reference_genome='GRCh38', contig_recoding=recode); mt.write('gnomad.mt', overwrite = True); ```. This also succeeded with Hail 0.2.108",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525:537,release,release,537,https://hail.is,https://github.com/hail-is/hail/issues/13249#issuecomment-1703341525,3,"['install', 'release']","['installed', 'release']"
Deployability,"The per-element code seems to end up in a separate method. Is that not standard? For example, the pipeline I'm using is an array of struct and the struct is always in a separate method",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13776#issuecomment-1747578071:98,pipeline,pipeline,98,https://hail.is,https://github.com/hail-is/hail/pull/13776#issuecomment-1747578071,1,['pipeline'],['pipeline']
Deployability,The pip install for pyspark installs the 2.2 version of Spark. Will that pyspark version work with Hail? The Getting Started Page only documents 2.0 and 2.1.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2003#issuecomment-330569535:8,install,install,8,https://hail.is,https://github.com/hail-is/hail/issues/2003#issuecomment-330569535,2,['install'],"['install', 'installs']"
Deployability,The pipeline I posted doesn't leave _1 lying around -- `va = va._1` overwrites `va`,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/488#issuecomment-235098604:4,pipeline,pipeline,4,https://hail.is,https://github.com/hail-is/hail/pull/488#issuecomment-235098604,1,['pipeline'],['pipeline']
Deployability,"The problem is it takes more than 7 minutes to schedule a trivial CI job and then a trivial deploy job, I could set the retries or try-delay higher, but what else could correct mean?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396:92,deploy,deploy,92,https://hail.is,https://github.com/hail-is/hail/pull/5639#issuecomment-474817396,1,['deploy'],['deploy']
Deployability,The problem seems to be a MatrixFilterEntries from the stack trace above. can we have the full pipeline?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384667398:95,pipeline,pipeline,95,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384667398,1,['pipeline'],['pipeline']
Deployability,"The reason that your original patch fixed the test you created is really the collision of unintended behaviors:. 1. the rebuilt MTs in the MatrixUnionRows in split_multi have different types because of the entry position; 2. the call to `upcast` for the rows was not checking that you were upcasting to a supertype, and was reordering struct fields. Adding an assertion to upcast caused failures elsewhere. I think the attack plan should be as follows:. 1. Add this assertion, and fix the failures caused by it (LD prune tests?); 2. Add the split_multi test, and fix the type violations created during PruneDeadFields rebuild. To make this easier, you can add a bunch of assertions about the type of the rebuilt MatrixIR nodes, which should cause debug-friendly errors.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474:30,patch,patch,30,https://hail.is,https://github.com/hail-is/hail/pull/4585#issuecomment-435486474,1,['patch'],['patch']
Deployability,"The recent PR #14086 was added to for a month before it was merged and the release made on Jan 12th. However the release date listed in the changelog remained the `Released 2023-12-08` date on which the PR was initiated. If there is a checklist for making releases, it may be worth adding “Update the release date in _change_log.md_ before merging the release PR” to it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14161#issuecomment-1891099203:75,release,release,75,https://hail.is,https://github.com/hail-is/hail/pull/14161#issuecomment-1891099203,7,"['Release', 'Update', 'release']","['Released', 'Update', 'release', 'releases']"
Deployability,"The remaining occurrences are not relevant to production work:; ```; /Users/dking/projects/hail/batch/batch/worker/worker.py:484: # * gcr.io/hail-vdc/hailgenetics/python-dill; /Users/dking/projects/hail/batch/batch/worker/worker.py:705: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/datasets/extract/extract_CADD.py:26: j.image(""gcr.io/broad-ctsa/datasets:050521""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:12: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:19: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/datasets/extract/extract_1000_Genomes_NYGC_30x_GRCh38.py:26: j.image(""gcr.io/broad-ctsa/datasets:041421""); /Users/dking/projects/hail/hail/scripts/update-terra-image.py:33:Image URL: `us.gcr.io/broad-dsp-gcr-public/{image_name}:{image_version}`; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:115: x = parse_docker_image_reference('gcr.io/hail-vdc/batch-worker:123fds312'); /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:116: assert x.domain == 'gcr.io'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:120: assert x.name() == 'gcr.io/hail-vdc/batch-worker'; /Users/dking/projects/hail/hail/python/test/hailtop/utils/test_utils.py:121: assert str(x) == 'gcr.io/hail-vdc/batch-worker:123fds312'; /Users/dking/projects/hail/hail/python/hail/docs/change_log.md:278:- (hail#12230) The python-dill Batch images in `gcr.io/hail-vdc` are no longer supported.; /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py:707: # DockerError(500, ""Head https://gcr.io/v2/genomics-tools/samtools/manifests/latest: unknown: Project 'project:genomics-tools' not found or deleted.""); /Users/dking/projects/hail/hail/python/hailtop/utils/utils.py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013:941,update,update-terra-image,941,https://hail.is,https://github.com/hail-is/hail/pull/12963#issuecomment-1531692013,1,['update'],['update-terra-image']
Deployability,"The resource names are changing to include the regions. So yes, this PR will use the latest resources. The backwards compatibility is for right when the front end starts up before the driver has the chance to fetch the latest products and their prices. Besides not getting a chance to dev deploy this change and make sure it works is whether we need to have a solution for users being able to select us-central if they don't want to pay the 10% increase in costs. I wanted to spec out how much work that would be, but got distracted by other things.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1164545882:289,deploy,deploy,289,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1164545882,1,['deploy'],['deploy']
Deployability,"The root cause of large memory usage is https://github.com/hail-is/hail/issues/13748 but we should be deploying Hail in a manner that has enough RAM (in this case, 4GiB is plenty). I'm following up with GVS to determine whether the JVM indeed has enough RAM.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13712#issuecomment-1743500460:102,deploy,deploying,102,https://hail.is,https://github.com/hail-is/hail/issues/13712#issuecomment-1743500460,1,['deploy'],['deploying']
Deployability,"The setuptools thing was actually a red herring, that error isn't fatal, and there was a version conflict elsewhere. I upgraded urllib3 and requests to resolve.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422:119,upgrade,upgraded,119,https://hail.is,https://github.com/hail-is/hail/pull/10349#issuecomment-843240422,1,['upgrade'],['upgraded']
Deployability,"The target creates an egg in `hail/hail/build/deploy/dist/`, alongside the wheel.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8997#issuecomment-646921877:46,deploy,deploy,46,https://hail.is,https://github.com/hail-is/hail/pull/8997#issuecomment-646921877,1,['deploy'],['deploy']
Deployability,"The terraform plan for `gcp-broad` has 4 changes after appropriately importing. The bucket, ci_config, and hail_ci_0_1_github_oauth_token are actually unchanged. I do not know why they are listed as a change. That secret currently has three files: hail-ci-0-1.key, oauth-token, user1, user2. AFAICT, only `user1` is used, so the new secret only has `user1`. ```; # module.ci[0].google_storage_bucket.bucket will be updated in-place; ~ resource ""google_storage_bucket"" ""bucket"" {; id = ""hail-ci-bpk3h""; # Warning: this attribute value will be marked as sensitive and will not; # display in UI output after applying this change. The value is unchanged.; ~ location = (sensitive value); name = ""hail-ci-bpk3h""; # Warning: this attribute value will be marked as sensitive and will not; # display in UI output after applying this change. The value is unchanged.; ~ storage_class = (sensitive value); # (8 unchanged attributes hidden). # (2 unchanged blocks hidden); }. # module.ci[0].kubernetes_secret.ci_config will be updated in-place; ~ resource ""kubernetes_secret"" ""ci_config"" {; id = ""default/ci-config""; # (3 unchanged attributes hidden). # (1 unchanged block hidden); }. # module.ci[0].kubernetes_secret.hail_ci_0_1_github_oauth_token will be updated in-place; ~ resource ""kubernetes_secret"" ""hail_ci_0_1_github_oauth_token"" {; id = ""default/hail-ci-0-1-github-oauth-token""; # (3 unchanged attributes hidden). # (1 unchanged block hidden); }. # module.ci[0].kubernetes_secret.hail_ci_0_1_service_account_key will be updated in-place; ~ resource ""kubernetes_secret"" ""hail_ci_0_1_service_account_key"" {; ~ data = (sensitive value); id = ""default/hail-ci-0-1-service-account-key""; # (2 unchanged attributes hidden). # (1 unchanged block hidden); }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12882#issuecomment-1507162756:415,update,updated,415,https://hail.is,https://github.com/hail-is/hail/pull/12882#issuecomment-1507162756,4,['update'],['updated']
Deployability,"The test failure here is spurious, happening because batch is going through a transition / upgrade and things are a little broken right now. Will make sure this merges when that's resolved.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556:91,upgrade,upgrade,91,https://hail.is,https://github.com/hail-is/hail/pull/10671#issuecomment-881473556,1,['upgrade'],['upgrade']
Deployability,"The unoptimized pipeline doesn't have the decorator right now, but I do want to version it so it's easier to find when I decide what we want to do with it. I also think that there's an optimization that will bring it back down to ~a few mins, rather than an hour.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500:16,pipeline,pipeline,16,https://hail.is,https://github.com/hail-is/hail/pull/6482#issuecomment-505873500,1,['pipeline'],['pipeline']
Deployability,The updated documents look good.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342#issuecomment-391751782:4,update,updated,4,https://hail.is,https://github.com/hail-is/hail/issues/3342#issuecomment-391751782,1,['update'],['updated']
Deployability,"The worker image installs `docker/requirements.txt` which pip installs `googlecloudprofiler` which requires gcc. I could probably figure out the actual requirements of the worker image and list those separately. That seems prudent anyway. However, in the long term, if we want to profiler the workers, we'd need build-essential. Installing build-essential adds 204MB.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801:17,install,installs,17,https://hail.is,https://github.com/hail-is/hail/pull/8030#issuecomment-582661801,3,"['Install', 'install']","['Installing', 'installs']"
Deployability,"There are a few small cosmetic changes in here that were a result of an updated pylint, but I put those in a separate commit to hopefully make that less confusing. There are a few follow-ups after this that I want to tackle; - simplifying the images for testing query (Dockerfile.hail-build, Dockerfile.hail-base, Dockerfile.hail-run). I think these are the only things that use `base_image` so we might be able to collapse a bunch of these; - updating to python 3.8 to avoid accidentally installing that in some of our images; - trying to produce eStargz images so that buildkit can lazily pull the base image when building new images. I hope that can bring some image build times down even further by not having to localize the installed pip dependencies when making changes to our python code.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548:72,update,updated,72,https://hail.is,https://github.com/hail-is/hail/pull/12578#issuecomment-1458470548,3,"['install', 'update']","['installed', 'installing', 'updated']"
Deployability,"There are now three check steps:; - check_hail (in the sense of $HAIL_HOME/hail directory), which checks the hail and hailtop packages, using the base image, from the source via `make check-hail`; - check_services, using the services image, except the benchmark service, which installs addition dependencies, from the source via `make -k check-services`, `-k` forces make to check all packages, even if one fails; - check_benchmark_service, which checks benchmark as installed in that image",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481:277,install,installs,277,https://hail.is,https://github.com/hail-is/hail/pull/9394#issuecomment-685253481,2,['install'],"['installed', 'installs']"
Deployability,"There is a [known issue](https://github.com/moby/moby/issues/41792) with the official Docker deb. If you uninstall docker and re-install it later, it might fail to start. The root cause is the `docker.socket` `systemd` unit failing to start because there are ""insufficient file descriptors available"". I think this is confusing verbiage. The socket's name must be `/var/run/docker.sock`. Clearly, if that filename is already in use, we cannot create a new socket at that filename. One of Google's [""Dataproc components""](https://cloud.google.com/dataproc/docs/concepts/components/overview) is Docker. I believe Google installed and then uninstalled docker in this image, thus leaving it in the broken state. For evidence of that:. <details>; <summary> find docker on a worker node of a *non-Hail* Dataproc cluster</summary>. ```; sudo find / -iname '*docker*'; ```. ```; /opt/conda/miniconda3/pkgs/dbus-1.13.6-h5008d03_3/info/recipe/patches/0004-disable-fd-limit-tests-not-supported-in-docker.patch; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/docker",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:129,install,install,129,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,3,"['install', 'patch']","['install', 'installed', 'patches']"
Deployability,"There is occasional use of this in other projects, e.g., gnomad_methods (`SimpleRichProgressBar` in this case). Do you consider these classes to be part of the API? Was it intended to rename these without any compatibility shim, e.g., having the old names as aliases for a while?. It's not the end of the world and gnomad_methods has already updated accordingly. It does however mean that older gnomad_methods is only compatible with hail ≤ 0.2.125 and newer gnomad_methods is only compatible with hail ≥ 0.2.126, which is an otherwise unnecessary lock-step restriction. ETA: gnomad_methods have now updated by removing the (apparently unused) progress bar reference, so now newer gnomad_methods is compatible with hail both ≤ 0.2.125 and ≥ 0.2.126 again. So this is no longer a significant problem for gnomad_methods, but remains FYI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13832#issuecomment-1788106993:342,update,updated,342,https://hail.is,https://github.com/hail-is/hail/pull/13832#issuecomment-1788106993,2,['update'],['updated']
Deployability,"There is one issue for the hail alias. The alias refers to; $SPARK_HOME/python/lib/py4j-0.10.3-src.zip However, the py4j zip file; varies from Spark version to spark version. For example, these are the; different versions for spark on our system. /share/pkg/spark/1.2.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:270,install,install,270,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,8,['install'],['install']
Deployability,"There remain a couple questions that a solution should answer:; 1. TCP or Unix Domain Socket? Current consensus feels that TCP is a reasonable and more portable way to go (allows for backend deployment over the web/in K8s for example); 2. Should we use just TCP or also use HTTP? If the Java backends can multiplex requests, HTTP sounds favorable, otherwise it's unclear to me what advantages it would give us over TCP + JSON. Ergonomically HTTP might be easier, but one tends have certain default expectations of HTTP servers (I would *assume* an HTTP server should be able to serve requests concurrently, are we just going to use all `POST`s?, etc.). Either way this feels like a minor adjustment.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491:191,deploy,deployment,191,https://hail.is,https://github.com/hail-is/hail/issues/13756#issuecomment-1743481491,1,['deploy'],['deployment']
Deployability,"There will probably still be a lot of partitions, but those that remain only include rows with keys matching those in `pcloadings`. This pipeline includes the special bgen variant filtering I added.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3953#issuecomment-406017529:137,pipeline,pipeline,137,https://hail.is,https://github.com/hail-is/hail/issues/3953#issuecomment-406017529,1,['pipeline'],['pipeline']
Deployability,"There's also #11428, which just merged and I forgot to write the changelog message for:. `hailtop.batch.build_python_image` now accepts a `show_docker_output` argument to toggle printing docker's output to the terminal while building container images",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11795#issuecomment-1109887634:171,toggle,toggle,171,https://hail.is,https://github.com/hail-is/hail/pull/11795#issuecomment-1109887634,1,['toggle'],['toggle']
Deployability,"There's more work to be done here. This adds a new route to the batch UI for getting a certain job group within a batch. It then, instead of listing all jobs, only lists the jobs that belong directly to the currently viewed job group and also shows the child job groups of the current job group. When picking up this PR I would make sure to go through the Batch development tutorial to make sure you are familiar with dev deploying. Then, read [this](https://github.com/hail-is/hail/blob/main/dev-docs/development-process.md#alternatives-to-dev-deploy) to learn about all the ways you can avoid dev deploying 😄 . If you are only making tweaks in the HTML templates, you don't need to keep deploying for every little change. Instead, run. ```bash; make devserver SERVICE=batch; ```. in your terminal and you'll get a local server that proxies the Batch that your `hail` installation is pointed to. You can then make changes to HTML and refresh your browser to see the results. Note that this is just rendering the HTML locally, and will have any effect on what's deployed, meaning you can't use it for python changes.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998:422,deploy,deploying,422,https://hail.is,https://github.com/hail-is/hail/pull/14600#issuecomment-2239854998,6,"['deploy', 'install']","['deploy', 'deployed', 'deploying', 'installation']"
Deployability,"There's something wrong here and I think it has to do with how I'm passing around the hadoop configuration. Closing this since I'm in the process of changing how spark is called, and I'll reopen once I've fixed this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906:93,configurat,configuration,93,https://hail.is,https://github.com/hail-is/hail/pull/5326#issuecomment-467585906,1,['configurat'],['configuration']
Deployability,"There's something wrong with this PR. The database migration step says it's successful, but the new database is never actually created. I think this is the same thing I saw with dev deploy and attributed it to the wrong cause. ```; +------------------------------------+; | Database |; +------------------------------------+; ...; | pr-9241-auth-zdyt4a4geys7 |; | pr-9241-batch-y0qvw1vpniad |; | pr-9241-ci-nkuua31y7nxn |; | pr-9241-test-instance-zxxeu6gotctw |; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9241#issuecomment-677664900:182,deploy,deploy,182,https://hail.is,https://github.com/hail-is/hail/pull/9241#issuecomment-677664900,1,['deploy'],['deploy']
Deployability,These strict pins belong in the pinned-requirements.txt not as a requirement for our users. We have a different PR which updates numpy to 1.24.2 https://github.com/hail-is/hail/pull/12898,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12895#issuecomment-1515071184:121,update,updates,121,https://hail.is,https://github.com/hail-is/hail/pull/12895#issuecomment-1515071184,1,['update'],['updates']
Deployability,"These three tests call out to [Plink](https://www.cog-genomics.org/plink2). If you install it from that link and add it to your global path, those tests will pass. If that sounds annoying, you can trust that these tests pass on our end!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-229943619:83,install,install,83,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-229943619,1,['install'],['install']
Deployability,"They weren't documented in any release yet:; https://hail.is/docs/0.2/api.html#top-level-functions. I'm OK removing them, since they won't really break pipelines, only citations. And am happy to be bothered by people asking questions about that!. If you feel strongly, let's add the code aliases back but not the documentation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6552#issuecomment-507804081:31,release,release,31,https://hail.is,https://github.com/hail-is/hail/pull/6552#issuecomment-507804081,2,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,"Things that remain to be done:; - overriding repartition. This isn't super trivial because if you have a huge pipeline that ends in a repartition down to 10 partitions, you're going to get 10 cores for the whole job.; - better ordering process on VCF import; - speed up joins by skipping ahead if the 'right' is behind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/555#issuecomment-238118077:110,pipeline,pipeline,110,https://hail.is,https://github.com/hail-is/hail/pull/555#issuecomment-238118077,1,['pipeline'],['pipeline']
Deployability,"This PR _explicitly_ changes the defaults. It optionally accepts the old values in an _attempt_ to not break current users. At worst, this would require another `hailctl auth login`. This is the patch I _wanted_ to write. ```patch; From aef878903d9249b542522082cba705eaf26d728a Mon Sep 17 00:00:00 2001; From: Christopher Vittal <christopher.vittal@gmail.com>; Date: Wed, 25 Sep 2019 14:55:42 -0400; Subject: [PATCH] [hailctl] Move default location for hail config directory; MIME-Version: 1.0; Content-Type: text/plain; charset=UTF-8; Content-Transfer-Encoding: 8bit. Now we try, in order:; $XDG_CONFIG_HOME/hail; ~/.config/hail. The XDG Base Directory Specification[1] is a freedesktop spec inteded to; define where applications should look for files they need to run. [1]: https://specifications.freedesktop.org/basedir-spec/basedir-spec-latest.html. I have enough 💩 in my home directory for applications I don't control,; I'd like to try to keep it clean when it comes to applications I do; control.; ---; hail/python/hailtop/auth/tokens.py | 4 ++--; hail/python/hailtop/config/__init__.py | 3 ++-; hail/python/hailtop/config/deploy_config.py | 4 +++-; hail/python/hailtop/hailctl/auth/login.py | 7 +++----; hail/python/hailtop/hailctl/dev/config/cli.py | 4 ++--; 5 files changed, 12 insertions(+), 10 deletions(-). diff --git a/hail/python/hailtop/auth/tokens.py b/hail/python/hailtop/auth/tokens.py; index 9de07dc42..e8c3fcccd 100644; --- a/hail/python/hailtop/auth/tokens.py; +++ b/hail/python/hailtop/auth/tokens.py; @@ -3,7 +3,7 @@ import os; import sys; import json; import logging; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; ; log = logging.getLogger('gear'); ; @@ -14,7 +14,7 @@ class Tokens(collections.abc.MutableMapping):; deploy_config = get_deploy_config(); location = deploy_config.location(); if location == 'external':; - return os.path.expanduser('~/.hail/tokens.json'); + return os.path.join(HAIL_CONFIG_DIR, 't",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:195,patch,patch,195,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,3,"['PATCH', 'patch']","['PATCH', 'patch']"
Deployability,This PR contains some fixes to the GCP deployment scripts. LMK if you want me to pull those into a separate PR.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12751#issuecomment-1458579094:39,deploy,deployment,39,https://hail.is,https://github.com/hail-is/hail/pull/12751#issuecomment-1458579094,1,['deploy'],['deployment']
Deployability,"This PR does not move us closer to testing and releasing built-once binaries. This is an important goal for me, but automating the deploy process as it exists is a prerequisite.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8550#issuecomment-613603023:131,deploy,deploy,131,https://hail.is,https://github.com/hail-is/hail/pull/8550#issuecomment-613603023,1,['deploy'],['deploy']
Deployability,"This SLF4J warning is nothing to be worried about -- we see that in our deployment here, and it doesn't affect the running or results in any way. I'll create an issue to resolve this though. Another note about running Hail: if you're running on one machine, using the script in `hail/build/install/hail/bin/hail` created by `gradle installDist` is the thing to do. But if you're deploying on a spark cluster, you're going to need to do the following:. `gradle shadowJar` : this builds a jar in `hail/build/libs/hail-all-spark.jar`, which can be run in parallel using the `spark-submit` command as below:. ```; /usr/bin/spark-submit --executor-memory 48g --executor-cores 24 --class org.broadinstitute.hail.driver.Main ~/hail/build/libs/hail-all-spark.jar --master yarn-client <hail commands here>; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/457#issuecomment-230431412:72,deploy,deployment,72,https://hail.is,https://github.com/hail-is/hail/issues/457#issuecomment-230431412,4,"['deploy', 'install']","['deploying', 'deployment', 'install', 'installDist']"
Deployability,"This added node performs the function that appears all over Python pipelines:; ```python; mt = mt.annotate_rows(foo = ht[mt.row_key]); ```; and ; ```; mt = mt.annotate_rows(foo = ht[mt.not_a_key_field]); ```. The code inside the execution is quite horrible when joining on a non-key field, but I expect it to be much faster than the previous Python implementation.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3974#issuecomment-408132885:67,pipeline,pipelines,67,https://hail.is,https://github.com/hail-is/hail/pull/3974#issuecomment-408132885,1,['pipeline'],['pipelines']
Deployability,This all would be a lot easier to reason about if at the top of CI it showed a DEPLOY batch and a PUBLISH batch. This could all be tracked fairly easily in the CI database/bucket.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11122#issuecomment-985734503:79,DEPLOY,DEPLOY,79,https://hail.is,https://github.com/hail-is/hail/pull/11122#issuecomment-985734503,1,['DEPLOY'],['DEPLOY']
Deployability,"This basically limits the number of nodes in the worker pool for a test/dev deployment of batch to (default 3, max 4), right? This looks fine to me; should I approve this and let you take off the WIP tag when it's good to merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9441#issuecomment-691312517:76,deploy,deployment,76,https://hail.is,https://github.com/hail-is/hail/pull/9441#issuecomment-691312517,1,['deploy'],['deployment']
Deployability,"This benchmark is in some ways bad. The real problem is in the compiler/orchestration, not in any execution. I feel like we need a `_do_nothing()` that doesn't execute any code, but runs a pipeline through everything in the compiler and stops short before anything that would submit a spark job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6629#issuecomment-511031021:189,pipeline,pipeline,189,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511031021,1,['pipeline'],['pipeline']
Deployability,This branch is up to date with master's `69ba846a3`. Let me know when #6978 lands and I'll update again. Maybe nudge @iitalics to review that?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-527604266:91,update,update,91,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527604266,1,['update'],['update']
Deployability,"This branch, notebook, ci and batch are deployed in my namespace now. Note, batch is tested pretty well, and we test CI comes up (which is about all you can do in a dev deploy).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619:40,deploy,deployed,40,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536245619,2,['deploy'],"['deploy', 'deployed']"
Deployability,"This broke CI, it can't update from batch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5655#issuecomment-475978868:24,update,update,24,https://hail.is,https://github.com/hail-is/hail/pull/5655#issuecomment-475978868,1,['update'],['update']
Deployability,"This can go in after we do a release and we notify the batch users about the change. After that, we should delete all the per-user buckets.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8834#issuecomment-634121647:29,release,release,29,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-634121647,1,['release'],['release']
Deployability,"This catches the problem -- you can try the diff you sent me Friday. Once this is merged, new version deploys will be blocked until a fix is in.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10520#issuecomment-847464819:102,deploy,deploys,102,https://hail.is,https://github.com/hail-is/hail/pull/10520#issuecomment-847464819,1,['deploy'],['deploys']
Deployability,This change can't possibly be causing the website_image issues. I'm going to force merge this and force merge a release to address the pressing hadoop connector issues. I will address the website_image issue after that is done.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11434#issuecomment-1055787996:112,release,release,112,https://hail.is,https://github.com/hail-is/hail/pull/11434#issuecomment-1055787996,1,['release'],['release']
Deployability,"This command uploads the intermediate files that are carried between jobs in a batch. Our tests should be sufficient for finding cases where the downloads are not possible. The container that downloads files (the setup container) uses the google alpine sdk image:; ```; # docker run google/cloud-sdk:237.0.0-alpine gsutil version -l; Unable to find image 'google/cloud-sdk:237.0.0-alpine' locally; 237.0.0-alpine: Pulling from google/cloud-sdk; 6c40cc604d8e: Pull complete ; ef6547e2e20f: Pull complete ; Digest: sha256:fc5a5a88eb49e646adac05ac6a352219d3d676a122fca0b90a2ae2ab091222bb; Status: Downloaded newer image for google/cloud-sdk:237.0.0-alpine; gsutil version: 4.37; checksum: 4b1e288eec2f799d8d0022adccf678cb (OK); boto version: 2.49.0; python version: 2.7.15 (default, Jan 24 2019, 16:32:39) [GCC 8.2.0]; OS: Linux 4.9.125-linuxkit; multiprocessing available: False; using cloud sdk: True; pass cloud sdk credentials to gsutil: True; config path(s): No config found; gsutil path: /google-cloud-sdk/bin/gsutil; compiled crcmod: True; installed via package manager: False; editable install: False; ```. The upload container uses batch_image, which does not have crcmod. I'm not sure it's required, but I'll add it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965:1044,install,installed,1044,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529589965,2,['install'],"['install', 'installed']"
Deployability,This data is from 25-35x whole genome sequencing data aligned to the hg38 reference. The VCF was generated by the GATK pipeline. The vcf used to create the pruned dataset was limited to SNVs and so it did not contain any indels or multallelic sites.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3490#issuecomment-386642665:119,pipeline,pipeline,119,https://hail.is,https://github.com/hail-is/hail/issues/3490#issuecomment-386642665,1,['pipeline'],['pipeline']
Deployability,"This diff uses the index on `instances.removed`. ```diff; diff --git a/batch/batch/driver/canceller.py b/batch/batch/driver/canceller.py; index d438a8519b..594b180221 100644; --- a/batch/batch/driver/canceller.py; +++ b/batch/batch/driver/canceller.py; @@ -371,10 +371,9 @@ SELECT attempts.*; FROM attempts; INNER JOIN jobs ON attempts.batch_id = jobs.batch_id AND attempts.job_id = jobs.job_id; LEFT JOIN instances ON attempts.instance_name = instances.name; -WHERE attempts.start_time IS NOT NULL; - AND attempts.end_time IS NULL; +WHERE attempts.end_time IS NULL; AND ((jobs.state != 'Running' AND jobs.state != 'Creating') OR jobs.attempt_id != attempts.attempt_id); - AND instances.`state` = 'active'; + AND instances.removed = 0; ORDER BY attempts.start_time ASC; LIMIT 300;; """""",; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14460#issuecomment-2221567514:69,a/b,a/batch,69,https://hail.is,https://github.com/hail-is/hail/issues/14460#issuecomment-2221567514,2,['a/b'],['a/batch']
Deployability,"This doesn't necessarily fix the problem, but I think they are good changes in the direction of what we know. It passed test_{batch, pipeline} on the first try, so that's a good sign. What I did:. 1. Lock down Job state transitions. Now only set_state and _mark_job_task_complete change _state, and they log identically. Explicitly enumerate the valid state transitions are check them in each function. Slightly clarified the transitions around Pending. Now Pending can only go to Ready. 2. If a state update fails (the Python object is stale), throw JobStateWriteFailure. If we have a stale picture, we clearly don't want to be doing anything else. 3. Handle a few more cases in update_job_with_pod: a pod without a job or a job that shouldn't have one, and a cancellable pod that hasn't been cancelled yet.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707:133,pipeline,pipeline,133,https://hail.is,https://github.com/hail-is/hail/pull/6618#issuecomment-510731707,2,"['pipeline', 'update']","['pipeline', 'update']"
Deployability,"This failed because of a Query bug. It's ready for another look. Once you're happy with the structure of the code, I'll do one last dev deploy and check everything is good.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1138707197:136,deploy,deploy,136,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1138707197,1,['deploy'],['deploy']
Deployability,"This failed with OpenJDK 11 installed. I don't know Scala, but my read of your checker is that it's looking for older versions, not newer ones. The docs explicitly say java 8 is required.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4896#issuecomment-444556003:28,install,installed,28,https://hail.is,https://github.com/hail-is/hail/issues/4896#issuecomment-444556003,1,['install'],['installed']
Deployability,"This is a little confusing and not especially well documented on our end. The `--tmpdir` option set within Hail defines the scratch space for certain Hail operations, but not Spark ones. The tempdir that's blowing up on you is the `spark.local.dir` setting (see http://spark.apache.org/docs/latest/configuration.html for more info). This directory is used when Spark needs to do a data shuffle, and currently the `splitmulti` command requires this. I assume you're submitting to a cluster with a `spark-submit` invocation. You can set spark settings by passing the `--conf` argument, as here:. ``` text; /usr/bin/spark-submit \ ; --conf spark.shuffle.compress=true \ ; --conf spark.local.dir=/correct/tmp/dir \; --class org.broadinstitute.hail.driver.Main \ ; JAR \; ...; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/902#issuecomment-251721691:298,configurat,configuration,298,https://hail.is,https://github.com/hail-is/hail/issues/902#issuecomment-251721691,1,['configurat'],['configuration']
Deployability,"This is a month out of date and CI has hit the status update limit, closing. Please push a fresh commit SHA when you're ready for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8333#issuecomment-612943506:54,update,update,54,https://hail.is,https://github.com/hail-is/hail/pull/8333#issuecomment-612943506,1,['update'],['update']
Deployability,"This is a quadratic task in releases, so it's not really feasible. We just need to be better about writing which interface is used in a post.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138:28,release,releases,28,https://hail.is,https://github.com/hail-is/hail/issues/1384#issuecomment-349720138,1,['release'],['releases']
Deployability,This is an issue with deploying on spark 1.6.0. Subsumed by #667,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/669#issuecomment-242106016:22,deploy,deploying,22,https://hail.is,https://github.com/hail-is/hail/issues/669#issuecomment-242106016,1,['deploy'],['deploying']
Deployability,This is an ok thing to do since we haven't released since those 1.6 files were created in #11151,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11246#issuecomment-1020333007:43,release,released,43,https://hail.is,https://github.com/hail-is/hail/pull/11246#issuecomment-1020333007,1,['release'],['released']
Deployability,"This is better, but I want to go more extreme:. - get rid of TransformedRegionValueAggregator and ZippedRegionValueAggregator. This is a compiler backend. Too much abstraction in your output! Let's compile that shit away.; - ExtractAggregators should return an Array[AggSum]. These are expressions ending in AggSum containing aggregator operations (filter, map, etc.) defined in terms of the aggregated element and associated context.; - Add a function AggSum => RegionValueAggregator. This is the way to generalize: make AggSum into Agg(op: AggOp) where AggOp (like unary and binary op) are all the possible aggregator types, and there is a function that maps the op to the corresponding RegionValueAggregator*; - compiling the Array[AggSum] should product a function that takes the array of aggregators and a single value (with context) of the collection we're aggregating over and updates them with that element. *I think you need an array of arguments to handle things like call_stats which are evaluated in the aggregator scope (the only scope available to evaluate something). Imagine you have `gs.filter(g => g.GT.isHet()).map(g => g.DP).sum() + gs.flatMap(g => g.PL).sum()`. The Array[AggSum] will be. ```; Array(AggSum(AggMap(; AggFilter(AggIn(...), ; ""g"", g.GT.isHet()),; ""g"", (getField (Ref ""g"") ""DP""),; AggSum(AggFlatMap(AggIn(...),; ""g"", (getField (Ref ""g"") ""PL))); ```. The generated function should look like:. ```; def f(aggs: Array[AggSum], region: MemoryBuffer, g: Long, mg: Boolean, ...) {; if (g.GT.isHet()) { // RV-ified, of course; val DP = g.DP // actually fieldOffset; aggs(0).seqOp(DP); }; for (PLi in g.PL) { // actually elementOffset; aggs(1).seqOp(PLi); }; }; ```. This is straight-line and should be fast. It immediately allows you to do common subexpression elimination on aggregator prefixes which is something that is quite common, that is, if you have `gs.filter(g => g.isHet).map(g => g.DP).mean() < 10 gs.filter.map(g => g.GQ).mean() < 50` then in the aggregation fu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2555#issuecomment-350868684:884,update,updates,884,https://hail.is,https://github.com/hail-is/hail/pull/2555#issuecomment-350868684,1,['update'],['updates']
Deployability,This is caused by VariantSpark monkey patching Hail. I've asked them to [stop doing this](https://github.com/aehrc/VariantSpark/issues/228).,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12859#issuecomment-1502215008:38,patch,patching,38,https://hail.is,https://github.com/hail-is/hail/issues/12859#issuecomment-1502215008,1,['patch'],['patching']
Deployability,This is going to be really challenging to deploy! Can you bump the instance version? The issue is the new CI build will probably get run on old batch workers that have the old copying mechanism.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10131#issuecomment-790044318:42,deploy,deploy,42,https://hail.is,https://github.com/hail-is/hail/pull/10131#issuecomment-790044318,1,['deploy'],['deploy']
Deployability,"This is great, not having to enumerate the dependencies. Hmm, this is potentially making the build 2x slower. Your branch:. > 17 | build_hail | Complete | Success 🎉 (0) | 8 minutes | log. A master deploy a few moments ago:. > 16 | build_hail | Complete | 0 | 4 minutes | log. The cluster might have been under load when your tests run. Can you do a bit of benchmarking to see if this is real?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082:197,deploy,deploy,197,https://hail.is,https://github.com/hail-is/hail/pull/6248#issuecomment-498529082,1,['deploy'],['deploy']
Deployability,"This is great, thanks @cseed! I've tried the gradlew option, and it worked well on Debian Jessie with java-8-oracle. `./gradlew installDist` worked, and the majority of the tests passed in `./gradlew check` (4 failed; I can give you the details if this is useful). On Ubuntu 16.04 with java-8-openjdk (the default) I get an error:. ```; :compileJava UP-TO-DATE; :compileScala; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:80: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:98: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; throw new AnnotationPathException(); ^; /home/jk/github/hail/src/main/scala/org/broadinstitute/hail/expr/Type.scala:424: not enough arguments for constructor AnnotationPathException: (msg: String)org.broadinstitute.hail.annotations.AnnotationPathException; case None => throw new AnnotationPathException(); ^; three errors found; :compileScala FAILED. FAILURE: Build failed with an exception.; ```. Is this a dependency on java-8-oracle do you think?. My immediate problem is solved, as I have hail running now, so this is just out of curiosity really. l think it would be good for new users if you could nail down the dependencies a bit more precisely. For testing and development, it's also really useful to be able to spin up a quick Ubuntu VM, apt-get install a few packages and make a fresh install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/594#issuecomment-240346120:128,install,installDist,128,https://hail.is,https://github.com/hail-is/hail/issues/594#issuecomment-240346120,3,['install'],"['install', 'installDist']"
Deployability,"This is indeed tricky, but I think there are other options here. For example, we could expand the spec so that a job specifies *either* a job_id or update_relative_job_id, and same for parents. This way the relative vs absolute job id is baked into the schema of the spec, and not inferred from the sign of the job id. Though similar in concept, I think that would be much less confusing. However,. > We can simplify things if we require all updates make two requests to the server to (1) get the start id and establish the update and then (2) submit new jobs with all absolute job IDs. I'd like to try this first. I feel like if we get a really solid API and it has a couple of superfluous requests in some edge cases, we will be able to come up with good performance shortcuts that don't muddle the normal path. Since the Query Driver currently lives the full life of the batch and is likely to stay that way for a while, it will satisfy these conditions without making any extra requests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685:442,update,updates,442,https://hail.is,https://github.com/hail-is/hail/pull/12010#issuecomment-1215951685,2,['update'],"['update', 'updates']"
Deployability,"This is just an update of #7951. Merged into a more recent master. . I still need to study it to see the filesystem burden. It does seem that the comb-ops are being distributed, but I ran it on gnomAD just to try it, and it failed with HDFS out of space during combops.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8370#issuecomment-604766994:16,update,update,16,https://hail.is,https://github.com/hail-is/hail/pull/8370#issuecomment-604766994,1,['update'],['update']
Deployability,"This is kind of an annoying problem to have because these pip installed versions are frozen in time. I feel like these steps are either redundant (nothing has changed), or will fail because we've updated the checks to improve on what we had at last release. What do these steps really do for us?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165:62,install,installed,62,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061003165,3,"['install', 'release', 'update']","['installed', 'release', 'updated']"
Deployability,"This is largely working, but IIRC was facing some bug in Azure. I would start by re-running the tests, then dev deploying and running any failing tests yourself.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14076#issuecomment-2239861081:112,deploy,deploying,112,https://hail.is,https://github.com/hail-is/hail/pull/14076#issuecomment-2239861081,1,['deploy'],['deploying']
Deployability,"This is mostly for the australians, it means that this commit must be fully deployed before they can deploy following migrations, since we need the changes to the batch-driver that this introduces before the introduce the cleanup migration.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12117#issuecomment-1245574443:76,deploy,deployed,76,https://hail.is,https://github.com/hail-is/hail/pull/12117#issuecomment-1245574443,2,['deploy'],"['deploy', 'deployed']"
Deployability,"This is mostly hooked up but not fully tested. I'd start by dev deploying this and submitting jobs, checking to see if any of the driver's requests to the worker (like in the `schedule_job` function) are rejected.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14622#issuecomment-2243018782:64,deploy,deploying,64,https://hail.is,https://github.com/hail-is/hail/pull/14622#issuecomment-2243018782,1,['deploy'],['deploying']
Deployability,"This is not because we forgot to unfreeze CI, we just have simply never added the dockerhub images to azure automatically. The couple that are there now (only 107 and 112) must have been uploaded manually. Because there are some build.yaml steps that run on deploy that are specific to the broad GCP instance (like maybe making a release), non-hail-vdc instances don't run the whole build.yaml pipeline on deploy, but a subset that are specified through terraform (this is how AUS and MS could decide to only deploy a subset of our services e.g. not monitoring. We somewhat recently added a step (separate from the `deploy` step) called `mirror_hailgenetics_images` that was entirely intended so that other hail deployments (including ourselves on Azure!) could pick up the images that we released to dockerhub. I never added that steps to the Azure CI's config. I have done that now. Somehow I had foreseen this incident happening and when it actually did any prior on it disappeared from my brain entirely.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390:258,deploy,deploy,258,https://hail.is,https://github.com/hail-is/hail/issues/13050#issuecomment-1572657390,8,"['deploy', 'pipeline', 'release']","['deploy', 'deployments', 'pipeline', 'release', 'released']"
Deployability,This is now available in 4.80.0 through 5.2.0. Work for this issue is:. 1. Upgrade to latest 4.x.x; 2. Encode cleanup policies in terraform.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13504#issuecomment-1773465652:75,Upgrade,Upgrade,75,https://hail.is,https://github.com/hail-is/hail/issues/13504#issuecomment-1773465652,1,['Upgrade'],['Upgrade']
Deployability,"This is now ready to be reviewed. @danking Could you please help me setup the tests to run on the CI?. @catoverdrive This is an example of the interface and the output generated. There's also a tests file in there. I'm happy to explain the design to you if you'd like. ```python3; from pipeline import Pipeline. p = Pipeline() # initialize a pipeline. # Define mapping for taking a file root to a set of output files; bfile = {'bed': '{root}.bed', 'bim': '{root}.bim', 'fam': '{root}.fam'}. # Import a file as a resource; file = p.read_input('gs://hail-jigold/random_file.txt'). # Import a set of input files as a resource group; input_bfile = p.read_input_group(bed='gs://hail-jigold/input.bed',; bim='gs://hail-jigold/input.bim',; fam='gs://hail-jigold/input.fam'). # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset = (subset; .label('subset'); .docker('ubuntu'); .declare_resource_group(tmp1=bfile, ofile=bfile); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:286,pipeline,pipeline,286,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,4,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,This is obviated by my imminent aggregator registry changes. The L suffix change should be a separate PR. I'll integrate the tests into my branch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090:111,integrat,integrate,111,https://hail.is,https://github.com/hail-is/hail/pull/1051#issuecomment-262054090,1,['integrat'],['integrate']
Deployability,"This is primarily a condensation and formatting of what you have above. Is there more to document?. ### `filteralleles`. #### Usage; - `-c | --condition <expr>`—a hail language expression, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | variant |; | `va` | variant annotations |; | `aIndex` | allele index |; - `--keep/--remove`—keep or remove the allele if the expression is true; - `-a | --annotation <expr>`—a hail language expression which may update the variant annotations based on the removed alleles, the following table describes the variables in the scope of `<expr>`. | Name | Description |; | --- | --- |; | `v` | the _new_ variant |; | `va` | the _old_ variant annotations |; | `aIndices` | an array of the old indices (such that `aIndices[newIndex] = oldIndex`) |. _NB:_ the allele indices are zero indexed and the zeroth index contains the reference. The `condition` expression will be executed for all alleles with index greater than zero. _NB2:_ if all alternate alleles are filtered the entire variant is filtered. #### Examples. ```; ... filteralleles; --keep; -c 'va.alleleQuality[aIndex] > 0.8'; -a 'va.info.AC = aIndices.map(oldIndex => va.info.AC[oldIndex]), va.info.AN = ...'; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/551#issuecomment-240216457:517,update,update,517,https://hail.is,https://github.com/hail-is/hail/issues/551#issuecomment-240216457,1,['update'],['update']
Deployability,"This is probably a memory leak. Your build is from 2 months ago, and there are several commits since then that change memory management and fix bugs. I'm nearly certain this problem won't exist in the latest build! Can you update? The current build will also be MUCH faster in many cases.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3901#issuecomment-403295450:223,update,update,223,https://hail.is,https://github.com/hail-is/hail/issues/3901#issuecomment-403295450,1,['update'],['update']
Deployability,"This is super cool! I am a big fan of the idea and the overall approach, particularly when it comes to setting up the tmp bucket and getting the permissions on it correct. Here's my high level thoughts. Sorry for the wall of text but I found these a little hard to articulate. ### Regarding number of prompts. I think this is my primary concern. There's a lot of great automation here, but it's a lot right off the bat. I think what this is aiming to do is make it quick and simple to start running batches and every time someone has to stop and ask someone a question as to how they should respond to some prompt that process gets longer and more complicated. I think it's worth considering what the first batch people should run might be and design for a minimal first experience. IMO, a temp bucket is an absolutely crucial piece of configuration before you can do anything interesting and configuring a temp bucket is something that `hailctl` can easily be very opinionated about. Container registry… I feel like there's harder questions there, and you can run a lot of cool batches before having to worry about provisioning your own. It's also not actually a part of the hailctl config (unless something has changed recently) so it feels a little unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:836,configurat,configuration,836,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,1,['configurat'],['configuration']
Deployability,"This is super useful, thanks @jigold! A few high level comments:. - I'd love to have this checked in, but I don't think it should be part of the regular tests, esp. when they run against the production database and this is designed to find/stress the limits of the database.; - Also, this seems most useful for benchmarking different database configuration and settings, and we don't want to vary the production database (and in some cases, we can't, like decreasing the database size).; - Therefore, I think we just have a module you can run that takes a database connection settings and n_jobs, batch_size, batch_parallelism and number of replicates, and runs the benchmark, not integrated with the build system. And .sql files to create/clean up tables. When we want to run it, we can just clone the repo and run it directly. Then we can think about wrapping it in a larger test to spinning up database instances with various node and disk sizes and MySQL settings and see how they perform.; - You explore number of jobs and batch size, but I think you should also measure amount of batch insert parallelism. You can use bounded_gather I sent you. Then basically these two tests correspond to parallelism=1 and parallelism=infinity.; - I think you can get rid of the pymysql version. No reason the async version should perform differently, no?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881:343,configurat,configuration,343,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538068881,2,"['configurat', 'integrat']","['configuration', 'integrated']"
Deployability,"This is what `hailctl` looks like:. ```. Usage: hailctl [OPTIONS] COMMAND [ARGS]... Manage and monitor hail deployments. ╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────╮; │ --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified │; │ shell. │; │ [default: None] │; │ --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified │; │ shell, to copy it or customize the │; │ installation. │; │ [default: None] │; │ --help Show this message and exit. │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Commands ───────────────────────────────────────────────────────────────────────────────────────────╮; │ batch Manage batches running on the batch service managed by the Hail team. │; │ config Manage Hail configuration. │; │ curl Issue authenticated curl requests to Hail infrastructure. │; │ version Print version information and exit. │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ```. This is what `hailctl batch submit --help` looks like:. ```. Usage: hailctl batch submit [OPTIONS] SCRIPT [ARGUMENTS]... Submit a batch with a single job that runs SCRIPT with the arguments ARGUMENTS. ╭─ Arguments ──────────────────────────────────────────────────────────────────────────────────────────╮; │ * script PATH Path to the script [default: None] [required] │; │ arguments [ARGUMENTS]... [default: None] │; ╰──────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Options ────────────────────────────────────────────────────────────────────────────────────────────╮; │ --files PATH Files or directories to add to the working directory of the │; │ job. │; │ [default: None] │; │ --name TEXT The name of the batch. │; │ --image-name TEXT Name of Docker image for the job │; │ [default: (hailgenetics/hail)] │; │ --ou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921:108,deploy,deployments,108,https://hail.is,https://github.com/hail-is/hail/pull/13109#issuecomment-1561081921,5,"['Install', 'configurat', 'deploy', 'install']","['Install', 'configuration', 'deployments', 'install-completion', 'installation']"
Deployability,"This isn't a hard change, but it is a big one. Let me know if you want me to break it up. OK, I think this is ready for a look. What I've tested:. - hand deploy new auth, router-resolver to default,; - tested login/logout flow on web (auth.hail.is/login, /logout) and hailctl (hailctl auth login/logout); - then deploy in my namespace:. ```; hailctl dev deploy -b cseed/hail:auth -s deploy_auth,deploy_router,deploy_notebook2; ```. - and test login/logout flow via notebook2 (internal.hail.is/cseed/notebook2, etc.) and hailctl, where access to internal is mediated by production (default namespace) credentials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:154,deploy,deploy,154,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,3,['deploy'],['deploy']
Deployability,"This isn't as easy as I had hoped. We have to sort out how to either let the container directly create overlay mounts or figure out how to get fuse-overlay working. I think for fuse-overlay, we might need to modify the VM image to include fuse-overlay. ```; + set +x; Using GOOGLE_APPLICATION_CREDENTIALS; + export TMPDIR=/io/; + TMPDIR=/io/; + retry buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; + buildah build -t us-docker.pkg.dev/hail-vdc/hail/git-make-bash:test-deploy-j6d7pph9mlzf -f /Dockerfile --cache-from us-docker.pkg.dev/hail-vdc/hail/cache --cache-to us-docker.pkg.dev/hail-vdc/hail/cache --layers /io; STEP 1/2: FROM us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04; Trying to pull us-docker.pkg.dev/hail-vdc/hail/ubuntu:20.04...; Getting image source signatures; Copying blob sha256:ca1778b6935686ad781c27472c4668fc61ec3aeb85494f72deb1921892b9d39e; Copying config sha256:88bd6891718934e63638d9ca0ecee018e69b638270fe04990a310e5c78ab4a92; Writing manifest to image destination; Storing signatures; time=\""2023-05-26T14:52:12Z\"" level=error msg=\""Unmounting /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged: invalid argument\""; Error: mounting new container: mounting build container \""45e0ed631d22b6e1de7945266efcf0b802aa3b919d6b6ebd529ded6fedc11cf9\"": creating overlay mount to /var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/merged, mount_data=\""lowerdir=/var/lib/containers/storage/overlay/l/ZCKOX3GV2VWHWT4DMPLYJGMJWL,upperdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/diff,workdir=/var/lib/containers/storage/overlay/dfc7702a226c7f2566c37f22a8636084e25da7ad1dcdf6a05eac8d3aa3b245a2/work,nodev,fsync=0,volatile\"": using mount program /usr/bin/fus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692:419,deploy,deploy-,419,https://hail.is,https://github.com/hail-is/hail/pull/13103#issuecomment-1564774692,2,['deploy'],['deploy-']
Deployability,This latent bug was triggered by https://github.com/hail-is/hail/commit/219f7a48d5592c7f2d86cd65f4134eec9d2ac680 which was released with 0.2.114.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14245#issuecomment-1924694888:123,release,released,123,https://hail.is,https://github.com/hail-is/hail/issues/14245#issuecomment-1924694888,1,['release'],['released']
Deployability,"This looks like good start. A few comments:; - I prefer using MySQL over auth0 mainly because it simplifies our eventual backup/restore story. If you think that's simpler overall, great. I don't see how integrating our db with their service does anything for us.; - I assume you're planning to pull the user data from MySQL during the login flow and add it to cookie? I think @danking @jigold and I are interested in nailing down the format for the cookie and seeing an example.; - I agree with @danking we should have an internal id field that's an integer. I think we should use that everywhere, and just use the auth0 id to look up the user record during login. So the integer id would be the primary key and the auth0 id would be unique with a secondary index.; - You need to get the GCP service account key and store it in a secret.; - The GCP service account needs permissions on the bucket. It should be bucket writer.; - Name ""user_secrets"" seems overly specific (buckets and service accounts are not secrets). ""user_data""?; - Please don't give the database a public IP.; - From a usability perspective, for user-visible names I have to say I really dislike long uuids and like the k8s-style short random string at the end. For k8s resource, you get this for free with the `generate_name` argument. For other stuff, long-term, this will potentially require retry logic to make it robust.; - I don't like this create table logic (FYI @danking @jigold). Most database users should not have permissions to create databases. There should be a k8s secret with the database root and a secret for each specific database application that only has access to that database.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731:203,integrat,integrating,203,https://hail.is,https://github.com/hail-is/hail/pull/5618#issuecomment-473583731,1,['integrat'],['integrating']
Deployability,"This means that hail (or something on which hail depends) is trying to call into BLAS. BLAS is a Fortran library and is often shipped with C bindings. The symbol `cblas_dgemv` is a C function. Your machine is likely missing `libcblas`. Can you post the output of these commands:; - `nm -g /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so` (this may say that the file doesn't exist, in which case skip the next command; - `objdump -TC /tmp/jniloader803664626041947143netlib-native_system-linux-x86_64.so`. Can you also answer these questions:; - What distribution are you using?; - What version of that distribution do you have?; - What package management tool do you use?. If you're on Ubuntu, can you tell me what version of `libatlas-dev` you have installed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-238879581:770,install,installed,770,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-238879581,1,['install'],['installed']
Deployability,"This particular issue is that I recently added a reference to an Array of IR in the JVM that is owned by python. When the owning object is destructed, the reference in Java will be released, enabling it to be garbage collected. Python won't run a GC pass inside a function without requesting it in general.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5658#issuecomment-475282912:181,release,released,181,https://hail.is,https://github.com/hail-is/hail/pull/5658#issuecomment-475282912,1,['release'],['released']
Deployability,"This patch and the contents of #13970 was sufficient to successfully run `make -C hail install SPARK_VERSION=3.4.0` (but is incompatible with Spark 3.3). ```diff; diff --git a/hail/build.gradle b/hail/build.gradle; index 1b65904484..d1feb0e578 100644; --- a/hail/build.gradle; +++ b/hail/build.gradle; @@ -40,7 +40,7 @@ tasks.withType(JavaCompile) {; }; ; project.ext {; - breezeVersion = ""1.1""; + breezeVersion = ""2.1.0""; ; sparkVersion = System.getProperty(""spark.version"", ""3.3.0""); ; diff --git a/hail/src/main/scala/is/hail/HailContext.scala b/hail/src/main/scala/is/hail/HailContext.scala; index 4e4063378b..4d2f9056a5 100644; --- a/hail/src/main/scala/is/hail/HailContext.scala; +++ b/hail/src/main/scala/is/hail/HailContext.scala; @@ -113,10 +113,10 @@ object HailContext {; ; {; import breeze.linalg._; - import breeze.linalg.operators.{BinaryRegistry, OpMulMatrix}; + import breeze.linalg.operators.{BinaryRegistry, HasOps, OpMulMatrix}; ; implicitly[BinaryRegistry[DenseMatrix[Double], Vector[Double], OpMulMatrix.type, DenseVector[Double]]].register(; - DenseMatrix.implOpMulMatrix_DMD_DVD_eq_DVD); + HasOps.impl_OpMulMatrix_DMD_DVD_eq_DVD); }; ; theContext = new HailContext(backend, branchingFactor, optimizerIterations); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13971#issuecomment-1792870445:5,patch,patch,5,https://hail.is,https://github.com/hail-is/hail/issues/13971#issuecomment-1792870445,2,"['install', 'patch']","['install', 'patch']"
Deployability,"This really does look great! I have two small suggestions:. 1. I feel like you should say `read_input` and rather than `write_input`. I'm thinking these commands are from the perspective of the pipeline since they are on Pipeline. 2. Rather than building a group and then declaring it, I think you can do both at once:. ```; # Remove duplicate samples from a PLINK dataset; subset = p.new_task(); subset.declare_resource_groups(tmp1={bed=""{root}.bed"", bim=""{root}.bim"", fam=""{root}.fam""}, ; ofile={...}); subset = (subset; .label('subset'); .command(f'plink --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452846400:194,pipeline,pipeline,194,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452846400,2,"['Pipeline', 'pipeline']","['Pipeline', 'pipeline']"
Deployability,"This seems right to me. In this vein of work, there's one more thing I want: we should build this Dockerfile with the hail wheel and then execute `pylint hail && pylint hailtop`. Pylint will look for uninstalled modules. This will save us from checking in (and eventually deploying) a hail package with bad dependencies. We should probably also run the python tests against this version of hail. This is a tru, local-mode user environment. ```; FROM ubuntu:18.04. ENV LANG C.UTF-8. RUN apt-get update && \; apt-get -y install \; openjdk-8-jdk-headless \; python3 python3-pip && \; rm -rf /var/lib/apt/lists/*. COPY hail.whl pylintrc ./; RUN pip install --no-cache-dir ./hail.whl; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882:272,deploy,deploying,272,https://hail.is,https://github.com/hail-is/hail/pull/7031#issuecomment-530112882,4,"['deploy', 'install', 'update']","['deploying', 'install', 'update']"
Deployability,"This seems to do it:; ```. In [1]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(32):; ...: j = b.new_job(); ...: j.command(f'cat >/dev/null {"" "".join(b.read_input(""gs://danking/foo.vcf"") for _ in range(1000))}'); ...: b.run(); /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/aiocloud/aiogoogle/user_config.py:29: UserWarning: You have specified the GCS requester pays configuration in both your spark-defaults.conf (/Users/dking/miniconda3/lib/python3.10/site-packages/pyspark/conf/spark-defaults.conf) and either an explicit argument or through `hailctl config`. For GCS requester pays configuration, Hail first checks explicit arguments, then `hailctl config`, then spark-defaults.conf.; warnings.warn(; /Users/dking/miniconda3/lib/python3.10/site-packages/hailtop/batch/backend.py:786: UserWarning: Using an image ubuntu:22.04 from Docker Hub. Jobs may fail due to Docker Hub rate limits.; warnings.warn(f'Using an image {image} from Docker Hub. '. https://batch.hail.is/batches/8090821 ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 33/33 0:00:00 0:01:37; batch 8090821 complete: success; Out[1]: <hailtop.batch_client.client.Batch at 0x1086d1bd0>. In [2]: import hailtop.batch as hb; ...: b = hb.Batch(backend=hb.ServiceBackend()); ...: for _ in range(300):; ...: j = b.new_job(); ...: j.command(f'echo {""a"" * 11 * 1024}'); ...: b.run(); ```. Perhaps related to creating a fresh service backend?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467:432,configurat,configuration,432,https://hail.is,https://github.com/hail-is/hail/issues/14051#issuecomment-1834376467,2,['configurat'],['configuration']
Deployability,"This sets the configuration permanently -- any following commands will use the overridden codecs. Setting a global option is almost certainly better than getting this kind of leakage, I think",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/826#issuecomment-248645129:14,configurat,configuration,14,https://hail.is,https://github.com/hail-is/hail/pull/826#issuecomment-248645129,1,['configurat'],['configuration']
Deployability,This should be fixed once https://github.com/hail-is/hail/pull/5655 goes in and the pr-builder has the standard python dependencies installed for the default python3 image.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834:132,install,installed,132,https://hail.is,https://github.com/hail-is/hail/pull/5624#issuecomment-475123834,1,['install'],['installed']
Deployability,This should be split into two PRs so as not to break users pipelines until we release the new version of hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8834#issuecomment-632136987:59,pipeline,pipelines,59,https://hail.is,https://github.com/hail-is/hail/pull/8834#issuecomment-632136987,2,"['pipeline', 'release']","['pipelines', 'release']"
Deployability,This should be updated in one of my current PRs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11966#issuecomment-1169115424:15,update,updated,15,https://hail.is,https://github.com/hail-is/hail/pull/11966#issuecomment-1169115424,2,['update'],['updated']
Deployability,"This should go in. For 0-argument functions, you should support both with and without parens for now. We can make it more strict after the 0.1 release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1411#issuecomment-283861433:143,release,release,143,https://hail.is,https://github.com/hail-is/hail/pull/1411#issuecomment-283861433,1,['release'],['release']
Deployability,"This should include performance experiments we use to drive design decisions (e.g., Array vs. Vector) so we know what assumptions change when we upgrade Scala/Spark/JVM or underlying abstractions.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/19#issuecomment-152269229:145,upgrade,upgrade,145,https://hail.is,https://github.com/hail-is/hail/issues/19#issuecomment-152269229,1,['upgrade'],['upgrade']
Deployability,This should probably not be in the release script. It should just run on every deploy. The original PR's title suggested this was our intention https://github.com/hail-is/hail/pull/13703,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14154#issuecomment-1889761766:35,release,release,35,https://hail.is,https://github.com/hail-is/hail/issues/14154#issuecomment-1889761766,2,"['deploy', 'release']","['deploy', 'release']"
Deployability,"This should work. I didn't test the `cleanup-db` because I wanted to leave the db I have running. Right now, you have to run this from inside the db-benchmark directory. I'm not sure how to easily make it directory agnostic and didn't want to put the time into that now. ```; cd db-benchmark; pip3 install -U ./. db-benchmark create-db --tier db-n1-standard-1 test-ad914f # this will assign a random name for the db if not specified . db-benchmark run test-ad914f --parallelism 5 --batch-sizes 1,10,100,1000,10000,100000 --chunk-size 1000 --n-replicates 10. db-benchmark cleanup-db test-ad914f; ```. By default, the k8s logs go to `benchmark.log`. I couldn't get them to print out nicely to the console. I'll try testing some of the database flags tomorrow and see if any of the suggestions on stack overflow help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807:298,install,install,298,https://hail.is,https://github.com/hail-is/hail/pull/7181#issuecomment-538785807,1,['install'],['install']
Deployability,"This specifically pertains to the *cgroup* of the user container. This command will probably *not* work inside the user container's mount namespace as 1. they wouldn't have `gcsfuse` installed and 2. we don't give them the capabilities necessary to set up a FUSE mount. `nsenter` should allow us to invoke `gcsfuse` inside the user's cgroup to attribute any memory usage to the user and not as part of the batch container. The tricky bit is that currently we run `gcsfuse`/`blobfuse` before the container is created. Because `crun` currently creates/destroys the cgroup, it does not yet exist when these mounts are set up. It *might* be possible to use [OCI hooks](https://github.com/opencontainers/runtime-spec/blob/main/config.md#prestart) to run `gcsfuse`/`blobfuse` after the container and corresponding cgroup is created but before user code is run.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13996#issuecomment-1804219578:183,install,installed,183,https://hail.is,https://github.com/hail-is/hail/issues/13996#issuecomment-1804219578,1,['install'],['installed']
Deployability,This still needs the following modifications to `install_bootstrap_dependencies.sh`:; - Also install the `gke-gcloud-auth-plugin`; - Install the docker buildx plugin,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14625#issuecomment-2239160292:93,install,install,93,https://hail.is,https://github.com/hail-is/hail/pull/14625#issuecomment-2239160292,2,"['Install', 'install']","['Install', 'install']"
Deployability,"This was a huge pain. I think I got everything, but it would be great if you can double check. I added two new images since earlier in case you already ran the script. The things I omitted to fix didn't have Makefiles with build steps and are deployed infrequently.; - blog; - notebook/worker/Dockerfile; - notebook/images/Dockerfile; - docker/python-dill. Memory already has the jinja templating in the Makefile.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9660#issuecomment-719690473:243,deploy,deployed,243,https://hail.is,https://github.com/hail-is/hail/pull/9660#issuecomment-719690473,1,['deploy'],['deployed']
Deployability,"This was a mix of a couple of issues. #12021 should fix the infinite redirect loop caused by your accounts not being developer accounts. I've upgraded you but even so there's a chance of us accidentally picking up your old account's session instead of your new shiny broadinstitute accounts, which will again land you with a 401. If you log out and back in does it work now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11954#issuecomment-1180671583:142,upgrade,upgraded,142,https://hail.is,https://github.com/hail-is/hail/pull/11954#issuecomment-1180671583,1,['upgrade'],['upgraded']
Deployability,This was a problem with .crc files. The solution is to delete the .crc files. We need to update the renamesamples documentation explain this.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/347#issuecomment-214553591:89,update,update,89,https://hail.is,https://github.com/hail-is/hail/issues/347#issuecomment-214553591,1,['update'],['update']
Deployability,"This was fixed in #6478, which merged a week ago, but I guess the docs haven't been deployed since then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509626765:84,deploy,deployed,84,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509626765,1,['deploy'],['deployed']
Deployability,"This was just addressed in #11828. We'll make a release asap, and please let us know if you encounter issues in the new version. Sorry for the disruption!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11827#issuecomment-1124030641:48,release,release,48,https://hail.is,https://github.com/hail-is/hail/issues/11827#issuecomment-1124030641,1,['release'],['release']
Deployability,"This was resolved by @chrisvittal in https://github.com/hail-is/hail/pull/13385 and released in 0.2.121. Use; ```python3; combiner = hl.vds.new_combiner(..., call_fields=YOUR_CUSTOM_CALL_FIELDS_ARRAY); combiner.run(); ```; The combiner also, by default, recognizes `LPGT`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13382#issuecomment-1738205715:84,release,released,84,https://hail.is,https://github.com/hail-is/hail/issues/13382#issuecomment-1738205715,1,['release'],['released']
Deployability,"This will allow you to go conda-free: https://github.com/hail-is/hail/pull/5655. In particular, I modified batch to be conda-free as part of this PR. I added the common python dependencies (async stuff, including mysql stuff) to the default python3 installation in the image. I think my PR will obviate this one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257:249,install,installation,249,https://hail.is,https://github.com/hail-is/hail/pull/5616#issuecomment-475252257,1,['install'],['installation']
Deployability,"This will be fixed in the next release.; If you can't wait until then and you're comfortable patching this yourself, replace the contents of the affected file with the following:; ```yaml; dataproc:; init_notebook.py: gs://hail-common/hailctl/dataproc/0.2.129/init_notebook.py; vep-GRCh37.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh37.sh; vep-GRCh38.sh: gs://hail-common/hailctl/dataproc/0.2.129/vep-GRCh38.sh; wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl; pip_dependencies: aiodns==2.0.0|aiohttp==3.9.3|aiosignal==1.3.1|async-timeout==4.0.3|attrs==23.2.0|avro==1.11.3|azure-common==1.1.28|azure-core==1.30.1|azure-identity==1.15.0|azure-mgmt-core==1.4.0|azure-mgmt-storage==20.1.0|azure-storage-blob==12.19.0|bokeh==3.3.4|boto3==1.34.55|botocore==1.34.55|cachetools==5.3.3|certifi==2024.2.2|cffi==1.16.0|charset-normalizer==3.3.2|click==8.1.7|commonmark==0.9.1|contourpy==1.2.0|cryptography==42.0.5|decorator==4.4.2|deprecated==1.2.14|dill==0.3.8|frozenlist==1.4.1|google-auth==2.28.1|google-auth-oauthlib==0.8.0|humanize==1.1.0|idna==3.6|isodate==0.6.1|janus==1.0.0|jinja2==3.1.3|jmespath==1.0.1|jproperties==2.1.1|markupsafe==2.1.5|msal==1.27.0|msal-extensions==1.1.0|msrest==0.7.1|multidict==6.0.5|nest-asyncio==1.6.0|numpy==1.26.4|oauthlib==3.2.2|orjson==3.9.10|packaging==23.2|pandas==2.2.1|parsimonious==0.10.0|pillow==10.2.0|plotly==5.19.0|portalocker==2.8.2|py4j==0.10.9.5|pyasn1==0.5.1|pyasn1-modules==0.3.0|pycares==4.4.0|pycparser==2.21|pygments==2.17.2|pyjwt[crypto]==2.8.0|python-dateutil==2.9.0.post0|python-json-logger==2.0.7|pytz==2024.1|pyyaml==6.0.1|regex==2023.12.25|requests==2.31.0|requests-oauthlib==1.3.1|rich==12.6.0|rsa==4.9|s3transfer==0.10.0|scipy==1.11.4|six==1.16.0|sortedcontainers==2.4.0|tabulate==0.9.0|tenacity==8.2.3|tornado==6.4|typer==0.9.0|typing-extensions==4.10.0|tzdata==2024.1|urllib3==1.26.18|uvloop==0.19.0;sys_platform!=""win32""|wrapt==1.16.0|xyzservices==2023.10.1|yarl==1.9.4|; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651:31,release,release,31,https://hail.is,https://github.com/hail-is/hail/issues/14452#issuecomment-2045876651,2,"['patch', 'release']","['patching', 'release']"
Deployability,"This will prevent optimization around the filter intervals. Given the prevalence of filter intervals in the biggest, baddest pipelines, this is a concern.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209:125,pipeline,pipelines,125,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-454386209,1,['pipeline'],['pipelines']
Deployability,This will probably behave better with this: https://github.com/hail-is/hail/pull/7636. The four was roughly chosen to match the k8s maximum pool size so there is space for test deployments. One problem we're seeing now is preemptible workloads get scheduled on non-preemptible nodes meaning there isn't space for non-preemptible test workloads.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961:177,deploy,deployments,177,https://hail.is,https://github.com/hail-is/hail/pull/7728#issuecomment-565619961,1,['deploy'],['deployments']
Deployability,"Though I may instead modify the test first, will update here.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3785#issuecomment-399807879:49,update,update,49,https://hail.is,https://github.com/hail-is/hail/issues/3785#issuecomment-399807879,1,['update'],['update']
Deployability,"Thought a bit more about our conversation. I think it would be nice, in general, to not make the local user manually create the root path secret, and it also seems better to ensure they have the necessary gcloud permissions. What do you think about. ```sh; /pipeline-secrets/pipeline-test-0-1--hail-is.key:; kubectl get secret pipeline-test-0-1--hail-is-service-account-key -o json | jq -r '.[""data""][""pipeline-test-0-1--hail-is.key""]' > $@; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435:258,pipeline,pipeline-secrets,258,https://hail.is,https://github.com/hail-is/hail/pull/5762#issuecomment-479586435,4,['pipeline'],"['pipeline-secrets', 'pipeline-test-']"
Deployability,"Tim, I left the integration tests for now. I propose if you want them out, that we leave 2 cases for each of the type-combinations, so that we can inductively prove that our code can infer the correct unified type across nested IR (without the Ref shortcut)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509:16,integrat,integration,16,https://hail.is,https://github.com/hail-is/hail/pull/6990#issuecomment-530868509,1,['integrat'],['integration']
Deployability,"Timing on profile.vds (1KG, 2535 samples, 25956 variants) using three covariates (isFemale, PC1, PC2) with variant QC and export is 60-63s for wald and lrt, 10-12s for score, on one core locally (with ~2s for read). Wald example:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; variantqc \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t wald \; printschema \; exportvariants -c 'variant = v, beta = va.logreg.wald.beta, se = va.logreg.wald.se, zstat = va.logreg.wald.zstat, pval = va.logreg.wald.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded, nNotCalled = va.qc.nNotCalled, nHomRef = va.qc.nHomRef, nHet = va.qc.nHet, nHomVar = va.qc.nHomVar' -o ~/data/profileHail/profile.sex.pc1.pc2.wald.tsv; ```. ```; hail: info: timing:; read: 1.922s; variantqc: 34.869ms; annotatesamples table: 317.153ms; annotatesamples fam: 60.988ms; logreg: 783.904ms; printschema: 2.034ms; exportvariants: 1m2.3s; ```. By comparison, EPACTS on 1 core of interactive node took:. ```; b.wald; real 10m45.718s; user 6m39.888s. b.lrt; real 11m59.180s; user 5m23.047s. b.score; real 5m0.636s; user 0m28.382s. b.firth; real 25m17.602s; user 19m27.214s. q.lm; real 5m17.675s; user 2m18.712s; ```. More timing info and convergence analysis to come.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-239628493:249,install,install,249,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-239628493,1,['install'],['install']
Deployability,Timing on sampleqc:. Command: `./build/install/hail/bin/hail read -i profile225.vds sampleqc -o sampleqc.tsv`. cs_fastsqc:; read: 1.410s; sampleqc: 1m49.8s. master:; read: 1.449s; sampleqc: 2m48.9s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/93#issuecomment-163000784:39,install,install,39,https://hail.is,https://github.com/hail-is/hail/pull/93#issuecomment-163000784,1,['install'],['install']
Deployability,Timing:. Command: `time ~/hail/build/install/hail/bin/hail read -i ~/profile225.vds variantqc -o ~/variantqc.tsv`. With HWE:. timing: ; read: 1.449s; variantqc: 2m15.3s. real 2m19.420s; user 2m54.741s; sys 0m5.010s. Without HWE:. timing: ; read: 1.775s; variantqc: 2m11.8s. real 2m16.133s; user 2m51.038s; sys 0m5.092s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/73#issuecomment-156199705:37,install,install,37,https://hail.is,https://github.com/hail-is/hail/pull/73#issuecomment-156199705,1,['install'],['install']
Deployability,Tornado 6.0 was released 5 hours ago. https://pypi.org/project/tornado/#history,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5505#issuecomment-468797074:16,release,released,16,https://hail.is,https://github.com/hail-is/hail/issues/5505#issuecomment-468797074,1,['release'],['released']
Deployability,"Total minutes for test_hail_python went up to 162 compared to last deploy at 152. Total minutes for local was 294 compared to last deploy at 312. No change in service backend runtime (I think service backend is bottlenecked on test parallelism in default and non-preemptible cores in PR, so maybe a less interesting number). Maybe the doubling of the block size is having a negative effect? Anyway, let's benchmark on something more realistic than the Hail tests and assess.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12981#issuecomment-1536802594:67,deploy,deploy,67,https://hail.is,https://github.com/hail-is/hail/pull/12981#issuecomment-1536802594,2,['deploy'],['deploy']
Deployability,"True. I thought at least for the copy-paste tokens that this would be intentional. Looks like you can get access tokens from GCP that last up to 12 hours, but that could be insufficient for large workloads. If we need something arbitrarily long-lived, our current implementation might be our best bet short of some better integration with OIDC.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525:322,integrat,integration,322,https://hail.is,https://github.com/hail-is/hail/pull/13934#issuecomment-1785516525,1,['integrat'],['integration']
Deployability,Trying to debug a CI issue and this PR's status has exceeded the number of CI updates due to it being rather old. I'll reopen once I've figured out CI's issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8725#issuecomment-706201983:78,update,updates,78,https://hail.is,https://github.com/hail-is/hail/pull/8725#issuecomment-706201983,2,['update'],['updates']
Deployability,Trying to do dev install deploy.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6284#issuecomment-499938113:17,install,install,17,https://hail.is,https://github.com/hail-is/hail/pull/6284#issuecomment-499938113,2,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Two of your comments deal with my elimination of redundant sources of information, so I'll address both here. I'm basically thinking of us when we have to debug this system. It's confusing if there's two sources of truth or if we're manually calling `deploy.sh` to isolate issues with that from issues with gradle, I don't want to have two separate knobs to spin. If they accidentally get out of sync that's gonna be double confusing (imagine a PyPI version that disagrees with hail's internal version). How about we put these two versions into two single-line, plain text files and have _generated... load the files to initialize the variables?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512:251,deploy,deploy,251,https://hail.is,https://github.com/hail-is/hail/pull/4812#issuecomment-440805512,1,['deploy'],['deploy']
Deployability,"UPDATE:; It's not shuffling region values, but we're still doing a full shuffle to reorder instead of doing a map-side combine. This is _very_ bad. Take the case that you're aggregating to a single key -- this will shuffle ALL THE DATA into a single partition.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3641#issuecomment-391440637:0,UPDATE,UPDATE,0,https://hail.is,https://github.com/hail-is/hail/issues/3641#issuecomment-391440637,1,['UPDATE'],['UPDATE']
Deployability,"US datasets are copied and ready for use. Europe is in progress. Once everything is transitioned, we need to merge https://github.com/hail-is/hail/pull/14286 and release. Then loudly inform everyone of the loss of these buckets. Then we delete them before March 1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13009#issuecomment-1939792040:162,release,release,162,https://hail.is,https://github.com/hail-is/hail/issues/13009#issuecomment-1939792040,1,['release'],['release']
Deployability,"Update on this. I am getting the same errors when doing `group_cols_by` for another aggregation method. This is a matrixtable with 2 variants and 245k samples. ```python; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_gtstats_vals = mt.group_cols_by(mt.ancestry).aggregate(gt_stats_ancestry=hl.agg.call_stats(mt.GT, mt.alleles)); mt_gtstats_vals.gt_stats_ancestry.AF.export(af_ancestry_bucket); ```. ```; [Stage 23:> (0 + 1) / 1]; ---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /tmp/ipykernel_231/1465831350.py in <module>; ----> 1 mt_gtstats_vals.gt_stats_ancestry.AF.export(af_ancestry_bucket). <decorator-gen-634> in export(self, path, delimiter, missing, header). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/python3.7/site-packages/hail/expr/expressions/base_expression.py in export(self, path, delimiter, missing, header); 1068 **{output_col_name: hl.delimit(column_names, delimiter)}); 1069 file_contents = header_table.union(file_contents); -> 1070 file_contents.export(path, delimiter=delimiter, header=False); 1071 ; 1072 @typecheck_method(n=int, _localize=bool). <decorator-gen-1190> in export(self, output, types_file, header, parallel, delimiter). /opt/conda/lib/python3.7/site-packages/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 575 def wrapper(__original_func, *args, **kwargs):; 576 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 577 return __original_func(*args_, **kwargs_); 578 ; 579 return wrapper. /opt/conda/lib/py",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['Update'],['Update']
Deployability,"Update the following docs:; annotatevariants_expr.md; HailExpressionLanguage.md; splitmulti.md, these lines:. ```; 108 filtervariants expr -c 'va.info.AC[va.aIndex] < 10' --remove; 118 annotatevariants expr -c 'va.info.AC = va.info.AC[va.aIndex]'; ```. Update error message in AST. ```; 1905 Hint: For accessing `A'-numbered info fields in split variants, `va.info.field[va.aIndex]' is correct"""""".stripMargin); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/663#issuecomment-242074915:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/663#issuecomment-242074915,2,['Update'],['Update']
Deployability,"Update to this, tried running the same script with the bgen file as v1.2 instead (was v1.1 in initial posted issue), but it gives the same issue/stack trace:. ```; SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apac",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['Update'],['Update']
Deployability,Update: I got things to work if I reject all traffic to the metadata server except DNS.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7447#issuecomment-549387958:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/pull/7447#issuecomment-549387958,1,['Update'],['Update']
Deployability,Update: Tested: https://github.com/broadinstitute/hail/commit/736b61a3b4f576963dc78c913a6596adeb7cc65e. Jon is working on implicit option manipulation. Tim is working on INFO support and annotation infrastructure.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/16#issuecomment-158250176:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/16#issuecomment-158250176,1,['Update'],['Update']
Deployability,"Update: when I tested with chr1 with 32355811 variants at local computer using singularity instead of docker with 200g spark memory, it also failed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14168#issuecomment-1897619805:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/14168#issuecomment-1897619805,1,['Update'],['Update']
Deployability,"Update:. We fixed this issue by converting our sparse matrix tables to variant datasets using. ````; vds = hl.vds.VariantDataset.from_merged_representation(smt); ````; And then merging the vds tables with. ````; combiner = hl.vds.new_combiner(output_path, temp_dir, gvcf_paths); combiner.run(); ````",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12083#issuecomment-1222850431:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/12083#issuecomment-1222850431,1,['Update'],['Update']
Deployability,"Update:. We were able to resolve this issues using a combination of three strategies. 1) We enabled [autoscaling](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling) in our cluster. 2) We changed the spark partition defaults on our cluster to split data into 8,00 partitions. We had [read](https://cloud.google.com/dataproc/docs/support/spark-job-tuning) that this number could be changed to 3x the number of vCPUs on our cluster. Because we are using autoscaling, the number of vCPUs used is not predetermined. Because of this we started with 1x the maximum number of secondary workers in our cluster. Our maximum is set to 1000 n1-highmem-8 machines. These nodes contain 8 vCPUs each, so 8 x 1,000 = 8,000. After speaking with Google, we verified that we could have used 3x the maximum number of vCPUs to increase parallelism. With a maximum of 10 workers and 1,000 secondary workers, all n1-highmem-8 nodes, we could have increased our partition to 24,240. A sample cluster declaration using autoscaling and default shuffle partitions and parallelism of 8000 is below. 3) The hail team had informed us that ""You might try adding `block_size=2048` to your King invocation. That will reduce the memory requirements on the workers to ~1/4 of the default which should give ample room for the analysis."" Because of this, we changed the block size in king to ```block_size=2048```. After looking through the king [source code](https://hail.is/docs/0.2/_modules/hail/linalg/blockmatrix.html#BlockMatrix.default_block_size), we were able to determine the default block size is 4096. . ```; hailctl dataproc start cluster \; --vep GRCh38 \; --autoscaling-policy=MVP_autoscaling_policy \; --requester-pays-allow-annotation-db \; --packages gnomad \; --requester-pays-allow-buckets gnomad-public-requester-pays \; --secondary-worker-type=non-preemptible \; --master-machine-type=n1-highmem-8 \; --worker-machine-type=n1-highmem-8 \; --worker-boot-disk-size=1000 \; --preemptible-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12290#issuecomment-1284270117:0,Update,Update,0,https://hail.is,https://github.com/hail-is/hail/issues/12290#issuecomment-1284270117,1,['Update'],['Update']
Deployability,"Updated WIP query:. ```sql; SELECT group_resources.batch_id; , group_resources.update_id; , group_resources.job_group_id; , group_resources.inst_coll; , SUM(group_resources.n_creating_cancellable_jobs) AS n_creating_cancellable_jobs; , SUM(group_resources.n_ready_cancellable_jobs) AS n_ready_cancellable_jobs; , SUM(group_resources.n_running_cancellable_jobs) AS n_running_cancellable_jobs; , SUM(group_resources.ready_cancellable_cores_mcpu) AS ready_cancellable_cores_mcpu; , SUM(group_resources.running_cancellable_cores_mcpu) AS running_cancellable_cores_mcpu; , COUNT(*) as `count`; FROM job_group_inst_coll_cancellable_resources AS group_resources; INNER JOIN LATERAL (; SELECT 1; FROM job_groups AS G ; INNER JOIN job_group_self_and_ancestors AS D; ON G.batch_id = D.batch_id; AND G.job_group_id = D.job_group_id; LEFT JOIN job_groups_cancelled AS C ; ON C.id = G.batch_id; AND C.job_group_id = D.ancestor_id ; WHERE G.batch_id = group_resources.batch_id; AND G.job_group_id = group_resources.job_group_id ; AND G.time_completed IS NOT NULL; AND C.id IS NULL; LIMIT 1; ) AS T ON TRUE ; WHERE group_resources.batch_id >= ?; AND ( group_resources.batch_id; , group_resources.update_id; , group_resources.job_group_id; , group_resources.inst_coll; ) > ?; GROUP BY group_resources.batch_id; , group_resources.update_id; , group_resources.job_group_id; , group_resources.inst_coll; ORDER BY group_resources.batch_id ASC; , group_resources.update_id ASC; , group_resources.job_group_id ASC; , group_resources.inst_coll ASC; LIMIT 1000; ;; ```; The following is slightly faster and has the advantage of only returning results which accumulate more than one row, I'm just unsure that the ordering is correct:; ```sql; SELECT R.*; FROM job_groups AS G ; INNER JOIN job_group_self_and_ancestors AS D; ON G.batch_id = D.batch_id; AND G.job_group_id = D.job_group_id; LEFT JOIN job_groups_cancelled AS C ; ON C.id = G.batch_id; AND C.job_group_id = D.ancestor_id ; INNER JOIN LATERAL (; SELECT R.batch_id;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14623#issuecomment-2258948791:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/issues/14623#issuecomment-2258948791,1,['Update'],['Updated']
Deployability,Updated and ready for review,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/462#issuecomment-232471446:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/462#issuecomment-232471446,1,['Update'],['Updated']
Deployability,Updated baseline timings on 61a5d4834 with Hana's updates + `use_new_shuffle=1` on my pc:. ```; --------------------------------------; Timing query 'fast' with 5 repeats.; Initial: 8.920063795001624s ; Mean: 6.641188349500226s; --------------------------------------; Timing query 'slow' with 5 repeats.; Initial: 45.03917969699978s ; Mean: 42.99382913374939s; --------------------------------------; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1947450758:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1947450758,2,"['Update', 'update']","['Updated', 'updates']"
Deployability,Updated changelog,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8867#issuecomment-634712363:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/8867#issuecomment-634712363,1,['Update'],['Updated']
Deployability,"Updated docs, check the value of f at the endpoints have opposite sign.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2323#issuecomment-339088324:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/2323#issuecomment-339088324,1,['Update'],['Updated']
Deployability,Updated docs. Back to you,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/767#issuecomment-250774694:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/767#issuecomment-250774694,1,['Update'],['Updated']
Deployability,Updated docs. Should be good to go.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3872#issuecomment-401417677:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/3872#issuecomment-401417677,1,['Update'],['Updated']
Deployability,Updated impact description,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14741#issuecomment-2447598502:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/14741#issuecomment-2447598502,1,['Update'],['Updated']
Deployability,Updated in one of my current PRs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11965#issuecomment-1169116227:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/11965#issuecomment-1169116227,1,['Update'],['Updated']
Deployability,"Updated queries to return job groups that do not have an ancestor or self job group that has been cancelled. This logic now mirrors that of `delete_prev_cancelled_job_group_cancellable_resources_records`, only in anti-join form.; Previous query returned those job groups that do not have a cancellation record for itself or a descendent job group.; SQL is hard.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14645#issuecomment-2269734587:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/14645#issuecomment-2269734587,1,['Update'],['Updated']
Deployability,"Updated runtime for LRT is about 50s:. ```; ~/hail/build/install/hail/bin/hail \; --master local[1] \; read -i ~/data/profile.vds \; annotatesamples table -i ~/data/profile.ped -e IND_ID -t 'PC1: Double, PC2: Double' -r sa.pc \; annotatesamples fam -i ~/data/profile.fam \; logreg -y sa.fam.isCase -c sa.fam.isFemale,sa.pc.PC1,sa.pc.PC2 -t lrt \; exportvariants -c 'variant = v, beta = va.logreg.lrt.beta, chi2 = va.logreg.lrt.chi2, pval = va.logreg.lrt.pval, nIter = va.logreg.fit.nIter, converged = va.logreg.fit.converged, exploded = va.logreg.fit.exploded' -o ~/data/profileHail/temp.profile.sex.pc1.pc2.lrt.tsv; ```. ```; read: 2.193s; annotatesamples table: 243.817ms; annotatesamples fam: 22.080ms; logreg: 367.098ms; exportvariants: 50.473s; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/585#issuecomment-242580502:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/585#issuecomment-242580502,2,"['Update', 'install']","['Updated', 'install']"
Deployability,Updated significantly. Ready for another look.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/852#issuecomment-250934102:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/852#issuecomment-250934102,1,['Update'],['Updated']
Deployability,Updated so PCA avoids select when the expression is a field (similar to BlockMatrix.write_from_entry_expr). Hopefully good to go now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3262#issuecomment-377693701:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/3262#issuecomment-377693701,1,['Update'],['Updated']
Deployability,"Updated the first comment to be the merge commit comment. Preserving my original comment here:. The output is garbage, but at least it has useful information. Output for tests now look like this:. ```; io/test/test_batch.py::Test::test_batch ; -------------------------------- live log setup --------------------------------; 2020-01-29T21:03:40 INFO test.conftest conftest.py:8:log_before_after starting test; -------------------------------- live log call ---------------------------------; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:481:submit created batch 159; 2020-01-29T21:03:40 INFO batch_client.aioclient aioclient.py:517:submit closed batch 159; PASSED; ------------------------------ live log teardown -------------------------------; 2020-01-29T21:04:08 INFO test.conftest conftest.py:10:log_before_after ending test; ```. @danking this backs out your batch client warning log change. It couldn't have done what you want, since it set the batch client log to WARN, which was only used to make info logs, and override the root logger which can't be what you want. Separately, we should however give users advice about how to set the logger properly for their scripts to get the appropriate log messages (possibly with a help function).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/7990#issuecomment-579974984,1,['Update'],['Updated']
Deployability,"Updated to break up struct construction when building the decoder. Tweaks the size estimates so the method size is 2-4K on some large examples. @tpoterba @konradjk I retimed loading a small subset of fields with this change. I'm not quite sure why things changed so much, but I now see about 4.5x improvement on read, which is more in line with (and even better than) what I was hoping:. ```; In [2]: t = hl.read_table('sites.ht', _row_fields=['locus', 'alleles', 'AN_AFR']). In [3]: %%timeit; ...: t._force_count(); ...: ; 3.43 s ± 54.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each). In [4]: t = hl.read_table('sites.ht'). In [5]: %%timeit; ...: t._force_count(); ...: ; 15.3 s ± 45.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3669#issuecomment-392413464:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/3669#issuecomment-392413464,1,['Update'],['Updated']
Deployability,Updated. Script now needs `--force` on the ab initio VEP calls.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/414#issuecomment-224707193:0,Update,Updated,0,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224707193,1,['Update'],['Updated']
Deployability,"Updates made, site updated with changes matching what you should see if you dev deploy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8634#issuecomment-629587797:0,Update,Updates,0,https://hail.is,https://github.com/hail-is/hail/pull/8634#issuecomment-629587797,3,"['Update', 'deploy', 'update']","['Updates', 'deploy', 'updated']"
Deployability,"Updates needed for the following:. ```sh; > Task :compileTestScala; Pruning sources from previous analysis, due to incompatible CompileSetup.; /io/repo/hail/src/test/scala/is/hail/annotations/ScalaToRegionValue.scala:9: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, a); ^; /io/repo/hail/src/test/scala/is/hail/annotations/StagedRegionValueSuite.scala:487: type mismatch;; found : is.hail.expr.types.physical.PType; required: is.hail.expr.types.physical.PBaseStruct; SafeRow(t, region, copyOff); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:46: type mismatch;; found : is.hail.expr.types.virtual.TStruct; required: is.hail.expr.types.physical.PType; val argOff = ScalaToRegionValue(region, argT, argVs); ^; /io/repo/hail/src/test/scala/is/hail/expr/ir/Aggregators2Suite.scala:177: type mismatch;; found : is.hail.expr.types.virtual.Type; required: is.hail.expr.types.physical.PType; val voff = ScalaToRegionValue(region, stream.typ, lit); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:73: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; rvb.addAnnotation(t, r); ^; /io/repo/hail/src/test/scala/is/hail/nativecode/RegionValueIteratorSuite.scala:133: type mismatch;; found : is.hail.expr.types.physical.PTuple; required: is.hail.expr.types.virtual.Type; Error occurred in an application involving default arguments.; rvb.addAnnotation(t, r); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174:0,Update,Updates,0,https://hail.is,https://github.com/hail-is/hail/pull/6711#issuecomment-515048174,1,['Update'],['Updates']
Deployability,"Upgrading is a good instinct to have. I appreciate this PR. The story here is long, complex, and mundane. I've tried to include everything below. Python 3.6, (should) only appear in the linting image for python 3.6. The `hail-ubuntu` image explicitly installs Python 3.7. I'm happy to drop linting for Python 3.6 from build.yaml if compilers team is OK with that (ask Tim). We are already using Ubuntu 20.04 for our services and tests. See below for details, but `hail-ubuntu` is based on 20.04. We explicitly install JDK 8 in the `base` image. ---. This PR doesn't change the hail-ubuntu image which is the basis for nearly all our images. `DOCKER_ROOT_IMAGE` seems to have been introduced [here](https://github.com/hail-is/hail/pull/9660). That's my bad for not flagging in the review that this isn't actually a ""root image"". `DOCKER_ROOT_IMAGE` is just some Linux image with the standard utils. It was introduced as a distinct global concept when Docker Hub began enforcing rate limits (so we needed all our test images to live in GCR). If you look at the occurrences of `DOCKER_ROOT_IMAGE` or `docker_root_image` you'll find it's almost exclusively used in the tests except for one occurrence in `build-batch-worker-image-startup-gcp.sh`. We could just remove that line. That line is an attempt to keep a relatively recent version of the ubuntu image cached on the worker VM so that we can save some time when pulling the worker Docker image. In practice, the ubuntu image is extraordinarily tiny and quickly becomes out of date (because we rarely rebuild the VM). hail-ubuntu uses a timestamped ubuntu 20.04 tag: `ubuntu:focal-20201106`. I did this because we kept getting screwed by new ubuntu images getting released which were incompatible with us. We would only find out later when we changed the hail-ubuntu Dockerfile and triggered a refetch of the latest image at the `ubuntu:18.04` tag which included the breaking changes. You're correct that this is technical debt of ours. DOCKER_ROOT_I",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805:251,install,installs,251,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965578805,2,['install'],"['install', 'installs']"
Deployability,"Using the Master branch version and Spark 2.2.1, I am getting the same error. Is Spark 2.2,1 supported? Any suggestions?. ```; /gradlew -Dspark.version=2.2.1 shadowJar archiveZip; fdf130b2f5d4. FAILURE: Build failed with an exception. * Where:; Build file '/restricted/projectnb/genpro/github/hail/build.gradle' line: 57. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Unknown Spark version 2.2.1. Set breeze.version and py4j.version properties for Spark 2.2.1. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 6.187 secs; ```; ```; env|grep SPARK; SPARK_HOME=/share/pkg/spark/2.2.1/install; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3001#issuecomment-375939652:706,install,install,706,https://hail.is,https://github.com/hail-is/hail/issues/3001#issuecomment-375939652,1,['install'],['install']
Deployability,Vaguely feel like we should dev deploy and verify login still works,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11457#issuecomment-1057652421:32,deploy,deploy,32,https://hail.is,https://github.com/hail-is/hail/pull/11457#issuecomment-1057652421,1,['deploy'],['deploy']
Deployability,"Various changes:. 1. Locking is now done with a carefully restricted use of flock(), and the makefiles use; perl's rename command to get atomic rename, so they don't need to take locks. 2. The makefile conforms to the customary use-whatever-is-on-$PATH, with the slight wrinkle that; the full pathnames of the commands used will be visible in the build log - so if someone; picks up something weird we'll at least see it. 3. There is a cache of NativeModule objects, so that we won't do enormous numbers of; calls to dlopen/dlclose. This may help in shuffle code, which creates a new PackDecoder; for each RV. 4. The hash function on (options, source) is now beefed up to cope with having only a; few distinct values of options; and is modified with the output of ""$(CXX) --version"",; so that when you upgrade your compiler, you won't get hits on modules compiled with the old; compiler. 5. build.gradle has a new target ""nativeLibPrebuilt"", for updating the prebuilt/lib/linux-x86-64; or prebuilt/lib/darwin. 6. The committed prebuilt libraries are built thus:. darwin - On my MacOS laptop, with the default (clang-based) compiler, -march=sandybridge; From my reading, I believe this should be compatible withall MacBook Pro's; released since 2011, and all versions of MacOS since 10.9 (the first to use; libc++ rather than libstdc++ as the default C++ library) - we're now at 10.13,; with 10.14 arriving some time in the fall. linux-x86-64 - Built on my home desktop running Ubuntu-16.04 LTS, and g++-5.0.4, with; -fabi-version=9. In theory this should work with all systems based on g++5.x and; later. I made some effort to move std::string out of the interfaces between prebuilt; and dynamic code, which gives it some chance of working on systems based on; g++-4.x, but haven't tested that. I'm planning to fire up VM's either in cloud or under VirtualBox, to test this against Ubuntu-14.04,; Ubuntu-18.04, and the latest stable RHEL, which should cover most of the bases. In the interest of getti",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863:802,upgrade,upgrade,802,https://hail.is,https://github.com/hail-is/hail/pull/3973#issuecomment-413997863,1,['upgrade'],['upgrade']
Deployability,Verified on konrad's pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3522#issuecomment-387188320:21,pipeline,pipeline,21,https://hail.is,https://github.com/hail-is/hail/pull/3522#issuecomment-387188320,1,['pipeline'],['pipeline']
Deployability,"Version: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:; cpu: ""1""; memory: 500M; terminationMessagePath: /dev/termination-log; terminationMessagePolicy: File; volumeMounts:; - mountPath: /gsa-key; name: gsa-key",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13113,pipeline,pipeline,13113,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:de,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2540,deploy,deploy-,2540,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:15032,release,release,15032,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,WIP until I sort the release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13653#issuecomment-1781474810:21,release,release,21,https://hail.is,https://github.com/hail-is/hail/pull/13653#issuecomment-1781474810,4,['release'],['release']
Deployability,"Wait, how's that? Looks like it runs `pip uninstall hail ... install hail`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6437#issuecomment-504549671:61,install,install,61,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504549671,1,['install'],['install']
Deployability,Was tested with:. ```; #!/bin/bash. set -ex. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/small.vcf \; splitmulti \; vep -r va --config ~/hail/vep.properties \; write -o ~/small.vds. ./build/install/hail/bin/hail importvcf -n 4 ~/sample.vcf \; splitmulti \; annotatevariants vds -r va -i ~/small.vds \; vep -r va --config ~/hail/vep.properties \; annotatevariants expr -c 'va.transcript_consequences = va.transcript_consequences.toSet' \; annotatevariants expr -c 'va.regulatory_feature_consequences = va.regulatory_feature_consequences.toSet' \; annotatevariants expr -c 'va.colocated_variants = va.colocated_variants.toSet' \; write -o ~/sample2.vds. ./build/install/hail/bin/hail read -i ~/sample.vds comparevds -i ~/sample2.vds; ```. where sample.vcf is test/resources/sample.vcf and small.vcf is the half of the sample.vcf variants.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/414#issuecomment-224702021:53,install,install,53,https://hail.is,https://github.com/hail-is/hail/pull/414#issuecomment-224702021,4,['install'],['install']
Deployability,"We are installing Hail's dependencies by injecting the requirements.txt dependencies into the deploy.yaml file. We don't have a way to grab the dependencies from the other wheel at the moment. I'd prefer to defer this change to modify -- The PR is big enough as-is, and fixes the modify semantics to be what they are in latest cloudtools. I think we should explore if there's a way to get pip to print the package dependencies from the wheel, and then grep out pyspark and install.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048:7,install,installing,7,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500612048,3,"['deploy', 'install']","['deploy', 'install', 'installing']"
Deployability,We are likewise unhappy with the age of the image. The alternative is installing/building enough Perl modules to make VEP work which has also proved challenging. Most recently some LibXML library was failing its own tests inside our relatively recent Ubuntu-based image. We're still evaluating various options. We would like to continue to support 95. We will probably also add 105 at some point. Unclear how long we should support 95.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12946#issuecomment-1543045199:70,install,installing,70,https://hail.is,https://github.com/hail-is/hail/issues/12946#issuecomment-1543045199,1,['install'],['installing']
Deployability,"We are trying to setup hail `0.2.72` on spark `3.1.2` version. However, we are also facing similar error. * java version: `OpenJDK 64-Bit Server VM, 1.8.0_242`; * scala version: `2.12.10`; * py4j: `0.10.9`; * Python: `3.7.10`. <details>; <summary>Stacktrace</summary>. ```; Py4JJavaError: An error occurred while calling o126.exists.; : java.lang.NoClassDefFoundError: com/amazonaws/AmazonClientException; 	at java.lang.Class.forName0(Native Method); 	at java.lang.Class.forName(Class.java:348); 	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:2532); 	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2497); 	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2593); 	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3269); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3301); 	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:124); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3352); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3320); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:361); 	at is.hail.io.fs.HadoopFS.fileStatus(HadoopFS.scala:164); 	at is.hail.io.fs.FS.exists(FS.scala:183); 	at is.hail.io.fs.FS.exists$(FS.scala:181); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:70); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:282); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.Ca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610:523,Configurat,Configuration,523,https://hail.is,https://github.com/hail-is/hail/issues/10590#issuecomment-899322610,6,['Configurat'],['Configuration']
Deployability,"We believe that #9408 may have fixed this for release 0.2.57. We weren't able to replicate locally, though -- I know you're unblocked now, but if it's easy for you, could you try rerunning the failing script to see if that's the fix to your particular issue? Thanks!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-688857271:46,release,release,46,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-688857271,1,['release'],['release']
Deployability,We can decide the proper policy/procedure to update dependencies later. I think that we should find a way to be made aware of security issues in our dependencies so we can update them should an extraordinary circumstance arise.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362:45,update,update,45,https://hail.is,https://github.com/hail-is/hail/pull/4701#issuecomment-435204362,2,['update'],['update']
Deployability,"We can investigate building against an old version of GLIBC for release. In the mean time, I don't see any reason why you shouldn't be able to build locally against an older version of glibc.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-444549354:64,release,release,64,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-444549354,1,['release'],['release']
Deployability,"We definitely need a mechanism to force a stream pipeline (or sub-pipeline) to put all allocations in a single region, and avoid any region management overhead. Then, for example, in table lowering we can set a flag on any single-row stream processing to use a single region, preserving the existing behavior. I have some thoughts on how to do that. We can just pass an ""allocator"" to EmitStream, which is a Region factory, that stream nodes must use to create new regions. An allocator that creates new regions gives the ""free between rows"" behavior. To implement the ""within one row"" behavior, we can pass down an allocator that returns regions backed by a single fixed RegionMemory, with no-op freeing. Maybe that needs to be put in place before this can merge?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761:49,pipeline,pipeline,49,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661877761,2,['pipeline'],['pipeline']
Deployability,"We do still require it, seems like an oversight that it's no longer listed as required. This discuss post: https://discuss.hail.is/t/native-compilation-lz4-issue-on-centos-7-8-2003/2076. suggests that maybe once you've failed to build because you didn't have lz4, you might have to `make clean` and then try again having installed lz4. Maybe try that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10747#issuecomment-892875634:321,install,installed,321,https://hail.is,https://github.com/hail-is/hail/issues/10747#issuecomment-892875634,1,['install'],['installed']
Deployability,"We don't automate VEP tests as part of our deployment, though -- could I ask you to test the fix later today when it goes in?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3790#issuecomment-400290957:43,deploy,deployment,43,https://hail.is,https://github.com/hail-is/hail/issues/3790#issuecomment-400290957,1,['deploy'],['deployment']
Deployability,"We don't use near line or cold line storage, so this change should be okay. However, there is a requirement on who can download the files that I am concerned about. > Note that for such uploads, crcmod is required for downloading regardless of whether the parallel composite upload option is on or not. For some distributions this is easy (e.g., it comes pre-installed on macOS), but in other cases some users have found it difficult. Because of this, at present parallel composite uploads are disabled by default. Google is actively working with a number of the Linux distributions to get crcmod included with the stock distribution. Once that is done we will re-enable parallel composite uploads by default in gsutil.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251:359,install,installed,359,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529229251,1,['install'],['installed']
Deployability,We fixed all the known issues and install-editable now seems reliable and fast.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13691#issuecomment-1758609203:34,install,install-editable,34,https://hail.is,https://github.com/hail-is/hail/pull/13691#issuecomment-1758609203,1,['install'],['install-editable']
Deployability,"We have characterized this issue well, and it will be fixed in an imminent spark release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-290859153:81,release,release,81,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-290859153,1,['release'],['release']
Deployability,We have the HTML and CSS for the navbar duplicated on discuss. I think it's fine that way -- we don't update the code very often.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1024#issuecomment-316249540:102,update,update,102,https://hail.is,https://github.com/hail-is/hail/issues/1024#issuecomment-316249540,1,['update'],['update']
Deployability,"We just updated Nirvana public repository with the bugfix being merged with develop branch. (https://github.com/Illumina/Nirvana/tree/develop). I just realized that the supplementary annotation data haven't been updated (regenerated) yet, so no need to update the JSON schema in Nirvana.scala. This feature branch is good for testing current version of Nirvana (develop branch) and supplementary files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-379110828:8,update,updated,8,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-379110828,3,['update'],"['update', 'updated']"
Deployability,"We may want to use NumPy ndarray as local matrix now, to avoid interface duplication and grab all its functionality, even if there's a performance hit in moving between Python and Java (worst case, we go through disk). I'm going to close this while we strategize, will PR the BlockMatrix updates separately.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825:288,update,updates,288,https://hail.is,https://github.com/hail-is/hail/pull/3064#issuecomment-370149825,1,['update'],['updates']
Deployability,"We need to release after this. I suspect release will be rocky due to some deploy changes I made recently, so make sure I'm online when we do it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11153#issuecomment-993057032:11,release,release,11,https://hail.is,https://github.com/hail-is/hail/pull/11153#issuecomment-993057032,3,"['deploy', 'release']","['deploy', 'release']"
Deployability,"We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the driver was killed by the system.; Let's focus on the driver machines. In Run A, we used an n1-highmem-8 which is advertised to have 52GiB (53248 MiB). In Run B, we used an n1-highmem-16 which is advertised to have 104GiB (106,496 MiB). hailctl sets the JVM max heap size to 80% of the advertised RAM, so 42598 MiB (see ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:1368,pipeline,pipeline,1368,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,1,['pipeline'],['pipeline']
Deployability,"We now have latex integration for formulas, used pervasively in docs along with references to literature. Not sure if this issue calls for more?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/28#issuecomment-279514823:18,integrat,integration,18,https://hail.is,https://github.com/hail-is/hail/issues/28#issuecomment-279514823,1,['integrat'],['integration']
Deployability,"We ran the same pipeline with an n1-highmem-16 driver node and it made it through 50 sample groups (each sample group has ~4000 samples) before crashing. Unfortunately, we do not have the syslogs from this run. We also do not have the Hail log from this run. We do have the stdout/stderr from the Python process. There's not much of value there. The Python process exited with code 256. That doesn't make a lot of sense to me because exit codes should be an unsigned 8-bit integer. On a highmem-16, total RAM is 106,496 MiB. Hail's JVM will use 85,197 MiB. We establish above that the system uses about 9,500 MiB (unclear if it would use more on a larger VM). This all leaves 11,799 MiB for the Python process. That seems extremely generous, but apparently not?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832549389:16,pipeline,pipeline,16,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832549389,1,['pipeline'],['pipeline']
Deployability,"We recently put in a couple PRs that improve performance on these searches so thought I would update here. They were mostly changes to things upstream of the portion of code we have been focusing on and change how data is initially read in, but the biggest performance gain we got was adding `hl._set_flags(use_new_shuffle='1')`. A lot of the focus was around how we handle searches in multiple data types which has been out of the scope of this work so far, so for the search we've been profiling here its only came down to like 80 seconds, but figured its worth sharing. Hopefully this does not cause to catastrophic of a merge conflict for you guys. https://github.com/broadinstitute/seqr/pull/3873; https://github.com/broadinstitute/seqr/pull/3876",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13882#issuecomment-1945174111:94,update,update,94,https://hail.is,https://github.com/hail-is/hail/issues/13882#issuecomment-1945174111,1,['update'],['update']
Deployability,"We seem to be miscommunicating. My issue is that this change uses a knob (pixel ratio) to control something seemingly unrelated to the knob (opacity). This isn't a browser thing, it's a monitor thing. I have two screens one with device pixel ratio (DPR) 1 and one with DPR 2. Regardless of browser, on the DPR 1 monitor, the lines appear thicker than on the DPR 2 monitor. They must appear thicker! The pixels are bigger and less dense. My suggestion is this: https://github.com/hail-is/hail/compare/master...danking:viz. That change sets line width to 1 and uses opacity to reduce the brightness of the lines. My proposal looks, to me, exactly the same as this PR (which I've dev deployed into my namespace: https://internal.hail.is/dking/site/). I've compared both on my DPR 2 monitor and my DPR 1 monitor. . The reason I prefer the change linked above is that it is self-describing: we want the thinest possible lines that the monitor supports and we want them to be a bit faint.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918:681,deploy,deployed,681,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645699918,1,['deploy'],['deployed']
Deployability,We should be explicit about namespace in all our deployments,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4618#issuecomment-432384853:49,deploy,deployments,49,https://hail.is,https://github.com/hail-is/hail/pull/4618#issuecomment-432384853,1,['deploy'],['deployments']
Deployability,"We should not be linting already released hail, that's dumb. Where do we do that?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1062067581:33,release,released,33,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1062067581,1,['release'],['released']
Deployability,We should probably have SELECT FOR UPDATE on these queries as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12942#issuecomment-1525704055:35,UPDATE,UPDATE,35,https://hail.is,https://github.com/hail-is/hail/pull/12942#issuecomment-1525704055,1,['UPDATE'],['UPDATE']
Deployability,We should probably talk to the SEQR team as well: https://github.com/macarthur-lab/hail-elasticsearch-pipelines/blob/master/download_and_create_reference_datasets/v02/hail_scripts/write_combined_reference_data_ht.py,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6715#issuecomment-514336863:102,pipeline,pipelines,102,https://hail.is,https://github.com/hail-is/hail/issues/6715#issuecomment-514336863,1,['pipeline'],['pipelines']
Deployability,We should wait for this to go in once pipeline is actually being tested. Will work on that now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5962#issuecomment-488291829:38,pipeline,pipeline,38,https://hail.is,https://github.com/hail-is/hail/pull/5962#issuecomment-488291829,1,['pipeline'],['pipeline']
Deployability,"We spoke about this in person, but I had this mostly typed up in this buffer so I'll leave it here for future us. Some context on the docker build cache'ing situation. Originally, Docker would use any image layer it had as a cache source. This was noted as a severe security vulnerability because I could make an image that claims to be the result of `apt-get install curl` but actually was the result of `apt-get install virus`. In response, Docker banned the use of non-locally-built (i.e. from the Internet) image layers as cache sources. I believe there may have been other motivations as well, but I did not carefully investigate. https://github.com/moby/moby/issues/26065 documents the desire for a way to use non-locally-built images as a cache source. https://github.com/moby/moby/pull/26839 implements this. Unfortunately, and I cannot find documentation on this, `--cache-from X` means ""cache only from X"". If you pass multiple `--cache-from`s each one is used as a cache source, but it is not possible to say ""use all local images as a cache source"" (other than enumerating them all). [`--cache-from` was included in v1.13.0](https://github.com/moby/moby/releases/tag/v1.13.0), released January 2017. Another subtlety of `--cache-from` is that it does not pull the image in question if it is not found locally. I only found this documented [in a comment on the implementing PR](https://github.com/moby/moby/pull/26839#issuecomment-277383550). Docker seems to be in maintenance mode and all new development is going into Moby. The replacement for `docker build` is called [`buildkit`](https://github.com/moby/buildkit). Build Kit has a more reasonable cache'ing strategy wherein [one exports and imports ones cache](https://github.com/moby/buildkit#exportingimporting-build-cache-not-image-itself) to a trusted repository.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073:360,install,install,360,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474154073,4,"['install', 'release']","['install', 'released', 'releases']"
Deployability,"We switched to Python-based FSes which work fine (see below). The issue in Scala will be fixed by https://github.com/hail-is/hail/pull/13434. ---. Here's a folder with 10k files:; ```; (base) dking@wm28c-761 hail % gsutil ls gs://gcp-public-data--gnomad/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht/rows/parts/ | wc -l; 10000; ```. And here's Hail working correctly as of 0.2.120.; ```; (base) dking@wm28c-761 hail % HAIL_QUERY_BACKEND=batch ipython; Python 3.10.9 (main, Jan 11 2023, 09:18:18) [Clang 14.0.6 ]; Type 'copyright', 'credits' or 'license' for more information; IPython 8.12.2 -- An enhanced Interactive Python. Type '?' for help. In [1]: import hail as hl; ...: print(hl.version()); ...: x = hl.utils.hadoop_ls('gs://gcp-public-data--gnomad/release/2.1.1/ht/genomes/gnomad.genomes.r2.1.1.sites.ht/rows/parts/'); ...: len(x); 0.2.120-f00f916faf78. Out[1]: 10000; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12932#issuecomment-1703524347:254,release,release,254,https://hail.is,https://github.com/hail-is/hail/issues/12932#issuecomment-1703524347,2,['release'],['release']
Deployability,We tried updated to zstd-jni 1.5.5-11 from 1.5.5-2. 4 Failures. [6873](https://batch.hail.is/batches/8093977/jobs/6873) execute(...)_stage2_table_native_writer_job4933	Failed		13s 631ms	$0.0001; [7157](https://batch.hail.is/batches/8093977/jobs/7157) execute(...)_stage2_table_native_writer_job5217	Failed		15s 919ms	$0.0001; [8854](https://batch.hail.is/batches/8093977/jobs/8854) execute(...)_stage2_table_native_writer_job6914	Failed		1 minute 12s	$0.0006; [12795](https://batch.hail.is/batches/8093977/jobs/12795) execute(...)_stage2_table_native_writer_job10855	Failed		21s 305ms	$0.0002,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13979#issuecomment-1843738845:9,update,updated,9,https://hail.is,https://github.com/hail-is/hail/issues/13979#issuecomment-1843738845,1,['update'],['updated']
Deployability,"We update our docs with every release. Spark3 support went in between 0.2.64 and 0.2.65, which was released at the end of last week. Now says: ; > Hail should work with any Spark 3.1.1 cluster built with Scala 2.12. It's possible to compile Hail against other versions of Spark/Scala, but this is what we test against.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10351#issuecomment-827844732:3,update,update,3,https://hail.is,https://github.com/hail-is/hail/issues/10351#issuecomment-827844732,3,"['release', 'update']","['release', 'released', 'update']"
Deployability,"We want to have this in the changelog properly, and to go through a few PRs betweeen now and the next release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13174#issuecomment-1587884319:102,release,release,102,https://hail.is,https://github.com/hail-is/hail/pull/13174#issuecomment-1587884319,1,['release'],['release']
Deployability,We will be skipping Spark 3.4.x and jumping to Spark 3.5.x because Google Dataproc has skipped to 3.5.x. https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13971#issuecomment-1876070344:173,release,release-,173,https://hail.is,https://github.com/hail-is/hail/issues/13971#issuecomment-1876070344,1,['release'],['release-']
Deployability,We will need to update the build server to create a `gradle.properties` file before execution.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201772:16,update,update,16,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201772,1,['update'],['update']
Deployability,"We'll probably switch to 2.12 when there's a PySpark release with 2.12, which there isn't in the 2.4 series (aside from one patch version (2.4.2, nothing else). This has been moved to Asana for task scheduling.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651:53,release,release,53,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-613655651,2,"['patch', 'release']","['patch', 'release']"
Deployability,"We're 11x slower than plink now:. ``` bash; # time plink --bfile profile225 --genome ; PLINK v1.90b3.38 64-bit (7 Jun 2016) https://www.cog-genomics.org/plink2; (C) 2005-2016 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to plink.log.; Options in effect:; --bfile profile225; --genome. 16384 MB RAM detected; reserving 8192 MB for main workspace.; 224885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to plink.nosex .; Using up to 4 threads (change this with --threads).; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.952416.; 224885 variants and 2535 people pass filters and QC.; Note: No phenotypes present.; IBD calculations complete. ; Finished writing plink.genome .; plink --bfile profile225 --genome 67.81s user 1.03s system 297% cpu 23.147 total. # time ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o hail.genome; hail: info: running: read -i profile225-splitmulti.vds; [Stage 0:> (0 + 0) / 4]SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".; SLF4J: Defaulting to no-operation (NOP) logger implementation; SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.; [Stage 1:============================================> (3 + 1) / 4]hail: info: running: ibd -o hail.genome; [Stage 8:======================================================> (62 + 3) / 65]hail: info: while writing:; hail.genome; merge time: 6.980s; hail: info: timing:; read: 2.953s; ibd: 4m12.3s; total: 4m15.2s; ../hail/build/install/hail/bin/hail read -i profile225-splitmulti.vds ibd -o 840.77s user 23.05s system 332% cpu 4:19.75 total. # dc; 60 4 * 19 +; 5 k; 23 / p; 11.26086; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345:976,install,install,976,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-260512345,2,['install'],['install']
Deployability,"We're not exactly the layout you mentioned because our `test` folder is a python package (it has an `__init__.py`). Because of this, Python will search up the path from, say, `test_context.py`, to the `test.hail` ""package"", to the `test` ""package"", then it will stop searching up at `test`'s parent directory `python`, but it _will_ look for sibling directories. Ergo, when `test_context.py` says `import hail as hl`, it goes up to `python` then finds `python/hail` and uses that instead of the pip installed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951:499,install,installed,499,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454465951,1,['install'],['installed']
Deployability,"We're several months away from Python 3.10. > On Apr 05, 2024 drop support for Python 3.9 (initially released on Oct 05, 2020). https://numpy.org/neps/nep-0029-deprecation_policy.html",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13852#issuecomment-1769474476:101,release,released,101,https://hail.is,https://github.com/hail-is/hail/pull/13852#issuecomment-1769474476,1,['release'],['released']
Deployability,"We've hit issues like this on some installations of Python, but I still have no idea what is the root cause inside Python. It looks like sometimes the submodule `bar` needs to be in the `__all__` of `foo` to import like as `import foo.bar.baz as bazz`, and sometimes not.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6762#issuecomment-516427230:35,install,installations,35,https://hail.is,https://github.com/hail-is/hail/issues/6762#issuecomment-516427230,1,['install'],['installations']
Deployability,"We've recently updated from 0.2.126ish to 0.2.130ish, and encountered some teething issues with the new (to us) metadata server. Jobs using `gsutil` failed as their attempts to get credentials from the server resulted in 404. There seems to have been two problems:. 1. As shown (also via `curl`) in [batch 454410](https://batch.hail.populationgenomics.org.au/batches/454410/jobs/1), our `gsutil` queried for `http://169.254.169.254/computeMetadata/v1/instance/service-accounts` (without a final `/`) which resulted in a 404. I don't know if there's a more elegant way for the server to accept both, rather than just adding a route with and without. 2. With that fixed, [batch 454418](https://batch.hail.populationgenomics.org.au/batches/454418/jobs/1) shows a failure within `GetInstanceScopes()`. This is failing because the metadata server does not implement the `…/scopes` endpoint. PR #14019 implemented only so much as is needed for `hail` and `gcloud` to get access tokens for hail GSAs so they can then make API calls to GCS or Hail Batch, but we seem to have needed a bit more. Not sure why you didn't encounter this yourselves: possibly sufficiently different versions of `gsutil` or the cloud SDK, or perhaps you are better at remembering to use `gcloud` rather than `gsutil` than we are!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331:15,update,updated,15,https://hail.is,https://github.com/hail-is/hail/pull/14566#issuecomment-2132539331,1,['update'],['updated']
Deployability,We've seen this error before on other deployments with no easy fix. I'll continue to investigate over the next couple days and get back to you.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1003#issuecomment-256194596:38,deploy,deployments,38,https://hail.is,https://github.com/hail-is/hail/issues/1003#issuecomment-256194596,1,['deploy'],['deployments']
Deployability,We've switched to Jenkins. Required tools are installed there.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/106#issuecomment-240001202:46,install,installed,46,https://hail.is,https://github.com/hail-is/hail/issues/106#issuecomment-240001202,1,['install'],['installed']
Deployability,"Welcome @ryerobinson, and thanks for the Pull Request! Could you report exactly the error that you saw when building hail? `sys_platform!='win32'` is necessary to install the dependencies on Windows, so there is probably an alternative solution to the error that works for all platforms.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1230330783:163,install,install,163,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1230330783,1,['install'],['install']
Deployability,"Well yes. What I mean is in an automated fashion. We haven't deployed any builds in around a day because of this error, the deploy job keeps restarting and it was very difficult for me to interrogate what was going on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816:61,deploy,deployed,61,https://hail.is,https://github.com/hail-is/hail/pull/5489#issuecomment-468434816,2,['deploy'],"['deploy', 'deployed']"
Deployability,"Well, I look forward to your PR fixing this. I tested the fix with dev deploy before and after my change and it fixed it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9571#issuecomment-705059061:71,deploy,deploy,71,https://hail.is,https://github.com/hail-is/hail/pull/9571#issuecomment-705059061,1,['deploy'],['deploy']
Deployability,"Well, I promise you it looks fine. We should figure out letting other devs log into dev deployed auth.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8805#issuecomment-629409014:88,deploy,deployed,88,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629409014,1,['deploy'],['deployed']
Deployability,Wenhan is giving this a spin on her pipeline.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14232#issuecomment-1922105974:36,pipeline,pipeline,36,https://hail.is,https://github.com/hail-is/hail/pull/14232#issuecomment-1922105974,1,['pipeline'],['pipeline']
Deployability,What do you think about a solution where we only upload/send the last 50MB of the log file and then worry about a nicer interface later on? This will also patch up a liability we've had where someone could write a huge log file that we'd then have to pay for.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12852#issuecomment-1570828503:155,patch,patch,155,https://hail.is,https://github.com/hail-is/hail/issues/12852#issuecomment-1570828503,1,['patch'],['patch']
Deployability,What does full-deploy mean? Can we not make this change online anymore?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12117#issuecomment-1245516418:15,deploy,deploy,15,https://hail.is,https://github.com/hail-is/hail/pull/12117#issuecomment-1245516418,1,['deploy'],['deploy']
Deployability,What were the other config changes you had to make to Auth to get it to deploy? I didn't see any here.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11005#issuecomment-949802668:72,deploy,deploy,72,https://hail.is,https://github.com/hail-is/hail/pull/11005#issuecomment-949802668,1,['deploy'],['deploy']
Deployability,Whatever is failing here is likely different from the interval pipeline failures seen in https://github.com/hail-is/hail/issues/13748 and related tickets because GVS team has confirmed that 0.2.126 reduces peak RAM usage from >50GB to 11GB.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1791064886:63,pipeline,pipeline,63,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1791064886,1,['pipeline'],['pipeline']
Deployability,"When I check my spark configuration, it appears that:; My spark version is version 1.6.0; Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67). scala> System.getProperty(""java.version""); res0: String = 1.7.0_67. scala> val rdd = sc.parallelize(0 to 1000, 4); scala> rdd.mapPartitions { it => Iterator(System.getProperty(""java.version"")) }.collect(); res1: Array[String] = Array(1.7.0_67, 1.7.0_67, 1.7.0_67, 1.7.0_67). It seems I should also update the java relate to the Spark cluster, Thank you !",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/825#issuecomment-248826154:22,configurat,configuration,22,https://hail.is,https://github.com/hail-is/hail/issues/825#issuecomment-248826154,2,"['configurat', 'update']","['configuration', 'update']"
Deployability,"When I try applying this patch:. ```diff; diff --git a/batch/batch/worker/worker.py b/batch/batch/worker/worker.py; index 9d3d89498b..a9106b7a60 100644; --- a/batch/batch/worker/worker.py; +++ b/batch/batch/worker/worker.py; @@ -1228,22 +1228,28 @@ class Container:; return config; ; async def _get_in_container_user(self) -> Tuple[int, int]:; + # https://docs.docker.com/engine/reference/builder/#user; assert self.image.image_config; user = self.image.image_config['Config']['User']; if not user:; return 0, 0; if "":"" in user:; - uid, gid = user.split("":""); - else:; - uid, gid = await self._read_user_from_rootfs(user); - return int(uid), int(gid); + user, group = user.split("":""); + try:; + return int(user), int(group); + except ValueError:; + return await self._read_uid_gid_from_rootfs(user); + try:; + return int(user), 0; + except ValueError:; + return await self._read_uid_gid_from_rootfs(user); ; - async def _read_user_from_rootfs(self, user) -> Tuple[str, str]:; + async def _read_uid_gid_from_rootfs(self, user: str) -> Tuple[int, int]:; with open(f'{self.image.rootfs_path}/etc/passwd', 'r', encoding='utf-8') as passwd:; for record in passwd:; if record.startswith(user):; _, _, uid, gid, _, _, _ = record.split("":""); - return uid, gid; + return int(uid), int(gid); raise ValueError(""Container user not found in image's /etc/passwd""); ; def _mounts(self, uid: int, gid: int) -> List[MountSpecification]:; ```. I can run the container successfully but it fails with the following error:. ```; mkdir: cannot create directory '/io/batch': Permission denied; ```. Which is caused by the following line inserted by the `hailtop.batch` client library:. ```; mkdir -p /io/batch/4c8107/NjztN; ```. so images with non-existent users would fail to run on batch regardless of whether we were using crun / docker / podman, because Batch assumes that the user in the container will have permission to write to `/io`, regardless of whether the job requires input/output files.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13679#issuecomment-1728424448:25,patch,patch,25,https://hail.is,https://github.com/hail-is/hail/issues/13679#issuecomment-1728424448,3,"['a/b', 'patch']","['a/batch', 'patch']"
Deployability,"When I updated the interval_list to this format: `1:1-10000`, it worked.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/319#issuecomment-212160076:7,update,updated,7,https://hail.is,https://github.com/hail-is/hail/issues/319#issuecomment-212160076,1,['update'],['updated']
Deployability,"When TeamCity is using SSL, we can enable GitHub to push notify TeamCity of commits. We'll need to create a TeamCity user for github and install the credentials in the TeamCity Service hook for the broadinstitute/hail project.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-242212918:137,install,install,137,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-242212918,1,['install'],['install']
Deployability,"When https://github.com/ijl/orjson/pull/457 merges and is released, we can update to that latest version of orjson. Do not update before that happens.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14299#issuecomment-1957208462:58,release,released,58,https://hail.is,https://github.com/hail-is/hail/issues/14299#issuecomment-1957208462,3,"['release', 'update']","['released', 'update']"
Deployability,"Where does this command get run? We need to make sure that crcmod is installed for the version of python that is running the gsutil command, not the version that is running batch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529304476:69,install,installed,69,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529304476,1,['install'],['installed']
Deployability,"While we're making breaking changes to the `hailctl` interface... there is a beta feature to start/stop Dataproc clusters. Once this is released, it could create some confusion that `hailctl dataproc start` runs `gcloud dataproc clusters create` instead of `gcloud dataproc clusters start`. Likewise for `hailctl dataproc stop` and `gcloud dataproc clusters delete`. Should we rename `hailctl dataproc` start/stop to create/delete?. https://cloud.google.com/dataproc/docs/guides/dataproc-start-stop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070:136,release,released,136,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767171070,1,['release'],['released']
Deployability,"Whoops, sorry for not getting back to this! Basically, I'd like to see a test of just the heap structure exercised in a large number of ways (basically doing a compare + insert in all sorts of different configurations). You're kind of doing this with the large TakeBy test, but I'd prefer to see a simpler test with many, many more inserts being done and tested for correctness. This is basically to flush out any edge cases that you wouldn't be hitting with a more basic/structured test that wouldn't reach the correct internal state to trigger it. . I've found that creating a simple test structure that mimics the desired end result and using the random generator to generate comparison tests (with the count set pretty high) is generally a pretty good way of sanity checking and flushing these bugs out, rather than writing specific test cases---usually if I've written a test for a specific case, I won't have missed it when coding, and if I've missed an edge case when coding, I won't think to test it. I think I'm pushing on this extra hard because the generated code is at a level of complexity where I can look at it and say ""yeah, this looks generally right"" but I'm not sure that I trust myself, as the reviewer, to guarantee that it's accounted for every single insert configuration correctly, if that makes sense.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530:203,configurat,configurations,203,https://hail.is,https://github.com/hail-is/hail/pull/6942#issuecomment-530961530,2,['configurat'],"['configuration', 'configurations']"
Deployability,Will be live when we deploy the docs for the next version!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8004#issuecomment-580325987:21,deploy,deploy,21,https://hail.is,https://github.com/hail-is/hail/issues/8004#issuecomment-580325987,1,['deploy'],['deploy']
Deployability,Will integrate with new build system and re-pull.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5906#issuecomment-487407741:5,integrat,integrate,5,https://hail.is,https://github.com/hail-is/hail/pull/5906#issuecomment-487407741,1,['integrat'],['integrate']
Deployability,Will merge when tests pass. Make a post on discourse updates to introduce the new command!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1028#issuecomment-257695252:53,update,updates,53,https://hail.is,https://github.com/hail-is/hail/pull/1028#issuecomment-257695252,1,['update'],['updates']
Deployability,Will open a dev post with updated thoughts when I get to updating my thoughts.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-461585416:26,update,updated,26,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-461585416,1,['update'],['updated']
Deployability,"With @danking 's recent work on our custom CI service, we also need to update the docker image anytime the environment changes. From the repo head:. ```; make prime-the-engines; make push-hail-ci-build-image; ```. This will update the `hail-ci-build-image` file (a pointer to google container registry)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4185#issuecomment-414755012:71,update,update,71,https://hail.is,https://github.com/hail-is/hail/pull/4185#issuecomment-414755012,2,['update'],['update']
Deployability,"With `hailtop` installed, I get:; ```; $ rm -f hail/upload-remote-test-resources && make -C hail upload-remote-test-resources ; make: Entering directory '/home/edmund/.local/src/hail/hail'; # # If hailtop.aiotools.copy gives you trouble:; # gcloud storage cp -r src/test/resources/\* gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/; # gcloud storage cp -r python/hail/docs/data/\* gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/; python3 -m hailtop.aiotools.copy -vvv 'null' '[\; {""from"":""src/test/resources"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/test/resources/""},\; {""from"":""python/hail/docs/data"",""to"":""gs://hail-test-ezlis/edmund/hail-test-resources/doctest/data/""}\; ]' --timeout 600; Traceback (most recent call last):; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 197, in _run_module_as_main; return _run_code(code, main_globals, None,; File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/runpy.py"", line 87, in _run_code; exec(code, run_globals); File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 211, in <module>; asyncio.run(main()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/asyncio/runners.py"", line 44, in run; return loop.run_until_complete(main); File ""uvloop/loop.pyx"", line 1517, in uvloop.loop.Loop.run_until_complete; File ""/home/edmund/.local/src/hail/hail/python/hailtop/aiotools/copy.py"", line 182, in main; files = json.loads(args.files); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/__init__.py"", line 346, in loads; return _default_decoder.decode(s); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/edmund/.pyenv/versions/3.9.17/lib/python3.9/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 2 (char 1); make:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163:15,install,installed,15,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1887744163,1,['install'],['installed']
Deployability,"With regard to #1314, this PR substitutes use of IntIterator in count(genotypes=True), linreg, logreg, lmmreg, computing the normalizations that go into RRM and GRM, and ExportPlink (got rid of ByteArrayBuilder there as well). I'm leaving IBD alone until @johnc1231 has his updates in and we can discuss. And hardcalls has the fakeRef issue to be dealt with later. ToStandardizedIndexRowMatrix is now ToHWENormalizedIndexedRowMatrix, alongside ToNormalizedIndexedRowMatrix and ToNormalizedRowMatrix. These all in turn map over genotype-array generating functions in RegressionUtils. I reimplemented that function for ToHWENormalizedIndexedRowMatrix as RegressionUtils.toHWENormalizedGtArray. The only methods using ToHWENormalizedIndexedRowMatrix are PCA and GRM, which both require split vds. So I've required the same on ToHWENormalizedIndexedRowMatrix for consistency and to take advantage of IntIterator in it's current state. We can circle back to the right more general approach later. I've also changed logreg to mapPartitions and wrote RegressionUtils.mutateLastColumn so logreg no longer constructs a new covariate matrix per variants, but rather mutates the last (gt) column of a shared matrix. This helped drive, for example, the 4x speedup on the score test.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906:274,update,updates,274,https://hail.is,https://github.com/hail-is/hail/pull/1425#issuecomment-285811906,1,['update'],['updates']
Deployability,With this new approach I'm trying to use a PHP script to fetch the metadata from SQLite and pass it off as a JSON file to the client to build the documentation web page. It looks like the script isn't running properly on the CI server -- I guess maybe there is some configuration to be done to allow PHP scripts to run on the web server?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1936#issuecomment-311068359:266,configurat,configuration,266,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-311068359,1,['configurat'],['configuration']
Deployability,"Working on it. Gradle/Scala tests pass. Python (Python 3.6.7) also probably pass, I encountered a missing R library error on 501, will re-run in the morning with it installed. Need to better understand the context of the changes, and whether any additional tests needed to cover them. edit: For instance, the serializers don't have tests, but it may not matter if they don't introduce public functionality (beyond that consumed by tested functions).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536:165,install,installed,165,https://hail.is,https://github.com/hail-is/hail/pull/5138#issuecomment-456288536,1,['install'],['installed']
Deployability,"Worth noting that this is also going to update JDK from 8 -> 11. That should be fine, as far as I can tell hail query works with both now, but we don't run hail tests with JDK 11, we do so with JDK 8. . Probably we should take some time to drop support for Spark 2, Python 3.6, and Java 8.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11046#issuecomment-965346335:40,update,update,40,https://hail.is,https://github.com/hail-is/hail/pull/11046#issuecomment-965346335,1,['update'],['update']
Deployability,Would moving back to Spark 2.2 fix this issue? Or should I just wait for a new release with the fix?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5744#issuecomment-480405929:79,release,release,79,https://hail.is,https://github.com/hail-is/hail/issues/5744#issuecomment-480405929,1,['release'],['release']
Deployability,Wow this is supremely annoying. https://github.com/jupyter/notebook/issues/3397 Jupyter is basically known to be broken for the normal use case of most Asyncio libraries. The recommended fix is to use a third party monkey patch. I'll revisit this if 1kg download continue to be a frequent issue.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8218#issuecomment-599765193:222,patch,patch,222,https://hail.is,https://github.com/hail-is/hail/pull/8218#issuecomment-599765193,1,['patch'],['patch']
Deployability,"Wow, talk about a *tour de force* of debugging, well done!!. ---. OK, so this kinda makes sense. We are importing our own copies of the GCS libraries and renaming them all to `is.hail.relocated....`. We do this so that we're not stuck with whatever version Dataproc is including. We pin our dataproc image version to `2.1.2-debian11` (see [here](https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/start.py#L147)) which [was released in January 2023](https://cloud.google.com/dataproc/docs/release-notes#January_23_2023). The latest available version of [Dataproc's Debian images](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) is 2.1.25-debian11 which depends on GoogleCloudDataproc hadoop connector version [2.2.15](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/2.2.15) which relies on Google Cloud Storage client library version [2.22.3](https://github.com/GoogleCloudDataproc/hadoop-connectors/commit/8b79f025ef5e8231de827f4c620cd23e230c3489). I have [a PR](https://github.com/hail-is/hail/pull/13732) to upgrade us to 2.27.1 because the library broke retries in versions [2.25.0, 2.27.0). AFAICT, Google's image version page only shows the most recent five. There's no way to go back further in time. Luckily, the way back machine has [a March 2023 capture](https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) which includes our version. 2.1.2-debian11 used Google Cloud Dataproc hadoop connector version [2.2.9](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.9) This version of the hadoop connector was [using some alpha version of a gRPC version of the cloud storage library](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/18f6e9f1c745e1854d76bea9362e2332898d8895/pom.xml#L96C1-L97C1). I'm not sure what's up with that. OK, here's my proposal: let's change that IMAGE_VERSION to the latest one ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645:452,release,released,452,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645,4,['release'],"['release-', 'release-notes', 'released', 'releases']"
Deployability,"Ya nearly, this is a subset of the changes that I made and deployed into a Terra dev environment. So technically I didn't deploy this branch (because things in the batch code base would have broken) but these are all the config/build system changes",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14450#issuecomment-2060017171:59,deploy,deployed,59,https://hail.is,https://github.com/hail-is/hail/pull/14450#issuecomment-2060017171,2,['deploy'],"['deploy', 'deployed']"
Deployability,Ya this was the previous deploy taking forever because of QoB tests restarting after preemption,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12353#issuecomment-1284573461:25,deploy,deploy,25,https://hail.is,https://github.com/hail-is/hail/pull/12353#issuecomment-1284573461,1,['deploy'],['deploy']
Deployability,"Yeah I meant what point in the pipeline. One could check on `j.image` (or `hb.Batch(..., default_image=...)`), but maybe `b.run` is best - is it only triggered once per batch, or once per offending job, or once per offending unique image?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9661#issuecomment-720515903:31,pipeline,pipeline,31,https://hail.is,https://github.com/hail-is/hail/pull/9661#issuecomment-720515903,1,['pipeline'],['pipeline']
Deployability,"Yeah you're right about `pyspark` though, if we `pip install pyspark` then we needn't edit PYTHONPATH or set `SPARK_HOME` afaik.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454466582:53,install,install,53,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454466582,1,['install'],['install']
Deployability,"Yeah, Dan is fixing the underlying CI issue regarding cloudtools update, hopefully it then works",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4347#issuecomment-422132872:65,update,update,65,https://hail.is,https://github.com/hail-is/hail/pull/4347#issuecomment-422132872,1,['update'],['update']
Deployability,"Yeah, is this going to deploy new jars only on 2.4 or also 2.2 for a while? This is good in the long run, but might be nice to keep 2.2 going in case there are any unintended consequences",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5756#issuecomment-479597161:23,deploy,deploy,23,https://hail.is,https://github.com/hail-is/hail/pull/5756#issuecomment-479597161,1,['deploy'],['deploy']
Deployability,"Yeah, let's make that a separate thing, I want to like SSH into other nodes and verify it's doing what I thought, and I was also reluctant to add a new option to the HailContext right before we released 0.2.32 in case it slid in first",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583572766:194,release,released,194,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583572766,1,['release'],['released']
Deployability,"Yeah, so, this is my bad. The dynamic-base is used at doc-generation-time. Changes to that are not reflected until the docs are regenerated (i.e. a new PyPI version of Hail is released). The short-term fix is to release Hail. The long-term fix is to not make changes to dynamic-base.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10277#issuecomment-813617813:176,release,released,176,https://hail.is,https://github.com/hail-is/hail/pull/10277#issuecomment-813617813,2,['release'],"['release', 'released']"
Deployability,"Yeah, that error indicates that those are old format VDS's, so Hail won't load them unless someone with write access to hail-common uses the ""write_partioning"" method to update them. I'll handle that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615:170,update,update,170,https://hail.is,https://github.com/hail-is/hail/issues/1683#issuecomment-295745615,1,['update'],['update']
Deployability,"Yeah, that's true. I've updated this PR to include the default gce deploy config for the default namespace, which I think is generally what you'd want for both testing and in production; I'm not sure under what circumstances you'd want separate service namespaces?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476:24,update,updated,24,https://hail.is,https://github.com/hail-is/hail/pull/9437#issuecomment-691245476,2,"['deploy', 'update']","['deploy', 'updated']"
Deployability,"Yeah, this was broken in a recent update to the gcloud libraries. For now, following the command to do:; ```; gcloud config set dataproc/region VALUE; ```; will generate the best user experience. We'll fix this for the next version though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077:34,update,update,34,https://hail.is,https://github.com/hail-is/hail/issues/8078#issuecomment-584868077,1,['update'],['update']
Deployability,"Yeah. As it stands, PCRelate isn't installable without some effort on modern systems. IIRC, I enabled some by-default-disabled repositories on the cloud machines and then installed the netCDF package. I think the package is named `ncdf`. After this, you should be able to install the packages using `biocLite` as above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377930321:35,install,installable,35,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377930321,3,['install'],"['install', 'installable', 'installed']"
Deployability,"Yeah. I think the main issue is that we need to run update-alternatives after we install a new version of Python. If query-build is just for native compiler stuff, then I'm fine with using 3.8 there. I'll see if I can get back to this PR later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12526#issuecomment-1355202497:52,update,update-alternatives,52,https://hail.is,https://github.com/hail-is/hail/pull/12526#issuecomment-1355202497,2,"['install', 'update']","['install', 'update-alternatives']"
Deployability,"Yep, I bumped into the Breeze bug a while back while trying to upgrade Hail to Spark 3 internally, and realized it'd be a blocker downstream. I've only seen issues on the Python side; for example:; ```; ______________________________________________________________________________ Tests.test_block_matrix_entries ______________________________________________________________________________. self = <test.hail.linalg.test_linalg.Tests testMethod=test_block_matrix_entries>. @fails_local_backend(); def test_block_matrix_entries(self):; n_rows, n_cols = 5, 3; rows = [{'i': i, 'j': j, 'entry': float(i + j)} for i in range(n_rows) for j in range(n_cols)]; schema = hl.tstruct(i=hl.tint32, j=hl.tint32, entry=hl.tfloat64); table = hl.Table.parallelize([hl.struct(i=row['i'], j=row['j'], entry=row['entry']) for row in rows], schema); table = table.annotate(i=hl.int64(table.i),; j=hl.int64(table.j)).key_by('i', 'j'); ; ndarray = np.reshape(list(map(lambda row: row['entry'], rows)), (n_rows, n_cols)); ; for block_size in [1, 2, 1024]:; block_matrix = BlockMatrix.from_numpy(ndarray, block_size); entries_table = block_matrix.entries(); self.assertEqual(entries_table.count(), n_cols * n_rows); self.assertEqual(len(entries_table.row), 3); > self.assertTrue(table._same(entries_table)); E AssertionError: False is not true. test/hail/linalg/test_linalg.py:868: AssertionError; ----------------------------------------------------------------------------------- Captured stdout call ------------------------------------------------------------------------------------; Table._same: rows differ:; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=1.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=3.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(entry=2.0)]; R: [Struct(entry=0.0)]; Row mismatch:; L: [Struct(e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406:63,upgrade,upgrade,63,https://hail.is,https://github.com/hail-is/hail/pull/9524#issuecomment-701539406,1,['upgrade'],['upgrade']
Deployability,"Yep, it's exactly the lack of filtering based on the exact interval. I patched htsjdk to not do the `getIntv` check and got identical results. To what I see for some of the queries of my reader.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4824#issuecomment-442568586:71,patch,patched,71,https://hail.is,https://github.com/hail-is/hail/pull/4824#issuecomment-442568586,1,['patch'],['patched']
Deployability,"Yep, just waiting on final (unrelated) fixes to go through on the v3.1.1; nuclear variant release files before pinging them about copying over all; our changed files. On Wed, Mar 10, 2021 at 3:29 PM Dan King ***@***.***> wrote:. > I'll wait for Grace to chime in about the desired public location.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/hail-is/hail/pull/10169#issuecomment-796049879>, or; > unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABNSGHRVFLHDPK26N3MKTXLTC7CBTANCNFSM4Y42HNKA>; > .; >",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10169#issuecomment-797031640:90,release,release,90,https://hail.is,https://github.com/hail-is/hail/pull/10169#issuecomment-797031640,1,['release'],['release']
Deployability,"Yep, sigh. And I don't want to specify these args on the master node since that would break workflows that do use LMMs and do svd on master node. So I guess I'll add them specifically for the hail in pipeline docker based benchmarking system since we know that's running in local mode and actually want that to have only one core. Given that, should I still leave this executor arguments in place or pull them back out?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583459034:200,pipeline,pipeline,200,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583459034,1,['pipeline'],['pipeline']
Deployability,Yes I'm good with this. I just wanted the pipeline changes to be done separately.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5844#issuecomment-482728341:42,pipeline,pipeline,42,https://hail.is,https://github.com/hail-is/hail/pull/5844#issuecomment-482728341,1,['pipeline'],['pipeline']
Deployability,Yes we could do that. I added the WIP tag. Sometimes PR testing is easier than dev deploy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8552#issuecomment-613658889:83,deploy,deploy,83,https://hail.is,https://github.com/hail-is/hail/pull/8552#issuecomment-613658889,1,['deploy'],['deploy']
Deployability,"Yes! I had a slight scare with foreign key/prefix joins, but it looks good now! All of our pipelines use tables and it appears to be on par with MT joins",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4026#issuecomment-420021337:91,pipeline,pipelines,91,https://hail.is,https://github.com/hail-is/hail/issues/4026#issuecomment-420021337,1,['pipeline'],['pipelines']
Deployability,"Yes! I want `make deploy` to always mean ""`kubectl apply` this service's kubernetes configuration"" and/or ""push to appropriate public repository"" (c.f. hail's python lib). Cotton can comment more directly on lets encrypt, but there's an issue wrt sharing a volume between two pods that isn't easily resolved. I'm not exactly sure how `make run` is intended to be used.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158:18,deploy,deploy,18,https://hail.is,https://github.com/hail-is/hail/issues/5413#issuecomment-467103158,2,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,"Yes, CI generates the config for gateway which needs the root domain name tied to the cluster, e.g. `hail.is` or `azure.hail.is`. But if it uses the environment variable name `HAIL_DOMAIN`, it will clobber the deploy config field `domain` in test namespaces which should be `internal.hail.is` not `hail.is`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14164#issuecomment-1898947090:210,deploy,deploy,210,https://hail.is,https://github.com/hail-is/hail/pull/14164#issuecomment-1898947090,1,['deploy'],['deploy']
Deployability,"Yes, I receive the same bug today, when I follow the instruction: https://hail.is/docs/0.2/install/other-cluster.html ; I believe this is a bug caused by pip or setuptools. If you downgrade the pip to 8.1.1, you will get another bug. . > cd build/deploy; python3 setup.py -q sdist bdist_wheel; sed '/^pyspark/d' python/requirements.txt | xargs python3 -m pip install -U; Exception:; Traceback (most recent call last):; File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/basecommand.py"", line 209, in main; status = self.run(options, args); File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/commands/install.py"", line 287, in run; wheel_cache; File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/basecommand.py"", line 270, in populate_requirement_set; wheel_cache=wheel_cache; File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/req/req_install.py"", line 230, in from_line; wheel_cache=wheel_cache, constraint=constraint); File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/req/req_install.py"", line 77, in __init__; req = pkg_resources.Requirement.parse(req); File ""/home/ubuntu/.local/lib/python3.6/site-packages/pip/_vendor/pkg_resources/__init__.py"", line 3036, in parse; req, = parse_requirements(s); ValueError: not enough values to unpack (expected 1, got 0)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10352#issuecomment-826316427:91,install,install,91,https://hail.is,https://github.com/hail-is/hail/issues/10352#issuecomment-826316427,4,"['deploy', 'install']","['deploy', 'install']"
Deployability,"Yes, all of that (export changes, transmute, and updated select) was part of my plan. I was trying to keep the PRs short. This one is already 250 lines. If the issue is the method name `select_fields`, then I can change that now.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2225#issuecomment-331238111:49,update,updated,49,https://hail.is,https://github.com/hail-is/hail/pull/2225#issuecomment-331238111,1,['update'],['updated']
Deployability,"Yes, for example:. ```; $ gsutil ls gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/index.html; gs://hail-ci-0-1/deploy/3b20406ba582d5aebac0c71b6b41f3509e2e1887/job.log; ```. where the hash is from master.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859:53,deploy,deploy,53,https://hail.is,https://github.com/hail-is/hail/issues/4437#issuecomment-438146859,3,['deploy'],['deploy']
Deployability,"Yes, it was the two extraClassPaths that got it running. It would make sense that the --jars parameter also update the paths.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3342#issuecomment-380560144:108,update,update,108,https://hail.is,https://github.com/hail-is/hail/issues/3342#issuecomment-380560144,1,['update'],['update']
Deployability,"Yes, it would. Slightly unfair test in my case because I was installing a dev build in preparation for the workshop Cotton is giving tomorrow, but thought I should report it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2978#issuecomment-370499062:61,install,installing,61,https://hail.is,https://github.com/hail-is/hail/issues/2978#issuecomment-370499062,1,['install'],['installing']
Deployability,"Yes, will fix. I'll do a build tomorrow from scratch on a VM and make sure docs are up to date. Did you then ultimately get your build to work after installing those things?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10747#issuecomment-893937331:149,install,installing,149,https://hail.is,https://github.com/hail-is/hail/issues/10747#issuecomment-893937331,1,['install'],['installing']
Deployability,"You can run into issues if you modify/upgrade python packages in the same jupyter notebook session, so be sure you close and reopen your notebooks after any `pip install` command. Maybe that was the cause?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13230#issuecomment-1630918748:38,upgrade,upgrade,38,https://hail.is,https://github.com/hail-is/hail/issues/13230#issuecomment-1630918748,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"You installed Hail using pip, right? You're running locally?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-500772207:4,install,installed,4,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-500772207,1,['install'],['installed']
Deployability,You might be thinking of `hail-ci-deploy-hail-is-hail`? This is the secret mounted by the CI when running a deploy job for `github.com/hail-is/hail`. There is also `ci-deploy-0-1--nealelab-cloudtools`. These are in the `_deploy_secrets` map of `prs.py`. The CI itself does not care what is in these secrets beyond their existence. I'll add a commit documenting the secrets of hail more broadly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4552#issuecomment-430393417:34,deploy,deploy-hail-is-hail,34,https://hail.is,https://github.com/hail-is/hail/pull/4552#issuecomment-430393417,3,['deploy'],"['deploy', 'deploy-', 'deploy-hail-is-hail']"
Deployability,You should be able to use the test/deploy script from internal-resources: https://github.com/hail-is/internal-resources/blob/master/gcp_build.sh.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/744#issuecomment-248415882:35,deploy,deploy,35,https://hail.is,https://github.com/hail-is/hail/issues/744#issuecomment-248415882,1,['deploy'],['deploy']
Deployability,"You should benchmark linear search vs priority queue in the merge itself. You can either implement both and compare on a representative benchmark in context, or code up the merge code itself into a targeted benchmark, something along the lines of:. Generate 10 million (or more, large enough to get a stable measurement) random input integers `Array[A]` in a wrapper class `class A(i: Int)` to simulate `RegionValue`. We're aiming to merge N = ~100 elements, so put 100 into either an array or priority heap, pull out the smallest values, and update them with new input values until the input values and array are both exhausted. Test with N = 10, 100, 1000. If you do the latter, post the benchmark. I think we'll need to re-visit this trade-off again when we switch to C++.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101:543,update,update,543,https://hail.is,https://github.com/hail-is/hail/pull/4396#issuecomment-428335101,1,['update'],['update']
Deployability,You want me to set up the deploy logic to match batch and the other services?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4446#issuecomment-424728688:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/pull/4446#issuecomment-424728688,1,['deploy'],['deploy']
Deployability,"Your previous post includes a warning message that your `SPARK_HOME`, in that shell, is set to:; ```; /opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2; ```; Could you try setting `SPARK_HOME` in the configuration file to `/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2` and then try again to create a `HailContext`?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337610577:228,configurat,configuration,228,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337610577,1,['configurat'],['configuration']
Deployability,"Yup! I got a better understanding why PRs stuck on merge conflicts keep getting updated all of the time in my own tinkering, but I want to be sure of the general case before following up and this will be a useful metric.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10723#issuecomment-893589750:80,update,updated,80,https://hail.is,https://github.com/hail-is/hail/pull/10723#issuecomment-893589750,1,['update'],['updated']
Deployability,Yup! Mitja and I have been talking and sharing code on this issue. But clearly there is some work to be done for this functionality to be integrated naturally in Hail. And obviously the phasing stuff would be neat :),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/104#issuecomment-279858875:138,integrat,integrated,138,https://hail.is,https://github.com/hail-is/hail/issues/104#issuecomment-279858875,1,['integrat'],['integrated']
Deployability,"[First successful master build with this configuration](https://ci.hail.is/viewLog.html?buildId=493&buildTypeId=HailSourceCode_HailCi&tab=buildLog&consoleStyle=false#_state=116,125&focus=123). PR builds will also trigger the three compiles before the `clean test createDocs`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/742#issuecomment-245038405:41,configurat,configuration,41,https://hail.is,https://github.com/hail-is/hail/issues/742#issuecomment-245038405,1,['configurat'],['configuration']
Deployability,"[Here](https://github.com/hail-is/hail/blob/62f606c5dd2f013ba7b43049f415ac0914bd6cf9/batch/batch/worker/worker.py#L1539-L1544) is where we specify the mount options for the `/io` mount, and [here](https://github.com/hail-is/hail/blob/62f606c5dd2f013ba7b43049f415ac0914bd6cf9/batch/batch/worker/worker.py#L1103) is the bulk of the container config and a link to the container runtime specification. For the most part the mount configuration maps to Linux mount options. I suspect we need to provide a mapping so that the `io` directory in the container is owned by the appropriate user and not just root.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13154#issuecomment-1588391511:426,configurat,configuration,426,https://hail.is,https://github.com/hail-is/hail/issues/13154#issuecomment-1588391511,1,['configurat'],['configuration']
Deployability,[SciPy >=1.11.0 drops support for Python 3.8](https://docs.scipy.org/doc/scipy/release/1.11.0-notes.html); > This release requires Python 3.9+ and NumPy 1.21.6 or greater.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13259#issuecomment-1644384753:79,release,release,79,https://hail.is,https://github.com/hail-is/hail/pull/13259#issuecomment-1644384753,2,['release'],['release']
Deployability,"[Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeExcep",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6395,install,install,6395,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"[This ](; https://github.com/broadinstitute/gnomad_methods/blob/e2142ee69da38bb5ec89513121329d28ba9262d8/gnomad/utils/constraint.py#L27) seems to be the point in the pipeline where the join occurs. ```python; def annotate_with_mu(; ht: hl.Table,; mutation_ht: hl.Table,; mu_annotation: str = ""mu_snp"",; ) -> hl.Table:; """"""; Annotate SNP mutation rate for the input Table. .. note::. Function expects that`ht` includes`mutation_ht`'s key fields. Note that these; annotations don't need to be the keys of `ht`. :param ht: Input Table to annotate.; :param mutation_ht: Mutation rate Table.; :param mu_annotation: The name of mutation rate annotation in `mutation_ht`.; Default is 'mu_snp'.; :return: Table with mutational rate annotation added.; """"""; mu = mutation_ht.index(*[ht[k] for k in mutation_ht.key])[mu_annotation]; return ht.annotate(; **{mu_annotation: hl.case().when(hl.is_defined(mu), mu).or_error(""Missing mu"")}; ); ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13486#issuecomment-1692614127:166,pipeline,pipeline,166,https://hail.is,https://github.com/hail-is/hail/issues/13486#issuecomment-1692614127,1,['pipeline'],['pipeline']
Deployability,"_From @alexb-3 on September 30, 2015 22:4_. pro-tip: To add references, first open `bibfile.bib` in BibDesk (comes with standard MacTeX installation). Then find your paper in Google Scholar, click the `cite` button, select `BibTeX` and paste into BibDesk. You may need to reformat slightly; I remove irrelevant fields, and proper names in the title requiring capital letters should go in braces, e.g. {Hardy}--{W}einberg.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/47#issuecomment-152273044:136,install,installation,136,https://hail.is,https://github.com/hail-is/hail/issues/47#issuecomment-152273044,1,['install'],['installation']
Deployability,"_From @jbloom22 on October 28, 2015 21:2_. Hail runtime for linreg with 10 PCs on profile.vds: 56s, 59s, 58s.; Hail runtime for variantqc on profile.vds: 35s, 34s, 35s.; Plink runtime for linreg with 10 PCs on profile.vds: 13s, 13s, 13s.; Hail runtime (8 cores) for linreg with 10 PCs on profile.vds: 23s, 25s, 23s. LINREG:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds linreg -f ~/data/profile.fam -c ~/data/profile.cov -o ~/data/profile.linreg. read: 1407.415486; linreg: 58336.701622. VARIANTQC:; /Users/jbloom/k3/build/install/k3/bin/k3 read -i ~/data/profile.vds variantqc -o ~/data/profile.variantqc. read: 1417.763771; variantqc: 35466.355219. PLINK:; create bed/bim/fam:; ./plink --vcf ~/data/profile.vcf.bgz. run regression:; time ./plink --bfile plink --double-id --pheno ~/data/profile.pheno; --allow-no-sex --covar ~/data/profile.covar --linear --out; ~/data/plinkTest. PLINK v1.90b3w 64-bit (3 Sep 2015) https://www.cog-genomics.org/plink2; (C) 2005-2015 Shaun Purcell, Christopher Chang GNU General Public License v3; Logging to /Users/Jon/data/plinkTest.log.; Options in effect:; --allow-no-sex; --bfile plink; --covar /Users/Jon/data/profile.covar; --double-id; --linear; --out /Users/Jon/data/plinkTest; --pheno /Users/Jon/data/profile.pheno; 16384 MB RAM detected; reserving 8192 MB for main workspace.; 24885 variants loaded from .bim file.; 2535 people (0 males, 0 females, 2535 ambiguous) loaded from .fam.; Ambiguous sex IDs written to /Users/Jon/data/plinkTest.nosex .; 2535 phenotype values present after --pheno.; Using 1 thread.; Warning: This run includes BLAS/LAPACK linear algebra operations which; currently disregard the --threads limit. If this is problematic, you; may want to recompile against single-threaded BLAS/LAPACK.; --covar: 10 covariates loaded.; Before main variant filters, 2535 founders and 0 nonfounders present.; Calculating allele frequencies... done.; Total genotyping rate is 0.907692.; 24885 variants and 2535 people pass filt",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/50#issuecomment-152273684:348,install,install,348,https://hail.is,https://github.com/hail-is/hail/issues/50#issuecomment-152273684,2,['install'],['install']
Deployability,"___/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.0.2; /_/. Using Python version 3.5.2 (default, Jul 12 2017 14:00:23); SparkSession available as 'spark'. In [1]: from hail import *; ---------------------------------------------------------------------------; ImportError Traceback (most recent call last); <ipython-input-1-3181c5d8fca5> in <module>(); ----> 1 from hail import *. /opt/Software/hail/python/hail/__init__.py in <module>(); ----> 1 import hail.expr; 2 from hail.representation import *; 3 from hail.context import HailContext; 4 from hail.dataset import VariantDataset; 5 from hail.expr import *. /opt/Software/hail/python/hail/expr.py in <module>(); 1 import abc; 2 from hail.java import scala_object, Env, jset; ----> 3 from hail.representation import Variant, AltAllele, Genotype, Locus, Interval, Struct, Call; 4 ; 5 . /opt/Software/hail/python/hail/representation/__init__.py in <module>(); ----> 1 from hail.representation.variant import Variant, Locus, AltAllele; 2 from hail.representation.interval import Interval; 3 from hail.representation.genotype import Genotype, Call; 4 from hail.representation.annotations import Struct; 5 from hail.representation.pedigree import Trio, Pedigree. /opt/Software/hail/python/hail/representation/variant.py in <module>(); 1 from hail.java import scala_object, Env, handle_py4j; ----> 2 from hail.typecheck import *; 3 ; 4 class Variant(object):; 5 """""". /opt/Software/hail/python/hail/typecheck/__init__.py in <module>(); ----> 1 from check import *; 2 ; 3 __all__ = ['typecheck',; 4 'typecheck_method',; 5 'none',. ImportError: No module named 'check'. In [2]: hc = HailContext(sc); ---------------------------------------------------------------------------; NameError Traceback (most recent call last); <ipython-input-2-2e980fcce31d> in <module>(); ----> 1 hc = HailContext(sc). NameError: name 'HailContext' is not defined. In [3]: ; ```; There are still some errors, is there something wrong with my configurations?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609:3980,configurat,configurations,3980,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-321152609,1,['configurat'],['configurations']
Deployability,"_fetchall; > async for row in tx.execute_and_fetchall(sql, args, query_name):\n File ""/usr/local/lib/python3.9/dist-packages/gear/database.py"", line 257, in execute_and_fetchall; > await cursor.execute(sql, args)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 239, in execute; > await self._query(query)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/cursors.py"", line 457, in _query; > await conn.query(q)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 469, in query; > await self._read_query_result(unbuffered=unbuffered)\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 683, in _read_query_result; > await result.read()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 1164, in read; > first_packet = await self.connection._read_packet()\n File ""/usr/local/lib/python3.9/dist-packages/aiomysql/connection.py"", line 652, in _read_packet; > packet.raise_for_error()\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/protocol.py"", line 219, in raise_for_error; > err.raise_mysql_exception(self._data)\n File ""/usr/local/lib/python3.9/dist-packages/pymysql/err.py"", line 150, in raise_mysql_exception; > raise errorclass(errno, errval); > pymysql.err.OperationalError: (1054, ""Unknown column 'cancelled.id' in 'on clause'""); > ```. This error strikes me as odd because `cancelled.id` has been updated to `cancelled.batch_id` in `delete_prev_cancelled_job_group_cancellable_resources_records`:. [batch/driver/main.py](https://github.com/hail-is/hail/blob/c6e3c660035379e6fe3f96fb4385f8b3c7e8d436/batch/batch/driver/main.py#L1474). Based on the error, it looks like the `main.py` being executed at `/usr/local/lib/python3.9/dist-packages/batch/driver/main.py` is still using the old version of the code, the changes from the PR were not correctly reflected in the environment. Is it possible that we might be missing a `pip install` step to ensure the latest code is deployed?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053:2152,update,updated,2152,https://hail.is,https://github.com/hail-is/hail/pull/14672#issuecomment-2353226053,3,"['deploy', 'install', 'update']","['deployed', 'install', 'updated']"
Deployability,"_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': Non",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2320,pipeline,pipeline,2320,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,```; $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtersamples --remove -c true filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.789s; filtersamples: 48.752ms; filtervariants: 20.286ms; exportvariants: 1m33.0s; ```. Surprised `filtersamples --remove --all` wasn't significantly faster.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/300#issuecomment-210845452:15,install,install,15,https://hail.is,https://github.com/hail-is/hail/pull/300#issuecomment-210845452,1,['install'],['install']
Deployability,```; $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.860s; filtervariants: 47.061ms; exportvariants: 1m32.1s. $ ./build/install/hail/bin/hail --master 'local[1]' read -i ~/profile225.vds filtersamples --remove --all filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.766s; filtersamples: 1.671ms; filtervariants: 39.474ms; exportvariants: 1m31.5s. $ ./build/install/hail/bin/hail --master 'local[1]' read --skip-genotypes -i ~/profile225.vds filtervariants --keep -c 'va.info.AF[0] < 0.01' exportvariants -c 'v' -o variants.tsv; hail: info: timing:; read: 1.844s; filtervariants: 44.972ms; exportvariants: 3.336s; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/300#issuecomment-210843547:15,install,install,15,https://hail.is,https://github.com/hail-is/hail/pull/300#issuecomment-210843547,3,['install'],['install']
Deployability,"```; (py311) jigold@wm349-8c4 hail % gcloud artifacts repositories set-cleanup-policies hail \; --project=hail-vdc \; --location=us \; --policy=/Users/jigold/projects/hail/infra/gcp-broad/gcp-ar-cleanup-policy.txt \; --no-dry-run; WARNING: Python 3.5-3.7 will be deprecated on August 8th, 2023. Please use Python version 3.8 and up. If you have a compatible Python interpreter installed, you can use it by setting; the CLOUDSDK_PYTHON environment variable to point to it. Updated repository [hail].; Dry run is disabled.; ```. I checked the UI and it seems correct now.; <img width=""420"" alt=""Screenshot 2023-11-15 at 7 50 06 AM"" src=""https://github.com/hail-is/hail/assets/1693348/630f0481-c24a-4e41-9350-ef091cc62b1b"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14001#issuecomment-1812482207:377,install,installed,377,https://hail.is,https://github.com/hail-is/hail/pull/14001#issuecomment-1812482207,2,"['Update', 'install']","['Updated', 'installed']"
Deployability,"```; * installing *source* package ‘ncdf4’ ...; ** package ‘ncdf4’ successfully unpacked and MD5 sums checked; configure.ac: starting; checking for nc-config... no; -----------------------------------------------------------------------------------; Error, nc-config not found or not executable. This is a script that comes with the; netcdf library, version 4.1-beta2 or later, and must be present for configuration; to succeed. If you installed the netcdf library (and nc-config) in a standard location, nc-config; should be found automatically. Otherwise, you can specify the full path and name of; the nc-config script by passing the --with-nc-config=/full/path/nc-config argument; flag to the configure script. For example:. ./configure --with-nc-config=/sw/dist/netcdf4/bin/nc-config. Special note for R users:; -------------------------; To pass the configure flag to R, use something like this:. R CMD INSTALL --configure-args=""--with-nc-config=/home/joe/bin/nc-config"" ncdf4. where you should replace /home/joe/bin etc. with the location where you have; installed the nc-config script that came with the netcdf 4 distribution.; -----------------------------------------------------------------------------------; ERROR: configuration failed for package ‘ncdf4’; * removing ‘/usr/local/lib/R/3.3/site-library/ncdf4’; ERROR: dependency ‘ncdf4’ is not available for package ‘GWASTools’; * removing ‘/usr/local/lib/R/3.3/site-library/GWASTools’; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057:7,install,installing,7,https://hail.is,https://github.com/hail-is/hail/issues/3273#issuecomment-377701057,6,"['INSTALL', 'configurat', 'install']","['INSTALL', 'configuration', 'installed', 'installing']"
Deployability,```; Building dependency tree...; Reading state information...; [91mE: Unable to locate package python-setuptools; The command '/bin/sh -c apt-get install gcc python-setuptools && pip uninstall crcmod && pip install --no-cache-dir -U crcmod' returned a non-zero code: 100; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160:148,install,install,148,https://hail.is,https://github.com/hail-is/hail/pull/7024#issuecomment-529650160,2,['install'],['install']
Deployability,"```; tpoterba@tim2-m:~$ sudo `which pip` install /usr/lib/spark/python/; Processing /usr/lib/spark/python; Complete output from command python setup.py egg_info:. If you are installing pyspark from spark source, you must first build Spark and; run sdist. To build Spark with maven you can run:; ./build/mvn -DskipTests clean package; Building the source dist is done in the Python directory:; cd python; python setup.py sdist; pip install dist/*.tar.gz. ----------------------------------------; Command ""python setup.py egg_info"" failed with error code 255 in /tmp/pip-req-build-r4aje_7z/; You are using pip version 10.0.1, however version 19.1.1 is available.; You should consider upgrading via the 'pip install --upgrade pip' . ```. :(",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508:41,install,install,41,https://hail.is,https://github.com/hail-is/hail/pull/6297#issuecomment-500615508,5,"['install', 'upgrade']","['install', 'installing', 'upgrade']"
Deployability,"`batches`, and `jobs`. The currently used version of these tables are `aggregated_*_resources_v2` with the emphasis on ""v2"". We've created new ""v3"" tables already that are almost identical to ""v2"" except we're deduplicating resources with the exact same billing rate to save space in the database. We're currently double writing to both the ""v2"" and ""v3"" tables. When we added the new ""v3"" tables, data from all newly created batches since then have been correctly populating both tables. However, we need to back fill the tables with old records. We've currently finished back-populating 3 out of 4 of the ""v3"" tables. The last one that remains is `aggregated_job_resources_v3`. I estimate the time to back fill this table will be on the order of 12 hours. How the backfilling works is we have a flag / Boolean Column on the equivalent `v2` table that says whether the rows have been migrated to the `v3` table already. There's then a trigger already in place that every time a row in the `v2` table is updated such as `update aggregated_job_resources_v2 set migrated = 1 where ...`, then we check whether the row has already been ""migrated"". If it hasn't then we insert new rows into the ""v3"" table with the deduped resource ID as the key. The exact trigger is . ```sql; DROP TRIGGER IF EXISTS aggregated_job_resources_v2_after_update $$; CREATE TRIGGER aggregated_job_resources_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:1400,update,updated,1400,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,2,['update'],"['update', 'updated']"
Deployability,`brew install netcdf` installed a version of gcc that messed a bunch of stuff up.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3281#issuecomment-377948180:6,install,install,6,https://hail.is,https://github.com/hail-is/hail/pull/3281#issuecomment-377948180,2,['install'],"['install', 'installed']"
Deployability,"`gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2139,update,update-args,2139,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,1,['update'],['update-args']
Deployability,"`hail` was hanging after all commands completed when running kudu commands against the quickstart. From the thread dump, it looked like it was spinning in the kudu client. Shutting down the kudu context seemed to fix the problem. See any problems with this patch? Also, I removed latest. It didn't seem to be used. ```; diff --git a/src/main/scala/org/kududb/spark/KuduContext.scala b/src/main/scala/org/kududb/spark/KuduContext.scala; index c48dcd4..71be7d2 100644; --- a/src/main/scala/org/kududb/spark/KuduContext.scala; +++ b/src/main/scala/org/kududb/spark/KuduContext.scala; @@ -41,8 +41,6 @@ class KuduContext(@transient sc: SparkContext,. val broadcastedKuduMaster = sc.broadcast(kuduMaster). - LatestKuduContextCache.latest = this; -; /**; * A simple enrichment of the traditional Spark RDD foreachPartition.; * This function differs from the original in that it offers the; @@ -169,10 +167,6 @@ class KuduContext(@transient sc: SparkContext,; def fakeClassTag[T]: ClassTag[T] = ClassTag.AnyRef.asInstanceOf[ClassTag[T]]; }. -object LatestKuduContextCache {; - var latest:KuduContext = null; -}; -; object KuduClientCache {; var kuduClient: KuduClient = null; var asyncKuduClient: AsyncKuduClient = null; @@ -195,4 +189,14 @@ object KuduClientCache {; asyncKuduClient; }. + def close() {; + if (kuduClient != null) {; + kuduClient.close(); + kuduClient = null; + }; + if (asyncKuduClient != null) {; + asyncKuduClient.close(); + asyncKuduClient = null; + }; + }; }; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/242#issuecomment-220667612:257,patch,patch,257,https://hail.is,https://github.com/hail-is/hail/pull/242#issuecomment-220667612,1,['patch'],['patch']
Deployability,"`make install-wheel` and then `PYTHONPATH="""" python -c ""import hail; hail.init()""`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6437#issuecomment-504542922:6,install,install-wheel,6,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504542922,1,['install'],['install-wheel']
Deployability,`make install-wheel` changes my local environment?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6437#issuecomment-504548967:6,install,install-wheel,6,https://hail.is,https://github.com/hail-is/hail/pull/6437#issuecomment-504548967,1,['install'],['install-wheel']
Deployability,"`pip install -e .`; Defaulting to user installation because normal site-packages is not writeable; Obtaining file:///home/skr/hail2/hail; Installing build dependencies ... done; Checking if build backend supports build_editable ... done; Getting requirements to build editable ... error; error: subprocess-exited-with-error; ; × Getting requirements to build editable did not run successfully.; │ exit code: 1; ╰─> [14 lines of output]; error: Multiple top-level packages discovered in a flat-layout: ['tls', 'gear', 'hail', 'auth', 'blog', 'infra', 'batch', 'query', 'docker', 'memory', 'devbin', 'gateway', 'website', 'grafana', 'notebook', 'graphics', 'datasets', 'monitoring', 'web_common', 'prometheus', 'letsencrypt'].; ; To avoid accidental inclusion of unwanted files or directories,; setuptools will not proceed with this build.; ; If you are trying to create a single distribution with multiple packages; on purpose, you should not rely on automatic discovery.; Instead, consider the following options:; ; 1. set up custom discovery (`find` directive with `include` or `exclude`); 2. use a `src-layout`; 3. explicitly set `py_modules` or `packages` with a list of names; ; To find more information, look for ""package discovery"" on setuptools docs.; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: subprocess-exited-with-error. × Getting requirements to build editable did not run successfully.; │ exit code: 1; ╰─> See above for output.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290:5,install,install,5,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1502112290,3,"['Install', 'install']","['Installing', 'install', 'installation']"
Deployability,"`skopeo` is part of the standard Ubuntu 2**2**.04 repo, but probably that's a bit more difficult to update to? Building Hail on 22.04 seems to work fine though.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11859#issuecomment-1137980297:100,update,update,100,https://hail.is,https://github.com/hail-is/hail/pull/11859#issuecomment-1137980297,1,['update'],['update']
Deployability,a dev deploy of test-dataproc https://ci.hail.is/batches/8123862,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14270#issuecomment-1936173509:6,deploy,deploy,6,https://hail.is,https://github.com/hail-is/hail/pull/14270#issuecomment-1936173509,1,['deploy'],['deploy']
Deployability,"a#L339-L368. The temporary location is drawn as 62 choose 22. So, odds of collision are 3 * 10^16. ~~I can't find the referenced case analysis in Google's latest code. [It is present in this fork](https://github.com/leogamas/java-storage/blob/2af8dfd95cdebc9e4d8252b0bbe3f092844d9f2c/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java#L68-L198) from a few years ago.~~. Here's the [referenced case analysis in 2.17.1](https://github.com/googleapis/java-storage/blame/v2.17.1/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java). There seems to have been a rewrite [two months ago](https://github.com/googleapis/java-storage/blame/main/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java) (here's [the main commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:26",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:1217,release,released,1217,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['release'],['released']
Deployability,"action for a current 2.1.0 user:; ```bash; dking@wmb16-359 # gradle -Dspark.verison=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.781 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); 2.1.0; dking@wmb16-359 # gradle -Dspark.version=2.1.0 compileScala. FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 42. * What went wrong:; A problem occurred evaluating root project 'hail'.; > The spark version must now be explicitly specified in the `gradle.properties`; file. Do *not* specify it with `-Dspark.version`. This version *must* match the; version of the spark installed on the machine or cluster that will execute; hail. You can override the setting in `gradle.properties` with a command line; like:. ./gradlew -PsparkVersion=2.1.1 shadowJar. The previous implicit, default spark version was 2.0.2. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.778 secs; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020:1080,install,installed,1080,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201020,1,['install'],['installed']
Deployability,"actually, let's just release now. We can make another release at the end of the week -- there are a couple of major performance improvements queued up.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482:21,release,release,21,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603482,2,['release'],['release']
Deployability,added notes for all three new bug fixes in this release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13265#issuecomment-1654506415:48,release,release,48,https://hail.is,https://github.com/hail-is/hail/pull/13265#issuecomment-1654506415,1,['release'],['release']
Deployability,"addressed comments, will update with results on realistic data shortly",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2494#issuecomment-348580922:25,update,update,25,https://hail.is,https://github.com/hail-is/hail/pull/2494#issuecomment-348580922,1,['update'],['update']
Deployability,"ah, annoying. It's hard to figure out when stuff is actually going to be executed in make. We can fix this by moving those definitions inside the target, I think:; ```; pypi-deploy: check-pypi wheel; 	 TWINE_USERNAME=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-username) \ ; 	 TWINE_PASSWORD=$(shell cat $(HAIL_TWINE_CREDS_FOLDER)/pypi-password) \; 	 twine upload build/deploy/dist/*; ```. mind making that change in a separate branch?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676:174,deploy,deploy,174,https://hail.is,https://github.com/hail-is/hail/pull/6318#issuecomment-501313676,2,['deploy'],['deploy']
Deployability,"ah, missed a comment--is there a reason the linux prebuilts needed to be updated?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4663#issuecomment-434492824:73,update,updated,73,https://hail.is,https://github.com/hail-is/hail/pull/4663#issuecomment-434492824,1,['update'],['updated']
Deployability,all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO: U,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:9034,install,install,9034,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"alled as per the logs (see below); ```; pyspark==3.3.3; # via -r hail/hail/python/requirements.txt`; ``` . * I do not see any `pyspark` in `/hail/hail/python/hailtop/hailctl/deploy.yaml`. * Checking in `/usr/lib/spark` I see reference of scala 2.12.15 same as in the hail logs; ```sh; $ cat /usr/lib/spark/RELEASE ; Spark 3.3.2-amzn-0.1 built for Hadoop 3.3.3-amzn-3.1; Build flags: -Divy.home=/home/release/.ivy2 -Dsbt.ivy.home=/home/release/.ivy2 -Duser.home=/home/release -Drepo.maven.org= -Dreactor.repo=file:///home/release/.m2/repository -Dhadoop.version=3.3.3-amzn-3.1 -Dyarn.version=3.3.3-amzn-3.1 -Dhive.version=2.3.9-amzn-3 -Dparquet.version=1.12.2-amzn-3 -Dprotobuf.version=2.5.0 -Dfasterxml.jackson.version=2.13.4 -Dfasterxml.jackson.databind.version=2.13.4 -Dcommons.httpclient.version=4.5.9 -Dcommons.httpcore.version=4.4.11 -Daws.java.sdk.version=1.12.446 -Daws.kinesis.client.version=1.12.0 -Daws.kinesis.producer.version=0.12.9 -Dscala.version=2.12.15 -DrecompileMode=all -Dmaven.deploy.plugin.version=2.8.2 -Dmaven.scaladoc.skip -Pyarn -Phadoop-3.2 -Phive -Phive-thriftserver -Psparkr -Pspark-ganglia-lgpl -Pnetlib-lgpl -Pscala-2.12 -Pkubernetes -Pvolcano -Pkinesis-asl -DskipTests; ```; I still did not found why scala is downgraded to 2.12.13. <details><summary>Hail logs</summary>; <p>; # Build Hail #; WARNING: Package(s) not found: hail; REVISION is set to ""13536b531342a263b24a7165bfeec7bd02723e4b"" which is different from old value """"; printf ""13536b531342a263b24a7165bfeec7bd02723e4b"" > env/REVISION; echo 13536b531342a263b24a7165bfeec7bd02723e4b > python/hail/hail_revision; SHORT_REVISION is set to ""13536b531342"" which is different from old value """"; printf ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; pr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:1233,deploy,deploy,1233,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,ame in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:13541,release,release,13541,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,ame in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$arguments'; + '[' -z f ']'; + echo WHEEL_FOR_AZURE=f; WHEEL_FOR_AZURE=f; + for varname in '$arguments'; + '[' -z g ']'; + echo WEBSITE_TAR=g; WEBSITE_TAR=g; + exit 1; make: *** [release] Error 1; ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE= make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE= \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:13694,deploy,deploy,13694,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy']
Deployability,"ances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8537,install,install,8537,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"and yes, will tag and deploy to PyPI after it merges.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4921#issuecomment-445271169:22,deploy,deploy,22,https://hail.is,https://github.com/hail-is/hail/pull/4921#issuecomment-445271169,1,['deploy'],['deploy']
Deployability,"ark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modif",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8321,install,install,8321,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"ark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing m",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1523,install,install,1523,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"at immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc start` argument, but `--num-secondary-worker-local-ssds` is not. This could cause some confusion/annoyance. This could potentially be reduced by minimizing the number of `gcloud dataproc` arguments that are defined as `hailctl dataproc` arguments. For example, `--num-worker-local-ssds` is just passed through to `gcloud`, so there's no real need for it to be a `hailctl dataproc start` argument. On the other hand, it is nice to have some of those `gcloud` arguments show up in `hailctl dataproc start --help`.; - While making breaking changes, it would be nice if the `--configuration`/`--gcloud-configuration` argument was consistent across `hailctl dataproc` commands. For example, currently `hailctl dataproc start` takes `--configuration` but `hailctl dataproc submit` takes `--gcloud-configuration`. It would also be nice to standardize on kebab case for all arguments. There are some other arguments that use underscores (ex. `--init_timeout` to `hailctl dataproc start`).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:2799,configurat,configuration,2799,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,4,['configurat'],['configuration']
Deployability,at; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5456,deploy,deploy-,5456,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"ataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has happened in 2/8 test_dataproc steps that I have run myself or seen run. The more workers you have, the higher the chance at least one worker fails. As @bpblanken suggested [here](https://github.com/hail-is/hail/issues/12936#issuecomment-1589956412), restarting docker on a failed worker works. Docker starts fine. However, I missed a subtlety: we must restart *after* installation but *before* we try to pull our VEP docker image. I also added a sleep in hopes that gives various things a chance to die off.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13565,update,update,13565,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,5,"['install', 'update']","['install', 'installation', 'update']"
Deployability,"atch-jigold-yrxul be created in? This project will incur costs for storing your Hail generated data. (Example: hail-jigold): hail-jigold; Which region does your data reside in? (Example: us-central1): us-central1; Do you want to set a lifecycle policy (automatically delete files after a time period) on the bucket hail-batch-jigold-yrxul? [y/n]: y; After how many days should files be automatically deleted from bucket hail-batch-jigold-yrxul? (30): 15; Created bucket hail-batch-jigold-yrxul in project hail-jigold.; Updated bucket hail-batch-jigold-yrxul in project hail-jigold with lifecycle rule set to 15 days and labels {'bucket': 'hail-batch-jigold-yrxul', 'owner': 'jigold', 'data_type': 'temporary'}.; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-yrxul in project hail-jigold.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-yrxul/batch/tmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Use an existing bucket and give permissions:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init ; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:1415,CONFIGURAT,CONFIGURATION,1415,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIGURAT'],['CONFIGURATION']
Deployability,"batch deployment is still failing; [batch.log](https://github.com/hail-is/hail/files/2507610/batch.log); ```; a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6: digest: sha256:3e72c4e3d33d3009fcd08cdaf4e8601535eadce37c3004d6371f802638aa09f5 size: 2002; echo ""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" > batch-image; sed -e ""s,@sha@,$(git rev-parse --short=12 HEAD),"" \; -e ""s,@image@,$(cat batch-image),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:6,deploy,deployment,6,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,10,"['Deploy', 'configurat', 'deploy']","['Deployment', 'configuration', 'deployment', 'deployments']"
Deployability,"batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'e",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2078,pipeline,pipeline,2078,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: status.podIP; - name: POD_NAME; valueFrom:; fieldRef:; apiVersion: v1; fieldPath: metadata.name; image: konradjk/saige:0.35.8.2.2; imagePullPolicy: IfNotPresent; name: main; resources:; requests:",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:13013,pipeline,pipeline,13013,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,bcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgene,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:2871,deploy,deploy-,2871,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,"btw, I think we can get rid of the deployment's notebook-secrets, wanted to double check (I believe it just handled basic auth).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-463477507:35,deploy,deployment,35,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-463477507,1,['deploy'],['deployment']
Deployability,"bump, I want to get this in before we release again",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10278#issuecomment-824386759:38,release,release,38,https://hail.is,https://github.com/hail-is/hail/pull/10278#issuecomment-824386759,1,['release'],['release']
Deployability,bump. I have a stack of these in the pipeline and want to keep them moving.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2301#issuecomment-337368303:37,pipeline,pipeline,37,https://hail.is,https://github.com/hail-is/hail/pull/2301#issuecomment-337368303,1,['pipeline'],['pipeline']
Deployability,bumping this because it's probably killing certain pipelines,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4483#issuecomment-428374032:51,pipeline,pipelines,51,https://hail.is,https://github.com/hail-is/hail/issues/4483#issuecomment-428374032,1,['pipeline'],['pipelines']
Deployability,c; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-v,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4903,deploy,deploy-,4903,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"can we please get this in? If there's a reason I'm not seeing to be cautious, I want to hear it. But for my own sanity, I'd like another line of defense against needing to rewrite benchmark-on-pipeline again.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260:193,pipeline,pipeline,193,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-583466260,1,['pipeline'],['pipeline']
Deployability,can you include more info? I think a fresh install of conda will include this binary on the path now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6738#issuecomment-515162578:43,install,install,43,https://hail.is,https://github.com/hail-is/hail/issues/6738#issuecomment-515162578,1,['install'],['install']
Deployability,cc: @cseed @tpoterba is this an OK way to track things that should be done before we release 0.3?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5128#issuecomment-454219734:85,release,release,85,https://hail.is,https://github.com/hail-is/hail/issues/5128#issuecomment-454219734,1,['release'],['release']
Deployability,cker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6270,deploy,deploy-,6270,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"clude/linux build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/Na; tiveCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before buildi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:15836,release,release-notes,15836,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['release'],['release-notes']
Deployability,"crossposting from a message I sent to the variants team. ---. #### executive summary. Excess JVM memory use is almost certainly not the issue. I've taken a close look at the import_gvs.py loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the drive",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:303,pipeline,pipelines,303,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,1,['pipeline'],['pipelines']
Deployability,"d @record_method decorators (the latter is now required, else history goes histrionic). The configuration file, init script, and resource files are in`gs://hail-common/nirvana`. The init script `gs://hail-common/nirvana/nirvana-init-GRCh37.sh` makes local copies of the resource files and .net: ; ```; #!/bin/bash. mkdir -p /nirvana/Data/Cache; mkdir -p /nirvana/Data/References; mkdir -p /nirvana/Data/SupplementaryDatabase. #Data is copied for use with Nirvana 1.6.2 as of June 19 2017; gsutil -m cp -r gs://hail-common/nirvana/Data/Cache/24/GRCh37 /nirvana/Data/Cache; gsutil -m cp gs://hail-common/nirvana/Data/References/5/Homo_sapiens.GRCh37.Nirvana.dat /nirvana/Data/References; gsutil -m cp -r gs://hail-common/nirvana/Data/SupplementaryDatabase/39/GRCh37 /nirvana/Data/SupplementaryDatabase; gsutil -m cp -r gs://hail-common/nirvana/netcoreapp1.1 /nirvana; gsutil -m cp gs://hail-common/nirvana/nirvana-cloud-GRCh37.properties /nirvana. chmod -R 777 /nirvana. apt-get -y install curl libunwind8 gettext; curl -sSL -o dotnet.tar.gz https://go.microsoft.com/fwlink/?linkid=843453; mkdir -p /opt/dotnet && sudo tar zxf dotnet.tar.gz -C /opt/dotnet; ln -s /opt/dotnet/dotnet /usr/local/bin; ```. The properties file `nirvana-cloud-GRCh37.properties` points Nirvana to these local resources:; ```; hail.nirvana.location = /nirvana/netcoreapp1.1/Nirvana.dll; hail.nirvana.cache = /nirvana/Data/Cache/GRCh37/Ensembl84; hail.nirvana.reference = /nirvana/Data/References/Homo_sapiens.GRCh37.Nirvana.dat; hail.nirvana.supplementaryAnnotationDirectory = /nirvana/Data/SupplementaryDatabase/GRCh37; ```. I started a cluster with the init script and ran Nirvana on all of `profile225.vcf`, and later exported results for just a region bounding the gene CABIN1:; ```; from hail import *; hc = (HailContext()). (hc; .import_vcf(path='gs://jbloom/profile225.vcf.bgz'); .filter_multi(); .nirvana(block_size=10000, config='/nirvana/nirvana-cloud-GRCh37.properties'); .variants_table(); .filter(expr='v.start > ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701:1149,install,install,1149,https://hail.is,https://github.com/hail-is/hail/pull/2377#issuecomment-340889701,1,['install'],['install']
Deployability,"d6ad5cd428cd""; Normal Created 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Created container; Normal Started 1m kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq ; NAME READY STATUS RESTARTS AGE; notebook-worker-9szt8 0/1 Running 0 49s. Started container; Warning Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:1817,install,install,1817,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,1,['install'],['install']
Deployability,"d: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30831,install,install,30831,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the clus",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2039,update,update,2039,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['update'],"['update', 'update-hail-version']"
Deployability,deploy successful,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8524#issuecomment-611797230:0,deploy,deploy,0,https://hail.is,https://github.com/hail-is/hail/pull/8524#issuecomment-611797230,1,['deploy'],['deploy']
Deployability,deployed,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8428#issuecomment-607465228:0,deploy,deployed,0,https://hail.is,https://github.com/hail-is/hail/pull/8428#issuecomment-607465228,3,['deploy'],['deployed']
Deployability,deploying now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9797#issuecomment-738988331:0,deploy,deploying,0,https://hail.is,https://github.com/hail-is/hail/pull/9797#issuecomment-738988331,1,['deploy'],['deploying']
Deployability,dev deploy should be up for whoever reviews: https://internal.hail.is/dking/batch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8805#issuecomment-629377969:4,deploy,deploy,4,https://hail.is,https://github.com/hail-is/hail/pull/8805#issuecomment-629377969,1,['deploy'],['deploy']
Deployability,docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments';,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6054,deploy,deploy-,6054,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=doc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4146,deploy,deploy-,4146,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9622,deploy,deploy-,9622,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b \; HAIL_GENETICS_HAILTOP_IMAGE=c \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e \; WHEEL_FOR_AZURE=f \; WEBSITE_TAR=g \; bash scripts/release.sh; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:10597,deploy,deploy,10597,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy']
Deployability,"don't approve until deployment is working again, please",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6312#issuecomment-501314794:20,deploy,deployment,20,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-501314794,1,['deploy'],['deployment']
Deployability,"e `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 3520, 51 row lock(s), undo log entries 30; MySQL thread id 1941930, OS thread handle 140298159240960, query id 1869168731 10.32.3.8 dgoldste update; INSERT INTO batch_inst_coll_cancellable_resources (batch_id, inst_coll, token, n_running_cancellable_jobs, running_can; cellable_cores_mcpu); VALUES (OLD.batch_id, OLD.inst_coll, rand_token, -1, -OLD.cores_mcpu); ON DUPLICATE KEY UPDATE; n_running_cancellable_jobs = n_running_cancellable_jobs - 1,; running_cancellable_cores_mcpu = running_cancellable_cores_mcpu - OLD.cores_mcpu; *** (2) HOLDS THE LOCK(S):; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409370 lock_mode X locks rec but not gap; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263027 page no 15 n bits 232 index PRIMARY of table `dgoldste`.`batch_inst_coll_cancellable_res; ources` trx id 644409370 lock_mode X locks rec but not gap waiting; Record lock, heap no 23 PHYSICAL RECORD: n_fields 10; compact format; info bits 0; 0: len 8; hex 8000000000000007; asc ;;; 1: len 8; hex 73746",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:2480,UPDATE,UPDATE,2480,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,1,['UPDATE'],['UPDATE']
Deployability,"e a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking if `n_jobs == n_complete`. If we don't have a `SELECT ... FOR UPDATE`, I couldn't convince myself that there wouldn't be a race condition where a batch completion event isn't accidentally missed. . I think the healing loop would only happen if the batch driver got restarted in between:; 1. CALL mark_job_complete() in the database; 2. mark_batch_complete(). If 1. errors, the operation is aborted entirely. On second thought, it might be possible to use an optimistic locking strategy, but that's more complicated and I'd have to think about it some more. > It seems to me like it would be preferable to instead first update application code to mark the batch complete if it is not complete, then remove the now redundant marking complete of the batch from the trigger. Then there is no delay after the migration where batches are not complete for some time. Ah, good point!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:2937,update,update,2937,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,3,"['UPDATE', 'update']","['UPDATE', 'update']"
Deployability,"e any of the hard problems (whereas shared_ptr; very much does). Now I realize that people writing books about C++ write a good deal about move semantics and; unique_ptr. My interpretation is that there's a lot of writing about it because it involves concepts; which simply don't occur in any other commonly-used languages, and as such it requires a; good deal of explanation and justification because it's peculiar and unfamiliar. I suggest that; other languages haven't invented this concept because it's a) confusing and b) not particularly; useful. There's one really good thing you get from move semantics: the ability to resize a std::vector<T>; or std::unordered_map<T> without constructing deep copies of each T object. In the cases; where that's useful, it's very useful for optimizing performance without totally bypassing all your; abstraction mechanisms. The other ways people attempt to exploit move semantics are IMO; just a bad idea: if you want to pass around a large expensive-to-create object, then do it the; Java way by putting it on the heap and passing around some kind of reference, and *everyone* can understand it, not just experts in modern C++. Another angle on this debate would be to look at some open-source C++ projects and see how; often they actually use unique_ptr and/or std::move. My guess is that it's much less common; in practice than you might think from reading books about C++, because the overlap between; ""object ownership is passed around"" and ""... but we always know precisely who has ownership""; is not a very big part of the design space - compared to a whole lot of ""always owned by the object which created it"" and ""used in several places at once and we don't know who will be the last to drop it"". [Update: the LLVM codebase, including tests, is 2.00M lines of C++ (.h and .cpp files), of which; ""unique_ptr"" occurs 4500 times, and ""std::move"" 3558 times. That's a ""unique_ptr"" on average once every 444 lines, and ""std::move"" once every 562 lines.]",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489:1993,Update,Update,1993,https://hail.is,https://github.com/hail-is/hail/pull/3718#issuecomment-396683489,1,['Update'],['Update']
Deployability,"e changes:; - The `hailctl dataproc` subcommand now has `--beta`, `--configuration=`, `--dry-run`, `--project=` and `--zone=`. These apply to all commands. There is a `GcloudRunner` object that takes these options, is set to the click context user `obj` field, and is used by all hailctl dataproc commands to invoke gcloud. Note, not all dataproc subcommands invoke gcloud, but the current design doesn't differentiate. Note, with `click`, the subcommand options must go on the subcommand, so `hailctl dataproc stop --dry-run` is an error.; - hailctl no longer takes `--region` (for gcloud dataproc commands). I compute region in `GcloudRunner` by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this).; - I stripped all gcloud pass through args from `hailctl dataproc modify`. There aren't any left. Invoking `modify` now looks like:. ```; hailctl dataproc modify my-cluster \; --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; ```. The `extra` in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:1097,update,update-args,1097,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,1,['update'],['update-args']
Deployability,"e the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args`.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2751,update,update-args,2751,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,1,['update'],['update-args']
Deployability,"e.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1912,pipeline,pipeline,1912,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' '],MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3434,deploy,deploy-,3434,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']',MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:8910,deploy,deploy-,8910,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"eld: imputed int32; (63110, 64048); running omit filter; [Stage 1:> (0 + 1) / 1](63110, 52877); running pc_relate; [Stage 3:==================================================>(12794 + 2) / 12796]2020-08-20 10:14:38 Hail: INFO: hwe_normalized_pca: running PCA using 63110 variants.; [Stage 5:==================================================>(12795 + 1) / 12796]2020-08-20 10:14:59 Hail: INFO: pca: running PCA with 10 components...; [Stage 102:================================================>(12795 + 1) / 12796]Traceback (most recent call last):; File ""/restricted/projectnb/adgc/topmed.r2.analysis/pc_relate_pop2.py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/inst",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:5620,install,install,5620,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"ent/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11719,update,update,11719,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['update'],['update']
Deployability,"ent/server model, mostly just to make sure I understand: each HailContext has one ShuffleClient,. There should be a ShuffleClient per-shuffle. I think in our current model, this means one active ShuffleClient at any given time (because there's only one active shuffle at any given time). However, I intend Hail service pipelines to be able to use multiple concurrent shuffles, if useful. In particular, a ShuffleClient has as type and an encoding, so its only useful for one type of dataset. Though you could theoretically re-use the object on a different dataset of the same type by calling `start` again. > which communicates with a ShuffleServer (which only connects to one ShuffleClient). I intended the ShuffleServer to serve an arbitrary number of non-adversarial clients (perhaps two different users in nascent hail service). I think it's pretty secure, but I don't think `UUID.randomUUID().toString()` is cryptographically random, so an adversary could probably guess it and thus get access to shuffle data. Moreover, the ShuffleServer must support concurrent connections from all the workers of a Hail pipeline. `ShuffleServer.serve` starts a fresh server thread for every connection. During the read or write phases of the shuffler, the idea is 1:1 mappings from workers to connections to server `Handler` threads. > Every time the hail context wants some data to be shuffled, the server creates a new shuffle ID to associate with the shuffle (starting the shuffle) and then the client sends over the data and can then access it, in ranges, using get. As soon as it starts a new shuffle, it can no longer access the data from the previous shuffle (at least through current interfaces---the shuffle server never deletes shuffled data, so we could theoretically define a put and get that take the uuid and then keep accessing older shuffles as long as we know the uuid). Does that sound about right?. The hail leader node could keep multiple `ShuffleClient` objects around to access old data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052:1153,pipeline,pipeline,1153,https://hail.is,https://github.com/hail-is/hail/pull/8361#issuecomment-609940052,1,['pipeline'],['pipeline']
Deployability,"er details.; SLF4J: Class path contains SLF4J bindings targeting slf4j-api versions 1.7.x or earlier.; SLF4J: Ignoring binding found at [jar:file:/usr/lib/spark/jars/log4j-slf4j-impl-2.17.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See https://www.slf4j.org/codes.html#ignoredBindings for an explanation.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/. Using Python version 3.9.18 (main, Oct 25 2023 05:26:35); Spark context Web UI available at http://ip-192-168-125-39.ap-southeast-1.compute.internal:4040; Spark context available as 'sc' (master = yarn, app id = application_1698211907929_0001).; SparkSession available as 'spark'.; >>> import hail as hl; >>> hl.version(); '0.2.124-e739a95489e4'; hl.init(sc); pip-installed Hail requires additional configuration options in Spark referring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:2338,install,installed,2338,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,2,"['configurat', 'install']","['configuration', 'installed']"
Deployability,"er, maybe a bit longer to deploy",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4340#issuecomment-421627054:26,deploy,deploy,26,https://hail.is,https://github.com/hail-is/hail/issues/4340#issuecomment-421627054,1,['deploy'],['deploy']
Deployability,er://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4023,deploy,deploy-,4023,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,er://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GI,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9499,deploy,deploy-,9499,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"ere before I delete the jobs. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/ba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1209,install,installed-,1209,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['install'],['installed-']
Deployability,"ername}-{database_name}-admin'; + self.user_username = f'{dev_username}-{database_name}-user'; else:; assert params.scope == 'test'; self._name = f'{params.code.short_str()}-{database_name}-{self.token}'; @@ -1030,7 +1032,7 @@ class CreateDatabaseStep(Step):; @staticmethod; def from_json(params: StepParameters):; json = params.json; - return CreateDatabaseStep(; + return CreateDatabase2Step(; params,; json['databaseName'],; json['namespace'],; @@ -1111,12 +1113,12 @@ EOF; attributes={'name': self.name},; secrets=[; {; - 'namespace': self.database_server_config_namespace,; + 'namespace': self.namespace,; 'name': 'database-server-config',; 'mount_path': '/sql-config',; }; ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; + service_account={'namespace': self.namespace, 'name': 'admin'},; input_files=input_files,; parents=[self.create_passwords_job] if self.create_passwords_job else self.deps_parents(),; network='private',; @@ -1125,42 +1127,4 @@ EOF; ); ; def cleanup(self, batch, scope, parents):; - if scope in ['deploy', 'dev'] or self.cant_create_database:; - return; -; - cleanup_script = f'''; -set -ex; -; -commands=$(mktemp); -; -cat >$commands <<EOF; -DROP DATABASE IF EXISTS \\`{self._name}\\`;; -DROP USER IF EXISTS '{self.admin_username}';; -DROP USER IF EXISTS '{self.user_username}';; -EOF; -; -until mysql --defaults-extra-file=/sql-config/sql-config.cnf <$commands; -do; - echo 'failed, will sleep 2 and retry'; - sleep 2; -done; -; -'''; -; - self.cleanup_job = batch.create_job(; - CI_UTILS_IMAGE,; - command=['bash', '-c', cleanup_script],; - attributes={'name': f'cleanup_{self.name}'},; - secrets=[; - {; - 'namespace': self.database_server_config_namespace,; - 'name': 'database-server-config',; - 'mount_path': '/sql-config',; - }; - ],; - service_account={'namespace': DEFAULT_NAMESPACE, 'name': 'ci-agent'},; - parents=parents,; - always_run=True,; - network='private',; - regions=[REGION],; - ); + pass; diff --git a/ci/test/resources/bu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600:2825,deploy,deploy,2825,https://hail.is,https://github.com/hail-is/hail/pull/13022#issuecomment-1542233600,1,['deploy'],['deploy']
Deployability,"erring; to the path to the Hail Python module directory HAIL_DIR,; e.g. /path/to/python/site-packages/hail:; spark.jars=HAIL_DIR/backend/hail-all-spark.jar; spark.driver.extraClassPath=HAIL_DIR/backend/hail-all-spark.jar; spark.executor.extraClassPath=./hail-all-spark.jarRunning on Apache Spark version 3.3.2-amzn-0.1; SparkUI available at http://ip-192-168-110-167.ap-southeast-1.compute.internal:4040; Welcome to; __ __ <>__; / /_/ /__ __/ /; / __ / _ `/ / /; /_/ /_/\_,_/_/_/ version 0.2.124-e739a95489e4; LOGGING: writing to /mnt/tmp/hail/hail/hail-20231025-0729-0.2.124-e739a95489e4.log; >>> mt = hl.balding_nichols_model(n_populations=3, n_samples=500, n_variants=1_000); 2023-10-25 07:29:48.283 Hail: INFO: balding_nichols_model: generating genotypes for 3 populations, 500 samples, and 1000 variants...; >>> mt.count(); (1000, 500); ```. it seems working in command line using pyspark !. I need to test on jupyter notebook now... FYI the pyspark configs. ```sh ; - Classification: spark-defaults; ConfigurationProperties:; spark.jars: /opt/hail/backend/hail-all-spark.jar; spark.driver.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar; spark.executor.extraClassPath: /opt/hail/backend/hail-all-spark.jar:/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/emrfs/conf:/usr/share/aws/emr/emrfs/lib/*:/usr/share/aws/emr/emrfs/auxlib/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949:3413,Configurat,ConfigurationProperties,3413,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1778834949,1,['Configurat'],['ConfigurationProperties']
Deployability,ers/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z build/deploy/dist/hail-0.2.128-py3-none-any.whl ']'; + echo WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo GITHUB_OAUTH_HEADER_FILE=abc123; GITHUB_OAUTH_HEADER_FILE=abc123; + for varname in '$arguments'; + '[' -z abc123 ']'; + echo HAIL_GENETICS_HAIL_IMAGE=abc123; HAIL_GENETICS_HAIL_IMAGE=abc123; + for varname in '$arguments'; + '[' -z a ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a; + for varname in '$arguments'; + '[' -z b ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b; + for varname in '$arguments'; + '[' -z c ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=c; HAIL_GENETICS_HAILTOP_IMAGE=c; + for varname in '$arguments'; + '[' -z d ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d; + for varname in '$arguments'; + '[' -z e ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e; + for varname in '$argum,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:12044,deploy,deploy,12044,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy']
Deployability,"ert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Dev",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2564,deploy,deploy,2564,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,1,['deploy'],['deploy']
Deployability,es in the Makefile to demonstrate the functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1118,release,release,1118,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,essentially you need to add the following as configuration in Spark:. ```; HAIL_JAR_LOCATION=/path/to/python/site-packages/hail/hail-all-spark.jar; spark.jars=${HAIL_JAR_LOCATION}; spark.driver.extraClassPath=${HAIL_JAR_LOCATION}; spark.executor.extraClassPath=./hail-all-spark.jar; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7080#issuecomment-536711994:45,configurat,configuration,45,https://hail.is,https://github.com/hail-is/hail/issues/7080#issuecomment-536711994,1,['configurat'],['configuration']
Deployability,"eve the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the other hand, `--extra-gcloud-dataproc-clusters-update-args` is an awfully long argument name. > I plan to leave the --async option to stop, although it is pass through. > Then there is --files for submit. This is passed through, but --py-files is needed (it is not passed through, but modified). Do I leave --files? I'm currently inclined to. Agreed. I support having the most frequently used parameters as `hailctl` parameters, even if they are only simple pass throughs. My original comment about minimizing the number of simple pass through parameters was mainly directed toward `hailctl dataproc start`, which has several options than can be specified separately for master node, worker nodes, and secondary worker nodes. I wanted to avoid cases where, for example, `--worker-boot-disk-size` was a `hailctl` option, but `--secondary-worker-boot-disk-size` had to be specified after a `--` or with `--extra-gcloud-start-args",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:2599,update,update-args,2599,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,2,['update'],"['update', 'update-args']"
Deployability,"ew ""v3"" tables, data from all newly created batches since then have been correctly populating both tables. However, we need to back fill the tables with old records. We've currently finished back-populating 3 out of 4 of the ""v3"" tables. The last one that remains is `aggregated_job_resources_v3`. I estimate the time to back fill this table will be on the order of 12 hours. How the backfilling works is we have a flag / Boolean Column on the equivalent `v2` table that says whether the rows have been migrated to the `v3` table already. There's then a trigger already in place that every time a row in the `v2` table is updated such as `update aggregated_job_resources_v2 set migrated = 1 where ...`, then we check whether the row has already been ""migrated"". If it hasn't then we insert new rows into the ""v3"" table with the deduped resource ID as the key. The exact trigger is . ```sql; DROP TRIGGER IF EXISTS aggregated_job_resources_v2_after_update $$; CREATE TRIGGER aggregated_job_resources_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the b",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:1799,UPDATE,UPDATE,1799,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,1,['UPDATE'],['UPDATE']
Deployability,"extra -fPIC -ggdb -fno-strict-aliasing -I../resources/include -I/etc/alternatives/jre/include -I/etc/alternatives/jre/include/linux build/ibs.o build/Decoder.o build/Encoder.o build/Logging.o build/Na; tiveCodeSuite.o build/NativeLongFunc.o build/NativeModule.o build/NativePtr.o build/NativeStatus.o build/ObjectArray.o build/PartitionIterators.o build/Region.o build/Upcalls.o build/FS.o -o lib/linux-x86-64/libhail.so; cp -p -f lib/linux-x86-64/libboot.so lib/linux-x86-64/libhail.so ../../../prebuilt/lib/linux-x86-64/; make[1]: Leaving directory `/mnt/tmp/hail/hail/src/main/c'; ./gradlew shadowJar -Dscala.version=2.12.15 -Dspark.version=3.3.2 -Delasticsearch.major-version=7; Downloading https://services.gradle.org/distributions/gradle-8.3-bin.zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:15692,release,release,15692,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['release'],['release']
Deployability,"f ""13536b531342"" > env/SHORT_REVISION; HAIL_PIP_VERSION is set to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.prop",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:2947,deploy,deploy,2947,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"fixed up the dev requirements, and updated the pre-commit hook, and in the process realized i had only run `ruff format` on files that are checked by `make check-all`, so also ran it at the root of the repo, as the hook will run there. EDIT: i misunderstood how the pre-commit hooks work, and apparently it only runs them on the files you have changed, but i think it's still worth it to format across the repo, not just in the places we check with `make check-all`; and also, i personally was a grump and never used the pre-commit hooks, but now that i've had to test them out for this, i have seen the light ⚡. also i personally would love if our quotes were consistent as well! i'll ask on zulip how ppl feel about that",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14132#issuecomment-1883771570:35,update,updated,35,https://hail.is,https://github.com/hail-is/hail/pull/14132#issuecomment-1883771570,1,['update'],['updated']
Deployability,for future PRs can you make a branch on a forked repo and PR in from there? It's nice to have only released branches on the main repo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4332#issuecomment-421074006:99,release,released,99,https://hail.is,https://github.com/hail-is/hail/pull/4332#issuecomment-421074006,1,['release'],['released']
Deployability,"for future pull requests, can you open a PR from your forked repository? We like to keep the main repo free from non-released branches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5162#issuecomment-455362548:117,release,released,117,https://hail.is,https://github.com/hail-is/hail/pull/5162#issuecomment-455362548,1,['release'],['released']
Deployability,"force merged, hand deploying.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8428#issuecomment-607460857:19,deploy,deploying,19,https://hail.is,https://github.com/hail-is/hail/pull/8428#issuecomment-607460857,1,['deploy'],['deploying']
Deployability,force merging to fix deploy,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5644#issuecomment-474937917:21,deploy,deploy,21,https://hail.is,https://github.com/hail-is/hail/pull/5644#issuecomment-474937917,1,['deploy'],['deploy']
Deployability,fusermount needs root privileges which the container has. I tested this with dev deploy and made sure things were being unmounted properly.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8971#issuecomment-644964603:81,deploy,deploy,81,https://hail.is,https://github.com/hail-is/hail/pull/8971#issuecomment-644964603,1,['deploy'],['deploy']
Deployability,"ge),"" \; < deployment.yaml.in > deployment.yaml; kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batc",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1460,deploy,deployments,1460,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,1,['deploy'],['deployments']
Deployability,"genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8386,install,install,8386,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Cha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1588,install,install,1588,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"gger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cachetools-5.3.1 certifi-2023.7.22 cffi-1.15.1 charset-normalizer-3.2.0 commonmark-0.9.1 contourpy-1.1.0 cryptography-41.0.3 decorator-4.4.2 deprecated-1.2.14 dill-0.3.7 frozenlist-1.4.0 google-api-core-2.11.1 google-auth-2.22.0 google-auth-oauthlib-0.8.0 google-cloud-core-2.3.3 google-cloud-storag; e-2.10.0 google-crc32c-1.5.0 google-resumable-media-2.5.0 googleapis-common-protos-1.60.0 humanize-1.1.0 idna-3.4 isodate-0.6.1 janus-1.0.0 jinja2-3.1.2 jmespath-1.0.1 jproperties-2.1.1 markupsafe-2.1.3 msal-1.23.0 msal-extensions-1.0.0 msrest-0.7.1 multidict-6.0.4 nest-asyncio-1.5.7 numpy-1.25.2 oaut; hlib-3.2.2 orjson-3.9.5 packaging-23.1 pandas-2.1.0 parsimonious-0.10.0 pillow-10.0.0 plotly-5.16.1 por",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:42866,install,installed,42866,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['install'],['installed']
Deployability,"git hash looks like a standard Hail install, nevermind",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301754312:36,install,install,36,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301754312,1,['install'],['install']
Deployability,github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2200,install,install,2200,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,gnomAD is also exhausting memory (exit code 137) on their frequencies generating pipeline. They’re still exhausting memory after eliminating fork-joins in their pipeline. They were unintentionally invoking densification four times. We’ll need to sort out why even the simple frequencies blows memory.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13584#issuecomment-1712493167:81,pipeline,pipeline,81,https://hail.is,https://github.com/hail-is/hail/issues/13584#issuecomment-1712493167,2,['pipeline'],['pipeline']
Deployability,"gnomAD team has a Batch-based VQSR method. After gnomAD v4 is released and ASHG 2023 is over, we should lean on gnomAD team to get this into Hail mainline. https://github.com/broadinstitute/gnomad_methods/pull/470",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12905#issuecomment-1775395559:62,release,released,62,https://hail.is,https://github.com/hail-is/hail/issues/12905#issuecomment-1775395559,1,['release'],['released']
Deployability,"gold/random_file.txt DWRmR1Lh. # __TASK__1 read_input; cp gs://hail-jigold/input.bed Aw2arWP9.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim Aw2arWP9.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam Aw2arWP9.fam. # __TASK__4 subset; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE_GROUP__0=Aw2arWP9; __RESOURCE_GROUP__1=srXTmGQE; __RESOURCE__6=srXTmGQE.fam; __RESOURCE__10=8ueGZQqn; __RESOURCE__1=Aw2arWP9.bed; __RESOURCE__2=Aw2arWP9.bim; __RESOURCE__3=Aw2arWP9.fam; __RESOURCE_GROUP__2=ESEFn8Tm; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}&& awk '""'""'{ print $1, $2}'""'""' ${__RESOURCE__6} | sort | uniq -c | awk '""'""'{ if ($1 != 1) print $2, $3 }'""'""' > ${__RESOURCE__10}&& plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}'. # __TASK__5 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__3=K1TfWX3n; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 1 --out ${__RESOURCE_GROUP__3}'. # __TASK__6 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__4=8dRi0LwZ; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 2 --out ${__RESOURCE_GROUP__4}'. # __TASK__7 shapeit; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ gcr.io/shapeit /bin/bash -c '__RESOURCE_GROUP__2=ESEFn8Tm; __RESOURCE_GROUP__5=NIqfevqS; shapeit --bed-file ${__RESOURCE_GROUP__2} --chr 3 --out ${__RESOURCE_GROUP__5}'. # __TASK__8 merge; docker run -v /tmp//pipeline.S9YTZap5/:/tmp//pipeline.S9YTZap5/ -w /tmp//pipeline.S9YTZap5/ ubuntu /bin/bash -c '__RESOURCE__11=K1TfWX3n.haps; __RESOURCE__13=8dRi0LwZ.haps; __RESOURCE__15=NIqfevq",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938:1179,pipeline,pipeline,1179,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-454122938,1,['pipeline'],['pipeline']
Deployability,"got it, thanks. Databricks is on Spark 3.2, and people are asking about upgrades. . Please comment here once these Dataproc / EMR are up to Spark 3.2 and Hail has a PyPi release on 3.2, we can then upgrade the Databricks Documentation for Hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/11707#issuecomment-1085241895:72,upgrade,upgrades,72,https://hail.is,https://github.com/hail-is/hail/issues/11707#issuecomment-1085241895,3,"['release', 'upgrade']","['release', 'upgrade', 'upgrades']"
Deployability,"groups that have completed but have not been cancelled. ; The `job_groups` table describes if job group has been cancelled. Simply joining the job groups to `job_group_inst_coll_cancellable_resources` is prohibitively expensive due to explodes. My approach is to query for finished non-cancelled job groups from a last (batch, job) id then process records iteratively:. ```python; sql = """"""\; WITH T AS ( ; SELECT G.batch_id, G.job_group_id; FROM job_groups AS G ; INNER JOIN job_group_self_and_ancestors AS D; ON G.batch_id = D.batch_id; AND G.job_group_id = D.job_group_id; LEFT JOIN job_groups_cancelled AS C ; ON C.id = G.batch_id; AND C.job_group_id = D.ancestor_id ; WHERE G.batch_id >= ?; AND G.job_group_id > ?; AND G.time_completed IS NOT NULL; AND C.id IS NULL; ORDER BY G.batch_id ASC, G.job_group_id ASC; LIMIT 1000; ); SELECT group_resources.batch_id; , group_resources.update_id; , group_resources.job_group_id; , group_resources.inst_coll; , SUM(group_resources.n_creating_cancellable_jobs) AS n_creating_cancellable_jobs; , SUM(group_resources.n_ready_cancellable_jobs) AS n_ready_cancellable_jobs; , SUM(group_resources.n_running_cancellable_jobs) AS n_running_cancellable_jobs; , SUM(group_resources.ready_cancellable_cores_mcpu) AS ready_cancellable_cores_mcpu; , SUM(group_resources.running_cancellable_cores_mcpu) AS running_cancellable_cores_mcpu; , COUNT(*) as `count`; FROM job_group_inst_coll_cancellable_resources AS group_resources; INNER JOIN T USING (batch_id, job_group_id); GROUP BY group_resources.batch_id; , group_resources.update_id; , group_resources.job_group_id; , group_resources.inst_coll;; "". last_batch_id = 0; last_job_group_id = -1. while True:; rows = db.execute(sql, [last_batch_id, last_job_group_id]); ; if not rows:; break. for r in rows:; if r.count > 1:; delete records from job_group_inst_coll_cancellable_resources where token > 0; update record with accumulated result in r; ; last_batch_id = r.batch_id; last_job_group_id = r.job_group_id ; ````",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14623#issuecomment-2253530481:1934,update,update,1934,https://hail.is,https://github.com/hail-is/hail/issues/14623#issuecomment-2253530481,1,['update'],['update']
Deployability,"h pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12392,pipeline,pipeline,12392,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,6,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"h-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinCaseCtrl=TRUE; 2>&1 | tee ${__RESOURCE_FILE__749}; env:; - name: POD_IP; valueFrom:; fieldRef:; apiVersion: v1; fieldP",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12777,pipeline,pipeline,12777,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,h; +++ dirname -- scripts/release.sh; ++ cd -- scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.128 ']'; + echo HAIL_PIP_VERSION=0.2.128; HAIL_PIP_VERSION=0.2.128; + for varname in '$arguments'; + '[' -z 0.2.128-91d328e7fc84 ']'; + echo HAIL_VERSION=0.2.128-91d328e7fc84; HAIL_VERSION=0.2.128-91d328e7fc84; + for varname in '$arguments'; + '[' -z 91d328e7fc84686936ffd4f370c8c104b2d78b2a ']'; + echo GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename scripts/release.sh; ++ basename scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/t,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:15064,release,release,15064,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"h_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sparseSigmaFile=${__RESOURCE_FILE__9} --IsSingleVarinGroupTest=TRUE --IsOutputAFinC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12696,pipeline,pipeline,12696,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,hail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts\n2022-11-15 20:30:18.318 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts response 200\n2022-11-15 20:30:18.331 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff response 200\n2022-11-15 20:30:18.332 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: running job\n2022-11-15 20:30:18.333 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast\n2022-11-15 20:30:18.697 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast response 200\n2022-11-15 20:30:18.697 BatchClient: INFO: run: created update 2 for batch 6627669\n2022-11-15 20:30:18.697 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.802 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.852 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.866 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.917 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.934 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.985 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.999 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:19.049 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:19.068 Re,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:24604,update,update,24604,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['update'],['update']
Deployability,hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-doc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4274,deploy,deploy-,4274,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z x ']'; + echo WHEEL_FOR_AZURE=x; WHEEL_FOR_AZURE=x; + for varname in '$arguments'; + '[' -z /path/to/www.tar.gz ']'; + echo WEBSITE_TAR=/path/to/www.tar.gz; WEBSITE_TAR=/path/to/www.tar.gz; + exit 1. ```. ```sh; # WEBSITE_TAR=g WHEEL_FOR_AZURE=f HAIL_GENETICS_VEP_GRCH38_95_IMAGE=e HAIL_GENETICS_VEP_GRCH37_85_IMAGE=d HAIL_GENETICS_HAILTOP_IMAGE=c HAIL_GENETICS_HAIL_IMAGE_PY_3_11=b HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a HAIL_GENETICS_HAIL_IMAGE=abc123 GITHUB_OAUTH_HEADER_FILE=abc123 DEPLOY_REMOTE=origin make -C hail release; HAIL_PIP_VERSION=0.2.128 \; HAIL_VERSION=0.2.128-91d328e7fc84 \; GIT_VERSION=91d328e7fc84686936ffd4f370c8c104b2d78b2a \; REMOTE=origin \; WHEEL=build/deploy/dist/hail-0.2.128-py3-none-any.whl \; GITHUB_OAUTH_HEADER_FILE=abc123 \; HAIL_GENETICS_HAIL_IMAGE=abc123 \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=a \; HAIL_GENETIC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:9750,deploy,deploy-,9750,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hai,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5339,deploy,deploy-,5339,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:946); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:946); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.table_ir_1(Parser.scala:942); 	at is.hail.expr.ir.IRParser$.table_ir(Parser.scala:850); 	at is.hail.expr.ir.IRParser$.matrix_ir_1(Parser.scala:1112); 	at is.hail.expr.ir.IRParser$.matrix_ir(Parser.scala:999); 	at is.hail.expr.ir.IRParser$.ir_value_expr_1(Parser.scala:806); 	at is.hail.expr.ir.IRParser$.ir_value_expr(Parser.scala:517); 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1192); 	at is.hail.expr.ir.IRParser$$anonfun$parse_value_ir$2.apply(Parser.scala:1192); 	at is.hail.expr.ir.IRParser$.parse(Parser.scala:1186); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1192); 	at is.hail.expr.ir.IRParser$.parse_value_ir(Parser.scala:1191); 	at is.hail.expr.ir.IRParser.parse_value_ir(Parser.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:748); ```. My updated code is at https://github.com/chrisvittal/hail/commit/92d18b7c5e28db82f2502980b44d48b730d8f000,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718:4260,update,updated,4260,https://hail.is,https://github.com/hail-is/hail/pull/5435#issuecomment-467547718,1,['update'],['updated']
Deployability,"hail/python/setup.py: python_requires="">=3.6"",; hail/python/hail/backend/spark_backend.py: raise EnvironmentError('Hail with spark {} requires Python 3.6 or 3.7, found {}.{}'.format(; hail/python/requirements.txt:orjson==3.6.4; hail/python/hailtop/batch/batch.py: 3.6, 3.7, or 3.8.; hail/python/hailtop/batch/docker.py: f'Python versions other than 3.6, 3.7, or 3.8 (you are using {major_version}.{minor_version}) are not supported'); hail/python/hailtop/batch/batch_pool_executor.py: include Python 3.6 or later and must have the ``dill`` Python package; hail/python/hailtop/batch/batch_pool_executor.py: f'You must specify an image if you are using a Python version other than 3.6, 3.7, or 3.8 (you are using {version})'); hail/python/hailtop/batch/docs/cookbook/random_forest.rst:choose a suitable image for you if your Python version is 3.6, 3.7, or 3.8.; hail/python/hailtop/batch/docs/docker_resources.rst:exists to base your image on. An image that has Python preinstalled is `python:3.6-slim-stretch` and; hail/python/hailtop/batch/docs/tutorial.rst:f-strings were added to Python in version 3.6 and are denoted by the 'f' character; hail/python/hailtop/batch/backend.py: f""You must specify 'image' for Python jobs if you are using a Python version other than 3.6, 3.7, or 3.8 (you are using {version})""); docker/python-dill/push.sh:for version in 3.6 3.6-slim 3.7 3.7-slim 3.8 3.8-slim; docker/third-party/images.txt:python:3.6; docker/third-party/images.txt:python:3.6-slim; benchmark/python/setup.py: python_requires="">=3.6"",. Found a few more invocations of 3.6. I'm not sure when you want to update the `python_requires` and makes sense to leave the 3.6 docker images around for a little while, but the other places in the batch documentation and warnings seem a good thing to update now too.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11219#issuecomment-1011381290:1605,update,update,1605,https://hail.is,https://github.com/hail-is/hail/pull/11219#issuecomment-1011381290,2,['update'],['update']
Deployability,"hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello""],""grace"":""48h"",""recursive"":true}; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:2989,install,installed-,2989,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,2,['install'],['installed-']
Deployability,hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:4540,release,release,4540,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"hailtop/config/__init__.py; +++ b/hail/python/hailtop/config/__init__.py; @@ -1,5 +1,6 @@; -from .deploy_config import get_deploy_config; +from .deploy_config import HAIL_CONFIG_DIR, get_deploy_config; ; __all__ = [; + 'HAIL_CONFIG_DIR',; 'get_deploy_config'; ]; diff --git a/hail/python/hailtop/config/deploy_config.py b/hail/python/hailtop/config/deploy_config.py; index 627d1792c..7d2eeeca0 100644; --- a/hail/python/hailtop/config/deploy_config.py; +++ b/hail/python/hailtop/config/deploy_config.py; @@ -4,6 +4,8 @@ import logging; from aiohttp import web; ; log = logging.getLogger('gear'); +HAIL_CONFIG_DIR = os.path.join(os.environ.get('XDG_CONFIG_HOME', os.path.expanduser('~/.config')),; + 'hail'); ; ; class DeployConfig:; @@ -15,7 +17,7 @@ class DeployConfig:; def from_config_file(config_file=None):; if not config_file:; config_file = os.environ.get(; - 'HAIL_DEPLOY_CONFIG_FILE', os.path.expanduser('~/.hail/deploy-config.json')); + 'HAIL_DEPLOY_CONFIG_FILE', os.path.join(HAIL_CONFIG_DIR, 'deploy-config.json')); if os.path.isfile(config_file):; with open(config_file, 'r') as f:; config = json.loads(f.read()); diff --git a/hail/python/hailtop/hailctl/auth/login.py b/hail/python/hailtop/hailctl/auth/login.py; index 343de7bda..e740f7b3d 100644; --- a/hail/python/hailtop/hailctl/auth/login.py; +++ b/hail/python/hailtop/hailctl/auth/login.py; @@ -5,7 +5,7 @@ import webbrowser; import aiohttp; from aiohttp import web; ; -from hailtop.config import get_deploy_config; +from hailtop.config import HAIL_CONFIG_DIR, get_deploy_config; from hailtop.auth import get_tokens, namespace_auth_headers; ; ; @@ -77,9 +77,8 @@ Opening in your browser.; ; tokens = get_tokens(); tokens[auth_ns] = token; - dot_hail_dir = os.path.expanduser('~/.hail'); - if not os.path.exists(dot_hail_dir):; - os.mkdir(dot_hail_dir, mode=0o700); + if not os.path.exists(HAIL_CONFIG_DIR):; + os.makedirs(HAIL_CONFIG_DIR, mode=0o700); tokens.write(); ; if auth_ns == 'default':; diff --git a/hail/python/hailtop/hai",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902:3225,deploy,deploy-config,3225,https://hail.is,https://github.com/hail-is/hail/pull/7125#issuecomment-535602902,1,['deploy'],['deploy-config']
Deployability,hand deploy successful,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8523#issuecomment-611792189:5,deploy,deploy,5,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611792189,1,['deploy'],['deploy']
Deployability,hand deploying,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5681#issuecomment-475979215:5,deploy,deploying,5,https://hail.is,https://github.com/hail-is/hail/pull/5681#issuecomment-475979215,4,['deploy'],['deploying']
Deployability,hand deploying batch.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9728#issuecomment-730648821:5,deploy,deploying,5,https://hail.is,https://github.com/hail-is/hail/pull/9728#issuecomment-730648821,1,['deploy'],['deploying']
Deployability,hand deploying latest version,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8524#issuecomment-611796466:5,deploy,deploying,5,https://hail.is,https://github.com/hail-is/hail/pull/8524#issuecomment-611796466,1,['deploy'],['deploying']
Deployability,hand deploying now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8500#issuecomment-611043999:5,deploy,deploying,5,https://hail.is,https://github.com/hail-is/hail/pull/8500#issuecomment-611043999,2,['deploy'],['deploying']
Deployability,hand deploying the last working master that is:; ```; * | | | | 49ec05df2 - (7 hours ago) [query] Throw a validation error for queries that read/write same path (#8327) - Tim Poterba (HEAD); ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434:5,deploy,deploying,5,https://hail.is,https://github.com/hail-is/hail/pull/8523#issuecomment-611791434,1,['deploy'],['deploying']
Deployability,"hat seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the one our deploy expects. ### mTLS. This PR will land. Things will break because the unmanaged services (router-resolver, gateway, internal-gateway) do not speak TLS. I'll manually deploy them. The default namespace and new PR namespaces should now function properly. Developers will need to redeploy from master. With this in place, I will make another PR with two main changes:; - enable client verification, and; - modify create_certs.py to load the unmanaged certificates from `default` rather than the local namespace.; That PR should pass all the tests (batch pods will speak TLS to internal-gateway; internal-gateway will speak TLS to PR batch using a client certificate PR batch trusts; etc.). Merge that PR. Everything will function correctly; however, the unmanaged services will not verify client certificates. I manually deploy and now everyone is verifying client certificates.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:3297,deploy,deploy,3297,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,3,['deploy'],['deploy']
Deployability,"he Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00096.g.vcf.gz',; 'gs://user-nrru16jaxrwmnzkv5f35xfibg/HG00268.g.vcf.gz']; # ...; ## Works fine; print(vcfs); ```. ### TODO:; Manually replicate on a Dataproc cluster. Currently working on this, have a Java gateway closed error during hl.init(), which could be caused by a misspecified JAVA_HOME. So at the moment, I either believe it's either a permission issue, or some Dataproc configuration issue. If you have suggestions, I'd love to hear them. cc @tpoterba",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:3290,install,install,3290,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,2,"['configurat', 'install']","['configuration', 'install']"
Deployability,"he StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:5312,CONFIGURAT,CONFIGURATION,5312,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIGURAT'],['CONFIGURATION']
Deployability,he functionality in these examples:. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \ ; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE= \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for varname in '$arguments'; + '[' -z origin ']'; + echo REMOTE=origin; REMOTE=origin; + for varname in '$arguments'; + '[' -z /path/to/the.whl ']'; + echo WHEEL=/path/to/the.whl; WHEEL=/path/to/the.whl; + for varname in '$argu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:1158,release,release,1158,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,"he/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 19776612}},; 'end': {'locus': {'contig': 'chr20', 'position': 21144633}},; 'includeStart': True,; 'includeEnd': True},; ]; parts_str = json.dumps(parts); vcfs = hl.import_vcfs(gvcfs, parts_str). ## Works fine; print(vcfs); ```; 2) Docker install based on Dockerfile.hail-build (built on top of the hail base image). This does use the gcs connector, and some sa key that has access to the bucket I specified. ```python; gvcfs = ['gs://",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:2470,install,install,2470,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['install'],['install']
Deployability,heh https://forge.rust-lang.org/release/rollups.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11482#issuecomment-1059287503:32,release,release,32,https://hail.is,https://github.com/hail-is/hail/pull/11482#issuecomment-1059287503,1,['release'],['release']
Deployability,"hi tpoterba.; I installed the new version of hail, following the instructions:; $ git clone https://github.com/broadinstitute/hail.git; $ cd hail; $ ./gradlew shadowJar; And all the commands went well.; **********************; $ ./gradlew shadowJar. ........ 24 warnings found; :processResources; :classes; :shadowJar. BUILD SUCCESSFUL; **********************; But when I tried to import hail, it went wrong; [root@tele-3 hail]# python; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> from hail import *; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/opt/Software/hail/python/hail/__init__.py"", line 3, in <module>; from hail.context import HailContext; File ""/opt/Software/hail/python/hail/context.py"", line 3, in <module>; from hail.typecheck import *; File ""/opt/Software/hail/python/hail/typecheck/__init__.py"", line 1, in <module>; from check import *; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 1, in <module>; from decorator import decorator, getargspec; ImportError: cannot import name getargspec. How can I fix it?. Thanks",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1818#issuecomment-302037736:16,install,installed,16,https://hail.is,https://github.com/hail-is/hail/issues/1818#issuecomment-302037736,1,['install'],['installed']
Deployability,hmm it seems to have finally updated,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-418778114:29,update,updated,29,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-418778114,1,['update'],['updated']
Deployability,"hon/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9;; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi;; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz;; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda;; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0;; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx;; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt;; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d;; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript; /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19}; --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747}; --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8}; --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20}; --sp",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:12617,pipeline,pipeline,12617,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,how did you install Hail? `pip install hail` or from source?,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8423#issuecomment-607431839:12,install,install,12,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607431839,2,['install'],['install']
Deployability,"http library for simple responses, and much closer for anything involving database calls). Sanic also uses httptools and uvloop, but has more stuff.. so yeah maybe a bit slower than Starlette, or not, but the diff will probably be small. Regarding the benchmark you linked, it is benchmarking the power of sleep. There is something deeply wrong with their results. Sanic has 1800 timeouts, vs 200 for aiohttp, and 3x the connection errors. Fine, so Sanic is super slow. But look at their non-db tests. Sanic is >2x as fast, 0 timeouts. They aren't using anything Sanic specific to query the database, and both use the same event loop. Adding asyncio Postgres to two programs that fundamentally differ mainly in how the handle http requests and responses, shows the one that is faster at http requests/responses (Sanic) becoming much slower, and in fact reversing its relationship to Aiohttp. This is strange to say the least. I was really curious about this, so I ran the bench. First, I upgraded Sanic to a recent version. Then I ran their test. In short, their results were not what I found. Sanic is 50% faster, and the timeouts are what you'd expect. 26 timeouts for Sanic, 45 for aiohttp. Sanic Run 1:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 640.64ms 947.31ms 7.97s 85.89%; Req/Sec 385.62 137.55 2.32k 77.21%; 274143 requests in 1.00m, 41.70MB read; Socket errors: connect 0, read 2072, write 0, timeout 26; Requests/sec: 4563.11; Transfer/sec: 710.67KB. Sanic Run 2:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 615.91ms 878.25ms 7.86s 85.85%; Req/Sec 391.30 118.76 1.61k 72.83%; 278943 requests in 1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 72",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:3625,upgrade,upgraded,3625,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['upgrade'],['upgraded']
Deployability,https://dev.hail.is/t/rfc-batch-pipeline-ci-roadmap/136/5,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6569#issuecomment-510158677:32,pipeline,pipeline-ci-roadmap,32,https://hail.is,https://github.com/hail-is/hail/pull/6569#issuecomment-510158677,1,['pipeline'],['pipeline-ci-roadmap']
Deployability,https://discuss.hail.is/t/on-mac-os-x-how-do-i-install-and-use-java-8-if-i-already-have-a-different-version-of-java-installed/831/2,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6299#issuecomment-516552799:47,install,install-and-use-java-,47,https://hail.is,https://github.com/hail-is/hail/issues/6299#issuecomment-516552799,2,['install'],"['install-and-use-java-', 'installed']"
Deployability,https://github.com/broadinstitute/install-gcs-connector/pull/6 is merged.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14158#issuecomment-1932836017:34,install,install-gcs-connector,34,https://hail.is,https://github.com/hail-is/hail/pull/14158#issuecomment-1932836017,1,['install'],['install-gcs-connector']
Deployability,"huh, interesting. Thanks for update!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3053#issuecomment-444175470:29,update,update,29,https://hail.is,https://github.com/hail-is/hail/issues/3053#issuecomment-444175470,1,['update'],['update']
Deployability,"ials. Note, to do this I copied the production oauth2 key to my namespace. We shouldn't do this in general and should create a shared dev oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev de",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1600,deploy,deploy,1600,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,1,['deploy'],['deploy']
Deployability,"igger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches ta",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:1393,update,update,1393,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,1,['update'],['update']
Deployability,"ill get output like this:; > ; > ```; > hello world; > ```; >; > If you like, you can also use target-specific variables (see [Target-specific Variable Values](https://www.gnu.org/software/make/manual/html_node/Target_002dspecific.html)) to obtain a tighter correspondence between the variable and the recipe that uses it. It seems to me like there are not any great choices. Putting the JSON into a Make variable seems too magical and likely to confuse a newbie editing this file. Using escaped double quotes is less legible than literal JSON. Putting the whole JSON array on one line is quite long. I guess we can go with double quotes for now. I tested on Make 3.81 and Make 4.4.1. The first EDIT and the original comment follow for context. ---. EDIT: Nope, I still appear to be wrong. Hold on. ---. I have bash 3.2.57; ```; (base) dking@wm28c-761 /tmp % make print-shell; /bin/sh; (base) dking@wm28c-761 /tmp % /bin/sh --version; GNU bash, version 3.2.57(1)-release (arm64-apple-darwin22); Copyright (C) 2007 Free Software Foundation, Inc.; ```. Looks like this was an intentionally backwards incompatible change [in Make 4.0](https://git.savannah.gnu.org/cgit/make.git/tree/NEWS?h=4.0&id=52191d9d613819a77a321ad6c3ab16e1bc73c381#n18) which removed the POSIX-compatible behavior on which our Makefile relies:; ```; * WARNING: Backward-incompatibility!; If .POSIX is specified, then make adheres to the POSIX backslash/newline; handling requirements, which introduces the following changes to the; standard backslash/newline handling in non-recipe lines:; * Any trailing space before the backslash is preserved; * Each backslash/newline (plus subsequent whitespace) is converted to a; single space; ```. They seem to have [broken the behavior in order to fix something else](https://git.savannah.gnu.org/cgit/make.git/commit/?id=391456a) and then added a [`.POSIX`](https://git.savannah.gnu.org/cgit/make.git/commit/?id=88f1bc8) escape hatch for Makefiles that want POSIX compatibility. For the Ma",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324:2443,release,release,2443,https://hail.is,https://github.com/hail-is/hail/pull/14138#issuecomment-1894411324,1,['release'],['release']
Deployability,"in=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/foo; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. User does not give permissions to existing remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: n ; WARNING: Please verify service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com has the role ""roles/storage.objectAdmin"" or both ""roles/storage.objectViewer"" and ""roles/storage.objectCreator"" roles for bucket hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-central1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp; batch/regions=us-central1; batch/backend=service; query/backend=batch; ```. Not existing user-specified remote tmpdir:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://my-bucket/foo/bar; ERROR: You do not have sufficient permissions to get information about bucket my-bucket or it does not exist. If the bucket exists, ask a project administrator to give you the permission ""storage.buckets.get"" or assign you the StorageAdmin role in Google Cloud Storage.; Aborted.; ```. Existing remote tmpdir in wrong region:; ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucke",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:3569,CONFIGURAT,CONFIGURATION,3569,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['CONFIGURAT'],['CONFIGURATION']
Deployability,"information there (e.g. email), although for now; > I don't think we need to, as everyone who signed up for the next tutorial; > filled out a questionnaire. For the tutorial, I'll just put a password. I think for Stanley Center stuff we; should use GCP auth. > I'm getting proxy timeouts. We need an ready endpoint and something on the; > client side to poll and redirect. Actually, awesome if it doesn't poll but; > uses, say, websockets, and the server watches the pod for a notification for; > k8s (or does this and also polls, which seems to be our standard pattern). The proxy timeouts might be because I shut the whole thing down? But yeah, I; also saw timeouts if a pod can't be scheduled right away. > Should we have an auto-scaling non-preemptible pool and schedule these there?. We already have such a pool, and these pods do not tolerate the preemptible; taint, so they are forced to get scheduled on non-preemptibles. > If we do that, to optimize startup time, we should have imagePullPolicy: Never; > and then pull the image on startup and push it on update. I think `imagePullPolicy: Never` is a bad idea. If there's a bug where the image; is not present, then we get stuck. I think we should rely on k8s to pull the 5GB; jupyter image in a reasonable time period. If we cannot rely on that, we just; start up N nodes before the tutorial, ssh to each and pull the image. If; somehow the image disappears, `imagePullPolicy: IfNotPresent` ensures we just; experience a delay rather than complete interruption. > When do you reap jupyter pods? jupyterhub has a simple management console that; > lets you shut down notebooks. I just run `make clean-jobs`, but we could add a delete endpoint and a little; web page. > I don't think you can do this dynamically using headers. Blueprints seem to be; > the answer in Flask:; > https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. ah, cool. > Is there a reason you didn't make it a subdomain? I though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878:1669,update,update,1669,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431185878,1,['update'],['update']
Deployability,inputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2018,install,install,2018,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"install-dev-deps doubles the ""no change"" build time from 4s to 8s on my machine. I already thing 4s is too long. I'd rather not add that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9403#issuecomment-704552873:0,install,install-dev-deps,0,https://hail.is,https://github.com/hail-is/hail/pull/9403#issuecomment-704552873,1,['install'],['install-dev-deps']
Deployability,"install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker by hand into this worker of a *non-Hail* Dataproc cluster, it just works. ---. I also tried to replicate the failure using an initialization action, but that also just worked.; ```; gcloud dataproc clusters create dk-test2 --initialization-actions=gs://hail-common/dk-test.sh; ```; `gs://hail-common/dk-test.sh`:; ```; apt-get update; apt-get -y install \; apt-transport-https \; ca-certificates \; curl \; gnupg2 \; software-properties-common \; tabix; curl -fsSL https://download.docker.com/linux/debian/gpg | sudo apt-key add -; sudo add-apt-repository ""deb [arch=amd64] https://download.docker.com/linux/debian $(lsb_release -cs) stable""; apt-get update; apt-get install -y --allow-unauthenticated docker-ce; ```. ---. Our users often report this error. In my experience, it has happened in 2/8 test_dataproc steps that I have run myself or seen run. The more workers you have, the higher the chance at least one worker fails. As @bpblanken suggested [here](https://github.com/hail-is/hail/issues/12936#issuecomment-15",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:13223,install,install,13223,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['install'],['install']
Deployability,institute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(I,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9157,deploy,deploy,9157,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,"iobank/imp/ad.v1/vcf/ukbb.hg38.imputed.chr22.dose.vcf.gz to mt /project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr22.mt; [Stage 1:====> (59 + 24) / 741]---------------------------------------------------------------------------; FatalError Traceback (most recent call last); /restricted/projectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1420,install,install,1420,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['install'],['install']
Deployability,"ion name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of `modify --help` is included below.; - I plan to leave the `--async` option to stop, although it is pass through.; - Then there is `--files` for submit. This is passed through, but `--py-files` is needed (it is not passed through, but modified). Do I leave `--files`? I'm currently inclined to.; - Finally, I need to strip out the pass through arguments for start like I did with update. ```; $ hailctl dataproc modify --help; Usage: hailctl dataproc modify [OPTIONS] CLUSTER_NAME. Modify an existing Dataproc cluster. 'hailctl dataproc modify' works by calling 'gcloud dataproc clusters; update' and then updating the Hail version if '--update-hail-version' or '; --wheel' is specified. You can pass arguments to the 'update' command; with the option '--extra-gcloud-update-args'. The following 'gcloud dataproc clusters update' options may be useful:. --num-workers=NUM_WORKERS: New number of worker machines, minimum 2. --num-secondary-workers=NUM_SECONDARY_WORKERS: New number of secondary; (preemptible) worker machines. --graceful-decommission-timeout=GRACEFUL_DECOMMISSION_TIMEOUT: Graceful; decommissioning allows removing nodes from the cluster without; interrupting jobs in progress. Timeout specifies how long to wait for; jobs in progress to finish before forcefully removing nodes (and; potentially interrupting jobs). Timeout defaults to 0 if not set (for; forceful decommission), and the maximum allowed timeout is 1 day. At most one of the following may be set:. --expiration-time=EXPIRATION_TIME: The time when cluster will be auto-; deleted. --max-age=MAX_AGE: The lifespan of the cluster before it is auto-; deleted, such as '60m' or '1d'. --no-max-age: Cancel the cluster auto-deletion by maximum cluster age,; as ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772:2169,update,update,2169,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767112772,2,['update'],"['update', 'update-args']"
Deployability,"ion strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a no-downtime rotation is:; 1. create fresh certs; 2. create the trust lists including a principal's fresh cert and previous generation cert; 3. update all the secrets; 4. somehow ensure everyone has the latest secrets?; 5. notify all servers to refresh their certificates (nginx: send SIGHUP, aiohttp: we have to write something). We could stick a generation uuid in the secrets and keep refreshing services until the certificate uuid they read is the",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:2291,update,update,2291,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,3,"['configurat', 'update']","['configuration', 'update']"
Deployability,"istributed"" which I think is your proposal). Blocks aren't heterogenous in our current setup (last block can be short) and I think that needs to be clarified in your proposal. Slicing has similar complications. I think the blocking should specify the list of sizes of each block, which makes it closed under slicing. I agree the user interface should do implicit broadcast but in the IR it should be explicit. > If we want to support sparse tensors. I vote we punt on sparse vectors for the time being. (In the sparse case, I think sparsity of the output should be inferred by analysis rather than specified. I think statically knowing the sparsity relationship is going to a rarity.). The key point here is that Patrick's operations are incredibly general so this proposal needs to be paired with an compilation strategy that guarantees his operations are never actually realized directly (e.g. deforestation, fusing operations (e.g. broadcast) into their consumers) while also generating BLAS-level performance. I propose we plan to build against BLAS while we investigate more general codegen strategies. To that end, I've suggested we read up on a few related projects in study group: taco (hat tip Patrick), TensorComprehensions, TVM (tensor virtual machine) and Halide. Might be other relevant ones (TF/XLA?) Few questions to ask:; - Can we steal good ideas for our IR?; - Can we target their IR?; - Can we integrate this with our stack?; - What's the performance like (compile time, runtime)?. We'll need some more prosaic operators:; - constructors: from array with shape, from shape and a function of indices; - get shape.; - index: get an element from a tuple of integers. This is only allowed for local arrays.; - slice: get another ndarray from a sequence of integers, ranges, or array of indices.; - reshape. Maybe local only? Or clarified in the distributed case. I think we'll need some additional basic operations supported by numpy (e.g. concatenate, tile), but this is a good start.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120:1576,integrat,integrate,1576,https://hail.is,https://github.com/hail-is/hail/pull/5190#issuecomment-459208120,1,['integrat'],['integrate']
Deployability,"it appears that batch deploy is still broken due to the issue alex notes, I'll fix that. not sure if that's related to infinite loops.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5503#issuecomment-469326697:22,deploy,deploy,22,https://hail.is,https://github.com/hail-is/hail/pull/5503#issuecomment-469326697,1,['deploy'],['deploy']
Deployability,it will pass once the next cloud tools version is deployed.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4241#issuecomment-417700063:50,deploy,deployed,50,https://hail.is,https://github.com/hail-is/hail/pull/4241#issuecomment-417700063,1,['deploy'],['deployed']
Deployability,"ittle unusual in this flow. I still think that it is helpful to set people up with an AR and keep them from footguns, but maybe that can go in a separate command that the initial init command points to once you're done? Something along the lines of ""if you get to the point where you need to upload custom container images, you can use hailctl to set up a registry""?. Another thing that gives me a little pause is the wording around google projects. I get that you need one to create a bucket, but I think we should just make sure to steer clear of the implication that you are ""selecting a GCP project to use for Hail Batch"", because that implies some link or ownership that isn't there. But I think there's a quick fix here: for a given resource that we *are* creating for hail use, like the temp bucket, ask for the name first and then ask which project it should be created in, using the projects listed in gcloud as choices with the option to write in your own. ### Regarding number of checks; I think it'd be good to avoid warnings when possible. From looking at this I see a pattern of; 1. Ask a leading question; 2. Emit a warning if the user selects the alternative option instead of the suggested option. I think I would prefer instead to ask a leading question and in the prompt explain why the alternative option might be undesirable. Then when they make a decision just move on. On a broader note, I think we should focus on having good documentation and linking to it over having perfectly thorough ; explanations in the CLI. At some point in an interactive setup if it gets longwinded I start spamming enter, but if it was quick and at the end it said something to the effect of: ""Your current configuration could result in excess cloud cost. See the documentation <here> about common pitfalls and how to avoid them"", I might decide to read through that FAQ with a more discerning eye. This is just my opinion though, I would be curious to see if other folks disagree regarding the UX.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012:2955,configurat,configuration,2955,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1648633012,1,['configurat'],['configuration']
Deployability,ized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Ann,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9297,deploy,deploy,9297,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,just a note that we're waiting for this PR to go in for the 0.2.27 release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7479#issuecomment-553897323:67,release,release,67,https://hail.is,https://github.com/hail-is/hail/pull/7479#issuecomment-553897323,1,['release'],['release']
Deployability,"kes sense. We are importing our own copies of the GCS libraries and renaming them all to `is.hail.relocated....`. We do this so that we're not stuck with whatever version Dataproc is including. We pin our dataproc image version to `2.1.2-debian11` (see [here](https://github.com/hail-is/hail/blob/main/hail/python/hailtop/hailctl/dataproc/start.py#L147)) which [was released in January 2023](https://cloud.google.com/dataproc/docs/release-notes#January_23_2023). The latest available version of [Dataproc's Debian images](https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) is 2.1.25-debian11 which depends on GoogleCloudDataproc hadoop connector version [2.2.15](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/2.2.15) which relies on Google Cloud Storage client library version [2.22.3](https://github.com/GoogleCloudDataproc/hadoop-connectors/commit/8b79f025ef5e8231de827f4c620cd23e230c3489). I have [a PR](https://github.com/hail-is/hail/pull/13732) to upgrade us to 2.27.1 because the library broke retries in versions [2.25.0, 2.27.0). AFAICT, Google's image version page only shows the most recent five. There's no way to go back further in time. Luckily, the way back machine has [a March 2023 capture](https://web.archive.org/web/20230307225815/https://cloud.google.com/dataproc/docs/concepts/versioning/dataproc-release-2.1) which includes our version. 2.1.2-debian11 used Google Cloud Dataproc hadoop connector version [2.2.9](https://github.com/GoogleCloudDataproc/hadoop-connectors/releases/tag/v2.2.9) This version of the hadoop connector was [using some alpha version of a gRPC version of the cloud storage library](https://github.com/GoogleCloudDataproc/hadoop-connectors/blob/18f6e9f1c745e1854d76bea9362e2332898d8895/pom.xml#L96C1-L97C1). I'm not sure what's up with that. OK, here's my proposal: let's change that IMAGE_VERSION to the latest one and see if that fixes things. If that works, let's just merge and forget this happene",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645:1094,upgrade,upgrade,1094,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1738196645,1,['upgrade'],['upgrade']
Deployability,"kubectl apply -f deployment.yaml; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""apps/v1beta2, Resource=deployments"", GroupVersionKind: ""apps/v1beta2, Kind=Deployment""; Name: ""batch-deployment"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""apps/v1beta2"" ""kind"":""Deployment"" ""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""] ""name"":""batch-deployment"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""replicas"":'\x01' ""selector"":map[""matchLabels"":map[""app"":""batch""]] ""template"":map[""metadata"":map[""labels"":map[""app"":""batch"" ""hail.is/sha"":""9d4cd6d6e0a0""]] ""spec"":map[""containers"":[map[""image"":""gcr.io/broad-ctsa/batch:a8466a39326493a8d0acb9347f3f640127e7a082fb85471dc56e57c7960d62c6"" ""name"":""batch"" ""ports"":[map[""containerPort"":'\u1388']]]] ""serviceAccountName"":""batch-svc""]]]]}; from server for: ""deployment.yaml"": deployments.apps ""batch-deployment"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get deployments.apps in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Error from server (Forbidden): error when retrieving current configuration of:; Resource: ""/v1, Resource=services"", GroupVersionKind: ""/v1, Kind=Service""; Name: ""batch"", Namespace: ""batch-pods""; Object: &{map[""apiVersion"":""v1"" ""kind"":""Service"" ""metadata"":map[""labels"":map[""app"":""batch""] ""name"":""batch"" ""namespace"":""batch-pods"" ""annotations"":map[""kubectl.kubernetes.io/last-applied-configuration"":""""]] ""spec"":map[""ports"":[map[""protocol"":""TCP"" ""targetPort"":'\u1388' ""port"":'P']] ""selector"":map[""app"":""batch""]]]}; from server for: ""deployment.yaml"": services ""batch"" is forbidden: User ""system:serviceaccount:batch-pods:deploy-svc"" cannot get services in the namespace ""batch-pods"": Unknown user ""system:serviceaccount:batch-pods:deploy-svc""; Makefile:45: recipe for target 'deploy-batch' failed; make: *** [deploy-batch] Error 1; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914:1484,deploy,deployment,1484,https://hail.is,https://github.com/hail-is/hail/issues/4609#issuecomment-432377914,11,"['configurat', 'deploy']","['configuration', 'deploy-batch', 'deploy-svc', 'deployment', 'deployments']"
Deployability,l-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/ha,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5222,deploy,deploy-,5222,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"l/src/test/scala/is/hail/fs/FSSuite.scala#L339-L368. The temporary location is drawn as 62 choose 22. So, odds of collision are 3 * 10^16. ~~I can't find the referenced case analysis in Google's latest code. [It is present in this fork](https://github.com/leogamas/java-storage/blob/2af8dfd95cdebc9e4d8252b0bbe3f092844d9f2c/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java#L68-L198) from a few years ago.~~. Here's the [referenced case analysis in 2.17.1](https://github.com/googleapis/java-storage/blame/v2.17.1/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java). There seems to have been a rewrite [two months ago](https://github.com/googleapis/java-storage/blame/main/google-cloud-storage/src/main/java/com/google/cloud/storage/BlobWriteChannel.java) (here's [the main commit](https://github.com/googleapis/java-storage/commit/1b52a1053130620011515060787bada10c324c0b)). That landed in [2.25.0](https://github.com/googleapis/java-storage/releases/tag/v2.25.0) which was released in July. ```; is.hail.relocated.com.google.cloud.storage.StorageException: Unable to recover in upload.; This may be a symptom of multiple clients uploading to the same upload session. For debugging purposes:; uploadId: https://storage.googleapis.com/upload/storage/v1/b/hail-test-ezlis/o?name=fs-suite-tmp-2LzGioRNy6RqIS2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 8388608; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 	at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 	at is.hail.relocated.com.google.cloud.storage.BlobWri",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:1185,release,releases,1185,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['release'],['releases']
Deployability,leinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started servi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8814,install,install,8814,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"les generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-batch-jigold-oxmmp/bar/foo; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-batch-jigold-oxmmp? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-batch-jigold-oxmmp.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-jigold-test-multi-regional.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-we",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:5564,configurat,configuration,5564,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,1,['configurat'],['configuration']
Deployability,"lient_secrets_file(; - self._credentials_file, scopes=self.scopes, state=flow_dict['state']; + self._credentials_file, scopes=GoogleFlow.scopes, state=flow_dict['state']; ); flow.redirect_uri = flow_dict['callback_uri']; flow.fetch_token(code=request.query['code']); @@ -71,17 +95,56 @@ class GoogleFlow(Flow):; email = token['email']; return FlowResult(email, email, token); ; + @staticmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_config(oauth2_client, GoogleFlow.scopes); + credentials = flow.run_local_server(); + return {; + 'client_id': credentials.client_id,; + 'client_secret': credentials.client_secret,; + 'refresh_token': credentials.refresh_token,; + 'type': 'authorized_user',; + }; +; +; + @staticmethod; + async def get_identity_uid_from_access_token(session: httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + oauth2_client_audience = oauth2_client['installed']['client_id']; + try:; + userinfo = await retry_transient_errors(; + session.get_read_json,; + 'https://www.googleapis.com/oauth2/v3/tokeninfo',; + params={'access_token': access_token},; + ); + if userinfo['aud'] != oauth2_client_audience and userinfo['aud'] != userinfo['sub']:; + return None; +; + email = userinfo['email']; + if email.endswith('iam.gserviceaccount.com'):; + return userinfo['sub']; + # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email.; + return email; + except httpx.ClientResponseError as e:; + if e.status in (400, 401):; + return None; + raise; +; +; +class AadJwk(TypedDict):; + kid: str; + x5c: List[str]; +; ; class AzureFlow(Flow):; + _aad_keys: Optional[List[AadJwk]] = None; +; def __init__(self, credentials_file: str):; with open(credentials_file, encoding='utf-8') as f:; data = json.loads(f.read()); ; tenant_id = data['tenant']; authority = f'https://login.microsoftonline.com/{tenant_id}'; - clie",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:3823,install,installed,3823,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,1,['install'],['installed']
Deployability,"link --bfile {input_bfile} --make-bed {subset.tmp1}'); .command(f""awk '{{ print $1, $2}}' {subset.tmp1.fam} | sort | uniq -c | awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}""; )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile=rgb_shapeit); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(); ```. ```bash; #! /usr/bash; set -ex. # define tmp directory; __TMP_DIR__=/tmp//pipeline.yG41vqpS/. # __TASK__0 write_input; cp gs://hail-jigold/random_file.txt ${__TMP_DIR__}/rsfKylng. # __TASK__1 write_input; cp gs://hail-jigold/input.bed ${__TMP_DIR__}/xJONBVn7.bed. # __TASK__2 write_input; cp gs://hail-jigold/input.bim ${__TMP_DIR__}/xJONBVn7.bim. # __TASK__3 write_input; cp gs://hail-jigold/input.fam ${__TMP_DIR__}/xJONBVn7.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=${__TMP_DIR__}/xJONBVn7; __RESOURCE_GROUP__1=${__TMP_DIR__}/TB7ZUbj8; __RESOURCE__6=${__TMP_DIR__}/TB7ZUbj8.fam; __RESOURCE__10=${__TMP_DIR__}/EVeRHf7V; __RESOURCE__1=${__TMP_DIR__}/xJONBVn7.bed; __RESOURCE__2=${__TMP_DIR__}/xJONBVn7.bim; __RESOURCE__3=${__TMP_DIR__}/xJONBVn7.fam; __RESOURCE_GROUP__2=${__TMP_DIR__}/MXBQugBx; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESO",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741:2105,pipeline,pipeline,2105,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-452753741,1,['pipeline'],['pipeline']
Deployability,"logging; import urllib.parse; -from typing import ClassVar, List; +from typing import Any, Dict, List, Optional, TypedDict; ; import aiohttp.web; import google.auth.transport.requests; import google.oauth2.id_token; import google_auth_oauthlib.flow; +import jwt; import msal; ; -from gear.cloud_config import get_global_config; +from hailtop import httpx; +from hailtop.utils import retry_transient_errors; +; +log = logging.getLogger('auth'); ; ; class FlowResult:; - def __init__(self, login_id: str, email: str, token: dict):; + def __init__(self, login_id: str, email: str, refresh_token: str):; self.login_id = login_id; self.email = email; - self.token = token; + self.refresh_token = refresh_token; ; ; class Flow(abc.ABC):; @@ -35,9 +44,24 @@ class Flow(abc.ABC):; """"""Concludes the OAuth2 flow by returning the user's identity and credentials.""""""; raise NotImplementedError; ; + @staticmethod; + @abc.abstractmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + """"""Performs an OAuth2 flow for credentials installed on the user's machine.""""""; + raise NotImplementedError; +; + @staticmethod; + @abc.abstractmethod; + async def get_identity_uid_from_access_token(session: httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + """"""; + Validate a user-provided access token. If the token is valid, return the identity; + to which it belongs. If it is not valid, return None.; + """"""; + raise NotImplementedError; +; ; class GoogleFlow(Flow):; - scopes: ClassVar[List[str]] = [; + scopes = [; 'https://www.googleapis.com/auth/userinfo.profile',; 'https://www.googleapis.com/auth/userinfo.email',; 'openid',; @@ -48,7 +72,7 @@ class GoogleFlow(Flow):; ; def initiate_flow(self, redirect_uri: str) -> dict:; flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(; - self._credentials_file, scopes=self.scopes, state=None; + self._credentials_file, scopes=GoogleFlow.scopes, state=None; ); flow.redirect_uri = redire",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:1574,install,installed,1574,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,1,['install'],['installed']
Deployability,"looked into the failing tests concerning randomness, and tracked down the source of the failures:. - `ApplySeeded` is an `AbstractApplyNode`. this means the interpreter will try to ""memoize"" the function definition so as to generate it only once even if you call it in multiple places in the IR. ; - memoization is based on referential equality, so they need to be the exact same IR object in order for reuse to trigger; - the ""seeded function"" implementations used by the test suite will create a new randomness state per generated function; - the Interpret pipeline used to rewrite each ApplySeeded node to be different objects, but with your changes they are the same, so their functions get memoized, thus sharing the same state. im not entirely sure which of these was doing the wrong thing, but IMO your changes in this PR should not have introduced any problems",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727:559,pipeline,pipeline,559,https://hail.is,https://github.com/hail-is/hail/pull/7567#issuecomment-556266727,1,['pipeline'],['pipeline']
Deployability,"m info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still app",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1000,Release,Release,1000,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['Release'],['Release']
Deployability,"main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURCE_FILE__747} --GMMATmodelFile=${__RESOURCE_FILE__6} --varianceRatioFile=${__RESOURCE_FILE__8} --SAIGEOutputFile=${__RESOURCE_FILE__748} --groupFile=${__RESOURCE_FILE__20} --sparseSigmaFi",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16822,pipeline,pipeline,16822,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"me directive sets the request scheme, hostname and port that; # the server uses to identify itself. This is used when creating; # redirection URLs. In the context of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/sub",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1550,configurat,configuration,1550,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['configurat'],['configuration']
Deployability,"mplete; WARNING | 2019-06-25 12:37:07,676 | batch.py | mark_complete:495 | job (2554, 4) has pod batch-2554-job-4-main-cc8d4 which is terminated but has no timing information. {'api_version': 'v1',; 'kind': 'Pod',; 'metadata': {'annotations': None,; 'cluster_name': None,; 'creation_timestamp': datetime.datetime(2019, 6, 25, 3, 9, 4, tzinfo=tzlocal()),; 'deletion_grace_period_seconds': 30,; 'deletion_timestamp': datetime.datetime(2019, 6, 25, 12, 37, 37, tzinfo=tzlocal()),; 'finalizers': None,; 'generate_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1677,pipeline,pipeline,1677,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,6,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"my strategy here was just to undo everything added in [this commit](https://github.com/hail-is/hail/commit/12e0f497db0f3e5453f870495e48e44191b315f4), except the version upgrades, so i'm not sure if there are some changes i'm making here that are unnecessary or produce weird results as far as what all ends up in the jar or anything. from running `jar -tf` on the jar produced by the current `main` and the one produced by the commit prior to the one i'm partially reverting, it looked like the updates to the config only added things to the jar, rather than removing any, so hopefully that should be fine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792:169,upgrade,upgrades,169,https://hail.is,https://github.com/hail-is/hail/pull/13759#issuecomment-1743654792,2,"['update', 'upgrade']","['updates', 'upgrades']"
Deployability,"n issue](https://github.com/moby/moby/issues/41792) with the official Docker deb. If you uninstall docker and re-install it later, it might fail to start. The root cause is the `docker.socket` `systemd` unit failing to start because there are ""insufficient file descriptors available"". I think this is confusing verbiage. The socket's name must be `/var/run/docker.sock`. Clearly, if that filename is already in use, we cannot create a new socket at that filename. One of Google's [""Dataproc components""](https://cloud.google.com/dataproc/docs/concepts/components/overview) is Docker. I believe Google installed and then uninstalled docker in this image, thus leaving it in the broken state. For evidence of that:. <details>; <summary> find docker on a worker node of a *non-Hail* Dataproc cluster</summary>. ```; sudo find / -iname '*docker*'; ```. ```; /opt/conda/miniconda3/pkgs/dbus-1.13.6-h5008d03_3/info/recipe/patches/0004-disable-fd-limit-tests-not-supported-in-docker.patch; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/nbclassic-0.5.6-pyhb4ecaf3_1/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/pkgs/notebook-6.2.0-py38h578d9bd_0/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/nbclassic/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile; /opt/conda/miniconda3/lib/python3.8/site-packages/notebook/static/components/codemirror/mode/dockerfile/dockerfile.js; /opt/g",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:993,patch,patch,993,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['patch'],['patch']
Deployability,"n.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MODE which does not exist; ELASTIC_MAJOR_VERSION is set ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3427,deploy,deploy,3427,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"nces=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authenticat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1739,install,install,1739,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"nerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_stable_binary-amd64_Packages; ```. </details>. There is a `/run/docker.sock` but notice it is not `/var/run/...`. However, if I install Docker ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:12259,install,install,12259,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['install'],['install']
Deployability,nfun$1.apply(Main.scala:144); at org.broadinstitute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:8972,deploy,deploy,8972,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,"ng -; Let's compare with & without Hail install; I don't know where to find `load-spark-env.sh` so I only print the env once. ## Without Hail. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; /usr/bin/spark-shell; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-shell; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; /usr/bin/spark-submit; -rwxr-xr-x 1 root root 141 Jul 19 15:54 /usr/bin/spark-submit; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /___/ .__/\_,_/_/ /_/\_\ version 3.3.2-amzn-0.1; /_/; ; Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.21; Branch ; Compiled by user release on 2023-07-19T15:12:33Z; Revision ; Url ; Type --help for more information.; ```. ```sh; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; /usr/bin/spark-class; -rwxr-xr-x 1 root root 140 Jul 19 15:54 /usr/bin/spark-class; SPARK_SCALA_VERSION=; ```; <details><summary>>>>>>>>>>> before load-spark-env.sh <<<<<<<<<</summary>; <p>; ```sh; XDG_SESSION_ID=38; HOSTNAME=ip-192-168-96-172; TERM=xterm-256color; SHELL=/bin/bash; HISTSIZE=1000; SSH_CLIENT=103.37.196.84 57805 22; QTDIR=/usr/lib64/qt-3.3; QTINC=/usr/lib64/qt-3.3/include; SSH_TTY=/dev/pts/0; USER=hadoop; LS_COLORS=rs=0:di=38;5;27:ln=38;5;51:mh=44;38;5;15:pi=40;38;5;11:so=38;5;13:do=38;5;5:bd=48;5;232;38;5;11:cd=48;5;232;38;5;3:or=48;5;232;38;5;9:mi=05;48;5;232;38;5;15:su=48;5;196;38;5;15:sg=48;5;11;38;5;16:ca=48;5;196;38;5;226:tw=48;5;10;38;5;16:ow=48;5;10;38;5;21:st=48;5;21;38;5;15:ex=38;5;34:*.tar=38;5;9:*.tgz=38;5;9:*.arc=38;5;9:*.arj=38;5;9:*.taz=38;5;9:*.lha=38;5;9:*.lz4=38;5;9:*.lzh=38;5;9:*.lzma=38;5;9:*.tlz=38;5;9:*.txz=38;5;9:*.tzo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045:980,release,release,980,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1772153045,1,['release'],['release']
Deployability,"ng Unhealthy 3s (x7 over 1m) kubelet, gke-vdc-non-preemptible-pool-0106a51b-pgxq Readiness probe failed: Get http://10.32.12.42:8888/instance/notebook-worker-service-j7bp9/login: dial tcp 10.32.12.42:8888: getsockopt: connection refused. Regarding binding; he should also be bound to localhost. The service definition has 80 forwarded to an internal 8888. Here is his worker Dockerfile (no cmd starting the notebook server, unless implemented by one of the installed extensions automatically). ```; FROM jupyter/scipy-notebook; MAINTAINER Hail Team <hail@broadinstitute.org>. USER root; RUN apt-get update && apt-get install -y \; openjdk-8-jre-headless \; && rm -rf /var/lib/apt/lists/*; USER jovyan. RUN pip install --no-cache-dir \; 'jupyter-spark<0.5' \; hail==0.2.8 \; jupyter_contrib_nbextensions \; && \; jupyter serverextension enable --user --py jupyter_spark && \; jupyter nbextension install --user --py jupyter_spark && \; jupyter contrib nbextension install --user && \; jupyter nbextension enable --user --py jupyter_spark && \; jupyter nbextension enable --user --py widgetsnbextension && \; jupyter nbextension enable --user collapsible_headings/main && \; jupyter nbextension enable --user move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-service-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; **labels}),; spec=service_spec); svc = k8s.create_namespaced_service(; 'default',; service_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:2002,install,install,2002,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,2,['install'],['install']
Deployability,"nginx should need the services (i.e. domain names) to exist, not the deployments.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10736#issuecomment-891282462:69,deploy,deployments,69,https://hail.is,https://github.com/hail-is/hail/pull/10736#issuecomment-891282462,1,['deploy'],['deployments']
Deployability,nit__.py'; adding 'hailtop/hailctl/dev/ci_client.py'; adding 'hailtop/hailctl/dev/cli.py'; adding 'hailtop/hailctl/dev/config.py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00;,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29120,install,install,29120,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['install'],['install']
Deployability,"none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hailctl/dataproc/0.2.129; gcloud storage objects update -r gs://hail-common/hailctl/dataproc/0.2.129 --add-acl-grant=entity=AllUsers,role=READER; gcloud storage objects update ""gs://hail-common/hailctl/dataproc/0.2.129/*"" --temporary-hold; ```. Note the following:; - mill is not invoked; - deploy.yaml is re-made with the correct uris; - the wheel is built",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1546,deploy,deploy,1546,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,7,"['deploy', 'update']","['deploy', 'update']"
Deployability,"not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on teensy data.; - Integrate RV with C/C++, so we can transmit data much more efficiently between Python and Java.; - Rethink the expr language function registry, because many functions there can be implemented in terms of others in Python.; - add back in de novo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554:1571,integrat,integrative,1571,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554,2,"['Integrat', 'integrat']","['Integrate', 'integrative']"
Deployability,"o to install all of my python packages (scipy/uvloop/etc). ```; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1062,install,installer,1062,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['installer']
Deployability,"oblems outlined above. The tradeoff made is that we have to refer to the object (ex: `subset`). So instead of thinking of writing the command as a template where `inputs` specifies how to substitute into the template, writing commands in this interface is the same as using Python to generate the correct string to output. I'm not sure that I like this better. One of the advantages of writing commands as templates is they are reusable. In the latter case, the strings are reusable if they are written as `.format()` templates instead of `f-strings`. So maybe it's approximately the same, but there's an extra step to define the inputs to `format`. I tried hacking the Python AST to not have to refer to the object, but I think it's going to be difficult to get the AST parsing exactly right and not have too many implicit rules within our language. I also considered writing a DSL, but found that it's hard to specify the part with the `shapeit_output` in a DSL. ```python3; from pyapi import Pipeline, resource_group; p = Pipeline(). input_bfile = p.new_resource_group(bed=""gs://jigold/input_root.bed"",; bim=""gs://jigold/input_root.bim"",; fam=""gs://jigold/input_root.fam""). def bfile(root):; return resource_group(root, lambda x: {""bed"": x + "".bed"", ""bim"": x + "".bim"", ""fam"": x + "".fam""}). subset = (p.new_task(); .label('subset')); subset = subset; .command(f'plink --bfile {input_bfile} --make-bed {bfile(subset.tmp1)}'); .command(""awk '{print $1, $2}'"" +; subset.tmp1.fam +; "" | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > "" +; subset.tmp2); .command(f""plink --bfile {input_bfile} --remove {subset.tmp2} --make-bed {bfile(subset.ofile)}"")). def shapeit_output(root):; return resource_group(root, lambda x: {""haps"": x + "".haps"", ""log"": x + "".log""}). for contig in [str(x) for x in range(1, 4)]:; shapeit = (p.new_task(); 		.label('shapeit')); shapeit = (shapeit; 		.command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit_output(shapeit.ofile)}')). merger = (p.new_t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662:1095,Pipeline,Pipeline,1095,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-451295662,2,['Pipeline'],['Pipeline']
Deployability,ocker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$arguments'; + '[' -z 0.2.123-abcdef123 ']'; + echo HAIL_VERSION=0.2.123-abcdef123; HAIL_VERSION=0.2.123-abcdef123; + for varname in '$arguments'; + '[' -z abcdef123 ']'; + echo GIT_VERSION=abcdef123; GIT_VERSION=abcdef123; + for v,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6389,deploy,deploy-,6389,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,ocker_puller_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_pusher_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/docker_appender_.py; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_appender_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_puller_.cpython-39.pyc; /usr/lib/google-cloud-sdk/lib/third_party/containerregistry/tools/__pycache__/docker_pusher_.cpython-39.pyc; /usr/local/share/google/dataproc/npd-config/docker-monitor-counter.json; /usr/local/share/google/dataproc/npd-config/docker-monitor.json; /usr/local/share/google/dataproc/npd-config/health-checker-docker.json; /usr/local/share/google/dataproc/npd-config/docker-monitor-filelog.json; /usr/local/share/google/dataproc/bdutil/fluentd/container_logging/plugin/test/Dockerfile; /usr/local/share/google/dataproc/bdutil/components/initialize/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/post-install/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/activate/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/components/shared/docker.sh; /usr/local/share/google/dataproc/bdutil/components/pre-uninstall/docker-ce.sh; /usr/local/share/google/dataproc/bdutil/configure_docker.sh; /run/docker.sock; /tmp/dataproc/uninstall/docker-ce; /tmp/dataproc/components/uninstall/docker-ce.running; /tmp/dataproc/components/uninstall/docker-ce.done; /tmp/dataproc/components/pre-uninstall/docker-ce.running; /tmp/dataproc/components/pre-uninstall/docker-ce.done; /etc/apt/preferences.d/docker-ce.pref; /etc/apt/preferences.d/docker-ce-cli.pref; /etc/apt/sources.list.d/docker.list; /var/lib/apt/lists/download.docker.com_linux_debian_dists_buster_InRelease; /var/lib/apt/lists/download.docker.com_linux_debian_dists_,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751:12106,install,install,12106,https://hail.is,https://github.com/hail-is/hail/issues/12936#issuecomment-1709120751,1,['install'],['install']
Deployability,"ogle Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the driver was killed by the system.; Let's focus on the driver machines. In Run A, we used an n1-highmem-8 which is advertised to have 52GiB (53248 MiB). In Run B, we used an n1-highmem-16 which is advertised to have 104GiB (106,496 MiB). hailctl sets the JVM max heap size to 80% of the advertised RAM, so 42598 MiB (see hailctl's --master-memory-fraction). In Run A (the only run for which we have syslogs), based on the driver's syslog, before Spark starts, the system has already allocated 8500 MiB to Linux/Google/Dataproc daemons. Moreover, the actual RAM of the system (as reported by the earlyoom daemon) is 52223 MiB (51 GiB, 1GiB less than Google advertises for n1-highmem-8). Assuming these daemons never release their memory, all our user code must fit in 43723 MiB. Since the JVM's max heap is 42598 MiB, Python (and indeed, anything else on the system) is limited to allocating 1125 MiB. I assume that an n1-highmem-16 uses the same amount of memory for system daemons, so I'd expect just over ten GiB that is used neither by system daemons nor the JVM. Assuming that's right, I can't explain why the oomkiller killed the JVM in Run B.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:2709,release,release,2709,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,1,['release'],['release']
Deployability,"oh, nice! Is this the issue we talked about that one time a few months ago?. I would love to be able to sort by ""recently updated"" accurately again!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10723#issuecomment-893571812:122,update,updated,122,https://hail.is,https://github.com/hail-is/hail/pull/10723#issuecomment-893571812,1,['update'],['updated']
Deployability,"ojectnb/ukbiobank/ad/analysis/ad.v1/vcf2mt.py in <module>; 6 mt=""/project/ukbiobank/imp/ad.v1/mt/ukbb.hg38.imputed.chr""+chr+"".mt""; 7 print(""Converting vcf ""+vcf+"" to mt ""+ mt); ----> 8 hl.import_vcf(vcf,force_bgz=True).write(mt). <decorator-gen-891> in write(self, output, overwrite, stage_locally, _codec_spec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:1705,install,install,1705,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['install'],['install']
Deployability,"ok, I've updated this PR so that billing projects can be ""open"", ""closed"", or ""deleted"". Allowed flow is `open` <--> `closed` --> `deleted`. Closed billing projects will still show up in the UI and billing reports; deleted projects show up in neither, but batches belonging to deleted billing projects will still show up in the batches table unless they're deleted. There's no endpoint here for deleting closed projects; I'll add an API endpoint in the billing project API PR so that I can run the tests properly there, but currently don't intend to add a button to the UI.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9579#issuecomment-705774492:9,update,updated,9,https://hail.is,https://github.com/hail-is/hail/pull/9579#issuecomment-705774492,1,['update'],['updated']
Deployability,"ok, updated setup.py. ready.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6214#issuecomment-497901535:4,update,updated,4,https://hail.is,https://github.com/hail-is/hail/pull/6214#issuecomment-497901535,1,['update'],['updated']
Deployability,"ok. this question about what n_jobs means for a job group is also blocking this query that's needed for the rest of the tests to pass. ```mysql; # compute global number of new ready jobs from root job group; INSERT INTO user_inst_coll_resources (user, inst_coll, token, n_ready_jobs, ready_cores_mcpu); SELECT user, inst_coll, 0, @n_ready_jobs := COALESCE(SUM(n_ready_jobs), 0), @ready_cores_mcpu := COALESCE(SUM(ready_cores_mcpu), 0); FROM job_groups_inst_coll_staging; JOIN batches ON batches.id = job_groups_inst_coll_staging.batch_id; WHERE batch_id = in_batch_id AND update_id = in_update_id AND job_group_id = 0; GROUP BY `user`, inst_coll; ON DUPLICATE KEY UPDATE; n_ready_jobs = n_ready_jobs + @n_ready_jobs,; ready_cores_mcpu = ready_cores_mcpu + @ready_cores_mcpu;; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1932776114:664,UPDATE,UPDATE,664,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1932776114,1,['UPDATE'],['UPDATE']
Deployability,"okay, so this makes no sense to me, and i don't understand gradle at all really, but i tried reproducing the issue with each recent release until i found the one where it started presenting (0.2.123), then tried it on every commit in between the previous release and that one, and found that the issue started presenting after https://github.com/hail-is/hail/pull/13551 merged. i tried reverting that commit on the current `main` and confirmed the issue stopped showing up. i also tried downgrading just the `google-cloud-storage` version back to 2.17.1, since that was bumped in that commit, but the issue still presented.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13690#issuecomment-1737808187:132,release,release,132,https://hail.is,https://github.com/hail-is/hail/issues/13690#issuecomment-1737808187,2,['release'],['release']
Deployability,"om the `copy_files` build step. Because I wanted `service_base_image` to depend on `copy_files`, I had to move the whole step after `copy_files`. This is a limitation in build.yaml: a step must appear *after* steps on which it depends. I also had to move `check_services` for the same reason: it depends on `service_base_image`. File dependencies in build.yaml work like this:; 1. For runImage steps, you can only copy out-of or copy into `/io` (the reasoning is a bit complicated and somewhat historical).; 2. For buildImage steps, you can copy out-of or copy into `/`; 3. the `to` of an `output` specifies a file path in a ""filesystem"" that another step can access if it `dependsOn` the outputting step; 4. the `from` of an `input` specifies a file path in the aforementioned ""filesystem""; the filesystem contains all `outputs` from steps in the inputting step's `dependsOn` clause. We also have a `docker/Makefile` which is an emergency manual build system. I update that so that `hail_version` appears in the root of the docker context. The `service-base` uses the entire repository as its docker context, so I place hail_version at the root of the repository. I moved the `version` function from `hailtop.hailctl` into `hailtop`. It seems broadly useful and isn't specific to hailctl in anyway. Your concern about loading from pkg_resources repeated seems well-founded, so I went ahead and loaded the hail_version at package import time. This seems likely to ensure we learn about a missing hail_version file as early as possible (presumably at service start-time). This also means all hailtop installs need a hail_version file. I only found two other places that use hailtop. One of them was a completely unused Dockerfile. I deleted that (`Dockerfile.hailtop`). The other was Dockerfile.ci-test, which I updated to copy the hail_version file just like service-base. https://github.com/hail-is/hail/compare/main...danking:add-version-endpoint. Do you want to cherry-pick that change onto your a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401:1389,update,update,1389,https://hail.is,https://github.com/hail-is/hail/pull/10085#issuecomment-789279401,1,['update'],['update']
Deployability,"on had a different error (#13721 to be exact).; 3. In run 3, two partitions had this error. After my fix [2] for this issues bug, the #13721 bug became super common! I saw it 50 times in my first run:; ```; Caused by: is.hail.relocated.com.google.cloud.storage.StorageException: Missing Range header in response; 	|> PUT https://storage.googleapis.com/upload/storage/v1/b/aou_tmp/o?name=tmp/hail/icullIwHC8dQXtq8JU2uDW/aggregate_intermediates/-ntpjdAQ9sKaR8lK26cV0p5790a4d87-9035-41ae-afc6-326f710d9a89&uploadType=resumable&upload_id=ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	|> content-range: bytes */*; 	| ; 	|< HTTP/1.1 308 Resume Incomplete; 	|< content-length: 0; 	|< content-type: text/plain; charset=utf-8; 	|< x-guploader-uploadid: ADPycdtl5JSqwvftT4W190_-ueC032I_oZcwLAlVVMFkqp06W4eY8b-XMwf8DeT7If9I7uIgmI_PLCuFsExsT0aEh2b4FrHtAiUktumQbvgl1U0icw; 	| ; ```. Luckily, that one is actually trivial to fix, we just need to [update to the latest GCS client; library](https://github.com/hail-is/hail/issues/13721#issuecomment-1737924344). # Test Code. ```python3; import hail as hl; import gnomad.utils.sparse_mt. tmp_dir = 'gs://danking/tmp/'; vds_file = 'gs://neale-bge/bge-wave-1.vds'; out = 'gs://danking/foo.vcf.bgz'. vds = hl.vds.read_vds(vds_file); mt = hl.vds.to_dense_mt(vds); t = gnomad.utils.sparse_mt.default_compute_info(mt); t = t.annotate(info=t.info.drop('AS_SB_TABLE')); t = t.annotate(info = t.info.drop(; 'AS_QUALapprox', 'AS_VarDP', 'AS_SOR', 'AC_raw', 'AC', 'AS_SB'; )); t = t.drop('AS_lowqual'). hl.methods.export_vcf(dataset = t, output = out, tabix = True). ```. # Failing Batch (in my namespace). https://internal.hail.is/dking/batch/batches/8?q=state%3Dbad. # Footnotes. [1] I was using `de009fdb89`, which I pushed as [`fixes-sans-gcs-read-fix`](https://github.com/hail-is/hail/compare/main...danking:hail:fixes-sans-gcs-read-fix).; [2] `64c4c6e248`, part of [`unblock-wenhan`](https://github.com",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184:1375,update,update,1375,https://hail.is,https://github.com/hail-is/hail/issues/13409#issuecomment-1737926184,1,['update'],['update']
Deployability,"on; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3193,deploy,deploy,3193,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"on=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723e4b' >> src/main/resources/build-info.properties; echo 'branch=HEAD' >> src/main/resources/build-info.properties; echo 'date=2023-10-19T03:09:40Z' >> src/main/resources/build-info.properties; echo 'sparkVersion=3.3.2' >> src/main/resources/build-info.properties; echo 'hailPipVersion=0.2.124' >> src/main/resources/build-info.properties; creating env/HAIL_DEBUG_MOD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3359,deploy,deploy,3359,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,"onsistencies (like #9587). A nitpick though... is there a better name for the click context attribute than ""obj""?. > hailctl no longer takes --region (for gcloud dataproc commands). I compute region in GcloudRunner by checking dataproc/region or falling back to determining the region from the zone. I error if the region and zone are incompatible (gcloud would also do this). If consistency with `gcloud dataproc` is desired, I think the opposite (determining zone from cluster region) would be preferable. `gcloud dataproc` commands take a `--region` argument. [`--zone` is an optional argument for `gcloud dataproc clusters create`](https://cloud.google.com/sdk/gcloud/reference/dataproc/clusters/create#--zone). When a cluster's zone is needed to run `gcloud compute` commands, it can be determined using `gcloud dataproc clusters describe <cluster> --format json`. `hailctl dataproc diagnose` currently does this. I believe the only reason that we currently require a zone be provided either in gcloud configuration or on the command line is to maintain backwards compatibility. `cloudtools` and earlier versions of `hailctl` had a default value for the `--zone` option of `hailctl dataproc start` (I think it was `us-central1-b`). > I stripped all gcloud pass through args from hailctl dataproc modify. There aren't any left. Invoking modify now looks like:; > ; > ```; > hailctl dataproc modify my-cluster \; > --extra-glcoud-update-args='---num-workers=2 --num-secondary-workers=100'; > ```; >; > The extra in the option name sounds a little weird since they are the only options (and the command isn't run if they aren't specified), but I'm leaving it for consistency for now. I moved the help text from the removed options into the help for the modify command itself. The output of modify --help is included below. I have mixed feelings on this one. On the one hand, `--extra-gcloud-update-args` sounds like it is extra arguments for a `gcloud update` command, which isn't a thing. On the ot",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393:1713,configurat,configuration,1713,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-767168393,1,['configurat'],['configuration']
Deployability,"oof, this is bad enough we might want to do yet another release",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6629#issuecomment-511016826:56,release,release,56,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511016826,1,['release'],['release']
Deployability,"oops I did tell Beryl we'd get #6978 in the release, though",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6971#issuecomment-527603770:44,release,release,44,https://hail.is,https://github.com/hail-is/hail/pull/6971#issuecomment-527603770,1,['release'],['release']
Deployability,"optimization?. The issue is that I started with #13475 and after your insightful comment about keeping the batches and job groups tables in sync, I realized that rather than using the batch_after_update trigger to keep the job groups and batches table states identical, we should just go ahead and directly add a double update to the job groups and batches table wherever a batches update occurs in our current code base. Unfortunately, I got stuck with the MJC trigger with these lines of code:. ```sql; UPDATE batches_n_jobs_in_complete_states; SET n_completed = (@new_n_completed := n_completed + 1),; n_cancelled = n_cancelled + (new_state = 'Cancelled'),; n_failed = n_failed + (new_state = 'Error' OR new_state = 'Failed'),; n_succeeded = n_succeeded + (new_state != 'Cancelled' AND new_state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:1258,UPDATE,UPDATE,1258,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,1,['UPDATE'],['UPDATE']
Deployability,"or.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls grou",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8452,install,install,8452,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"or.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1654,install,install,1654,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,park.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracke,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8876,install,install,8876,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"pec). /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/typecheck/check.py in wrapper(__original_func, *args, **kwargs); 558 def wrapper(__original_func, *args, **kwargs):; 559 args_, kwargs_ = check_all(__original_func, args, kwargs, checkers, is_method=is_method); --> 560 return __original_func(*args_, **kwargs_); 561; 562 return wrapper. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/matrixtable.py in write(self, output, overwrite, stage_locally, _codec_spec); 2146 """"""; 2147; -> 2148 self._jvds.write(output, overwrite, stage_locally, _codec_spec); 2149; 2150 def globals_table(self) -> Table:. /share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py in __call__(self, *args); 1131 answer = self.gateway_client.send_command(command); 1132 return_value = get_return_value(; -> 1133 answer, self.gateway_client, self.target_id, self.name); 1134; 1135 for temp_arg in temp_args:. /share/pkg/hail/2018-10-31/install/build/distributions/hail-python.zip/hail/utils/java.py in deco(*args, **kwargs); 208 raise FatalError('%s\n\nJava stack trace:\n%s\n'; 209 'Hail version: %s\n'; --> 210 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; 211 except pyspark.sql.utils.CapturedException as e:; 212 raise FatalError('%s\n\nJava stack trace:\n%s\n'. FatalError: OutOfMemoryError: Java heap space. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 76 in stage 1.0 failed 1 times, most recent failure: Lost task 76.0 in stage 1.0 (TID 77, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOfRange(Arrays.java:3664); at java.lang.String.<init>(String.java:207); at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567); at java.nio.CharBuffer.toString(CharBuffer.java:1241); at org.apache.hadoop.io.Text.decode(Text.java:412); at org.apache.hadoop.io.Text.decode(Text.java:389); at org.apache.hadoop.io.T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:2018,install,install,2018,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['install'],['install']
Deployability,"ping @chrisvittal ; There is 1 failure left, which seems difficult to replicate, and for which I have developed a (small) prior belief that it may be uncorrelated with this PR's changes. . The failure is in the Dataproc test of [cluster-tests/cluster-read-vcfs-check.py, line 143 of HadoopFS, in _fileSystem, new hadoop.fs.Path(filename).getFileSystem(conf)](https://ci2.hail.is/jobs/17044/log). This calls hadoop.fs.FileSystem.get , which in turn calls Configuration.get (instance method). ```java; // In Path.java; public FileSystem getFileSystem(Configuration conf) throws IOException {; return FileSystem.get(this.toUri(), conf);; }. // In FileSystem.java; public static FileSystem get(Configuration conf) throws IOException {; return get(getDefaultUri(conf), conf);; }. public static FileSystem get(final URI uri, final Configuration conf,; final String user) throws IOException, InterruptedException {; String ticketCachePath =; conf.get(CommonConfigurationKeys.KERBEROS_TICKET_CACHE_PATH);; UserGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configurat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:454,Configurat,Configuration,454,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,4,['Configurat'],['Configuration']
Deployability,please refer to this page for up-to-date installation instructions: https://hail.is/docs/0.2/getting_started.html,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8423#issuecomment-607438752:41,install,installation,41,https://hail.is,https://github.com/hail-is/hail/issues/8423#issuecomment-607438752,1,['install'],['installation']
Deployability,ploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5010,deploy,deploy-,5010,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,put wip to prioritize the release fixing PR,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14308#issuecomment-1948836511:26,release,release,26,https://hail.is,https://github.com/hail-is/hail/pull/14308#issuecomment-1948836511,1,['release'],['release']
Deployability,"py"", line 128, in <module>; pc_rel = hl.pc_relate(mt.GT, 0.01, k=10, statistics='kin',min_kinship=0.0883); File ""<decorator-gen-1543>"", line 2, in pc_relate; [Stage 103:> (0 + 15) / 16] File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.util",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6237,install,install,6237,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,py'; adding 'hailtop/hailctl/hdinsight/__init__.py'; adding 'hailtop/hailctl/hdinsight/cli.py'; adding 'hailtop/hailctl/hdinsight/start.py'; adding 'hailtop/hailctl/hdinsight/submit.py'; adding 'hailtop/utils/__init__.py'; adding 'hailtop/utils/filesize.py'; adding 'hailtop/utils/process.py'; adding 'hailtop/utils/rate_limiter.py'; adding 'hailtop/utils/rates.py'; adding 'hailtop/utils/rich_progress_bar.py'; adding 'hailtop/utils/serialization.py'; adding 'hailtop/utils/time.py'; adding 'hailtop/utils/utils.py'; adding 'hailtop/utils/validate/__init__.py'; adding 'hailtop/utils/validate/validate.py'; adding 'hail-0.2.124.dist-info/METADATA'; adding 'hail-0.2.124.dist-info/WHEEL'; adding 'hail-0.2.124.dist-info/entry_points.txt'; adding 'hail-0.2.124.dist-info/top_level.txt'; adding 'hail-0.2.124.dist-info/RECORD'; emoving build/bdist.linux-x86_64/wheel; python3 -m pip install 'pip-tools==6.13.0' && bash ../check_pip_requirements.sh python; Defaulting to user installation because normal site-packages is not writeable; Collecting pip-tools==6.13.0; Downloading pip_tools-6.13.0-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.2/53.2 kB 15.7 MB/s eta 0:00:00; Collecting build; Downloading build-1.0.3-py3-none-any.whl (18 kB); Collecting click>=8; Downloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading i,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:29212,install,installation,29212,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['install'],['installation']
Deployability,"py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:spa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30811,update,update,30811,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['update'],['update']
Deployability,"r, I like the greater flexibility in providing parameters. A few concerns though:; - For hailctl arguments that are also gcloud arguments (for example, `--project` to `hailctl dataproc start`), what happens if a user provides them in both places (for example, `hailctl dataproc start --project=project-a cluster-name -- --project=project-b`)? One nice attribute of the current parsing method is that it does not allow this, since in most cases the hailctl argument shadows the gcloud argument of the same name.; - It looks like this creates some inconsistency in how the same argument must be provided to different `hailctl dataproc` commands. For example, `--project` can be directly provided to `hailctl dataproc start`, but it would have to go after the `--` for `hailctl dataproc list` or `hailctl dataproc modify`. That seems likely to be surprising/annoying for users. This could be solved by moving such flags (`--project`, `--region`, and `--configuration` are the ones that immediately come to mind) to the `hailctl dataproc` group level or by adding definitions for those arguments to all `hailctl dataproc` commands.; - On the subject of `--`, a current issue with `hailctl dataproc submit` is that it does not support `--` for specifying parameters to the submitted script like `gcloud dataproc jobs submit` does. Thus, for example, you cannot currently submit a script that has a `--files` argument because `--files` will be interpreted as an argument to `hailctl dataproc submit` instead of the submitted script. It would be nice to support that behavior in `hailctl dataproc submit`. However, with this parsing approach, supporting script arguments like that might conflict with accepting pass through arguments to `gcloud dataproc jobs submit` such as `--async`, `--bucket`, etc. And more minor:; - For `hailctl dataproc start` especially, it could seem pretty arbitrary to a user which arguments go before `--` vs after. For example, `--num-worker-local-ssds` is a `hailctl dataproc s",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034:1176,configurat,configuration,1176,https://hail.is,https://github.com/hail-is/hail/pull/9842#issuecomment-757016034,1,['configurat'],['configuration']
Deployability,r.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_GRCH37_85_IMAGE HAIL_GENETICS_VEP_GRCH38_95_IMAGE WHEEL_FOR_AZURE WEBSITE_TAR'; + for varname in '$arguments'; + '[' -z 0.2.123 ']'; + echo HAIL_PIP_VERSION=0.2.123; HAIL_PIP_VERSION=0.2.123; + for varname in '$argum,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:6163,deploy,deploy-,6163,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"r; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.fo",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6777,install,install,6777,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"rGroupInformation ugi =; UserGroupInformation.getBestUGI(ticketCachePath, user);; return ugi.doAs(new PrivilegedExceptionAction<FileSystem>() {; @Override; public FileSystem run() throws IOException {; return get(uri, conf);; }; });; }; ```. For some reasons the line numbers reported in CI log don't quite match up (using either IntelliJ's goto def - which could say be the result of referencing a different copy on the system - or the [2.7.1 branch on GitHub](https://github.com/apache/hadoop/blob/branch-2.7.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java)), so I followed the parameterization. Still need to figure out why lines reported don't match, but I've seen line number differences before between that reported for the compiled binary, and the uncompiled source. Lines of evidence:; 1) The line specified in the ci log suggests that Hadoop's fileSystem.open() command fails. It appears from examining the line and source, that the Hadoop Configuration object could be null, which suggests a serialization error in HadoopFS. However, there are many others tests that by touch HadoopFS serialization, and none of them have problems. If it's not a serialization error (say the URI object that hadoop looks for is null, or CACHE is null), it would not seem PR specific. 2) On local, with or without the google storage connector, I cannot replicate the error in cluster-read-vcfs.py. Attempts to replicate:; 1) Local hail install, not using google storage connector, and reading 2 local vcfs:. ```python; gvcfs = ['./HG00096.g.vcf.gz',; './HG00268.g.vcf.gz']; hl.init(default_reference='GRCh38'); parts = [; {'start': {'locus': {'contig': 'chr20', 'position': 17821257}},; 'end': {'locus': {'contig': 'chr20', 'position': 18708366}},; 'includeStart': True,; 'includeEnd': True},; {'start': {'locus': {'contig': 'chr20', 'position': 18708367}},; 'end': {'locus': {'contig': 'chr20', 'position': 19776611}},; 'includeStart': True,; 'includeEnd': True},; {'",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803:1991,Configurat,Configuration,1991,https://hail.is,https://github.com/hail-is/hail/pull/6083#issuecomment-494037803,1,['Configurat'],['Configuration']
Deployability,"rc.zip; /share/pkg/spark/1.3.1/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.4.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.5.0/install/python/lib/py4j-0.8.2.1-src.zip; /share/pkg/spark/1.6.0/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/1.6.1/install/python/lib/py4j-0.9-src.zip; /share/pkg/spark/2.0.0/install/python/lib/py4j-0.10.1-src.zip; /share/pkg/spark/2.1.0/install/python/lib/py4j-0.10.4-src.zip. So I got the following error since I was using Spark 2.1.0 which has; py4j-0.10.4-src.zip instead of py4j-0.10.3-src.zip in the alias. >>> import pyhail; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File; ""/restricted/projectnb/genpro/github/hail/python/pyhail/__init__.py"", line; 1, in <module>; from pyhail.context import HailContext; File ""/restricted/projectnb/genpro/github/hail/python/pyhail/context.py"",; line 1, in <module>; from pyspark.java_gateway import launch_gateway; File ""/share/pkg/spark/2.1.0/install/python/pyspark/__init__.py"", line; 44, in <module>; from pyspark.context import SparkContext; File ""/share/pkg/spark/2.1.0/install/python/pyspark/context.py"", line 29,; in <module>; from py4j.protocol import Py4JError; ImportError: No module named py4j.protocol. The following will fix the issue. Essentially it sets PYJ4 to the py4j zip; file found in SPARK_HOME. Then uses that to set the PYTHONPATH. *PYJ4*=`ls $SPARK_HOME/python/lib/py4j*.zip`; alias hail=""PYTHONPATH=$SPARK_HOME/python:*$PYJ4*:$HAIL_HOME/python; SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python"". On Thu, Jan 12, 2017 at 11:21 PM, cseed <notifications@github.com> wrote:. > We now have a Getting Started the python API:; >; > https://hail.is/pyhail/getting_started.html; >; > Please give it a spin and let us know if you run into any problems. The; > documentation for the python API is nearly complete, but the Tutorial and; > General Reference section are still being ported to python and will need; > another week or ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799:1288,install,install,1288,https://hail.is,https://github.com/hail-is/hail/issues/1218#issuecomment-272537799,1,['install'],['install']
Deployability,"re: `pip install -e`, yeah, I should probably understand how all this python packaging stuff works so I can make an informed decision about that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5130#issuecomment-454467812:9,install,install,9,https://hail.is,https://github.com/hail-is/hail/pull/5130#issuecomment-454467812,1,['install'],['install']
Deployability,"relates to this (which we need to update to Python, I made a separate issue):; http://discuss.hail.is/t/save-pcs-for-projection/46. There are several issues requesting more flexible PCA at high abstraction level. We should make a game plan.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/442#issuecomment-279578856:34,update,update,34,https://hail.is,https://github.com/hail-is/hail/issues/442#issuecomment-279578856,1,['update'],['update']
Deployability,replaced by https://github.com/hail-is/hail/pull/14084; everything else is already updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14024#issuecomment-1846221998:83,update,updated,83,https://hail.is,https://github.com/hail-is/hail/pull/14024#issuecomment-1846221998,1,['update'],['updated']
Deployability,"res 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". spark-submit\; --executor-cores 4\; --executor-memory 40G\; --driver-memory 10g\; --driver-cores 2\; --num-executors 10\; --conf spark.yarn.appMasterEnv.LD_LIBRARY_PATH=$LD_LIBRARY_PATH\; --conf spark.yarn.appMasterEnv.PYTHONPATH=$PYTHONPATH\; --conf spark.yarn.appMasterEnv.PATH=$PATH\; --jars $HAIL_HOME/build/libs/hail-all-spark.jar \; --master yarn\; --deploy-mode client \; --conf spark.driver.memory=5G\; --conf spark.executor.memory=30G\; --conf spark.driver.extraClassPath=\""$HAIL_HOME/build/libs/hail-all-spark.jar\"" \; --conf spark.executor.extraClassPath=./hail-all-spark.jar \; --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \; --conf spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator\; ""$@"". ```; Here is a sample of the yarn log....; ...; SLF4J: Class path contains multiple SLF4J bindings.; SLF4J: Found binding in [jar:file:/data04/hadoop/yarn/local/usercache/farrell/filecache/40/__spark_libs__5184408978318087972.zip/slf4j-log4j12-1.7.16.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: Found binding in [jar:file:/usr/hdp/2.4.0.0-169/hadoop/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]; SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.; SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]; 19/01/22 13:11:40 WARN util.NativeCodeLoader: U",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258:1684,deploy,deploy-mode,1684,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456518258,1,['deploy'],['deploy-mode']
Deployability,rk.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2080,install,install,2080,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,ro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8996,install,install,8996,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"robeTime: null; lastTransitionTime: ""2019-06-25T12:37:07Z""; status: ""True""; type: PodScheduled; containerStatuses:; - image: konradjk/saige:0.35.8.2.2; imageID: """"; lastState: {}; name: main; ready: false; restartCount: 0; state:; waiting:; reason: ContainerCreating; hostIP: 10.128.0.8; phase: Pending; qosClass: Burstable; startTime: ""2019-06-25T12:37:07Z""; + kubectl describe pod batch-2554-job-4-main-vsk7h -n batch-pods; Name: batch-2554-job-4-main-vsk7h; Namespace: batch-pods; Priority: 500000; PriorityClassName: user; Node: gke-vdc-preemptible-pool-9c7148b2-4gq2/10.128.0.8; Start Time: Tue, 25 Jun 2019 08:37:07 -0400; Labels: app=batch-job; hail.is/batch-instance=cd50b95a89914efb897965a5e982a29d; uuid=0c8e6bfd45294d738957b42a3874e25e; Annotations: <none>; Status: Pending; IP: ; Containers:; main:; Container ID: ; Image: konradjk/saige:0.35.8.2.2; Image ID: ; Port: <none>; Host Port: <none>; Command:; /bin/bash; -c; set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; __RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; __RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; __RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; __RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; __RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; __RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; __RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; __RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; Rscript /usr/local/bin/step2_SPAtests.R --vcfFile=${__RESOURCE_FILE__18} --vcfFileIndex=${__RESOURCE_FILE__19} --vcfField=GT --minMAF=0 --minMAC=1 --maxMAFforGroupTest=0.5 --chrom=chr1 --sampleFile=${__RESOURC",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:16599,pipeline,pipeline,16599,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,6,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"rs.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still appeared.**. /opt/BioDir/jdk/jdk1.8.0_91/bin/java: symbol lookup error: /tmp/jniloader7277009897699512423netlib-native_system-linux-x86_64.so: undefined symbol: cblas_dgemv. FAILURE: Build failed with an exception.; - What went wrong:; Execution failed for task ':test'.; ; > Process 'Gradle Test Executor 1' finished with non-zero exit value 127; - Try:; ; ## Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output.; ; #######The output info was collected in the file as follow:; [gradle_check_info1.txt](https",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:1553,update,updates,1553,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,1,['update'],['updates']
Deployability,"ry code work:; - Add the rest of the core methods from VDS/KT to api2 (#2591 does most for KT, order_by is the only outstanding KT method that's not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on teensy data.; - Integrate RV with C/C++, so we can transmit data much more efficiently between Python and Java.; - R",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554:1135,patch,patching,1135,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554,1,['patch'],['patching']
Deployability,"s"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Network/networkInterfaces/batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun-nic"",; ""primary"": null,; ""resourceGroup"": ""dgoldste""; }; ]; },; ""osProfile"": {; ""adminPassword"": null,; ""adminUsername"": ""batch-worker"",; ""allowExtensionOperations"": true,; ""computerName"": ""batch-worker-pr-11144-default-nbthv8fduvd6-standard-i4sun"",; ""customData"": null,; ""linuxConfiguration"": {; ""disablePasswordAuthentication"": true,; ""patchSettings"": {; ""assessmentMode"": ""ImageDefault"",; ""patchMode"": ""ImageDefault""; },; ""provisionVmAgent"": true,; ""ssh"": {; ""publicKeys"": [; {; ""keyData"": ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEven",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:11629,patch,patchSettings,11629,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,2,['patch'],"['patchMode', 'patchSettings']"
Deployability,s-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WEBSITE_TAR=/path/to/www.tar.gz \; hail/scripts/release.sh. +++ dirname -- hail/scripts/release.sh; ++ cd -- hail/scripts; ++ pwd; + SCRIPT_DIR=/Users/dking/projects/hail/hail/scripts; + arguments='HAIL_PIP_VERSION HAIL_VERSION GIT_VERSION REMOTE WHEEL GITHUB_OAUTH_HEADER_FILE HAIL_GENETICS_HAIL_IMAGE HAIL_GENETICS_HAIL_IMAGE_PY_3_10 HAIL_GENETICS_HAIL_IMAGE_PY_3_11 HAIL_GENETICS_HAILTOP_IMAGE HAIL_GENETICS_VEP_G,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5945,deploy,deploy-,5945,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"s/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; --; 872 | amazon-ebs: rm -rf build/deploy; 873 | amazon-ebs: mkdir -p build/deploy; 874 | amazon-ebs: mkdir -p build/deploy/src; 875 | amazon-ebs: cp ../README.md build/deploy/; 876 | amazon-ebs: rsync -r \; 877 | amazon-ebs: --exclude '.eggs/' \; 878 | amazon-ebs: --exclude '.pytest_cache/' \; 879 | amazon-ebs: --exclude '__pycache__/' \; 880 | amazon-ebs: --exclude 'benchmark_hail/' \; 881 | amazon-ebs: --exclude '.mypy_cache/' \; 882 | amazon-ebs: --exclude 'docs/' \; 883 | amazon-ebs: --exclude 'dist/' \; 884 | amazon-ebs: --exclude 'test/' \; 885 | amazon-ebs: --exclude '*.log' \; 886 | amazon-ebs: python/ build/deploy/; 887 | amazon-ebs: # Clear the bdist build cache before building the wheel; 888 | amazon-ebs: cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; 889 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/installer.py:30: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer.; 890 | ==> amazon-ebs: SetuptoolsDeprecationWarning,; 891 | ==> amazon-ebs: /usr/local/lib/python3.7/site-packages/setuptools/command/install.py:37: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.; 892 | ==> amazon-ebs: setuptools.SetuptoolsDeprecationWarning,; 893 | amazon-ebs: sed '/^pyspark/d' python/requirements.txt \| grep -v '^#' \| xargs python3 -m pip install -U; 894 | amazon-ebs: Collecting aiohttp==3.8.1; 895 | amazon-ebs: Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB); 896 | amazon-ebs: ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 68.3 MB/s eta 0:00:00; 897 | amazon-ebs: Collecting aiohttp_session<2.8,>=2.7; 898 | amazon-ebs: Downloading aiohttp_session-2.7.0-py3-none-any.whl (14 kB); 899 | amazon-ebs: Collecting asyncinit<0.3,>=0.2.4; 900 | amazon-ebs",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691:1120,install,installer,1120,https://hail.is,https://github.com/hail-is/hail/pull/12136#issuecomment-1255177691,1,['install'],['installer']
Deployability,"s/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java/default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/python:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-2",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:8419,install,install,8419,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"s/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:1621,install,install,1621,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,s/vep-grch38-95:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; + for varname in '$arguments'; + '[' -z '' ']'; + echo. + usage; + cat; ++ basename hail/scripts/release.sh; ++ basename hail/scripts/release.sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5117,deploy,deploy-,5117,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['deploy'],['deploy-']
Deployability,"s_mcpu = free_cores_mcpu + cur_cores_mcpu; WHERE instances_free_cores_mcpu.name = in_instance_name; *** (1) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409381 lock_mode X locks rec but not gap waiting; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) TRANSACTION:; TRANSACTION 644409370, ACTIVE 0 sec inserting; mysql tables in use 6, locked 6; 39 lock struct(s), heap size 3520, 51 row lock(s), undo log entries 30; MySQL thread id 1941930, OS thread handle 140298159240960, query id 1869168731 10.32.3.8 dgoldste update; INSERT INTO batch_inst_coll_cancellable_resources (batch_id, inst_coll, token, n_running_cancellable_jobs, running_can; cellable_cores_mcpu); VALUES (OLD.batch_id, OLD.inst_coll, rand_token, -1, -OLD.cores_mcpu); ON DUPLICATE KEY UPDATE; n_running_cancellable_jobs = n_running_cancellable_jobs - 1,; running_cancellable_cores_mcpu = running_cancellable_cores_mcpu - OLD.cores_mcpu; *** (2) HOLDS THE LOCK(S):; RECORD LOCKS space id 1263041 page no 3 n bits 264 index PRIMARY of table `dgoldste`.`instances_free_cores_mcpu` trx i; d 644409370 lock_mode X locks rec but not gap; Record lock, heap no 192 PHYSICAL RECORD: n_fields 4; compact format; info bits 0; 0: len 30; hex 62617463682d776f726b65722d64676f6c647374652d7374616e64617264; asc batch-worker-dgoldste-standard; (tot; al 36 bytes);; 1: len 6; hex 00002668e81a; asc &h ;;; 2: len 7; hex 710000071136b3; asc q 6 ;;; 3: len 4; hex 800029fe; asc ) ;;. *** (2) WAITING FOR THIS LOCK TO BE GRANTED:; RECORD LOCKS space id 1263027 page no 15 n bits 232 index PRIMARY of table `dgoldste`.`batch_inst_coll_cancellabl",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116:2242,update,update,2242,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1036370116,1,['update'],['update']
Deployability,"s_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this audit in chunks here as these tables are massive and a single audit query would take hours. The bounds of the audit for these chunks are on the order of `(batch_id, job_id)` rather than `(batch_id, job_id, resource_id)` which was used for the actual updates. This is because the resource_ids can differ between ""v2"" and ""v3"", so we just check the overall job adds up to the same usage after deduplicating the resource IDs on both tables. I recommend looking at the main function towards the bottom of the script and then working your way through it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2817,update,update,2817,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,3,['update'],"['update', 'updates']"
Deployability,"ser move_selected_cells/main. COPY ./resources/ /home/jovyan; ```. And the actual worker creation in notebook.py. ```py; def start_pod(jupyter_token, image, labels={}):; print(""IMAGE IN START IS"", image); pod_id = uuid.uuid4().hex; service_spec = kube.client.V1ServiceSpec(; selector={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id},; ports=[kube.client.V1ServicePort(port=80, target_port=8888)]); service_template = kube.client.V1Service(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-service-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; **labels}),; spec=service_spec); svc = k8s.create_namespaced_service(; 'default',; service_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS; ); pod_spec = kube.client.V1PodSpec(; containers=[; kube.client.V1Container(; command=[; 'jupyter',; 'notebook',; f'--NotebookApp.token={jupyter_token}',; f'--NotebookApp.base_url=/instance/{svc.metadata.name}/'; ],; name='default',; image=image,; ports=[kube.client.V1ContainerPort(container_port=8888)],; resources=kube.client.V1ResourceRequirements(; requests={'cpu': '1.601', 'memory': '1.601G'}),; readiness_probe=kube.client.V1Probe(; http_get=kube.client.V1HTTPGetAction(; path=f'/instance/{svc.metadata.name}/login',; port=8888)))]); pod_template = kube.client.V1Pod(; metadata=kube.client.V1ObjectMeta(; generate_name='notebook-worker-',; labels={; 'app': 'notebook-worker',; 'hail.is/notebook-instance': INSTANCE_ID,; 'uuid': pod_id,; 'svc_name': svc.metadata.name,; **labels; },),; spec=pod_spec); pod = k8s.create_namespaced_pod(; 'default',; pod_template,; _request_timeout=KUBERNETES_TIMEOUT_IN_SECONDS,; ). return svc, pod; ``` . In the pod definition he runs `jupyter notebook --NotebookApp.token={jupyter_token} --NotebookApp.base_url=/instance/{svc.metadata.name}/` which overrides yours. I'm going to try to run headless mode, and if not, maybe we should install jupyterlab?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218:4291,install,install,4291,https://hail.is,https://github.com/hail-is/hail/pull/5243#issuecomment-460097218,1,['install'],['install']
Deployability,sh; usage: release.sh. All arguments are specified by environment variables. For example:. HAIL_PIP_VERSION=0.2.123; HAIL_VERSION=0.2.123-abcdef123; GIT_VERSION=abcdef123; REMOTE=origin; WHEEL=/path/to/the.whl; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc; WHEEL_FOR_AZURE=/path/to/wheel/for/azure; WEBSITE_TAR=/path/to/www.tar.gz; release.sh; + echo. + echo 'WHEEL_FOR_AZURE is unset or empty'; WHEEL_FOR_AZURE is unset or empty; + exit 1; ```. ```sh; # HAIL_PIP_VERSION=0.2.123 \; HAIL_VERSION=0.2.123-abcdef123 \; GIT_VERSION=abcdef123 \; REMOTE=origin \; WHEEL=/path/to/the.whl \; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file \; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc \; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc \; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc \; HAIL_GENETICS_VEP_GRCH38_95_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95:deploy-123abc \; WHEEL_FOR_AZURE=x \; WE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:5546,release,release,5546,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,1,['release'],['release']
Deployability,since 0.2 entered beta I'm not sure I've seen a single pipeline with globals manipulation,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4027#issuecomment-408648225:55,pipeline,pipeline,55,https://hail.is,https://github.com/hail-is/hail/issues/4027#issuecomment-408648225,1,['pipeline'],['pipeline']
Deployability,sk :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding 'hail/matrixtable.py'; adding 'hail/table.py'; adding 'hail/backend/__init__.py'; adding 'hail/backend/backend.py'; adding 'hail/backend/hail-all-spark.jar'; adding 'hail/backend/local_backend.py'; adding 'hail/backend/py4j_backend.py'; adding 'hail/backend/service_backend.py'; adding 'hail/backend/spark_backend.py'; adding 'hail/experimental/__init__.py'; adding 'hail/experimental/codec.py'; adding 'hail/experimental/compile.py'; adding 'hail/experimental/datasets.json'; adding 'hail/experimental/datasets.py'; adding 'hail/experimental/db.py'; addi,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:17018,install,installing,17018,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['install'],['installing']
Deployability,spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg/lz4/1.8.3/install/lib:/share/pkg/gcc/7.2.0/install/lib64:/share/pkg/gcc/7.2.0/install/lib; spark.yarn.appMasterEnv.PATH=/share/pkg/spark/2.2.1/install/bin:/share/pkg/lz4/1.8.3/install/bin:/share/pkg/gcc/7.2.0/install/bin:/usr3/bustaff/farrell/anaconda_envs/hail2/bin:/share/pkg/anaconda3/5.2.0/install/bin:/usr/java/default/jre/bin:/usr/java; /default/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/dell/srvadmin/bin:/usr3/bustaff/farrell/bin:/usr3/bustaff/farrell/bin; spark.yarn.appMasterEnv.PYTHONPATH=/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip:/share/pkg/spark/2.2.1/install/python:/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip:/share/pkg/spark/2.2.1/install/py; thon:/share/pkg/spark/2.2.1/install/python/lib/py4j-*-src.zip; spark.yarn.dist.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.yarn.isPython=true; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:21 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:21 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify p; ermissions: Set(); 2019-01-22 13:11:21 Utils: INFO: Successfully started service 'sparkDriver' on port 38253.; 2019-01-22 13:11:21 SparkEnv: INFO: Registering MapOutputTracker; 2019-01-22 13:11:21 SparkEnv: INFO: Registering BlockManagerMaster; 2019-01-22 13:11:21 BlockManagerMasterEndpoint: INFO:,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:2240,install,install,2240,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['install'],['install']
Deployability,"state != 'Error' AND new_state != 'Failed'); WHERE id = in_batch_id;. # Grabbing an exclusive lock on batches here could deadlock,; # but this IF should only execute for the last job; IF @new_n_completed = total_jobs_in_batch THEN; UPDATE batches; SET time_completed = new_timestamp,; `state` = 'complete'; WHERE id = in_batch_id;; END IF;; ```. We can do the double update in the IF statement to both the job groups table for job_group_id = 0 and for the batches table in #13475. However, this SQL code / approach will eventually need to be changed for the full job group implementation. I don't know how to compute `@new_n_completed` grouped by job group and then `total_jobs_in_batch` would need to be computed per job group as well. I don't think you can use for loops in SQL. It might be possible to do this with temporary tables, but I thought it would be better to take a detour from adding job groups and get rid of how we currently do the batch update in MJC to allow for job groups in the future before putting in job groups tables so that I could slot in the appropriate state and time_completed updates to both batches and job_groups tables in the same place rather than relying on a trigger for the updates. I can think about which set of changes should go first (I'm not wedded to either PR coming first -- just thought this way was conceptually easier to understand when there was just a batches table). I haven't 100% convinced myself this change to tokenize the `batches_n_jobs_in_complete_states` table is absolutely necessary for nested job groups, but it seemed like we would want the performance improvements regardless. > 2. How come marking the batch as complete is moved into a separate transaction as marking the job complete? If it were in the same transaction wouldn't we not need this healing loop?. This is for performance reasons to avoid serialization and race conditions. If the update occurs in the same transaction, then each transaction will be serialized checking ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732:1980,update,update,1980,https://hail.is,https://github.com/hail-is/hail/pull/13513#issuecomment-1701597732,3,['update'],"['update', 'updates']"
Deployability,still failing:; ```gsutil cat gs://hail-ci-0-1/deploy/c28a3f9863a0\*/job-log ```; ```; + gcloud -q auth activate-service-account --key-file=/secrets/gcr-push-service-account-key.json; Activated service account credentials for: [gcr-push@broad-ctsa.iam.gserviceaccount.com]; + gcloud -q auth configure-docker; Docker configuration file updated.; + make push-batch; docker build -t batch .; Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.38/build?buildargs=%7B%7D&cachefrom=%5B%5D&cgroupparent=&cpuperiod=0&cpuquota=0&cpusetcpus=&cpusetmems=&cpushares=0&dockerfile=Dockerfile&labels=%7B%7D&memory=0&memswap=0&networkmode=default&rm=1&session=nt5ube8nzit2kbdia2afrfify&shmsize=0&t=batch&target=&ulimits=null&version=1: dial unix /var/run/docker.sock: connect: permission denied; make: *** [build-batch] Error 1; Makefile:14: recipe for target 'build-batch' failed. ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708:47,deploy,deploy,47,https://hail.is,https://github.com/hail-is/hail/issues/4443#issuecomment-424716708,3,"['configurat', 'deploy', 'update']","['configuration', 'deploy', 'updated']"
Deployability,still no updates,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13598#issuecomment-1839221748:9,update,updates,9,https://hail.is,https://github.com/hail-is/hail/issues/13598#issuecomment-1839221748,1,['update'],['updates']
Deployability,superceded by newer release,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14212#issuecomment-1917831497:20,release,release,20,https://hail.is,https://github.com/hail-is/hail/pull/14212#issuecomment-1917831497,6,['release'],['release']
Deployability,"t to ""0.2.124"" which is different from old value """"; printf ""0.2.124"" > env/HAIL_PIP_VERSION; echo 0.2.124-13536b531342 > python/hail/hail_version; echo 0.2.124 > python/hail/hail_pip_version; cp -f python/hail/hail_version python/hailtop/hail_version; printf 'hail_version=""0.2.124-13536b531342"";' > python/hail/docs/_static/hail_version.js; printf 'hail_pip_version=""0.2.124""' >> python/hail/docs/_static/hail_version.js; cloud_base is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" which is different from old value """"; printf ""gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in vep-GRCh37.sh vep-GRCh38.sh init_notebook.py; do \; echo "" $FILE: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-30-day/hailctl/dataproc/hadoop-dev/0.2.124-13536b531342/hail-0.2.124-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; BRANCH is set to ""HEAD"" which is different from old value """"; printf ""HEAD"" > env/BRANCH; SPARK_VERSION is set to ""3.3.2"" which is different from old value """"; printf ""3.3.2"" > env/SPARK_VERSION; echo '[Build Metadata]' > src/main/resources/build-info.properties; echo 'user=hadoop' >> src/main/resources/build-info.properties; echo 'revision=13536b531342a263b24a7165bfeec7bd02723",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:3003,deploy,deploy,3003,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,1,['deploy'],['deploy']
Deployability,t-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff\n2022-11-15 20:30:18.264 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts\n2022-11-15 20:30:18.318 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Fcontexts response 200\n2022-11-15 20:30:18.331 Requester: INFO: request POST http://memory.hail/api/v1alpha/objects?q=gs%3A%2F%2Fhail-test-dmk9z%2FparallelizeAndComputeWithIndex%2Fpty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY%3D%2Ff response 200\n2022-11-15 20:30:18.332 ServiceBackend$: INFO: parallelizeAndComputeWithIndex: pty4D81uzQk6XN9LVVebj6KNvkh8SC3EzvXjgv6-LMY=: running job\n2022-11-15 20:30:18.333 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast\n2022-11-15 20:30:18.697 Requester: INFO: request POST http://batch.hail/api/v1alpha/batches/6627669/update-fast response 200\n2022-11-15 20:30:18.697 BatchClient: INFO: run: created update 2 for batch 6627669\n2022-11-15 20:30:18.697 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.802 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.852 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.866 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.917 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.934 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669 response 200\n2022-11-15 20:30:18.985 Requester: INFO: request GET http://batch.hail/api/v1alpha/batches/6627669\n2022-11-15 20:30:18.999 Requester: INF,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:24409,update,update-fast,24409,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['update'],['update-fast']
Deployability,"tHandler: INFO: Started o.s.j.s.ServletContextHandler@5cae8477{/executors/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@3f5a136b{/executors/threadDump,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@1c36c598{/executors/threadDump/json,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@35dfb92d{/static,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@85877e{/,null,AVAILABLE,@Spark}; [farrell@scc-hadoop ukb.v3]$ cat /restricted/projectnb/ukbiobank/ad/analysis/ukb.v3/hail-20190122-1311-0.2.4-d602a3d7472d.log; 2019-01-22 13:11:20 SparkContext: INFO: Running Spark version 2.2.1; 2019-01-22 13:11:20 NativeCodeLoader: WARN: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 2019-01-22 13:11:21 SparkContext: INFO: Submitted application: Hail; 2019-01-22 13:11:21 SparkContext: INFO: Spark configuration:; spark.app.name=Hail; spark.driver.extraClassPath=""/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar""; spark.driver.memory=5G; spark.executor.cores=4; spark.executor.extraClassPath=./hail-all-spark.jar; spark.executor.instances=10; spark.executor.memory=40G; spark.hadoop.io.compression.codecs=org.apache.hadoop.io.compress.DefaultCodec,is.hail.io.compress.BGzipCodec,is.hail.io.compress.BGzipCodecTbi,org.apache.hadoop.io.compress.GzipCodec; spark.hadoop.mapreduce.input.fileinputformat.split.minsize=1048576; spark.kryo.registrator=is.hail.kryo.HailKryoRegistrator; spark.logConf=true; spark.master=yarn; spark.repl.local.jars=file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar; spark.serializer=org.apache.spark.serializer.KryoSerializer; spark.submit.deployMode=client; spark.ui.showConsoleProgress=false; spark.yarn.appMasterEnv.LD_LIBRARY_PATH=/share/pkg",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:7311,configurat,configuration,7311,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['configurat'],['configuration']
Deployability,"te_name': 'batch-2554-job-4-main-',; 'generation': None,; 'initializers': None,; 'labels': {'app': 'batch-job',; 'hail.is/batch-instance': 'cd50b95a89914efb897965a5e982a29d',; 'uuid': '3bf0b121f62d4cfea15cf187a21bc0ed'},; 'name': 'batch-2554-job-4-main-cc8d4',; 'namespace': 'batch-pods',; 'owner_references': None,; 'resource_version': '72793521',; 'self_link': '/api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-cc8d4',; 'uid': '968b4ba5-96f6-11e9-8aa3-42010a80015f'},; 'spec': {'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:1994,pipeline,pipeline,1994,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,2,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"ter, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:18); at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33); at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35); at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:18); at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:28); at is.hail.backend.spark.SparkBackend.is$hail$backend$spark$SparkBackend$$_execute(SparkBackend.scala:317); at i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:7074,install,install,7074,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,test_dataproc only runs on deploys AFAIK,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12510#issuecomment-1329684853:27,deploy,deploys,27,https://hail.is,https://github.com/hail-is/hail/pull/12510#issuecomment-1329684853,1,['deploy'],['deploys']
Deployability,"tested this out in my dev namespace with [this PR](https://github.com/hail-ci-test/ci-test-fhkslcwtu6ij/pull/1) in the associated test repo, which didn't merge until it was approved and had the `ci-test` commit status passing, while the `hail-ci-azure` commit status has failed (the test repo configuration requires one approval and just the `ci-test` commit status to be passing)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14676#issuecomment-2349266606:293,configurat,configuration,293,https://hail.is,https://github.com/hail-is/hail/pull/14676#issuecomment-2349266606,1,['configurat'],['configuration']
Deployability,tests pass rolling up,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9851#issuecomment-754182658:11,rolling,rolling,11,https://hail.is,https://github.com/hail-is/hail/pull/9851#issuecomment-754182658,1,['rolling'],['rolling']
Deployability,tests took 8m for this PR and 10m for another recent PR that didn't change pipeline,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7864#issuecomment-580362794:75,pipeline,pipeline,75,https://hail.is,https://github.com/hail-is/hail/pull/7864#issuecomment-580362794,1,['pipeline'],['pipeline']
Deployability,thank you for these changes.; I am facing trouble in installing this branch of the package. i tried. `pip install git+https://github.com/danking/hail.git@relax-variant-qc`. ```; git clone https://github.com/danking/hail.git --branch relax-variant-qc --single-branch; cd hail; pip install .; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12851#issuecomment-1501410278:53,install,installing,53,https://hail.is,https://github.com/hail-is/hail/pull/12851#issuecomment-1501410278,3,['install'],"['install', 'installing']"
Deployability,thank you for these changes.; I am facing trouble in installing this branch of the package. i tried. pip install git+https://github.com/danking/hail.git@relax-variant-qc. git clone https://github.com/danking/hail.git --branch relax-variant-qc --single-branch; cd hail; pip install .,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12844#issuecomment-1501448505:53,install,installing,53,https://hail.is,https://github.com/hail-is/hail/issues/12844#issuecomment-1501448505,3,['install'],"['install', 'installing']"
Deployability,"thank you very much for the reply,; when I upgrade the decorator from 3.4.0 to 4.1.2 , this error disappears：. ```; Installing collected packages: decorator; Found existing installation: decorator 3.4.0; Uninstalling decorator-3.4.0:; Successfully uninstalled decorator-3.4.0; Successfully installed decorator-4.1.2. ```. But there is another error, as follows：. ```; bash-4.2$ pyspark; WARNING: User-defined SPARK_HOME (/opt/cloudera/parcels/SPARK2-2.2.0.cloudera1-1.cdh5.12.0.p0.142354/lib/spark2) overrides detected (/opt/cloudera/parcels/SPARK2/lib/spark2).; WARNING: Running pyspark from user-defined location.; Python 2.7.5 (default, Nov 6 2016, 00:28:07) ; [GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; Setting default log level to ""WARN"".; To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).; Welcome to; ____ __; / __/__ ___ _____/ /__; _\ \/ _ \/ _ `/ __/ '_/; /__ / .__/\_,_/_/ /_/\_\ version 2.2.0.cloudera1; /_/. Using Python version 2.7.5 (default, Nov 6 2016 00:28:07); SparkSession available as 'spark'.; >>> from hail import *; >>> hc = hail.Context(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = hail.HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; NameError: name 'hail' is not defined; >>> hc = HailContext(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""<decorator-gen-470>"", line 2, in __init__; File ""/opt/Software/hail/python/hail/typecheck/check.py"", line 245, in _typecheck; return f(*args, **kwargs); File ""/opt/Software/hail/python/hail/context.py"", line 88, in __init__; parquet_compression, min_block_size, branching_factor, tmp_dir); TypeError: 'JavaPackage' object is not callable; ```; My Java version; ```; [root@mg opt]# java -version; java version ""1.8.0_91""; Java(TM) SE Runtime Environment (bui",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579:43,upgrade,upgrade,43,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-337132579,4,"['Install', 'install', 'upgrade']","['Installing', 'installation', 'installed', 'upgrade']"
Deployability,the configuration changes we made mean that it's perfectly natural to handle this PR!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/805#issuecomment-248415712:4,configurat,configuration,4,https://hail.is,https://github.com/hail-is/hail/pull/805#issuecomment-248415712,1,['configurat'],['configuration']
Deployability,"the most recent numbers, with some updates (this is basically read->densify->force count):; ```; size | new | old; 10 x 25000 | 0m 02.62 | 0m 01.05; 10 x 500000 | 0m 02.39 | 0m 04.91; 200 x 25000 | 0m 00.69 | 0m 05.92; 200 x 500000 | 0m 13.69 | 1m 03.08; 1000 x 25000 | 0m 02.17 | 0m 27.34; 1000 x 500000 | 1m 03.90 | 5m 13.64; 5000 x 25000 | 0m 09.13 | 2m 16.92; 5000 x 500000 | 5m 45.71 | 27m 53.85; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6653#issuecomment-511630081:35,update,updates,35,https://hail.is,https://github.com/hail-is/hail/pull/6653#issuecomment-511630081,1,['update'],['updates']
Deployability,the requirements updates seem fine.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11840#issuecomment-1308991106:17,update,updates,17,https://hail.is,https://github.com/hail-is/hail/pull/11840#issuecomment-1308991106,1,['update'],['updates']
Deployability,this actually isn't broken -- I fixed it (by adding a blank line before the bullet lists) on May 1. The problem is that we haven't deployed the website since then.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6190#issuecomment-498621962:131,deploy,deployed,131,https://hail.is,https://github.com/hail-is/hail/pull/6190#issuecomment-498621962,1,['deploy'],['deployed']
Deployability,"this image itself is failing for build reasons, I will merge. That should unstick everything by removing these steps from the deploy and pr paths.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8623#issuecomment-619208491:126,deploy,deploy,126,https://hail.is,https://github.com/hail-is/hail/pull/8623#issuecomment-619208491,1,['deploy'],['deploy']
Deployability,this is breaking deploy so I'm gonna use admin override to merge it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5007#issuecomment-448473891:17,deploy,deploy,17,https://hail.is,https://github.com/hail-is/hail/pull/5007#issuecomment-448473891,1,['deploy'],['deploy']
Deployability,"this is, uh, already deployed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10179#issuecomment-796851213:21,deploy,deployed,21,https://hail.is,https://github.com/hail-is/hail/pull/10179#issuecomment-796851213,1,['deploy'],['deployed']
Deployability,"this will dirty the working tree, right? should releaseJar clean up?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6134#issuecomment-494062117:48,release,releaseJar,48,https://hail.is,https://github.com/hail-is/hail/pull/6134#issuecomment-494062117,1,['release'],['releaseJar']
Deployability,titute.hail.Utils$.time(Utils.scala:1282); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:143); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1.apply(Main.scala:137); at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51); at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60); at scala.collection.mutable.ArrayOps$ofRef.foldLeft(ArrayOps.scala:108); at org.broadinstitute.hail.driver.Main$.runCommands(Main.scala:137); at org.broadinstitute.hail.driver.Main$.main(Main.scala:286); at org.broadinstitute.hail.driver.Main.main(Main.scala); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:497); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:674); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)java.lang.ClassCastException: java.lang.Integer cannot be cast to scala.collection.IndexedSeq; at org.broadinstitute.hail.expr.IndexOp$$anonfun$eval$224.apply(AST.scala:1894); at org.broadinstitute.hail.expr.AST$$anonfun$evalCompose$2.apply(AST.scala:129); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.expr.Parser$$anonfun$5$$anonfun$apply$7.apply(Parser.scala:168); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:71); at org.broadinstitute.hail.driver.AnnotateVariantsExpr$$anonfun$2$$anonfun$apply$2.apply(AnnotateVariantsExpr.scala:70);,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:9009,deploy,deploy,9009,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['deploy'],['deploy']
Deployability,"to be included in the dev docs:. the devserver uses the `default_namespace` configured via `hailctl dev config`, so make sure to change that to your dev namespace or the default namespace as desired. whatever service you're trying to hack on the UI of needs to be `pip install`ed, as well as `web_common` and `gear`; for example, `pip install -e web_common gear batch`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13632#issuecomment-1783089092:269,install,install,269,https://hail.is,https://github.com/hail-is/hail/pull/13632#issuecomment-1783089092,2,['install'],['install']
Deployability,"to become C++11-compliant. Systems using the earlier libstdc++; (which also typically have g++-4.8.x/4.9x) have the old-ABI not-fully-compliant std::string; (and std::list, but I've gone 20 years without ever using that). Later versions of libstdc++ have *both* flavors of std::string, but use namespaces to allow them; to coexist (but not to be interchangeable, so interfaces between old-ABI and new-ABI are; problematic). https://gcc.gnu.org/onlinedocs/libstdc++/manual/using_dual_abi.html. ""In the GCC 5.1 release libstdc++ introduced a new library ABI that includes new implementations of std::string and std::list. These changes were necessary to conform to the 2011 C++ standard which forbids Copy-On-Write strings and requires lists to keep track of their size. In order to maintain backwards compatibility for existing code linked to libstdc++ the library's soname has not changed and the old implementations are still supported in parallel with the new ones. This is achieved by defining the new implementations in an inline namespace so they have different names for linkage purposes, e.g. the new version of std::list<int> is actually defined as std::__cxx11::list<int>. Because the symbols for the new implementations have different names the definitions for both versions can be present in the same library."". OSX doesn't have this ABI-compatibility issue because for several years it has been using; libc++ as the default library, and libc++ is a post-C++11 rewrite-from-scratch implementation; of the required standard-library functionality. My understanding is that it is perfectly feasible to mix and match different g++/clang++compiler; versions and different libstdc++ versions. It just doesn't happen very much because both the; g++ version and the libstdc++ are chosen at the same time, early in building a Linux distribution,; and then are frozen throughout the release's lifetime to ensure interoperability of binaries. So debian8's version of g++ and libstdc++ doesn't change.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612:2353,release,release,2353,https://hail.is,https://github.com/hail-is/hail/pull/4422#issuecomment-424797612,1,['release'],['release']
Deployability,"to clarify, here's the new command to run:. ```; $ ./build/install/hail/bin/hail read ~/sample.vds \; splitmulti \; sampleqc -o ~/sampleqc.tsv \; variantqc \; exportvariants -o ~/variantqc.tsv -c 'Variant = v, va.qc.*' \; printschema; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1017#issuecomment-256800696:59,install,install,59,https://hail.is,https://github.com/hail-is/hail/issues/1017#issuecomment-256800696,1,['install'],['install']
Deployability,to/the.whl; + for varname in '$arguments'; + '[' -z /path/to/github/oauth/header/file ']'; + echo GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; GITHUB_OAUTH_HEADER_FILE=/path/to/github/oauth/header/file; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_10=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc ']'; + echo HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; HAIL_GENETICS_HAIL_IMAGE_PY_3_11=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hail:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc ']'; + echo HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; HAIL_GENETICS_HAILTOP_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/hailtop:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc ']'; + echo HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; HAIL_GENETICS_VEP_GRCH37_85_IMAGE=docker://us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85:deploy-123abc; + for varname in '$arguments'; + '[' -z docker://us-docker.pkg.dev/hail-vdc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409:3092,deploy,deploy-,3092,https://hail.is,https://github.com/hail-is/hail/pull/14323#issuecomment-1955223409,2,['deploy'],['deploy-']
Deployability,"tput; ```bash; cloud_base is set to ""gs://hail-common/hailctl/dataproc/0.2.129"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129"" > env/cloud_base; wheel_cloud_path is set to ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" which is different from old value ""gs://hail-30-day/hailctl/dataproc/edmund-dev/0.2.129-827516e474c3/hail-0.2.129-py3-none-any.whl""; mkdir -p env; printf ""gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" > env/wheel_cloud_path; rm -f python/hailtop/hailctl/deploy.yaml; echo ""dataproc:"" >> python/hailtop/hailctl/deploy.yaml; for FILE in init_notebook.py vep-GRCh37.sh vep-GRCh38.sh; do \; echo "" $FILE: gs://hail-common/hailctl/dataproc/0.2.129/$FILE"" >> python/hailtop/hailctl/deploy.yaml || exit 1; done; echo "" wheel: gs://hail-common/hailctl/dataproc/0.2.129/hail-0.2.129-py3-none-any.whl"" >> python/hailtop/hailctl/deploy.yaml; printf "" pip_dependencies: "" >> python/hailtop/hailctl/deploy.yaml; cat python/pinned-requirements.txt | sed '/^[[:blank:]]*#/d;s/#.*//' | grep -v pyspark | tr ""\n"" ""|||"" | tr -d '[:space:]' >> python/hailtop/hailctl/deploy.yaml; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; gcloud storage cp python/hailtop/hailctl/dataproc/resources/init_notebook.py python/hailtop/hailctl/dataproc/resources/vep-GRCh37.sh python/hailtop/hailctl/dataproc/resources/vep-GRCh38.sh build/deploy/dist/hail-0.2.129-py3-none-any.whl gs://hail-common/hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145:1210,deploy,deploy,1210,https://hail.is,https://github.com/hail-is/hail/pull/14453#issuecomment-2045927145,1,['deploy'],['deploy']
Deployability,"tus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Network/networkInterfaces/batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5-nic"",; ""primary"": null,; ""resourceGroup"": ""dgoldste""; }; ]; },; ""osProfile"": {; ""adminPassword"": null,; ""adminUsername"": ""batch-worker"",; ""allowExtensionOperations"": true,; ""computerName"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highcpu-robv5"",; ""customData"": null,; ""linuxConfiguration"": {; ""disablePasswordAuthentication"": true,; ""patchSettings"": {; ""assessmentMode"": ""ImageDefault"",; ""patchMode"": ""ImageDefault""; },; ""provisionVmAgent"": true,; ""ssh"": {; ""publicKeys"": [; {; ""keyData"": ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEvent",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:2286,patch,patchSettings,2286,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,2,['patch'],"['patchMode', 'patchSettings']"
Deployability,"tus"",; ""name"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m"",; ""networkProfile"": {; ""networkApiVersion"": null,; ""networkInterfaceConfigurations"": null,; ""networkInterfaces"": [; {; ""deleteOption"": ""Delete"",; ""id"": ""/subscriptions/22cd45fe-f996-4c51-af67-ef329d977519/resourceGroups/dgoldste/providers/Microsoft.Network/networkInterfaces/batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m-nic"",; ""primary"": null,; ""resourceGroup"": ""dgoldste""; }; ]; },; ""osProfile"": {; ""adminPassword"": null,; ""adminUsername"": ""batch-worker"",; ""allowExtensionOperations"": true,; ""computerName"": ""batch-worker-pr-11144-default-nbthv8fduvd6-highmem-13t6m"",; ""customData"": null,; ""linuxConfiguration"": {; ""disablePasswordAuthentication"": true,; ""patchSettings"": {; ""assessmentMode"": ""ImageDefault"",; ""patchMode"": ""ImageDefault""; },; ""provisionVmAgent"": true,; ""ssh"": {; ""publicKeys"": [; {; ""keyData"": ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQDKC9kZBCsWb78yJ1zcdhmrYEmjNEOaJN5RGuMBuoszUXGGOCJMFi6jMTgSVjTql90NchA0tWXAuooVLV++f8WIOwpP7TY1YzN1XCREyk8jKOMrIdCc22ueJlNvxmFfJhdDKBCy0eThWN2qFxQJ4p9SvzGlMd2r3nBy95v9f8WgkN8M/HTDwTsFafNT0arvHnmUY6rFHxQE9TgTRlH1/sZ7mMxzmVZ8NKI/wIXTkv53TbylBYbvkEXyVFl3OBj1MUvo17v99LGdQNFcAiWR/pRsDvXY415FzootwShgQpmvuPLP7buTqVcrRnwRr2hZcpaydOyEaErYsuEPiot0RPrvsIXSkSI4NlIbqO2i4gjfF8FwpDzyY0WtAvUbsY8dKxWXzcIWtQzUyYeqJq1R0Yh8p3ijmLgrkpAJTI/Lz8WT2foUFg7gYQwc9xbFN6aQzQwUQ0Y8s0DDvQqnbby12IXXHI+rjuh1TH8lIRPw/UsFInJn3WS1MBp4FRiXwRs9EwVhfeb+b8Z5rnaQ3RrmM8SY0kjg0i05rkMkygEnPuSec6qKYREHW8n4wbYQNhJvDW9RhUIGnzn3IQRJB57bOZ8xwPkZ97PM0WGsCMWwupSOuEk/NsFe69cZwbElYZJeqeA/bKKsmRsJ/tjzyYMLUlj4L++4GQIwPHgtjmQ9kUEeaw== dgoldste@wmce3-cb7\n"",; ""path"": ""/home/batch-worker/.ssh/authorized_keys""; }; ]; }; },; ""requireGuestProvisionSignal"": true,; ""secrets"": [],; ""windowsConfiguration"": null; },; ""plan"": null,; ""platformFaultDomain"": null,; ""priority"": ""Spot"",; ""provisioningState"": ""Succeeded"",; ""proximityPlacementGroup"": null,; ""resourceGroup"": ""dgoldste"",; ""resources"": null,; ""scheduledEvent",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686:7281,patch,patchSettings,7281,https://hail.is,https://github.com/hail-is/hail/pull/11144#issuecomment-990039686,2,['patch'],"['patchMode', 'patchSettings']"
Deployability,"ugh, that used to just use copy_image and skopeo did The Right Thing (i.e. copy if it the tag had been updated).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13536#issuecomment-1703112668:103,update,updated,103,https://hail.is,https://github.com/hail-is/hail/pull/13536#issuecomment-1703112668,1,['update'],['updated']
Deployability,"unfortunately, we won't know if this works until it merges because the CI adding the new permissions hasn't been deployed yet. I don't think this change will do any harm though if it's not right.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12059#issuecomment-1197094359:113,deploy,deployed,113,https://hail.is,https://github.com/hail-is/hail/pull/12059#issuecomment-1197094359,1,['deploy'],['deployed']
Deployability,"update: I made checking the keys in the RVD constructor optional, as it forced the broadcast of the partitioner every time we construct an RVD, and enabled it (via a flag HailContext.checkRVDKeys) only for the Scala tests. @tpoterba Thoughts on this? Should keep it enabled in production and just add a flag to disable it for the joint caller? If not, should we enable it for the Python tests? If so, we'll probably want some additional tests with it disabled so we at least test somewhat we release.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533:0,update,update,0,https://hail.is,https://github.com/hail-is/hail/pull/5424#issuecomment-466819533,2,"['release', 'update']","['release', 'update']"
Deployability,"update: `calculateKeyRanges` doesn't work correctly when there's only one element (which we never hit, since `coerce` considers those to be sorted), which was causing the shuffle to drop the only moved element in some of my tests (which is why the tests were running so much faster than the non-shuffling version). . I did fix something in the ReorderedPartitions dependencies that was causing an extra partition to be included in the dependencies during the `union_rows`, and now this branch (sans explode_rows changes) is running at about the same speed as SplitMulti was previously, although there's still generally noticeable bump for the first iteration. so @tpoterba I think this is probably set for review.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941:0,update,update,0,https://hail.is,https://github.com/hail-is/hail/pull/4076#issuecomment-410349941,1,['update'],['update']
Deployability,update: took 160s on profile225 (2.0GB as .vcf.gz). The size input to LD Prune (after filtering and split-multi) is 700MB (as mt). 1KG is 16MB as an mt. There's clearly a lot of overhead for small datasets.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5078#issuecomment-452419381:0,update,update,0,https://hail.is,https://github.com/hail-is/hail/pull/5078#issuecomment-452419381,1,['update'],['update']
Deployability,updated error:; ```; is.hail.utils.HailException: OrderedRVD error! Unexpected key in partition 1; Range bounds for partition 1: ([bar]-[foo]]; Key should be in partition 1: ([bar]-[foo]]; Invalid key: [quam]; ```; 🤔,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793:0,update,updated,0,https://hail.is,https://github.com/hail-is/hail/issues/4055#issuecomment-410039793,1,['update'],['updated']
Deployability,updated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5496#issuecomment-473080271:0,update,updated,0,https://hail.is,https://github.com/hail-is/hail/pull/5496#issuecomment-473080271,1,['update'],['updated']
Deployability,upgraded to `crime against humanity`. https://hail.zulipchat.com/#narrow/stream/127527-team/subject/Dataproc/near/135016050,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4038#issuecomment-426225971:0,upgrade,upgraded,0,https://hail.is,https://github.com/hail-is/hail/issues/4038#issuecomment-426225971,1,['upgrade'],['upgraded']
Deployability,"us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; WARNING: remote temporary directory ""gs://hail-batch-jigold-oxmmp/bar/foo"" is not located in the selected compute region for Batch jobs ""us-east1"".; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-batch-jigold-oxmmp/bar/foo; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```. Existing multiregional bucket:. ```; (py311) jigold@wm349-8c4 hail % hailctl batch init; Do you want to create a new bucket in project for temporary files generated by Hail? [y/n]: n; Enter a path to an existing remote temporary directory (ex: gs://my-bucket/batch/tmp): gs://hail-jigold-test-multi-regional; WARNING: remote temporary directory gs://hail-jigold-test-multi-regional is multi-regional. Using this bucket with the Batch Service will incur addtional ingress and egress fees.; Do you want to give service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read/write access to bucket hail-jigold-test-multi-regional? [y/n]: y; Granted service account jigold-59hi5@hail-vdc.iam.gserviceaccount.com read and write access to hail-jigold-test-multi-regional.; Which region do you want your jobs to run in? [us-central1/us-east1/us-east4/us-west1/us-west2/us-west3/us-west4]: us-east1; Which backend do you want to use for Hail Query? [spark/batch/local]: batch; --------------------; FINAL CONFIGURATION:; --------------------; global/domain=hail.is; batch/remote_tmpdir=gs://hail-jigold-test-multi-regional; batch/regions=us-east1; batch/backend=service; query/backend=batch; WARNING: Initialized Hail with warnings! The currently specified configuration will result in additional ingress and egress fees when using Hail Batch.; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568:6658,CONFIGURAT,CONFIGURATION,6658,https://hail.is,https://github.com/hail-is/hail/pull/13279#issuecomment-1679133568,2,"['CONFIGURAT', 'configurat']","['CONFIGURATION', 'configuration']"
Deployability,uvloop 0.19.0 is released; pushing an update to 0.19.0 now,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13853#issuecomment-1775553836:17,release,released,17,https://hail.is,https://github.com/hail-is/hail/pull/13853#issuecomment-1775553836,2,"['release', 'update']","['released', 'update']"
Deployability,"v oauth2 key. Alternatively, we should create a separate login flow doesn't use oauth2 but uses production credentials.; - and interactively tested notebook2 creating notebooks (but haven't tested the config of the notebooks themselves). Summary of changes:; - auth service that handles login/logout flow via Google OAuth2 and user verification via /userdata endpoint. Web sessions are stored in the aiohttp_session cookie (encrypted), command line sessions are stored in tokens file: tokens.json. Token files potentially contain tokens for multiple namespaces (e.g. default and cseed in the example workflow above).; - sessions are now started in the database, table `users.sessions`, which have session_id (32 random bytes, base64-encoded), user_id, creation time and max_age (for expiry); - I write notebook2 to use our async stack; - added a notion of ""deploy config"" that has three parts: location (one of external, k8s or gce), default_namespace (the default namespace to find services), and service_namespace (of overrides for specific services ... so e.g. you can use the default auth with batch in cseed). deploy_config main function is to construct URLs to contact services.; - JWTs and the jwt secret key are gone.; - Simplified configuration/data file handling by enforcing consistent defaults. File paths should be determined by the location, which is loaded from HAIL_DEPLOY_CONFIG_FILE. If that isn't set, I look in ~/.hail/deploy_config.json, and if that doesn't exist, use external/default. All other configuration files are determined by the location: the tokens file is in ~/.hail/tokens.json for external, in /user-tokens/tokens.json for k8s, etc. What remains:; - what a `hailctl dev config` to set the (local) deploy config for switching between default and dev namespaces.; - salt session IDs in the database; - dev oauth2 key; - add `dev deploy` service override option so we can use production/default auth with services deployed in dev namespaces. These are all pretty easy.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251:1983,configurat,configuration,1983,https://hail.is,https://github.com/hail-is/hail/pull/6892#issuecomment-527970251,5,"['configurat', 'deploy']","['configuration', 'deploy', 'deployed']"
Deployability,"valent `v2` table that says whether the rows have been migrated to the `v3` table already. There's then a trigger already in place that every time a row in the `v2` table is updated such as `update aggregated_job_resources_v2 set migrated = 1 where ...`, then we check whether the row has already been ""migrated"". If it hasn't then we insert new rows into the ""v3"" table with the deduped resource ID as the key. The exact trigger is . ```sql; DROP TRIGGER IF EXISTS aggregated_job_resources_v2_after_update $$; CREATE TRIGGER aggregated_job_resources_v2_after_update AFTER UPDATE ON aggregated_job_resources_v2; FOR EACH ROW; BEGIN; DECLARE new_deduped_resource_id INT;. IF OLD.migrated = 0 AND NEW.migrated = 1 THEN; SELECT deduped_resource_id INTO new_deduped_resource_id FROM resources WHERE resource_id = OLD.resource_id;. INSERT INTO aggregated_job_resources_v3 (batch_id, job_id, resource_id, `usage`); VALUES (NEW.batch_id, NEW.job_id, new_deduped_resource_id, NEW.usage); ON DUPLICATE KEY UPDATE; `usage` = `usage` + NEW.usage;; END IF;; END $$; ```. What this PR does is find the keys of all rows in the `aggregated_jobs_resources_v2` table in intervals of 100 rows. This is a ""chunk"". The reason is because we want to keep the transactions small and fast. I optimized this and found 100 rows worked best for performance. We then want to set `migrated=1` for all rows in the given chunk which activates the trigger and also maintains idempotency so we only run the update for each chunk once. . Most of the code in this PR is identifying the bounds of each chunk and then doing the update. We have a burn-in period at the beginning where we migrate chunks serially. Then we migrate the chunks in 10-way parallel. This is to get rid of deadlock errors due to row locks with the ""birthday problem"". Lastly, once all of the updates are complete, we run an audit that makes sure the ""v2"" and ""v3"" tables are equivalent and have the same total aggregate resource usage. I believe I also run this a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782:2223,UPDATE,UPDATE,2223,https://hail.is,https://github.com/hail-is/hail/pull/12849#issuecomment-1771141782,1,['UPDATE'],['UPDATE']
Deployability,"w.from_client_secrets_file(; - self._credentials_file, scopes=self.scopes, state=None; + self._credentials_file, scopes=GoogleFlow.scopes, state=None; ); flow.redirect_uri = redirect_uri; authorization_url, state = flow.authorization_url(access_type='offline', include_granted_scopes='true'); @@ -61,7 +85,7 @@ class GoogleFlow(Flow):; ; def receive_callback(self, request: aiohttp.web.Request, flow_dict: dict) -> FlowResult:; flow = google_auth_oauthlib.flow.Flow.from_client_secrets_file(; - self._credentials_file, scopes=self.scopes, state=flow_dict['state']; + self._credentials_file, scopes=GoogleFlow.scopes, state=flow_dict['state']; ); flow.redirect_uri = flow_dict['callback_uri']; flow.fetch_token(code=request.query['code']); @@ -71,17 +95,56 @@ class GoogleFlow(Flow):; email = token['email']; return FlowResult(email, email, token); ; + @staticmethod; + def perform_installed_app_login_flow(oauth2_client: Dict[str, Any]) -> Dict[str, Any]:; + flow = google_auth_oauthlib.flow.InstalledAppFlow.from_client_config(oauth2_client, GoogleFlow.scopes); + credentials = flow.run_local_server(); + return {; + 'client_id': credentials.client_id,; + 'client_secret': credentials.client_secret,; + 'refresh_token': credentials.refresh_token,; + 'type': 'authorized_user',; + }; +; +; + @staticmethod; + async def get_identity_uid_from_access_token(session: httpx.ClientSession, access_token: str, *, oauth2_client: dict) -> Optional[str]:; + oauth2_client_audience = oauth2_client['installed']['client_id']; + try:; + userinfo = await retry_transient_errors(; + session.get_read_json,; + 'https://www.googleapis.com/oauth2/v3/tokeninfo',; + params={'access_token': access_token},; + ); + if userinfo['aud'] != oauth2_client_audience and userinfo['aud'] != userinfo['sub']:; + return None; +; + email = userinfo['email']; + if email.endswith('iam.gserviceaccount.com'):; + return userinfo['sub']; + # We don't currently track user's unique GCP IAM ID (sub) in the database, just their email.; + r",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329:3327,Install,InstalledAppFlow,3327,https://hail.is,https://github.com/hail-is/hail/pull/13131#issuecomment-1668558329,1,['Install'],['InstalledAppFlow']
Deployability,"wait what, how did this auto-deploy?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4442#issuecomment-424518782:29,deploy,deploy,29,https://hail.is,https://github.com/hail-is/hail/pull/4442#issuecomment-424518782,1,['deploy'],['deploy']
Deployability,"wait, I take back my earlier comment -- of course hail needs to be installed to build the docs. The docs are built by navigating the module hierarchy and docstrings of the installed Hail module.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6996#issuecomment-530058611:67,install,installed,67,https://hail.is,https://github.com/hail-is/hail/pull/6996#issuecomment-530058611,2,['install'],['installed']
Deployability,"we could also update to something from within the last 2 years, too",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4262#issuecomment-418770000:14,update,update,14,https://hail.is,https://github.com/hail-is/hail/pull/4262#issuecomment-418770000,1,['update'],['update']
Deployability,"we hardcode the scala version to 2.11 at the moment:. https://github.com/hail-is/hail/blob/9ef5ebb362e70633a8e29b81768ad92f2853e6cb/hail/build.gradle#L37-L38. so this is pretty much to be expected. We'll make this flexible when Google Dataproc releases a GA image using 2.12, but could certainly document it before then.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8009#issuecomment-580354782:244,release,releases,244,https://hail.is,https://github.com/hail-is/hail/issues/8009#issuecomment-580354782,1,['release'],['releases']
Deployability,we need to make a release first,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6567#issuecomment-509642588:18,release,release,18,https://hail.is,https://github.com/hail-is/hail/issues/6567#issuecomment-509642588,1,['release'],['release']
Deployability,"we print a 2 or 3 more than that). The computeGrammianMatrix function is used by Spark SVD for tall-skinny matrices. It's defined on RowMatrix as:. ```; def computeGramianMatrix(): Matrix = {; val n = numCols().toInt; checkNumColumns(n); // Computes n*(n+1)/2, avoiding overflow in the multiplication.; // This succeeds when n <= 65535, which is checked above; val nt: Int = if (n % 2 == 0) ((n / 2) * (n + 1)) else (n * ((n + 1) / 2)). // Compute the upper triangular part of the gram matrix.; val GU = rows.treeAggregate(new BDV[Double](new Array[Double](nt)))(; seqOp = (U, v) => {; RowMatrix.dspr(1.0, v, U.data); U; }, combOp = (U1, U2) => U1 += U2). RowMatrix.triuToFull(n, GU.data); }; ```. dspr calls to the corresponding BLAS Level 2 command, which updates A to A + x.t \* x in place:; http://www.netlib.org/lapack/explore-html/d7/d15/group__double__blas__level2_ga22adb497a4f41eabc6a8dcac6f326183.html#ga22adb497a4f41eabc6a8dcac6f326183. Down the line we might also consider using BLAS level 3 dsyrk on each partition, which updates A to A + B.t \* B:; http://www.netlib.org/lapack/explore-html/d1/d54/group__double__blas__level3_gae0ba56279ae3fa27c75fefbc4cc73ddf.html#gae0ba56279ae3fa27c75fefbc4cc73ddf. @cseed The ArrayIndex exception on profile225k in the current master is concerning, and may be related to serialization. Here is the full stack trace:. ```; hail: grm: caught exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 3.0 failed 1 times, most recent failure: Lost task 5.0 in stage 3.0 (TID 45, localhost): java.lang.ArrayIndexOutOfBoundsException: 1048578; at com.esotericsoftware.kryo.util.IdentityObjectIntMap.clear(IdentityObjectIntMap.java:345); at com.esotericsoftware.kryo.util.MapReferenceResolver.reset(MapReferenceResolver.java:47); at com.esotericsoftware.kryo.Kryo.reset(Kryo.java:804); at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:570); at org.apache.spark.serializer.KryoSerializationStream.writeObjec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/801#issuecomment-247861703:1675,update,updates,1675,https://hail.is,https://github.com/hail-is/hail/pull/801#issuecomment-247861703,1,['update'],['updates']
Deployability,we'll update this when we update spark,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12298#issuecomment-1275484879:6,update,update,6,https://hail.is,https://github.com/hail-is/hail/pull/12298#issuecomment-1275484879,2,['update'],['update']
Deployability,"well, this is odd. I just rebuilt it for the 6th time on the HPC and it is registering the breeze function. It still fails these tests on the most current version pulled from the github today. ` Gradle suite > Gradle test > is.hail.methods.IBDSuite.ibdPlinkSameOnRealVCF FAILED; Gradle suite > Gradle test > is.hail.methods.IBDSuite.testIBDPlink FAILED; Gradle suite > Gradle test > is.hail.methods.ImputeSexSuite.testImputeSexPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.stats.InbreedingCoefficientSuite.testIbcPlinkVersion FAILED; Gradle suite > Gradle test > is.hail.methods.LinearMixedRegressionSuite.genAndFitLMM FAILED`. It fails out with 244 tests completed and 5 failed. I've attached the test report ; [tests.zip](https://github.com/hail-is/hail/files/795132/tests.zip). There are two differences that I can tell between the current build and the previous times I've tried. 1. I was using a local installation of spark when it worked, whereas now I am using the HPC's version of spark 2.1.0. However, it passed the tests just fine when I was using a local copy of spark 2.0.2 on both my laptop and HPC. . 2. Initially I followed the recommendations on the doc pages to setup the python path references to py4j under `alias hail=""PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$HAIL_HOME/python SPARK_CLASSPATH=$HAIL_HOME/build/libs/hail-all-spark.jar python""` This perhaps didn't export the PYTHONPATH to the py4j 10.4 .zip library if I hadn't run the `hail` command before I tried testing. My initial reaction was to just install a local copy of py4j via pip in my local copy of python since the tests were failing out with complaints about missing py4j module. That worked to get a little farther in the test script, to the point where it was failing out with the breeze function. But, since then I've re-jiggered the PYTHONPATH in the .bash_profile to always be defined to point to the SPARK_HOME version of py4j. This doesn't seem like it would be a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721:919,install,installation,919,https://hail.is,https://github.com/hail-is/hail/issues/1419#issuecomment-281846721,1,['install'],['installation']
Deployability,"wever, I endeavored in this PR to additionally prevent, for example, a compromised `notebook` from masquerading as `batch`. I agree that additionally verifying that the certificate came from a single root certificate (that we, perhaps, destroy after everything is signed) would additionally prevent a malicious user from inserting their certificates into the trusted certificates list. AFAICT, python's `ssl` module has no support for this verification strategy. We could probably build an SSLContext shim that contained two SSLContexts one with a root cert and one with the trusted certs and require certification verification to pass both. Seems easy to get wrong, so I'm inclined to not take this path. ### trusted cert lists. Yeah, it felt a little silly to duplicate the cert in each secret. However, this seems like the simplest approach if I require each principal to only trust a subset of incoming/outgoing principals. If I had one secret per principal, then I have to modify build.yaml or deployment.yamls if I modify the trust sets. That seemed error prone. If I had one secret with all the certs, then when a service starts up it has to select the trusted ones and only insert those into its certificate store. This seems OK, but a little harder to inspect. Duplicating a cert for each trust list to which it belongs occupies what seems like a good spot to me from a developer ergonomics perspective:; - O(trusts) modifications necessary to update/revoke the cert; - O(1) configuration to load a trust list; - no pod-start-time configuration; - the trust list is on the container's file system, so its easy to inspect. Small point: I don't pin the incoming certs yet due to the mTLS challenges. ### create on each deploy. Only creating certs if they don't exist is an easy change. Seems fine, though leaves unresolved how to rotate the certs. I guess I'm inclined to always recreate because it makes rotation the common case, forcing us to make it work well. I think the only way to do a n",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243:1837,deploy,deployment,1837,https://hail.is,https://github.com/hail-is/hail/pull/8561#issuecomment-617428243,1,['deploy'],['deployment']
Deployability,which do you mean?. - switch our exclusive spark testing/deployment from 2.2.0 to 2.3.X?; - test and deploy against 2.3.X as well as 2.2.0?; - something else?. I know people are using Hail with 2.3 successfully right now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4578#issuecomment-431602419:57,deploy,deployment,57,https://hail.is,https://github.com/hail-is/hail/issues/4578#issuecomment-431602419,2,['deploy'],"['deploy', 'deployment']"
Deployability,will add this to the style guide when I update it,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3065#issuecomment-370182016:40,update,update,40,https://hail.is,https://github.com/hail-is/hail/pull/3065#issuecomment-370182016,1,['update'],['update']
Deployability,"with the new mypy update, mypy complains if we don't use the type stubs for our dependencies. This is fixed in main (we add the type stubs) but not in the previously released pip hail, because well it's already released. One option is we decide we don't like this requirement and disable that for mypy (though I do enjoy having the type hints). The problem remains that we lint the released version with the `setup.cfg` on main, so this will fail if we ever tighten our linting. It's not clear to me why we want to lint already-released hail",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409:18,update,update,18,https://hail.is,https://github.com/hail-is/hail/pull/11502#issuecomment-1061947409,5,"['release', 'update']","['released', 'update']"
Deployability,"wnloading click-8.1.7-py3-none-any.whl (97 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 32.4 MB/s eta 0:00:00; Requirement already satisfied: pip>=22.2 in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (23.0.1); Collecting wheel; Using cached wheel-0.41.2-py3-none-any.whl (64 kB); Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from pip-tools==6.13.0) (58.1.0); Collecting packaging>=19.0; Downloading packaging-23.2-py3-none-any.whl (53 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.0/53.0 kB 18.3 MB/s eta 0:00:00; Collecting tomli>=1.1.0; Downloading tomli-2.0.1-py3-none-any.whl (12 kB); Collecting importlib-metadata>=4.6; Downloading importlib_metadata-6.8.0-py3-none-any.whl (22 kB); Collecting pyproject_hooks; Downloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB); Collecting zipp>=0.5; Downloading zipp-3.17.0-py3-none-any.whl (7.4 kB); Installing collected packages: zipp, wheel, tomli, packaging, click, pyproject_hooks, importlib-metadata, build, pip-tools; Successfully installed build-1.0.3 click-8.1.7 importlib-metadata-6.8.0 packaging-23.2 pip-tools-6.13.0 pyproject_hooks-1.0.0 tomli-2.0.1 wheel-0.41.2 zipp-3.17.0. [notice] A new release of pip is available: 23.0.1 -> 23.3; [notice] To update, run: pip3.9 install --upgrade pip; + for package in '$@'; + reqs=python/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtrackin",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:30451,Install,Installing,30451,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,"['Install', 'install']","['Installing', 'installed']"
Deployability,would prefer if #6313 went in first -- don't want to deoptimize anyone's pipelines!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6312#issuecomment-500791514:73,pipeline,pipelines,73,https://hail.is,https://github.com/hail-is/hail/pull/6312#issuecomment-500791514,1,['pipeline'],['pipelines']
Deployability,"wtf. It passes in azure, passes on my laptop under a number of configurations. passes in google on everything but QoB. What's going on",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12588#issuecomment-1397706166:63,configurat,configurations,63,https://hail.is,https://github.com/hail-is/hail/pull/12588#issuecomment-1397706166,1,['configurat'],['configurations']
Deployability,"xt of virtual hosts, the ServerName; # specifies what hostname must appear in the request's Host: header to; # match this virtual host. For the default virtual host (this file) this; # value is not decisive as it is used as a last resort host regardless.; # However, you must set it for any further virtual host explicitly.; ServerName hail.is; ServerAlias www.hail.is. ServerAdmin webmaster@localhost; DocumentRoot /var/www/html. RedirectMatch 404 /\.git. # Available loglevels: trace8, ..., trace1, debug, info, notice, warn,; # error, crit, alert, emerg.; # It is also possible to configure the loglevel for particular; # modules, e.g.; #LogLevel info ssl:warn. ErrorLog ${APACHE_LOG_DIR}/error.log; CustomLog ${APACHE_LOG_DIR}/access.log combined. # For most configuration files from conf-available/, which are; # enabled or disabled at a global level, it is possible to; # include a line for only one particular virtual host. For example the; # following line enables the CGI configuration for this host only; # after it has been globally disabled with ""a2disconf"".; #Include conf-available/serve-cgi-bin.conf; SSLCertificateFile /etc/letsencrypt/live/hail.is/fullchain.pem; SSLCertificateKeyFile /etc/letsencrypt/live/hail.is/privkey.pem; Include /etc/letsencrypt/options-ssl-apache.conf; </VirtualHost>. <VirtualHost *:443>; ServerName ci.hail.is; ServerAdmin webmaster@localhost. LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.so; LoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so; LoadModule headers_module /usr/lib/apache2/modules/mod_headers.so; LoadModule proxy_wstunnel_module /usr/lib/apache2/modules/mod_proxy_wstunnel.so. ProxyRequests Off; ProxyPreserveHost On; ProxyPass /app/subscriptions ws://localhost:8111/app/subscriptions connectiontimeout=240 timeout=1200; ProxyPassReverse /app/subscriptions ws://localhost:8111/app/subscriptions. ProxyPass / http://localhost:8111/ connectiontimeout=240 timeout=1200; ProxyPassReverse / http://localhost:81",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/674#issuecomment-243899170:1768,configurat,configuration,1768,https://hail.is,https://github.com/hail-is/hail/issues/674#issuecomment-243899170,1,['configurat'],['configuration']
Deployability,"y current rough list of things to be done before hail2 is as usable as hail1. It's still pretty long!. ## Necessary code work:; - Add the rest of the core methods from VDS/KT to api2 (#2591 does most for KT, order_by is the only outstanding KT method that's not moved to table there. Same needs to be done for VDS, this isn't too hard); - Add the non-core methods to `hail.methods` / `hail.genetics.methods`; - some stuff here is much harder than the rest, like `filter_alleles`; - This is mostly just labor, but some require more thought than others, like moving TDT to use hail2 expr; - Support intervals in the `index_*` methods. It's possible now to join by locus, but not using the `annotateLociTable` fast path.; - Move to Python 3 so argument order is preserved; - Test the hail2 api much more rigorously than we do now (at the very least, call each parameter branch for each method!; - Typecheck the expression language. This isn't super trivial, and making a nice system to integrate our `typecheck` module and expressions will require some thoughtful design work.; - Some more organization around the package: monkey patching with `import hail.genetics` is an idea I like, but want to think about the edge cases first. ## Documentation; - Document the `index_*` methods / joins; - Translate the _Hail Overview_ tutorial; - Make new tutorials to replace the 2 expr ones we have; - Fill in docs on api2 methods (they're not all there yet); - Fill in docs on expression language (things like __mul__ on NumericExpression haven't been documented); - Write ""integrative docs"" that provide how-tos for common types of workflows. Show the power of annotate / select / group_by/aggregate, etc. ## Longer term QoL:; - Move over tests to Python as much as possible. I looked at the linear regression suite and it can be moved entirely into Python without many problems.; - Write a type parser in Python. The nested calls into the JVM for Type._from_java make the library feel extremely sluggish on tee",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554:991,integrat,integrate,991,https://hail.is,https://github.com/hail-is/hail/pull/2588#issuecomment-352190554,1,['integrat'],['integrate']
Deployability,"y"", line 334, in call_api; _return_http_data_only, collection_formats, _preload_content, _request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a8",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11276,update,update,11276,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['update'],['update']
Deployability,"yeah, I agree. But we can also get that effect by having tiny data but big pipelines",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6629#issuecomment-511032001:75,pipeline,pipelines,75,https://hail.is,https://github.com/hail-is/hail/pull/6629#issuecomment-511032001,1,['pipeline'],['pipelines']
Deployability,"yeah, definitely a bug. If you've already got the patch, can you create a pull request?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/10087#issuecomment-783470432:50,patch,patch,50,https://hail.is,https://github.com/hail-is/hail/issues/10087#issuecomment-783470432,1,['patch'],['patch']
Deployability,"yeah, if/else should be wrapped in parens. Will submit a patch.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2653#issuecomment-355436505:57,patch,patch,57,https://hail.is,https://github.com/hail-is/hail/issues/2653#issuecomment-355436505,1,['patch'],['patch']
Deployability,"yep, cloudtools grabs the latest-deployed version.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5436#issuecomment-468749967:33,deploy,deployed,33,https://hail.is,https://github.com/hail-is/hail/pull/5436#issuecomment-468749967,1,['deploy'],['deployed']
Deployability,"yes, exact same problem. The VDSs are:; `gs://future-variant-calling/future-pipeline/future.vds`; and; `gs://future-variant-calling/old-pipeline/past.vds`. You should have access in case you want to try things.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2743#issuecomment-358477102:76,pipeline,pipeline,76,https://hail.is,https://github.com/hail-is/hail/issues/2743#issuecomment-358477102,2,['pipeline'],['pipeline']
Deployability,"yes, sure. will do that after practicing talk once more. Also, this did remove the logic that prevents a remapping of entries if the field is a top level entry field. This will make UKBB regression performance worst in the very near term. Should we patch this?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969:249,patch,patch,249,https://hail.is,https://github.com/hail-is/hail/pull/4535#issuecomment-430342969,1,['patch'],['patch']
Deployability,"ylinux2014_x86_64.whl (269 kB); Building wheels for collected packages: avro; Building wheel for avro (pyproject.toml): started; Building wheel for avro (pyproject.toml): finished with status 'done'; Created wheel for avro: filename=avro-1.11.2-py2.py3-none-any.whl size=119738 sha256=d7f238f86de270b449b018590930a06270766887328bdb51066eccff2cd696a6; Stored in directory: /home/hadoop/.cache/pip/wheels/e3/a2/1e/5c1be0865f4170a89de34e0a798f32f674a7eaf63a93272c7f; Successfully built avro; Installing collected packages: sortedcontainers, pytz, py4j, commonmark, azure-common, xyzservices, wrapt, uvloop, urllib3, tzdata, typing-extensions, tornado, tenacity, tabulate, six, regex, pyyaml, python-json-logger, pyjwt, pygments, pycparser, pyasn1, protobuf, portalocker, pillow, packaging, orjs; on, oauthlib, numpy, nest-asyncio, multidict, markupsafe, jmespath, idna, humanize, google-crc32c, frozenlist, dill, decorator, charset-normalizer, certifi, cachetools, avro, attrs, asyncinit, async-timeout, yarl, typer, scipy, rsa, rich, requests, python-dateutil, pyasn1-modules, plotly, parsimonious; , jproperties, jinja2, janus, isodate, googleapis-common-protos, google-resumable-media, deprecated, contourpy, cffi, aiosignal, requests-oauthlib, pycares, pandas, google-auth, cryptography, botocore, azure-core, aiohttp, s3transfer, msrest, google-auth-oauthlib, google-api-core, bokeh, azure-storage; -blob, azure-mgmt-core, aiodns, msal, google-cloud-core, boto3, azure-mgmt-storage, msal-extensions, google-cloud-storage, azure-identity; Attempting uninstall: packaging; Found existing installation: packaging 23.2; Uninstalling packaging-23.2:; Successfully uninstalled packaging-23.2; Successfully installed aiodns-2.0.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 asyncinit-0.2.4 attrs-23.1.0 avro-1.11.2 azure-common-1.1.28 azure-core-1.29.3 azure-identity-1.14.0 azure-mgmt-core-1.4.0 azure-mgmt-storage-20.1.0 azure-storage-blob-12.17.0 bokeh-3.2.2 boto3-1.28.41 botocore-1.31.; 41 cache",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:41653,Install,Installing,41653,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,"you have a doctest failure:; ```. =================================== FAILURES ===================================; ______________ [doctest] hail.experimental.ldscsim.get_cov_matrix ______________; 034 trait 1 & trait 4 :math:`r_g` = 0; 035 trait 2 & trait 3 :math:`r_g` = 0.9; 036 trait 2 & trait 4 :math:`r_g` = 0.15; 037 trait 3 & trait 4 :math:`r_g` = 1; 038 To obtain the covariance matrix corresponding to this scenario :math:`h^2` values are; 039 ordered according to user specification and :math:`r_g` values are ordered by the ; 040 order in which the corresponding genetic covariance terms will appear in the ; 041 covariance matrix, reading lines in the upper triangular matrix from left to; 042 right, top to bottom (read first row left to right, read second row left to ; 043 right, etc.), exluding the diagonal.; Differences (unified diff with -expected +actual):; @@ -1,4 +1,12 @@; -array([[0.1 , 0.06928203, 0.09899495, 0. ],; - [0.06928203, 0.3 , 0.22045408, 0.06363961],; - [0.09899495, 0.22045408, 0.2 , 0.34641016],; - [0. , 0.06363961, 0.34641016, 0.6 ]]); +covariance matrix is not positive semidefinite.; +adjusting rg values to make covariance matrix positive semidefinite; +0.4 -> 0.42023852645344634; +0.7 -> 0.5623351779264535; +0.0 -> 0.04812102869845767; +0.9 -> 0.7179252549815345; +0.15 -> 0.20103397396043554; +1.0 -> 0.7534523335539473; +(array([[0.1 , 0.07278745, 0.0795262 , 0.0117872 ],; + [0.07278745, 0.3 , 0.17585505, 0.08529149],; + [0.0795262 , 0.17585505, 0.2 , 0.26100354],; + [0.0117872 , 0.08529149, 0.26100354, 0.6 ]]), [0.42023852645344634, 0.5623351779264535, 0.04812102869845767, 0.7179252549815345, 0.20103397396043554, 0.7534523335539473]). ```. And you'll need to update `ldscsim.rst` because you changed the list of exposed functions, it looks like.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328:1716,update,update,1716,https://hail.is,https://github.com/hail-is/hail/pull/6825#issuecomment-519985328,1,['update'],['update']
Deployability,ython/requirements.txt; + pinned=python/pinned-requirements.txt; ++ mktemp; + new_pinned=/tmp/tmp.YoVBQEw8XF; ++ mktemp; + pinned_no_comments=/tmp/tmp.WRSKGgGEB8; ++ mktemp; + new_pinned_no_comments=/tmp/tmp.C8ggaXDHDt; + PATH=/usr/lib64/qt-3.3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/opt/aws/puppet/bin/:/home/hadoop/.local/bin:/home/hadoop/.local/bin; + pip-compile --quiet python/requirements.txt python/pinned-requirements.txt --output-file=/tmp/tmp.YoVBQEw8XF; WARNING: the legacy dependency resolver is deprecated and will be removed in future versions of pip-tools. The default resolver will be changed to 'backtracking' in pip-tools 7.0.0. Specify --resolver=backtracking to silence this warning.; + cat python/pinned-requirements.txt; + sed /#/d; + sed /#/d; + cat /tmp/tmp.YoVBQEw8XF; + diff /tmp/tmp.WRSKGgGEB8 /tmp/tmp.C8ggaXDHDt; sed '/^pyspark/d' python/pinned-requirements.txt | grep -v -e '^[[:space:]]*#' -e '^$' | tr '\n' '\0' | xargs -0 python3 -m pip install -U; Defaulting to user installation because normal site-packages is not writeable; Collecting aiodns==2.0.0; Using cached aiodns-2.0.0-py2.py3-none-any.whl (4.8 kB); Collecting aiohttp==3.8.5; Using cached aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB); Collecting aiosignal==1.3.1; Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB); Collecting async-timeout==4.0.3; Using cached async_timeout-4.0.3-py3-none-any.whl (5.7 kB); Collecting asyncinit==0.2.4; Using cached asyncinit-0.2.4-py3-none-any.whl (2.8 kB); Collecting attrs==23.1.0; Using cached attrs-23.1.0-py3-none-any.whl (61 kB); Collecting avro==1.11.2; Using cached avro-1.11.2.tar.gz (85 kB); Installing build dependencies: started; Installing build dependencies: finished with status 'done'; Getting requirements to build wheel: started; Getting requirements to build wheel: finished with status 'done'; Preparing metadata (pyproject.toml): started; Preparing metadata (pyproject.toml): finished with stat,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:31871,install,install,31871,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,2,['install'],"['install', 'installation']"
Deployability,"ython3.6/dist-packages/kubernetes/client/api_client.py"", line 168, in __call_api; _request_timeout=_request_timeout); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/api_client.py"", line 355, in request; headers=headers); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 231, in GET; query_params=query_params); File ""/usr/local/lib/python3.6/dist-packages/kubernetes/client/rest.py"", line 222, in request; raise ApiException(http_resp=r); INFO | 2019-06-25 12:37:07,703 | batch.py | mark_complete:501 | no logs for batch-2554-job-4-main-cc8d4 due to previous error, rescheduling pod; INFO | 2019-06-25 12:37:07,730 | batch.py | _create_pod:205 | created pod name: batch-2554-job-4-main-vsk7h for job (2554, 4), task main; INFO | 2019-06-25 12:37:07,788 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,846 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; INFO | 2019-06-25 12:37:07,881 | web_log.py | log:233 | 10.32.14.87 [25/Jun/2019:12:37:07 +0000] ""GET /api/v1alpha/batches/2669/jobs/1 HTTP/1.1"" 200 208 ""-"" ""Python/3.6 aiohttp/3.5.4""; INFO | 2019-06-25 12:37:07,906 | batch.py | update_job_with_pod:976 | update job (2554, 4) with pod batch-2554-job-4-main-vsk7h; ```. The new pod:; ```; + kubectl get pod batch-2554-job-4-main-vsk7h -n batch-pods -o yaml; apiVersion: v1; kind: Pod; metadata:; creationTimestamp: ""2019-06-25T12:37:07Z""; generateName: batch-2554-job-4-main-; labels:; app: batch-job; hail.is/batch-instance: cd50b95a89914efb897965a5e982a29d; uuid: 0c8e6bfd45294d738957b42a3874e25e; name: batch-2554-job-4-main-vsk7h; namespace: batch-pods; resourceVersion: ""72793526""; selfLink: /api/v1/namespaces/batch-pods/pods/batch-2554-job-4-main-vsk7h; uid: f1d2b3ad-9745-11e9-8aa3-42010a80015f; spec:; containers:; - command:; - /bin/bash; - -c; - set -ex; mkdir -p /io/pipeline/pipeline-f559bb010746/__TASK__3/; __RES",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:11405,update,update,11405,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,1,['update'],['update']
Deployability,"ython3.7/site-packages/hail/methods/statgen.py"", line 2007, in pc_relate; block_size=block_size); File ""<decorator-gen-1417>"", line 2, in from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 409, in from_entry_expr; center=center, normalize=normalize, axis=axis, block_size=block_size); File ""<decorator-gen-1429>"", line 2, in write_from_entry_expr; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/typecheck/check.py"", line 614, in wrapper; return __original_func(*args_, **kwargs_); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/linalg/blockmatrix.py"", line 698, in write_from_entry_expr; mt.select_entries(**{field: entry_expr})._write_block_matrix(path, overwrite, field, block_size); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/matrixtable.py"", line 4112, in _write_block_matrix; 'blockSize': block_size})); File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 296, in execute; result = json.loads(self._jhc.backend().executeJSON(jir)); File ""/share/pkg.7/spark/2.4.3/install/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py"", line 1257, in __call__; File ""/share/pkg.7/hail/0.2.46/install/lib/python3.7/site-packages/hail/backend/spark_backend.py"", line 41, in deco; 'Error summary: %s' % (deepest, full, hail.__version__, deepest)) from None; hail.utils.java.FatalError: FileNotFoundException: /scratch/.writeBlocksRDD-l5om7fTy3akZKCYbLDY4AD.crc (Too many open files). Java stack trace:; java.lang.RuntimeException: error while applying lowering 'InterpretNonCompilable'; at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(LoweringPipeline.scala:26); at is.hail.expr.ir.lowering.LoweringPipeline$$anonfun$apply$1.apply(Lowe",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:6625,install,install,6625,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['install'],['install']
Deployability,"zip; ............10%............20%.............30%............40%.............50%............60%.............70%............80%.............90%............100%. Welcome to Gradle 8.3!. Here are the highlights of this release:; - Faster Java compilation; - Reduced memory usage; - Support for running on Java 20. For more details see https://docs.gradle.org/8.3/release-notes.html. Starting a Gradle Daemon (subsequent builds will be faster). > Configure project :; WARNING: Hail primarily tested with Spark 3.3.0, use other versions at your own risk. > Task :shadedazure:compileJava NO-SOURCE; > Task :shadedazure:processResources NO-SOURCE; > Task :shadedazure:classes UP-TO-DATE; > Task :shadedazure:shadowJar; > Task :compileJava NO-SOURCE; > Task :compileScala; > Task :processResources; > Task :classes; > Task :shadowJar. BUILD SUCCESSFUL in 4m 20s; 4 actionable tasks: 4 executed; cp -f build/libs/hail-all-spark.jar python/hail/backend/hail-all-spark.jar; rm -rf build/deploy; mkdir -p build/deploy; mkdir -p build/deploy/src; cp ../README.md build/deploy/; rsync -r \; --exclude '.eggs/' \; --exclude '.pytest_cache/' \; --exclude '__pycache__/' \; --exclude 'benchmark_hail/' \; --exclude '.mypy_cache/' \; --exclude 'docs/' \; --exclude 'dist/' \; --exclude 'test/' \; --exclude '*.log' \; python/ build/deploy/; # Clear the bdist build cache before building the wheel; cd build/deploy; rm -rf build; python3 setup.py -q sdist bdist_wheel; WARNING: The wheel package is not available.; WARNING: The wheel package is not available.; installing to build/bdist.linux-x86_64/wheel; creating build/bdist.linux-x86_64/wheel/hail-0.2.124.dist-info/WHEEL; creating 'dist/hail-0.2.124-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it; adding 'hail/__init__.py'; adding 'hail/builtin_references.py'; adding 'hail/conftest.py'; adding 'hail/context.py'; adding 'hail/hail_logging.py'; adding 'hail/hail_pip_version'; adding 'hail/hail_revision'; adding 'hail/hail_version'; adding",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221:16452,deploy,deploy,16452,https://hail.is,https://github.com/hail-is/hail/issues/13837#issuecomment-1770502221,3,['deploy'],['deploy']
Deployability,"{'active_deadline_seconds': None,; 'affinity': None,; 'automount_service_account_token': None,; 'containers': [{'args': None,; 'command': ['/bin/bash',; '-c',; 'set -ex; mkdir -p '; '/io/pipeline/pipeline-f559bb010746/__TASK__3/; '; '__RESOURCE_FILE__747=/io/pipeline/pipeline-f559bb010746/inputs/5fa554a9; '; '__RESOURCE_FILE__19=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz.tbi; '; '__RESOURCE_FILE__18=/io/pipeline/pipeline-f559bb010746/inputs/eaaeaee5.vcf.gz; '; '__RESOURCE_FILE__6=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.rda; '; '__RESOURCE_FILE__749=/io/pipeline/pipeline-f559bb010746/__TASK__3/c60d4fd0; '; '__RESOURCE_FILE__9=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt_relatednessCutoff_0.125_2000_randomMarkersUsed.sparseSigma.mtx; '; '__RESOURCE_FILE__8=/io/pipeline/pipeline-f559bb010746/__TASK__0/b8c8bb11.gene.varianceRatio.txt; '; '__RESOURCE_FILE__748=/io/pipeline/pipeline-f559bb010746/__TASK__3/60d62d9d; '; '__RESOURCE_FILE__20=/io/pipeline/pipeline-f559bb010746/inputs/6d001f3e; '; 'Rscript /usr/local/bin/step2_SPAtests.R '; '--vcfFile=${__RESOURCE_FILE__18} '; '--vcfFileIndex=${__RESOURCE_FILE__19} '; '--vcfField=GT --minMAF=0 --minMAC=1 '; '--maxMAFforGroupTest=0.5 --chrom=chr1 '; '--sampleFile=${__RESOURCE_FILE__747} '; '--GMMATmodelFile=${__RESOURCE_FILE__6} '; '--varianceRatioFile=${__RESOURCE_FILE__8} '; '--SAIGEOutputFile=${__RESOURCE_FILE__748} '; '--groupFile=${__RESOURCE_FILE__20} '; '--sparseSigmaFile=${__RESOURCE_FILE__9} '; '--IsSingleVarinGroupTest=TRUE '; '--IsOutputAFinCaseCtrl=TRUE 2>&1 | tee '; '${__RESOURCE_FILE__749}'],; 'env': [{'name': 'POD_IP',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'status.podIP'},; 'resource_field_ref': None,; 'secret_key_ref': None}},; {'name': 'POD_NAME',; 'value': None,; 'value_from': {'config_map_key_ref': None,; 'field_ref': {'api_version': 'v1',; 'field_path': 'metadata.name'},; 'reso",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649:2423,pipeline,pipeline,2423,https://hail.is,https://github.com/hail-is/hail/issues/6466#issuecomment-505429649,4,['pipeline'],"['pipeline', 'pipeline-']"
Deployability,"| awk '{{ if ($1 != 1) print $2, $3 }}' > {subset.tmp2}""); .command(f""plink --bed {input_bfile.bed} --bim {input_bfile.bim} --fam {input_bfile.fam} --remove {subset.tmp2} --make-bed {subset.ofile}"". )). # Run shapeit for each contig from 1-3 with the output from subset; for contig in [str(x) for x in range(1, 4)]:; shapeit = p.new_task(); shapeit = (shapeit; .label('shapeit'); .declare_resource_group(ofile={'haps': ""{root}.haps"", 'log': ""{root}.log""}); .command(f'shapeit --bed-file {subset.ofile} --chr {contig} --out {shapeit.ofile}')). # Merge the shapeit output files together; merger = p.new_task(); merger = (merger; .label('merge'); .command('cat {files} >> {ofile}'.format(files="" "".join([t.ofile.haps for t in p.select_tasks('shapeit')]),; ofile=merger.ofile))). # Write the result of the merger to a permanent location; p.write_output(merger.ofile, ""gs://jigold/final_output.txt""). # Execute the pipeline; p.run(dry_run=True); ```. ```bash; #!/bin/bash; set -ex. # change cd to tmp directory; cd /tmp//pipeline.jlQrNJZW/. # __TASK__0 read_input; cp gs://hail-jigold/random_file.txt nfVpMp4n. # __TASK__1 read_input; cp gs://hail-jigold/input.bed 33qZtfwg.bed. # __TASK__2 read_input; cp gs://hail-jigold/input.bim 33qZtfwg.bim. # __TASK__3 read_input; cp gs://hail-jigold/input.fam 33qZtfwg.fam. # __TASK__4 subset; __RESOURCE_GROUP__0=33qZtfwg; __RESOURCE_GROUP__1=yibUlBkL; __RESOURCE__6=yibUlBkL.fam; __RESOURCE__10=29aBQihd; __RESOURCE__1=33qZtfwg.bed; __RESOURCE__2=33qZtfwg.bim; __RESOURCE__3=33qZtfwg.fam; __RESOURCE_GROUP__2=YXS0tQKi; plink --bfile ${__RESOURCE_GROUP__0} --make-bed ${__RESOURCE_GROUP__1}; awk '{ print $1, $2}' ${__RESOURCE__6} | sort | uniq -c | awk '{ if ($1 != 1) print $2, $3 }' > ${__RESOURCE__10}; plink --bed ${__RESOURCE__1} --bim ${__RESOURCE__2} --fam ${__RESOURCE__3} --remove ${__RESOURCE__10} --make-bed ${__RESOURCE_GROUP__2}. # __TASK__5 shapeit; __RESOURCE_GROUP__2=YXS0tQKi; __RESOURCE_GROUP__3=gidGmbcC; shapeit --bed-file ${__RESOURCE_GROUP__",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282:2098,pipeline,pipeline,2098,https://hail.is,https://github.com/hail-is/hail/pull/4937#issuecomment-453230282,1,['pipeline'],['pipeline']
Deployability,"─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ * name TEXT [default: None] [required] │; │ * script TEXT [default: None] [required] │; │ arguments [ARGUMENTS]... You should use -- if you want to pass option-like arguments through. [default: None] │; ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯; ╭─ Options ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮; │ --files TEXT Comma-separated list of files to add to the working directory of the Hail application. │; │ --pyfiles TEXT Comma-separated list of files (or directories with python files) to add to the PYTHONPATH. │; │ --properties -p TEXT Extra Spark properties to set. [default: None] │; │ --gcloud_configuration TEXT Google Cloud configuration to submit job (defaults to currently set configuration). [default: None] │; │ --dry-run --no-dry-run Print gcloud dataproc command, but don't run it. [default: no-dry-run] │; │ --region TEXT Compute region for the cluster. [default: None] │; │ --help Show this message and exit. │; ╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012:6418,configurat,configuration,6418,https://hail.is,https://github.com/hail-is/hail/pull/13447#issuecomment-1681403012,2,['configurat'],['configuration']
Deployability,"（1）yum info atlas-devel; root yum.repos.d $ yum info atlas-devel; Loaded plugins: fastestmirror, langpacks; base | 3.6 kB 00:00:00 ; extras | 3.4 kB 00:00:00 ; updates | 3.4 kB 00:00:00 ; (1/4): base/7/x86_64/group_gz | 155 kB 00:00:00 ; (2/4): extras/7/x86_64/primary_db | 160 kB 00:00:00 ; (3/4): base/7/x86_64/primary_db | 5.3 MB 00:00:09 ; (4/4): updates/7/x86_64/primary_db | 6.5 MB 00:00:32 ; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirrors.tuna.tsinghua.edu.cn; - updates: mirrors.tuna.tsinghua.edu.cn; Available Packages; Name : atlas-devel; Arch : i686; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). Name : atlas-devel; Arch : x86_64; Version : 3.10.1; Release : 10.el7; Size : 1.5 M; Repo : base/7/x86_64; Summary : Development libraries for ATLAS; URL : http://math-atlas.sourceforge.net/; License : BSD; Description : This package contains the libraries and headers for development; : with ATLAS (Automatically Tuned Linear Algebra Software). ## （2）I installed the “atlas-devel” , . root yum.repos.d $ yum install atlas-devel; Loaded plugins: fastestmirror, langpacks; Loading mirror speeds from cached hostfile; - base: mirror.bit.edu.cn; - epel: mirrors.neusoft.edu.cn; - extras: mirror.bit.edu.cn; - updates: mirror.bit.edu.cn; Resolving Dependencies; --> Running transaction check; ---> Package atlas-devel.x86_64 0:3.10.1-10.el7 will be installed; --> Processing Dependency: atlas = 3.10.1-10.el7 for package: atlas-devel-3.10.1-10.el7.x86_64; ............. Installed:; atlas-devel.x86_64 0:3.10.1-10.el7 . Dependency Installed:; atlas.x86_64 0:3.10.1-10.el7 . ## Complete!. ## ######**but when I excute the ""gradle check --info"" ，the error still",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/565#issuecomment-239729893:160,update,updates,160,https://hail.is,https://github.com/hail-is/hail/issues/565#issuecomment-239729893,4,"['Release', 'update']","['Release', 'updates']"
Deployability,👍 We can use the [tar](https://github.com/p12tic/libsimdpp/archive/v2.0-rc2.tar.gz) that he published [on github](https://github.com/p12tic/libsimdpp/releases/tag/v2.0-rc2). I'll get that change in today.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1092#issuecomment-261980358:150,release,releases,150,https://hail.is,https://github.com/hail-is/hail/pull/1092#issuecomment-261980358,1,['release'],['releases']
Deployability,🙏 https://github.com/conda/conda/pull/7385 🙏 ; They finally fixed it for the forthcoming 4.6.x release.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5022#issuecomment-449440250:95,release,release,95,https://hail.is,https://github.com/hail-is/hail/pull/5022#issuecomment-449440250,1,['release'],['release']
Energy Efficiency, 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3435,schedul,scheduler,3435,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency," (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8764,schedul,scheduler,8764,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency, 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGSchedulerEventProcessLoop failed; shutting down SparkContext; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200100,schedul,scheduler,200100,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency," 2019-01-22 13:12:06 DAGScheduler: INFO: Job 0 failed: fold at RVD.scala:603, took 14.445174 s; 2019-01-22 13:12:06 DAGScheduler: INFO: ResultStage 0 (fold at RVD.scala:603) failed in 14.237 s due to Stage cancelled because SparkContext was shut down; 2019-01-22 13:12:06 root: ERROR: SparkException: Job 0 cancelled because SparkContext was shut down; From org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.sca",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:207135,schedul,scheduler,207135,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency," Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5200,schedul,scheduler,5200,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency," ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:4859,reduce,reduce,4859,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['reduce'],['reduce']
Energy Efficiency, at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1680,schedul,scheduler,1680,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency," below needs to be ironed out. Should the number of jobs and state of the job group be recursive or specific to that job group? It's a bit weird for the billing and cancellation to be nested, but the number of jobs etc. are not. More concretely, if a child batch is running, should the parent also be running even if it has no direct child jobs that are running? Thoughts?; > ; > cc: @daniel-goldstein; > ; > ```sql; > UPDATE batches SET; > `state` = 'running',; > time_completed = NULL,; > n_jobs = n_jobs + expected_n_jobs; > WHERE id = in_batch_id;; > ; > ### FIXME FIXME what should the state be of nested job groups?; > UPDATE job_groups; > INNER JOIN (; > SELECT batch_id, job_group_id, CAST(COALESCE(SUM(n_jobs), 0) AS SIGNED) AS staged_n_jobs; > FROM job_groups_inst_coll_staging; > WHERE batch_id = in_batch_id AND update_id = in_update_id; > GROUP BY batch_id, job_group_id; > ) AS t ON job_groups.batch_id = t.batch_id AND job_groups.job_group_id = t.job_group_id; > SET `state` = 'running', time_completed = NULL, n_jobs = n_jobs + t.staged_n_jobs;; > ```. When you say ""billing and cancellation [is] nested"" do you mean that the bill for a group is the sum of the bill for all jobs directly in the group with all jobs in any descendent group?. Since we decided that groups are nested, my inclination is for everything to represent a sum total over the direct jobs and jobs within any descendant groups. From here on out ""sum total"" means exactly that. OK, so:. 1. In the UI (database should do what makes sense and is fast), the number of jobs should be the sum total *but* the pagination should page through the direct jobs.; 2. In the UI (same caveat), the total bill should be the sum total. (Including a sum of direct jobs cost seems fine if it is efficiently computable from the database).; 3. Yes, a group is running if any direct job or job within any descendant group is running (reasoning: if cancelling the group could cancel a running job, the UI needs to indicate that fact).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021:2455,efficient,efficiently,2455,https://hail.is,https://github.com/hail-is/hail/pull/14170#issuecomment-1940341021,1,['efficient'],['efficiently']
Energy Efficiency," loop and the related Hail Python code. No obvious accumulation of RAM use. AFAICT, the oomkiller keeps killing the pipelines. We need to stop this because the oomkiller (a) acts before the JVM GC can free things and (b) prevents us from getting JVM diagnostics on failure. We control the JVM's max heap with hailctl's --master-memory-fraction (default is 0.8 for 80% of the master machine type's advertised RAM). I suggest we set this down to 0.6 and continue using an n1-highmem-16 driver.; If Hail is (incorrectly) accumulating garbage memory per-group, we'll have a better chance diagnosing that with a running JVM instead of one that's been SIGKILL'ed. To understand what's going on, we gotta see what is using RAM in the n1-highmem-16 case. If I could SSH to the cluster, a simple solution is a screen with top -s 300 -n 100 >memory.log (I'd guess no more than 500KiB per hour of logs) and retrieve that file if the cluster fails. If we could get Google Monitoring set up to retrieve process-level memory statistics from the driver node that should also work. Just to be clear, I don't anticipate any changes to Hail in the next week that would change the memory use of this pipeline. There could be a memory leak, but I have no clews that lead to it. I realize this is an unsatisfying answer. I'm pretty perplexed as to what could be the issue here. #### technical details. We'll call the second to most recent run Run A and the most recent run Run B. Run A (like all runs before it) only manages two sample groups before failing. Run B made it through 50 groups before failing on 51. Why did they fail? The syslog for Run A is clear: the oomkiller killed Run A. We lack syslogs for Run B, so we cannot be certain but the lack of a JVM stack trace suggests to me that (a) the driver failed and (b) the driver was killed by the system.; Let's focus on the driver machines. In Run A, we used an n1-highmem-8 which is advertised to have 52GiB (53248 MiB). In Run B, we used an n1-highmem-16 which ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449:1147,Monitor,Monitoring,1147,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1832666449,1,['Monitor'],['Monitoring']
Energy Efficiency," o.s.j.s.ServletContextHandler@25004c63{/api,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@36f9d98a{/jobs/job/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 ContextHandler: INFO: Started o.s.j.s.ServletContextHandler@302922c9{/stages/stage/kill,null,AVAILABLE,@Spark}; 2019-01-22 13:11:22 SparkUI: INFO: Bound SparkUI to 0.0.0.0, and started at http://10.48.225.55:4040; 2019-01-22 13:11:23 DomainSocketFactory: WARN: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.; 2019-01-22 13:11:23 Client: INFO: Requesting a new application from cluster with 21 NodeManagers; 2019-01-22 13:11:23 Client: INFO: Verifying our application has not requested more than the maximum memory capability of the cluster (204800 MB per container); 2019-01-22 13:11:23 Client: INFO: Will allocate AM container, with 896 MB memory including 384 MB overhead; 2019-01-22 13:11:23 Client: INFO: Setting up container launch context for our AM; 2019-01-22 13:11:23 Client: INFO: Setting up the launch environment for our AM container; 2019-01-22 13:11:24 Client: INFO: Preparing resources for our AM container; 2019-01-22 13:11:24 HadoopFSCredentialProvider: INFO: getting token for: hdfs://scc/user/farrell; 2019-01-22 13:11:24 DFSClient: INFO: Created HDFS_DELEGATION_TOKEN token 11364 for farrell on ha-hdfs:scc; 2019-01-22 13:11:26 Client: WARN: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 2019-01-22 13:11:29 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_libs__5184408978318087972.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_libs__5184408978318087972.zip; 2019-01-22 13:11:30 Client: INFO: Uploading resource file:/restricted/projectnb/genpro/github/hail/hail/build/libs/hail-all-spark.jar -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/hail-all-",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:14559,allocate,allocate,14559,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['allocate'],['allocate']
Energy Efficiency, org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2479,schedul,scheduler,2479,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,"![Screen Shot 2022-06-10 at 10 08 59 AM](https://user-images.githubusercontent.com/106194/173083939-aea57012-ddcc-4240-9ad0-55163eb6df04.png). Average utilization is marginally improved (maybe 2.5% -> 5%), but total number of wasted cores goes way down because the total number of cores is 16 rather than 16 * 8 = 128. This also suggests the autoscaler/scheduler could be made substantially smarter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11902#issuecomment-1152405036:353,schedul,scheduler,353,https://hail.is,https://github.com/hail-is/hail/pull/11902#issuecomment-1152405036,1,['schedul'],['scheduler']
Energy Efficiency,""": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. T",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5081,reduce,reduce,5081,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['reduce'],['reduce']
Energy Efficiency,"""analysis_type=ApplyRecalibration input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false input=[(RodBinding name=input source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.unfiltered.vcf)] recal_file=(RodBinding name=recal_file source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recal) tranches_file=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.tranches out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub ts_filter_level=98.5 ignore_filter=null mode=SNP filter_mismatching_base_and_quals=false""; ##CombineVariants=""analysis_type=Combine",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:1162,monitor,monitorThreadEfficiency,1162,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['monitor'],['monitorThreadEfficiency']
Energy Efficiency,"""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": {; ""state"": ""RUNNING"",; ""stateStartTime"": ""2016-12-15T19:00:51.004Z""; },; ""clusterUuid"": ""fb371071-cdd1-4bed-bd1b-3ce3049d07e5"",; ""statusHistory"": [; {; ""state"": ""CREATING"",; ""stateStartTime"": ""2016-12-15T18:59:19.745Z""; }; ],; ""metrics"": {}; }; ```. # Analysis Thus Far. The job doesn't terminate on its own. After stopping the job in the Google Cloud UI, subsequent jobs don't seem to be accepted (i.e. they hang before I see the Spark progress bar). The source of the error is a log statement. Apparently a task failure is triggering a giant log statement. The [log statement (from Spark 2.0 branch)](https://github.com/apache/spark/blob/branch-2.0/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L693) seems innocuous. So the real question is why are the tasks failing?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:5695,schedul,scheduler,5695,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,3,['schedul'],['scheduler']
Energy Efficiency,$12.hasNext(Iterator.scala:440); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:5509,schedul,scheduler,5509,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; ```. _All_ of my workers had this error:; ```; java.lang.NegativeArraySizeException; 	at java.util.Arrays.copyOf(Arrays.java:3236); 	at is.hail.annotations.Region.ensure(Region.scala:139); 	at is.hail.annotations.Region.allocate(Region.scala:152); 	at is.hail.annotations.Region.allocate(Region.scala:159); 	at is.hail.expr.types.TContainer.allocate(TContainer.scala:127); 	at is.hail.annotations.RegionValueBuilder.fixupArray(RegionValueBuilder.scala:278); 	at is.hail.annotations.RegionValueBuilder.addRegionValue(RegionValueBuilder.scala:432); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:815); 	at is.hail.expr.MatrixMapRows$$anonfun$25$$anonfun$apply$19.apply(Relational.scala:804); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:914); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$16$$anon$3.next(OrderedRVD.scala:908); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$12.next(Iterator.scala:444); 	at is.hail.rv,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681:6781,allocate,allocate,6781,https://hail.is,https://github.com/hail-is/hail/issues/3508#issuecomment-387563681,1,['allocate'],['allocate']
Energy Efficiency,$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskScheduler,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202296,schedul,scheduler,202296,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,(Context.scala:15); 	at is.hail.utils.WithContext.map(Context.scala:27); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at is.hail.io.vcf.LoadVCF$$anonfun$14$$anonfun$apply$7.apply(LoadVCF.scala:301); 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440); 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:438); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3363,schedul,scheduler,3363,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201259,schedul,scheduler,201259,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCancelAllJobs(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1741); at org.apache.spark.util.Ev,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204906,schedul,scheduler,204906,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,(MethodInvoker.java:244); 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357); 	at py4j.Gateway.invoke(Gateway.java:280); 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132); 	at py4j.commands.CallCommand.execute(CallCommand.java:79); 	at py4j.GatewayConnection.run(GatewayConnection.java:214); 	at java.lang.Thread.run(Thread.java:745)java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310); 	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:206); 	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66); 	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96); 	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:81); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748)org.apache.spark.SparkException: Failed to get broadcast_4_piece0 of broadcast_4; 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at org.ap,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:6432,schedul,scheduler,6432,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,*sigh*. ```; E java.lang.RuntimeException: Stream is already closed.; E 	at com.azure.storage.common.StorageOutputStream.checkStreamState(StorageOutputStream.java:79); E 	at com.azure.storage.common.StorageOutputStream.flush(StorageOutputStream.java:89); E 	at is.hail.io.fs.AzureStorageFS$$anon$3.close(AzureStorageFS.scala:291); E 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159); E 	at is.hail.utils.package$.using(package.scala:640); E 	at is.hail.io.fs.FS.writePDOS(FS.scala:428); E 	at is.hail.io.fs.FS.writePDOS$(FS.scala:427); E 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend.$anonfun$parallelizeAndComputeWithIndex$3$adapted(ServiceBackend.scala:114); E 	at is.hail.backend.service.ServiceBackend$$anon$2.$anonfun$call$1(ServiceBackend.scala:122); E 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); E 	at is.hail.services.package$.retryTransientErrors(package.scala:124); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:122); E 	at is.hail.backend.service.ServiceBackend$$anon$2.call(ServiceBackend.scala:119); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:750); ```. Azure's `StorageOutputStream.close` method is not idempotent in the version that we use. It has been made idempotent in `12.18.0`. I would be surprised if spark let us upgrade to a version that recent,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901:811,adapt,adapted,811,https://hail.is,https://github.com/hail-is/hail/pull/12968#issuecomment-1532328901,1,['adapt'],['adapted']
Energy Efficiency,", line 2131, in count; File ""/share/pkg/spark/2.2.1/install/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8318,schedul,scheduler,8318,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,", thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.3M allocated (256.0K blocks / 2.1M chunks), regions.size = 4, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:10.794 : INFO: RegionPool: REPORT_THRESHOLD: 2.4M allocated (320.0K blocks / 2.1M chunks), regions.size = 5, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.286 : INFO: RegionPool: REPORT_THRESHOLD: 28.8M allocated (576.0K blocks / 28.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.657 : INFO: RegionPool: REPORT_THRESHOLD: 30.8M allocated (576.0K blocks / 30.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.683 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.2M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:43:11.709 : INFO: RegionPool: REPORT_THRESHOLD: 32.8M allocated (576.0K blocks / 32.3M chunks), regions.size = 9, 0 current java objects, thread 10: pool-2-thread-2; 2023-09-27 16:44:22.426 GoogleStorageFS$: INFO: createNoCompression: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.495 GoogleStorageFS$: INFO: close: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.620 GoogleStorageFS$: INFO: closed: gs://1-day/tmp/hail/TSOfOrgZUmbVixnRiKWQFB/aggregate_intermediates/-Pt3gNtQW5WoBdCTDPQiwHda9c265f2-fbd8-4f1b-bcde-fbf29180c347; 2023-09-27 16:44:22.621 : INFO: TaskReport: stage=0, partition=7028, attempt=0, peakBytes=62266032, peakBytesReadable=59.38 MiB, chunks requested=72126, cache hits=72121; 2023-09-27 16:44:22.622 : INFO: RegionPool: FREE: 59.4M allocated (25.2M blocks / 34.2M chunks), regions.size = 11, 0 current java objects, thread 10: pool-2-t",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:4874,allocate,allocated,4874,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['allocate'],['allocated']
Energy Efficiency,",0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RD",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1508,schedul,scheduler,1508,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,",; ""https://www.googleapis.com/auth/logging.write""; ]; },; ""masterConfig"": {; ""numInstances"": 1,; ""instanceNames"": [; ""cluster-2-m""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""workerConfig"": {; ""numInstances"": 2,; ""instanceNames"": [; ""cluster-2-w-0"",; ""cluster-2-w-1""; ],; ""imageUri"": ""https://www.googleapis.com/compute/v1/projects/cloud-dataproc/global/images/dataproc-1-1-20161212-154751"",; ""machineTypeUri"": ""https://www.googleapis.com/compute/v1/projects/broad-ctsa/zones/us-central1-f/machineTypes/n1-standard-4"",; ""diskConfig"": {; ""bootDiskSizeGb"": 10; }; },; ""softwareConfig"": {; ""imageVersion"": ""1.1.15"",; ""properties"": {; ""distcp:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""distcp:mapreduce.map.memory.mb"": ""3072"",; ""distcp:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""distcp:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:mapreduce.map.cpu.vcores"": ""1"",; ""mapred:mapreduce.map.java.opts"": ""-Xmx2457m"",; ""mapred:mapreduce.map.memory.mb"": ""3072"",; ""mapred:mapreduce.reduce.cpu.vcores"": ""2"",; ""mapred:mapreduce.reduce.java.opts"": ""-Xmx4915m"",; ""mapred:mapreduce.reduce.memory.mb"": ""6144"",; ""mapred:yarn.app.mapreduce.am.command-opts"": ""-Xmx4915m"",; ""mapred:yarn.app.mapreduce.am.resource.cpu-vcores"": ""2"",; ""mapred:yarn.app.mapreduce.am.resource.mb"": ""6144"",; ""spark:spark.driver.maxResultSize"": ""1920m"",; ""spark:spark.driver.memory"": ""3840m"",; ""spark:spark.executor.cores"": ""2"",; ""spark:spark.executor.memory"": ""5586m"",; ""spark:spark.yarn.am.memory"": ""5586m"",; ""spark:spark.yarn.am.memoryOverhead"": ""558"",; ""spark:spark.yarn.executor.memoryOverhead"": ""558"",; ""yarn:yarn.nodemanager.resource.memory-mb"": ""12288"",; ""yarn:yarn.scheduler.maximum-allocation-mb"": ""12288"",; ""yarn:yarn.scheduler.minimum-allocation-mb"": ""1024""; }; }; },; ""status"": ",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:4808,reduce,reduce,4808,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['reduce'],['reduce']
Energy Efficiency,"-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.910 root: INFO: RegionPool: REPORT_THRESHOLD: 512.0M allocated (111.9M blocks / 400.1M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:42.730 root: INFO: RegionPool: REPORT_THRESHOLD: 1.2G allocated (439.1M blocks / 781.5M chunks), regions.size = 5, 0 current java objects, thread 8: pool-1-thread-1""}, 'service_backend_debug_info': {'batch_attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'driver_cores': None, 'driver_memory': None, ...}} or 'batch.worker.jvm_entryway_protocol.EndOfStream' in {'batch_status': {'attributes': {'name': 'test_tiny_driver_has_tiny_memory'}, 'billing_project': 'test', 'closed': True, 'complete': True, ...}, 'job_status': {'attributes': {'name': 'driver'}, 'batch_id': 6627669, 'billing_project': 'test', 'cost': 0.0015413897092729028, ...}, 'log': {'main':",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:63861,allocate,allocated,63861,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,1,['allocate'],['allocated']
Energy Efficiency,-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2422); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2204); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2225); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2244); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2269); 	at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030); 	at org.apache.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5803,schedul,scheduler,5803,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,.Cipher.doFinal(Cipher.java:2463) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLCipher$T12GcmReadCipherGenerator$GcmReadCipher.decrypt(SSLCipher.java:1606) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decodeInputRecord(SSLSocketInputRecord.java:262) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketInputRecord.decode(SSLSocketInputRecord.java:190) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:109) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:9351,Meter,MeteredStream,9351,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['Meter'],['MeteredStream']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinsti,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6921,schedul,scheduler,6921,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.ge,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:6027,schedul,scheduler,6027,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpre,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9282,schedul,scheduler,9282,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202481,schedul,scheduler,202481,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13159,schedul,scheduler,13159,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,.SSLException: Tag mismatch!; 	at sun.security.ssl.Alert.createSSLException(Alert.java:133) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:331) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:274) ~[?:1.8.0_392]; 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:269) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:119) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1404) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1372) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73) ~[?:1.8.0_392]; 	at sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:966) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:284) ~[?:1.8.0_392]; 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[?:1.8.0_392]; 	at sun.net.www.MeteredStream.read(MeteredStream.java:134) ~[?:1.8.0_392]; 	at java.io.FilterInputStream.read(FilterInputStream.java:133) ~[?:1.8.0_392]; 	at sun.net.www.protocol.http.HttpURLConnection$HttpInputStream.read(HttpURLConnection.java:3460) ~[?:1.8.0_392]; 	at com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:164) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385) ~[?:1.8.0_392]; 	at is.hail.relocated.com.google.cloud.storage.StorageByteChannels$ScatteringByteChannelFacade.read(StorageByteChannels.java:242) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f8ce5c699083a8effa13127b0ba0c41ad.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.relocated.com.google.cloud.storage.ApiaryUnbufferedReadableByteChannel.read(ApiaryUnbufferedReadableByteChannel.java:113) ~[gs:__hail-query-ger0g_jars_dking_3xzj5v1p7z3y_38ae919f,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352:1052,Meter,MeteredStream,1052,https://hail.is,https://github.com/hail-is/hail/pull/14094#issuecomment-1852957352,1,['Meter'],['MeteredStream']
Energy Efficiency,.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12641,schedul,scheduler,12641,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSch,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:201104,schedul,scheduler,201104,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply(DAGScheduler.scala:723); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.doCanc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204751,schedul,scheduler,204751,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"._sock.recv_into(b); socket.timeout: timed out; ```. Seems that the simplest issue may be to increase `read_timeout` past 120 seconds, although depending on the causes of this issue, that may not eliminate the problem, and of course leaves a long delay, which may be unacceptable for the use-case. As for why read takes so long: not 100% sure yet, setting up batch and CI is still incomplete, and I have not triggered this error myself. My guess is that Kubernetes takes too long to generate the response, either due to garbage collection, or simply because the requested information takes N > 120 seconds to return. That would be a very long time for any reasonable response, so either the resource isn't ready and it waits, or there are network connectivity issues. If network issues, not sure what solutions are. If I were on AWS, I would think about using a larger instance, with a higher-bandwidth NIC.; * Possible connection: https://github.com/arangodb/arangodb/issues/7813 ; * Possible solution: Reduce work Kubernetes must do to return response. #### 2nd set of errors:; ```log; # Batch; ERROR	| 2018-12-18 21:25:00,095 	| server.py 	| run_forever:447 | run_forever: target kube_event_loop threw exception; Traceback (most recent call last):; File ""/usr/lib/python3.6/site-packages/urllib3/response.py"", line 601, in _update_chunk_length; self.chunk_left = int(line, 16); ValueError: invalid literal for int() with base 16: b''. # CI; ERROR	| 2018-12-18 21:25:22,041 	| app.py 	| log_exception:1761 | Exception on /refresh_batch_state [POST]; Traceback (most recent call last):; ...; File ""/home/hail-ci/.local/lib/python3.7/site-packages/batch/requests_helper.py"", line 11, in raise_on_failure; response=response; requests.exceptions.HTTPError: 500 Server Error for url http://batch.default/jobs. <!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 3.2 Final//EN"">; <title>500 Internal Server Error</title>; ```; Initiator seems; https://github.com/datawire/ambassador/issues/554. Solution may be to cat",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389:2030,Reduce,Reduce,2030,https://hail.is,https://github.com/hail-is/hail/issues/4984#issuecomment-450444389,1,['Reduce'],['Reduce']
Energy Efficiency,".apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/$JAR_FILE. PYHAIL_ZIP=gs://hail-common/pyhail-hail-is-master-$HASH.zip. gcloud dataproc jobs submit pyspark \; $script \; --cluster $cluster \; --files=$JAR \; --py-files=$PYHAIL_ZIP \; --properties=""spark.driver.extraClassPath=./$JAR_FILE,spark.executor.extraClassPath=./$JAR_FILE"" \; --; ```; cluster JSON:; ```; {; ""projectId"": ""broad-ctsa"",; ""clusterName"": ""cluster-2"",; ""config"": {; ""config",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:2170,schedul,scheduler,2170,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency,.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:2733,schedul,scheduler,2733,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachE,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203129,schedul,scheduler,203129,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.variant.MatrixTable.write(MatrixTable.scala:2393); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:3518,schedul,scheduler,3518,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:497); 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237); 	at py4j.reflection.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:5075,schedul,scheduler,5075,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1867,schedul,scheduler,1867,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:22); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:529); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMetho,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:4314,schedul,scheduler,4314,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.hail.utils.richUtils.RichContextRDD.writePartitions(RichContextRDD.scala:48); 	at is.hail.io.RichContextRDDRegionValue$.writeRows$extension(RowStore.scala:1096); 	at is.hail.rvd.RVD$class.write(RVD.scala:467); 	at is.hail.rvd.OrderedRVD.write(OrderedRVD.scala:32),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5963,schedul,scheduler,5963,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.table.Table.collect(Table.scala:841); 	at is.hail.table.Table.collectJSON(Table.scala:844); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.jav,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:5318,schedul,scheduler,5318,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at is.hail.rvd.RVD$class.count(RVD.scala:183); 	at is.hail.rvd.OrderedRVD.count(OrderedRVD.scala:19); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:471); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:469); 	at is.hail.utils.package$.time(package.scala:82); 	at is.hail.methods.LDPrune$.apply(LDPrune.scala:469); 	at is.hail.methods.LDPrune.apply(LDPrune.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:4234,schedul,scheduler,4234,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:771); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:88); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$7.apply(InterpretNonCompilable.scala:19); 	at is.hail.expr.ir.InterpretNonCompilable$$anonfun$7.apply(InterpretNonCompilable.scala:19); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1408,schedul,scheduler,1408,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,1,['schedul'],['scheduler']
Energy Efficiency,.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:200851,schedul,scheduler,200851,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply(DAGScheduler.scala:1529); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1457); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelAllJobs$1.apply$mcVI$sp(DAGScheduler.scala:723); at org.apache.spark.scheduler.DAGScheduler$$anonfun$doCancelA,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:204498,schedul,scheduler,204498,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,.broadinstitute.hail.RichPairRDD$$anonfun$mapValuesWithKey$extension$1$$anonfun$apply$5.apply(Utils.scala:459); at scala.collection.Iterator$$anon$11.next(Iterator.scala:328); at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:5917,schedul,scheduler,5917,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,.expr.ir.LowerOrInterpretNonCompilable$.rewrite$1(LowerOrInterpretNonCompilable.scala:53); E 	at is.hail.expr.ir.LowerOrInterpretNonCompilable$.apply(LowerOrInterpretNonCompilable.scala:72); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.transform(LoweringPass.scala:69); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$3(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.$anonfun$apply$1(LoweringPass.scala:16); E 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:81); E 	at is.hail.expr.ir.lowering.LoweringPass.apply(LoweringPass.scala:14); E 	at is.hail.expr.ir.lowering.LoweringPass.apply$(LoweringPass.scala:13); E 	at is.hail.expr.ir.lowering.LowerOrInterpretNonCompilablePass$.apply(LoweringPass.scala:64); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1(LoweringPipeline.scala:22); E 	at is.hail.expr.ir.lowering.LoweringPipeline.$anonfun$apply$1$adapted(LoweringPipeline.scala:20); E 	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36); E 	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33); E 	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38); E 	at is.hail.expr.ir.lowering.LoweringPipeline.apply(LoweringPipeline.scala:20); E 	at is.hail.expr.ir.CompileAndEvaluate$._apply(CompileAndEvaluate.scala:47); E 	at is.hail.backend.spark.SparkBackend._execute(SparkBackend.scala:454); E 	at is.hail.backend.spark.SparkBackend.$anonfun$executeEncode$2(SparkBackend.scala:490); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:75); E 	at is.hail.utils.package$.using(package.scala:635); E 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:17); E 	at is.hail.backend.ExecuteContext$.scoped(ExecuteContext,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229:4365,adapt,adapted,4365,https://hail.is,https://github.com/hail-is/hail/pull/12754#issuecomment-1456467229,1,['adapt'],['adapted']
Energy Efficiency,.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:507); 	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3469); 	at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174); 	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574); 	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521); 	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540); 	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365); 	at is.hail.io.fs.HadoopFSURL.<init>(HadoopFS.scala:76); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:88); 	at is.hail.io.fs.HadoopFS.parseUrl(HadoopFS.scala:85); 	at is.hail.io.fs.FS.exists(FS.scala:618); 	at is.hail.io.fs.FS.exists$(FS.scala:618); 	at is.hail.io.fs.HadoopFS.exists(HadoopFS.scala:85); 	at __C5Compiled.apply(Emit.scala); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$3$adapted(LocalBackend.scala:223); 	at is.hail.backend.ExecuteContext.$anonfun$scopedExecution$1(ExecuteContext.scala:144); 	at is.hail.utils.package$.using(package.scala:664); 	at is.hail.backend.ExecuteContext.scopedExecution(ExecuteContext.scala:144); 	at is.hail.backend.local.LocalBackend.$anonfun$_jvmLowerAndExecute$2(LocalBackend.scala:223); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend._jvmLowerAndExecute(LocalBackend.scala:223); 	at is.hail.backend.local.LocalBackend._execute(LocalBackend.scala:249); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$2(LocalBackend.scala:314); 	at is.hail.utils.ExecutionTimer.time(ExecutionTimer.scala:84); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1(LocalBackend.scala:309); 	at is.hail.backend.local.LocalBackend.$anonfun$execute$1$adapted(LocalBackend.scala:308); 	at is.hail.backend.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:78); 	at is.hail.utils.package,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699:1907,adapt,adapted,1907,https://hail.is,https://github.com/hail-is/hail/issues/13904#issuecomment-1973731699,4,['adapt'],['adapted']
Energy Efficiency,.hail.expr.ir.Emit.emitI(Emit.scala:815); 	at is.hail.expr.ir.Emit$.$anonfun$apply$4(Emit.scala:99); 	at is.hail.expr.ir.EmitCodeBuilder$.scoped(EmitCodeBuilder.scala:19); 	at is.hail.expr.ir.EmitCodeBuilder$.scopedCode(EmitCodeBuilder.scala:24); 	at is.hail.expr.ir.EmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1044); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.WrappedEmitMethodBuilder.emitWithBuilder$(EmitClassBuilder.scala:1095); 	at is.hail.expr.ir.EmitFunctionBuilder.emitWithBuilder(EmitClassBuilder.scala:1192); 	at is.hail.expr.ir.Emit$.apply(Emit.scala:97); 	at is.hail.expr.ir.Compile$.apply(Compile.scala:78); 	at is.hail.TestUtils$.eval(TestUtils.scala:256); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$5(TestUtils.scala:366); 	at scala.collection.immutable.Set$Set4.foreach(Set.scala:289); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4(TestUtils.scala:348); 	at is.hail.TestUtils$.$anonfun$assertEvalsTo$4$adapted(TestUtils.scala:339); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$3(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$2(ExecuteContext.scala:47); 	at is.hail.utils.package$.using(package.scala:618); 	at is.hail.annotations.RegionPool$.scoped(RegionPool.scala:13); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:46); 	at is.hail.backend.spark.SparkBackend.withExecuteContext(SparkBackend.scala:276); 	at is.hail.expr.ir.ExecuteContext$.$anonfun$scoped$1(ExecuteContext.scala:40); 	at is.hail.utils.ExecutionTimer$.time(ExecutionTimer.scala:52); 	at is.hail.expr.ir.ExecuteContext$.scoped(ExecuteContext.scala:39); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:339); 	at is.hail.TestUtils$.assertEvalsTo(TestUtils.scala:314); 	at is.hail.expr.ir.IRSuite.testStreamLenUnconsumedInnerStream(IRSuite.scala:1800); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604:2300,adapt,adapted,2300,https://hail.is,https://github.com/hail-is/hail/pull/10330#issuecomment-827119604,1,['adapt'],['adapted']
Energy Efficiency,.lang.reflect.Method.invoke(Method.java:498); E 	at is.hail.JVMEntryway$1.run(JVMEntryway.java:105); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748); E ; E java.util.concurrent.TimeoutException: Did not observe any item or terminal signal within 5000ms in 'flatMap' (and no fallback has been configured); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.handleTimeout(FluxTimeout.java:294); E 	at reactor.core.publisher.FluxTimeout$TimeoutMainSubscriber.doTimeout(FluxTimeout.java:279); E 	at reactor.core.publisher.FluxTimeout$TimeoutTimeoutSubscriber.onNext(FluxTimeout.java:418); E 	at reactor.core.publisher.FluxOnErrorResume$ResumeSubscriber.onNext(FluxOnErrorResume.java:79); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.propagateDelay(MonoDelay.java:270); E 	at reactor.core.publisher.MonoDelay$MonoDelayRunnable.run(MonoDelay.java:285); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:68); E 	at reactor.core.scheduler.SchedulerTask.call(SchedulerTask.java:28); E 	at java.util.concurrent.FutureTask.run(FutureTask.java:266); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180); E 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293); E 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); E 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); E 	at java.lang.Thread.run(Thread.java:748). ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222:5234,schedul,scheduler,5234,https://hail.is,https://github.com/hail-is/hail/pull/11883#issuecomment-1144890222,12,"['Schedul', 'schedul']","['ScheduledFutureTask', 'ScheduledThreadPoolExecutor', 'SchedulerTask', 'scheduler']"
Energy Efficiency,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2069); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:3677,schedul,scheduler,3677,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,2,['schedul'],['scheduler']
Energy Efficiency,.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinstitute.hail.driver.Main$$anonfun$runCommands$1$$anonfun$1.apply(Main.scala:144); at org.broadinst,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:7016,schedul,scheduler,7016,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.collect(RDD.scala:935); at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:139); at is.hail.rvd.RVD$.getKeyInfo(RVD.scala:1063); at is.hail.rvd.RVD$.makeCoercer(RVD.scala:1127); at is.hail.io.vcf.Ma,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635:6122,schedul,scheduler,6122,https://hail.is,https://github.com/hail-is/hail/issues/4755#issuecomment-438447635,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at ,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:9377,schedul,scheduler,9377,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.forea,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202576,schedul,scheduler,202576,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:944); at is.hail.expr.ir.functions.MatrixWriteBlockMatrix.execute(MatrixWriteBlockMatrix.scala:47); at is.hail.expr.ir.functions.WrappedMatrixToValueFunction.execute(RelationalFunctions.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13254,schedul,scheduler,13254,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,".scc.bu.edu:38836 with 21.2 GB RAM, BlockManagerId(4, scc-q07.scc.bu.edu, 38836, None); 2019-01-22 13:11:42 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.186:56622) with ID 1; 2019-01-22 13:11:42 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q02.scc.bu.edu:37399 with 21.2 GB RAM, BlockManagerId(1, scc-q02.scc.bu.edu, 37399, None); 2019-01-22 13:11:43 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.192:52326) with ID 5; 2019-01-22 13:11:43 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q08.scc.bu.edu:44677 with 21.2 GB RAM, BlockManagerId(5, scc-q08.scc.bu.edu, 44677, None); 2019-01-22 13:11:43 YarnSchedulerBackend$YarnDriverEndpoint: INFO: Registered executor NettyRpcEndpointRef(spark-client://Executor) (192.168.18.196:53932) with ID 2; 2019-01-22 13:11:43 YarnClientSchedulerBackend: INFO: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 2019-01-22 13:11:43 BlockManagerMasterEndpoint: INFO: Registering block manager scc-q12.scc.bu.edu:34720 with 21.2 GB RAM, BlockManagerId(2, scc-q12.scc.bu.edu, 34720, None); 2019-01-22 13:11:43 Hail: INFO: SparkUI: http://10.48.225.55:4040; 2019-01-22 13:11:43 Hail: INFO: Running Hail version 0.2.4-d602a3d7472d; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:45 CodecPool: INFO: Got brand-new decompressor [.gz]; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.<init> instruction count: 3; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 28; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.apply instruction count: 12; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method1 instruction count: 112; 2019-01-22 13:11:45 root: INFO: is/hail/codegen/generated/C0.method2 instruction count: 82;",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:22800,Schedul,SchedulerBackend,22800,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,2,"['Schedul', 'schedul']","['SchedulerBackend', 'scheduling']"
Energy Efficiency,.spark.SparkException: Job 0 cancelled because SparkContext was shut down; at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:820); at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:818); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1750); at org.apache.spark.util.EventLoop.stop(EventLoop.scala:83); at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1669); at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1928); at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1317); at org.apache.spark.SparkContext.stop(SparkContext.scala:1927); at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1872); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); at is.hail.rvd.RVD.count(RVD.scala:603); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply$mcJ$sp(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$$anonfun$apply$1.apply(Interpret.scala:725); at scala.Option.getOrElse(Option.scala:121); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:725); at is.hail.expr.ir.Interpret$.apply(Interpret.scala:107); at is.hail.expr.ir.Interpret$.apply(Interpret.scal,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:207503,schedul,scheduler,207503,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-docker.pkg.dev/hail-vdc/hail/grafana"",""us-docker.pkg.dev/hail-vdc/hail/hail-base"",""us-docker.pkg.dev/hail-vdc/hail/hail-build"",""us-docker.pkg.dev/hail-vdc/hail/hail-buildkit"",""us-docker.pkg.dev/hail-vdc/hail/hail-run"",""us-docker.pkg.dev/hail-vdc/hail/hail-run-tests"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python37"",""us-docker.pkg.dev/hail-vdc/hail/hail-pip-installed-python38"",""us-docker.pkg.dev/hail-vdc/hail/hail-ubuntu"",""us-docker.pkg.dev/hail-vdc/hail/memory"",""us-docker.pkg.dev/hail-vdc/hail/monitoring"",""us-docker.pkg.dev/hail-vdc/hail/notebook"",""us-docker.pkg.dev/hail-vdc/hail/notebook_nginx"",""us-docker.pkg.dev/hail-vdc/hail/prometheus"",""us-docker.pkg.dev/hail-vdc/hail/service-base"",""us-docker.pkg.dev/hail-vdc/hail/service-java-run-base"",""us-docker.pkg.dev/hail-vdc/hail/test-ci"",""us-docker.pkg.dev/hail-vdc/hail/test-monitoring"",""us-docker.pkg.dev/hail-vdc/hail/test-benchmark"",""us-docker.pkg.dev/hail-vdc/hail/test_hello_create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/website"",""us-docker.pkg.dev/hail-vdc/hail/ci-hello"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch37-85"",""us-docker.pkg.dev/hail-vdc/hail/hailgenetics/vep-grch38-95""],""grace"":""48h"",""recursive"":true,""tag_filter_all"":""cache-pr-.*""}; ```. ```; {""repos"":[""us-docker.pkg.dev/hail-vdc/hail/auth"",""us-docker.pkg.dev/hail-vdc/hail/base"",""us-docker.pkg.dev/hail-vdc/hail/base_spark_3_2"",""us-docker.pkg.dev/hail-vdc/hail/batch"",""us-docker.pkg.dev/hail-vdc/hail/batch-driver-nginx"",""us-docker.pkg.dev/hail-vdc/hail/batch-worker"",""us-docker.pkg.dev/hail-vdc/hail/benchmark"",""us-docker.pkg.dev/hail-vdc/hail/blog_nginx"",""us-docker.pkg.dev/hail-vdc/hail/ci"",""us-docker.pkg.dev/hail-vdc/hail/ci-intermediate"",""us-docker.pkg.dev/hail-vdc/hail/ci-utils"",""us-docker.pkg.dev/hail-vdc/hail/create_certs_image"",""us-docker.pkg.dev/hail-vdc/hail/echo"",""us-d",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545:1681,monitor,monitoring,1681,https://hail.is,https://github.com/hail-is/hail/issues/13603#issuecomment-1734249545,1,['monitor'],['monitoring']
Energy Efficiency,"/lib/py4j-0.10.4-src.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/py4j-0.10.4-src.zip; 2019-01-22 13:11:31 Client: INFO: Uploading resource file:/tmp/spark-1afae5c8-6de0-4d0d-8db4-c834966e0865/__spark_conf__963896229742184890.zip -> hdfs://scc/user/farrell/.sparkStaging/application_1542127286896_0174/__spark_conf__.zip; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls to: farrell; 2019-01-22 13:11:31 SecurityManager: INFO: Changing view acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: Changing modify acls groups to:; 2019-01-22 13:11:31 SecurityManager: INFO: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(farrell); groups with view permissions: Set(); users with modify permissions: Set(farrell); groups with modify permissions: Set(); 2019-01-22 13:11:31 Client: INFO: Submitting application application_1542127286896_0174 to ResourceManager; 2019-01-22 13:11:32 YarnClientImpl: INFO: Submitted application application_1542127286896_0174; 2019-01-22 13:11:32 SchedulerExtensionServices: INFO: Starting Yarn extension services with app application_1542127286896_0174 and attemptId None; 2019-01-22 13:11:33 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:33 Client: INFO:; client token: Token { kind: YARN_CLIENT_TOKEN, service: }; diagnostics: N/A; ApplicationMaster host: N/A; ApplicationMaster RPC port: -1; queue: default; start time: 1548180691687; final status: UNDEFINED; tracking URL: https://scc-hsn1.scc.bu.edu:8090/proxy/application_1542127286896_0174/; user: farrell; 2019-01-22 13:11:34 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:35 Client: INFO: Application report for application_1542127286896_0174 (state: ACCEPTED); 2019-01-22 13:11:36 Client: INFO: Application report for applica",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:17144,Schedul,SchedulerExtensionServices,17144,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Schedul'],['SchedulerExtensionServices']
Energy Efficiency,"0.4-src.zip/py4j/java_gateway.py"", line 1133, in __call__; File ""/restricted/projectnb/genpro/github/hail/hail/build/distributions/hail-python.zip/hail/utils/java.py"", line 210, in deco; hail.utils.java.FatalError: SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 0.0 failed 4 times, most recent failure: Lost task 7.3 in stage 0.0 (TID 86, scc-q16.scc.bu.edu, executor 26): ExecutorLostFailure (executor 26 exited caused by one of the running tasks) Reason: Slave lost; Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1517); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572:8416,schedul,scheduler,8416,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456417572,1,['schedul'],['scheduler']
Energy Efficiency,"00 on error. We could return a BadRequest error code with the message 'invalid spec' and then handle the MJC database call on the driver. I chose instead to have the worker to post job complete so we get the error message with the stack trace showing up in the UI as having the normal job flow seemed cleaner to me last week then special casing `schedule_job` on the driver. `post job complete` needs a job object to get the status to send back to the driver. However, a `Job` has two concrete implementations and we don't know which the bad job is because we can't get the spec. Furthermore, the `Job` class does a lot of work based on the spec right now. So I thought it was clearer to just create a new class that had the status, but nothing else. After writing this out, it's probably better to have the driver MJC upon error rather than from the worker. The code below would be more complicated. We'd have to get the traceback / error message from the response from the worker. ```python3; try:; await client_session.post(; f'http://{instance.ip_address}:5000/api/v1alpha/batches/jobs/create',; json=body,; timeout=aiohttp.ClientTimeout(total=2),; ); await instance.mark_healthy(); except aiohttp.ClientResponseError as e:; await instance.mark_healthy(); if e.status == 403:; log.info(f'attempt already exists for job {id} on {instance}, aborting'); if e.status == 503:; log.info(f'job {id} cannot be scheduled because {instance} is shutting down, aborting'); raise e; except Exception:; await instance.incr_failed_request_count(); raise; ```. And the error handling would look something like this:. ```python3; try:; body = await job_config(app, record, attempt_id); except Exception:; log.exception('while making job config'); status = {; 'version': STATUS_FORMAT_VERSION,; 'worker': None,; 'batch_id': batch_id,; 'job_id': job_id,; 'attempt_id': attempt_id,; 'user': record['user'],; 'state': 'error',; 'error': traceback.format_exc(),; 'container_statuses': {k: None for k in tasks},; }. ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078:1450,schedul,scheduled,1450,https://hail.is,https://github.com/hail-is/hail/pull/11391#issuecomment-1048213078,1,['schedul'],['scheduled']
Energy Efficiency,"1.00m, 42.46MB read; Socket errors: connect 0, read 2079, write 0, timeout 12; Requests/sec: 4642.59; Transfer/sec: 723.58KB. Sanic Run 3 (very large background task spike in last 1-2s of run):; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 543.65ms 839.00ms 7.93s 87.81%; Req/Sec 392.47 118.69 1.42k 73.81%; 279206 requests in 1.00m, 42.54MB read; Socket errors: connect 0, read 2101, write 0, timeout 35; Requests/sec: 4646.20; Transfer/sec: 724.97KB. Aiohttp Run 1:; alexkotlar:~/projects/aiohttp-vs-sanic-vs-japronto:$ wrk -d 60 -c 2000 -t 12 --timeout 8 http://localhost:8000/db; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 747.49ms 1.00s 7.88s 86.77%; Req/Sec 280.95 103.65 1.60k 79.52%; 199147 requests in 1.00m, 36.47MB read; Socket errors: connect 0, read 2058, write 1, timeout 45; Requests/sec: 3313.70; Transfer/sec: 621.36KB. Aiohttp Run 2:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 696.00ms 967.04ms 7.93s 86.48%; Req/Sec 289.87 115.90 1.90k 83.92%; 205188 requests in 1.00m, 37.54MB read; Socket errors: connect 0, read 2041, write 0, timeout 38; Requests/sec: 3414.95; Transfer/sec: 639.84KB. Aiohttp Run 3:; Running 1m test @ http://localhost:8000/db; 12 threads and 2000 connections; Thread Stats Avg Stdev Max +/- Stdev; Latency 670.88ms 898.81ms 7.89s 86.58%; Req/Sec 318.17 108.06 1.47k 74.96%; 226300 requests in 1.00m, 41.34MB read; Socket errors: connect 0, read 2053, write 0, timeout 19; Requests/sec: 3765.55; Transfer/sec: 704.34KB. Runs were interleaved two reduce chance that the programs would benefit from caching across runs. First run for each had somewhat more background tasks open. Starlette will be run later.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030:6360,reduce,reduce,6360,https://hail.is,https://github.com/hail-is/hail/pull/5242#issuecomment-461259030,1,['reduce'],['reduce']
Energy Efficiency,"1.1 in initial posted issue), but it gives the same issue/stack trace:. ```; SparkException: Job aborted due to stage failure: Task 1.0 in stage 5.0 (TID 2681) had a not serializable result: is.hail.io.bgen.Bgen12GenotypeIterator; Serialization stack:; 	- object not serializable (class: is.hail.io.bgen.Bgen12GenotypeIterator, value: Bgen12GenotypeIterator(0/0:.:.:.:GP=0.99798583984375,0.00201416015625,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:GP=1.0,0.0,0.0, 0/0:.:.:.:; ```; ```; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkConte",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1082,schedul,scheduler,1082,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,"1.6.2; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem total: 52223 MiB, swap total: 0 MiB; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: sending SIGTERM when mem <= 0.12% and swap <= 1.00%,; Nov 22 14:26:51 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: SIGKILL when mem <= 0.06% and swap <= 0.50%; ...; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: + echo 'All done'; Nov 22 14:30:05 vds-cluster-91f3f4c1-b737-m post-hdfs-startup-script[7747]: All done; Nov 22 14:30:06 vds-cluster-91f3f4c1-b737-m earlyoom[4115]: mem avail: 42760 of 52223 MiB (81.88%), swap free: 0 of 0 MiB ( 0.00%); ```. Notice:; 1. The total memory available on the machine is less than 52 GiB (= 53,248 MiB), indeed it is a full 1025 MiB below the advertised amount.; 2. Once all the components of the Dataproc cluster have started (but before any Hail Query jobs are submitted) the total memory available is already depleted to 42760 MiB. Recall that Hail allocates 41 GiB (= 41,984 MiB) to its JVM. This leaves the Python process and all other daemons on the system only 776 MiB of excess RAM. For reference `python3 -c 'import hail'` needs 206 MiB. ---. We must address this situation. It seems safe to assume that the system daemons will use a constant 9.5 GiB of RAM. Moreover the advertised RAM amount is at least 1 GiB larger than reality. I propose:; 1. The driver memory calculation in `hailctl dataproc` should take the advertised RAM amount, subtract 10.5 GiB, and then use 90% of the remaining value. For an n1-highmem-8, that reduces our allocation from 41 GiB to 37 GiB yielding an additional 4GiB to Python and deamon memory fluctuations.; 2. AoU RWB needs to review its memory settings for Spark driver nodes to ensure that the JVM is set to an appropriate maximum heap size. For what it's worth, I think the reason we didn't get an outcry from our local scientific community is that many of them have transitioned to Query-on-Batch where we have exact a",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790:2492,allocate,allocates,2492,https://hail.is,https://github.com/hail-is/hail/issues/13960#issuecomment-1836844790,1,['allocate'],['allocates']
Energy Efficiency,"15b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:252); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253); at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293); at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136); at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40); at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248); at scala.concurrent.Promise$class.complete(Promise.scala:55); at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153); at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326); at scala.concu",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:215010,schedul,scheduler,215010,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,"1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.506 root: INFO: RegionPool: REPORT_THRESHOLD: 256.2M allocated (192.0K blocks / 256.0M chunks), regions.size = 3, 0 current java objects, thr",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:62943,allocate,allocated,62943,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,"2079a85bce (41 minutes):. I could try to make the tests even more fine-grained and split up even more long-running tests. Seems like some of the bottlenecks I'm hitting now are:; 1. Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency.; 2. The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down). I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. <img width=""2032"" alt=""Screen Shot 2023-05-22 at 12 30 47"" src=""https://github.com/hail-is/hail/assets/106194/aaa3fbb7-176d-4487-b65e-586c235e2089"">; <img width=""541"" alt=""Screen Shot 2023-05-22 at 12 31 23"" src=""https://github.com/hail-is/hail/assets/106194/016f1089-d08d-4555-ae86-c01353f39c78"">",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015:471,reduce,reduce,471,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1557548015,1,['reduce'],['reduce']
Energy Efficiency,29); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1529); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 2019-01-22 13:12:06 YarnScheduler: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:202384,schedul,scheduler,202384,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,2pfXIoSO&uploadType=resumable&upload_id=ADPycdvZ5HhnGfOKt5TE1qXWiHpqIpZnXVTYWuWUCXNPRF9HqyCB-4LvRsxNX6SUWRgk13pYrzYaa9-wXlvNZt1oct0ptaEz0bS3; chunkOffset: 16777216; chunkLength: 0; localOffset: 268435456; remoteOffset: 285212672; lastChunk: false. 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:131); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.unrecoverableState(BlobWriteChannel.java:87); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.access$1000(BlobWriteChannel.java:35); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel$1.run(BlobWriteChannel.java:267); 		at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); 		at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103); 		at is.hail.relocated.com.google.cloud.RetryHelper.run(RetryHelper.java:76); 		at is.hail.relocated.com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 		at is.hail.relocated.com.google.cloud.storage.BlobWriteChannel.flushBuffer(BlobWriteChannel.java:189); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.flush(BaseWriteChannel.java:112); 		at is.hail.relocated.com.google.cloud.BaseWriteChannel.write(BaseWriteChannel.java:139); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.$anonfun$flush$1(GoogleStorageFS.scala:317); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.doHandlingRequesterPays(GoogleStorageFS.scala:299); 		at is.hail.io.fs.GoogleStorageFS$$anon$2.flush(GoogleStorageFS.scala:317); 		at is.hail.io.fs.FSPositionedOutputStream.write(FS.scala:227); 		at java.io.DataOutputStream.write(DataOutputStream.java:107); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1(FSSuite.scala:347); 		at is.hail.fs.FSSuite.$anonfun$testSeekMoreThanMaxInt$1$adapted(FSSuite.scala:341); 		at is.hail.utils.package$.using(package.scala:635); 		... 26 more. test is.hail.fs.gs.GoogleStorageFSSuite.testSeekMoreThanMaxInt FAILURE; ```,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911:9208,adapt,adapted,9208,https://hail.is,https://github.com/hail-is/hail/issues/12950#issuecomment-1704346911,1,['adapt'],['adapted']
Energy Efficiency,38); 	at is.hail.sparkextras.OrderedRDD$$anonfun$apply$7$$anon$2.hasNext(OrderedRDD.scala:210); 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408); 	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1763); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1134); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1899); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:3799,schedul,scheduler,3799,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,6); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1838); at is.hail.linalg.WriteBlocksRDD$$anonfun$62.apply(BlockMatrix.scala:1829); at scala.Array$.tabulate(Array.scala:331); at is.hail.linalg.WriteBlocksRDD.compute(BlockMatrix.scala:1829); at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90); at org.apache.spark.scheduler.Task.run(Task.scala:121); at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408); at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:12155,schedul,scheduler,12155,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1824); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1837); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1850); at org.apache.spark.SparkContext.runJob(SparkContext.scala:1921); at org.apache.spark.rdd.RDD.count(RDD.scala:1125); at org.broadinstitute.hail.driver.Count$.run(Count.scala:37); at org.broadinstitute.hail.driver.Count$.run(Count.scala:9); at org.broadinstitute.hail.driver.Command.runCommand(Command.scala:239); at org.broadinstitute.hail.driver.Main$.runCommand(Main.scala:120); at org.broadinst,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6824,schedul,scheduler,6824,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403:13062,schedul,scheduler,13062,https://hail.is,https://github.com/hail-is/hail/issues/9293#issuecomment-677718403,1,['schedul'],['scheduler']
Energy Efficiency,6_290(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply_region4_318(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply_region2_501(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at __C372collect_distributed_array_matrix_native_writer.apply(Unknown Source); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$16(BackendUtils.scala:91); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.annotations.RegionPool.scopedRegion(RegionPool.scala:162); 	at is.hail.backend.BackendUtils.$anonfun$collectDArray$15(BackendUtils.scala:90); 	at is.hail.backend.service.Worker$.$anonfun$main$12(Worker.scala:167); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at is.hail.services.package$.retryTransientErrors(package.scala:182); 	at is.hail.backend.service.Worker$.$anonfun$main$11(Worker.scala:166); 	at is.hail.backend.service.Worker$.$anonfun$main$11$adapted(Worker.scala:164); 	at is.hail.utils.package$.using(package.scala:637); 	at is.hail.backend.service.Worker$.main(Worker.scala:164); 	at is.hail.backend.service.Main$.main(Main.scala:14); 	at is.hail.backend.service.Main.main(Main.scala); 	... 11 more. Logs; Main; Log ; 2023-09-24 17:23:30.055 JVMEntryway: ERROR: Exception encountered in QoB cancel thread.; org.newsclub.net.unix.SocketClosedException: Not open; 	at org.newsclub.net.unix.AFCore.validFdOrException(AFCore.java:90) ~[jvm-entryway.jar:?]; 	at org.newsclub.net.unix.AFSocketImpl$AFInputStreamImpl.read(AFSocketImpl.java:510) ~[jvm-entryway.jar:?]; 	at java.io.DataInputStream.readInt(DataInputStream.java:388) ~[?:1.8.0_382]; 	at is.hail.JVMEntryway$2.run(JVMEntryway.java:136) ~[jvm-entryway.jar:?]; 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_382]; 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_382]; 	at java.util.concurrent.Executors$RunnableAdapter.call(,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888:4138,adapt,adapted,4138,https://hail.is,https://github.com/hail-is/hail/issues/13704#issuecomment-1734170888,1,['adapt'],['adapted']
Energy Efficiency,"9 complete - 8157265 bytes\n2022-11-15 20:31:41.330 ServiceBackend$: INFO: all results complete\n2022-11-15 20:31:41.331 root: INFO: executed D-Array [table_aggregate_singlestage] in 1m23.1s\n2022-11-15 20:31:41.331 root: INFO: RegionPool: REPORT_THRESHOLD: 8.2M allocated (192.0K blocks / 8.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.335 root: INFO: RegionPool: REPORT_THRESHOLD: 16.2M allocated (192.0K blocks / 16.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.339 root: INFO: RegionPool: REPORT_THRESHOLD: 24.2M allocated (192.0K blocks / 24.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.345 root: INFO: RegionPool: REPORT_THRESHOLD: 32.2M allocated (192.0K blocks / 32.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.351 root: INFO: RegionPool: REPORT_THRESHOLD: 40.2M allocated (192.0K blocks / 40.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.357 root: INFO: RegionPool: REPORT_THRESHOLD: 48.2M allocated (192.0K blocks / 48.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.362 root: INFO: RegionPool: REPORT_THRESHOLD: 56.2M allocated (192.0K blocks / 56.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.368 root: INFO: RegionPool: REPORT_THRESHOLD: 64.2M allocated (192.0K blocks / 64.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.374 root: INFO: RegionPool: REPORT_THRESHOLD: 72.2M allocated (192.0K blocks / 72.0M chunks), regions.size = 3, 0 current java objects, thread 8: pool-1-thread-1\n2022-11-15 20:31:41.414 root: INFO: RegionPool: REPORT_THRESHOLD: 128.2M allocated (192.0K blocks / 128.0M chunks), regions.size = 3, 0 current java objects, threa",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284:62760,allocate,allocated,62760,https://hail.is,https://github.com/hail-is/hail/pull/12470#issuecomment-1315959284,2,['allocate'],['allocated']
Energy Efficiency,9); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; script=$2. echo cluster = $cluster; echo script = $script. HASH=`gsutil cat gs://hail-common/latest-hash.txt`. JAR_FILE=hail-hail-is-master-all-spark2.0.2-$HASH.jar; JAR=gs://hail-common/,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1775,schedul,scheduler,1775,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency,931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at java.io.FilterOutputStream.close(FilterOutputStream.java:159) ~[?:1.8.0_382]; 	at is.hail.utils.package$.using(package.scala:677) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS(FS.scala:441) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.FS.writePDOS$(FS.scala:440) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.io.fs.RouterFS.writePDOS(RouterFS.scala:3) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$4$adapted(Worker.scala:124) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.$anonfun$main$13(Worker.scala:178) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) ~[scala-library-2.12.15.jar:?]; 	at is.hail.services.package$.retryTransientErrors(package.scala:182) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Worker$.main(Worker.scala:177) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main$.main(Main.scala:14) ~[gs:__hail-test-ezlis_dking_jars_ch4g3zvqceyo_09526a168d57dac1a26f8caa4ab49593931ed2ef.jar.jar:0.0.1-SNAPSHOT]; 	at is.hail.backend.service.Main.main(Main.scala,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943:14595,adapt,adapted,14595,https://hail.is,https://github.com/hail-is/hail/issues/13721#issuecomment-1737788943,1,['adapt'],['adapted']
Energy Efficiency,: INFO: Cancelling stage 0; 2019-01-22 13:12:06 DAGSchedulerEventProcessLoop: ERROR: DAGScheduler failed to cancel all jobs.; java.util.NoSuchElementException: key not found: 70; at scala.collection.MapLike$class.default(MapLike.scala:228); at scala.collection.AbstractMap.default(Map.scala:59); at scala.collection.mutable.HashMap.apply(HashMap.scala:65); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply$mcVJ$sp(TaskSchedulerImpl.scala:243); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3$$anonfun$apply$1.apply(TaskSchedulerImpl.scala:242); at scala.collection.mutable.HashSet.foreach(HashSet.scala:78); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:242); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2$$anonfun$apply$3.apply(TaskSchedulerImpl.scala:235); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:99); at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:230); at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40); at scala.collection.mutable.HashMap.foreach(HashMap.scala:99); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:235); at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$cancelTasks$2.apply(TaskSchedulerImpl.scala:234); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.TaskSchedulerImpl.cancelTasks(TaskSchedulerImpl.scala:234); at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages$1.apply$mcVI$sp(DAGScheduler.scala:1543); at org.apache.spark.s,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:203747,schedul,scheduler,203747,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['schedul'],['scheduler']
Energy Efficiency,:1555); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1125); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1850); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66); at org.apache.spark.scheduler.Task.run(Task.scala:88); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697); at scala.Option.foreach(Option.scala:236); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/660#issuecomment-242218633:6152,schedul,scheduler,6152,https://hail.is,https://github.com/hail-is/hail/issues/660#issuecomment-242218633,1,['schedul'],['scheduler']
Energy Efficiency,:893); 	at is.hail.rvd.OrderedRVD$$anonfun$apply$9$$anon$5.foreach(OrderedRVD.scala:631); 	at is.hail.io.RichRDDRegionValue$.writeRowsPartition(RowStore.scala:510); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.io.RichRDDRegionValue$$anonfun$writeRows$extension$1.apply(RowStore.scala:526); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:207); 	at is.hail.utils.richUtils.RichRDD$$anonfun$5.apply(RichRDD.scala:198); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsWithIndex$1$$anonfun$apply$25.apply(RDD.scala:820); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70); 	at org.apache.spark.scheduler.Task.run(Task.scala:86); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSchedu,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:1878,schedul,scheduler,1878,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:912); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:911); 	at is.hail.utils.richUtils.RichRDD$.writePartitions$extension(RichRDD.scala:209); 	at is.hail.io.RichRDDRegionValue$.writeRows$extension(RowStore.scala:526); 	at is.hail.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437:3353,schedul,scheduler,3353,https://hail.is,https://github.com/hail-is/hail/pull/2722#issuecomment-358198437,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1913); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1134); 	at is.hail.variant.VariantSampleMatrix.countVariants(VariantSampleMatrix.scala:810); 	at is.hail.variant.VariantDatasetFunctions$.count$extension(VariantDataset.scala:504); 	at is.hail.variant.VariantDatasetFunctions.count(VariantDataset.scala:494); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMeth,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882:4910,schedul,scheduler,4910,https://hail.is,https://github.com/hail-is/hail/pull/1552#issuecomment-287190882,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1873); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1906); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1219); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1161); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358); 	at org.apache.spark.rdd.Pa,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783:1702,schedul,scheduler,1702,https://hail.is,https://github.com/hail-is/hail/issues/2527#issuecomment-355985783,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1089); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1083); 	at is.hail.utils.richUtils.RichRDD$.exists$extension(RichRDD.scala:26); 	at is.hail.utils.richUtils.RichRDD$.forall$extension(RichRDD.scala:22); 	at is.hail.io.vcf.LoadVCF$.apply(LoadVCF.scala:286); 	at is.hail.HailContext.importVCFs(HailContext.scala:529); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(N,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807:4149,schedul,scheduler,4149,https://hail.is,https://github.com/hail-is/hail/issues/2076#issuecomment-338418807,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.sparkextras.ContextRDD.collect(ContextRDD.scala:143); 	at is.hail.utils.richUtils.RichContextRDD.writePartitions(RichContextRDD.scala:48); 	at is.hail.io.RichC,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719:5798,schedul,scheduler,5798,https://hail.is,https://github.com/hail-is/hail/pull/4128#issuecomment-412764719,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:935); 	at is.hail.table.Table.collect(Table.scala:841); 	at is.hail.table.Table.collectJSON(Table.scala:844); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	a,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410:5153,schedul,scheduler,5153,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-385847410,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1505); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1504); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1504); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1732); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1687); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1676); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2029); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2050); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2069); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2094); 	at org.apache.spark.rdd.RDD.count(RDD.scala:1158); 	at is.hail.rvd.RVD$class.count(RVD.scala:183); 	at is.hail.rvd.OrderedRVD.count(OrderedRVD.scala:19); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:471); 	at is.hail.methods.LDPrune$$anonfun$9.apply(LDPrune.scala:469); 	at is.hail.utils.package$.time(package.scala:82); 	at is.hail.methods.LDPrune$.apply(LDPrune.scala:469); 	at is.hail.methods.LDPrune.apply(LDPrune.scala); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.ref,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627:4069,schedul,scheduler,4069,https://hail.is,https://github.com/hail-is/hail/issues/3446#issuecomment-384671627,1,['schedul'],['scheduler']
Energy Efficiency,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2158); 	at org.apache.spark.rdd.RDD$$anonfun$fold$1.apply(RDD.scala:1098); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.fold(RDD.scala:1092); 	at is.hail.rvd.RVD.count(RVD.scala:660); 	at is.hail.methods.ForceCountTable.execute(ForceCount.scala:11); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:771); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:88); 	at is.hail.expr.ir.Interpret$.apply(Interpret.scala:59); 	at is.hail.expr.ir.Interp,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307:1243,schedul,scheduler,1243,https://hail.is,https://github.com/hail-is/hail/pull/6345#issuecomment-503757307,1,['schedul'],['scheduler']
Energy Efficiency,"; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:. Java stack trace:; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 4 times, most recent failure: Lost task 0.3 in stage 23.0 (TID 26) (all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal executor 5): ExecutorLostFailure (executor 5 exited caused by one of the running tasks) Reason: Container from a bad node: container_1691092255852_0001_01_000005 on host: all-of-us-56-w-0.c.terra-vpc-sc-8f5cdfd2.internal. Exit status: 137. Diagnostics: [2023-08-03 20:14:25.441]Container killed on request. Exit code is 137; [2023-08-03 20:14:25.442]Container exited with a non-zero exit code 137. ; [2023-08-03 20:14:25.442]Killed by external signal; .; Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2304); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2253); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2252); 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62); 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2252); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124); 	at scala.Option.foreach(Option.scala:407); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2491); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2433); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSched",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619:5292,schedul,scheduler,5292,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1664593619,1,['schedul'],['scheduler']
Energy Efficiency,; machdep.cpu.tlb.shared: 512; machdep.cpu.tlb.data.large: 32; machdep.cpu.tlb.data.small: 64; machdep.cpu.tlb.inst.large: 8; machdep.cpu.tlb.inst.small: 64; machdep.cpu.cache.size: 256; machdep.cpu.cache.L2_associativity: 8; machdep.cpu.cache.linesize: 64; machdep.cpu.arch_perf.fixed_width: 48; machdep.cpu.arch_perf.fixed_number: 3; machdep.cpu.arch_perf.events: 0; machdep.cpu.arch_perf.events_number: 7; machdep.cpu.arch_perf.width: 48; machdep.cpu.arch_perf.number: 4; machdep.cpu.arch_perf.version: 3; machdep.cpu.xsave.extended_state1: 1 0 0 0; machdep.cpu.xsave.extended_state: 7 832 832 0; machdep.cpu.thermal.energy_policy: 0; machdep.cpu.thermal.hardware_feedback: 0; machdep.cpu.thermal.package_thermal_intr: 1; machdep.cpu.thermal.fine_grain_clock_mod: 1; machdep.cpu.thermal.core_power_limits: 1; machdep.cpu.thermal.ACNT_MCNT: 1; machdep.cpu.thermal.thresholds: 2; machdep.cpu.thermal.invariant_APIC_timer: 1; machdep.cpu.thermal.dynamic_acceleration: 1; machdep.cpu.thermal.sensor: 1; machdep.cpu.mwait.sub_Cstates: 135456; machdep.cpu.mwait.extensions: 3; machdep.cpu.mwait.linesize_max: 64; machdep.cpu.mwait.linesize_min: 64; machdep.cpu.processor_flag: 4; machdep.cpu.microcode_version: 21; machdep.cpu.cores_per_package: 8; machdep.cpu.logical_per_package: 16; machdep.cpu.extfeatures: SYSCALL XD EM64T LAHF RDTSCP TSCI; machdep.cpu.leaf7_features: SMEP ERMS RDWRFSGS; machdep.cpu.features: FPU VME DE PSE TSC MSR PAE MCE CX8 APIC SEP MTRR PGE MCA CMOV PAT PSE36 CLFSH DS ACPI MMX FXSR SSE SSE2 SS HTT TM PBE SSE3 PCLMULQDQ DTES64 MON DSCPL VMX EST TM2 SSSE3 CX16 TPR PDCM SSE4.1 SSE4.2 x2APIC POPCNT AES PCID XSAVE OSXSAVE TSCTMR AVX1.0 RDRAND F16C; machdep.cpu.brand: 0; machdep.cpu.signature: 198313; machdep.cpu.extfeature_bits: 4967106816; machdep.cpu.leaf7_feature_bits: 641; machdep.cpu.feature_bits: 9203919201183202303; machdep.cpu.stepping: 9; machdep.cpu.extfamily: 0; machdep.cpu.extmodel: 3; machdep.cpu.model: 58; machdep.cpu.family: 6; machdep.cpu.brand_string: In,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543:2449,sensor,sensor,2449,https://hail.is,https://github.com/hail-is/hail/issues/1274#issuecomment-274242543,1,['sensor'],['sensor']
Energy Efficiency,"<img width=""1841"" alt=""Screen Shot 2022-02-10 at 4 59 10 PM"" src=""https://user-images.githubusercontent.com/24440116/153503683-a94528e0-769b-4dd9-bdb9-26867de5c445.png"">; The SQL query monitoring that @vrautela added makes this really stark: the MJC and MJS SQL procedures are having to retry multiple times due to deadlocks, while the deactivate instance procedure is just slow for some reason. (the left side is the average duration of the HTTP endpoint on the driver and the right is the average runtime of the corresponding SQL query happening within the endpoint)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11352#issuecomment-1035571407:185,monitor,monitoring,185,https://hail.is,https://github.com/hail-is/hail/pull/11352#issuecomment-1035571407,1,['monitor'],['monitoring']
Energy Efficiency,"=""analysis_type=CombineVariants input_file=[] read_buffer_size=null phone_home=STANDARD gatk_key=null tag=NA read_filter=[] intervals=[/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.padded.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=0 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=[(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snps.recalibrated.vcf), (RodBinding name=variant2 source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.indels.filtered.vcf)] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub genotypemergeoption=UNSORTED filteredrecordsmergetype=KEEP_IF_ANY_UNFILTERED multipleallelesmergetype=BY_TYPE rod_priority_list=null printComplexMerges=false filteredAreUncalled=false minimalVCF=false",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:3138,monitor,monitorThreadEfficiency,3138,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['monitor'],['monitorThreadEfficiency']
Energy Efficiency,"> @daniel-goldstein - do you know if compacting or deleting will impact other backgroud processes? Is this table used for anything else after a job group completes?; > ; > ; > ; > Also, should we also compact failed job groups?. I don't believe so, this should be fine. I would usually grep the codebase for the table name. IIRC you should see it used in:; - job insertion, clearly no longer relevant; - triggers which won't fire on cold batches; - the scheduler/fair share which should only care about rows with >0 values and active job groups. Off the top of my head, I see no reason to distinguish between different `complete` states.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/14623#issuecomment-2264264725:453,schedul,scheduler,453,https://hail.is,https://github.com/hail-is/hail/issues/14623#issuecomment-2264264725,1,['schedul'],['scheduler']
Energy Efficiency,"> @patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure?. In general, I don't know how to wrap pieces of a stream in methods, since streams need to be able to jump between labels defined by different stream nodes. Maybe to handle large `MakeStream`s, and decomposing other streams into multiple methods, we could compile some streams into iterators. That shouldn't be hard to do, and would let us experiment with the performance effects. (I've been nervous about stream code never getting jit compiled, and never considered this option. I think I like it.). > Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. Yeah, that seems unavoidable. If MakeArray(...) generates better code than ToArray(MakeStream(...)), is it worth keeping MakeArray, with a ToArray(MakeStream(...)) -> MakeArray(...) simplify rule?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391:706,efficient,efficient,706,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590861391,1,['efficient'],['efficient']
Energy Efficiency,"> Addressed all comments, should be good to go.; > ; > One question I did have is that I have to specify a region size in this aggregator, and I picked `TINY`. Does it matter? `TINY` does seem small compared to the kinds of operations that we'll do on dndarray and block matrix. That specifies the granularity of allocations. Each time the region needs more space than it has allocated, it allocates a new block of size determined by the size parameter. Unless it's trying to make a single allocation larger than the block size, in which case it allocates a block of exactly the desired size. The ideal thing here would be to make a single block with exactly the size needed for the state, but the current Region interface doesn't support that. The closest we can get is using the smallest block size, which is `TINIER`. Then the missing bit and pointer to the data would go in the initial tiny block, and the data would go there too if it's small enough, otherwise it would go in an allocation of exactly the right size.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320:376,allocate,allocated,376,https://hail.is,https://github.com/hail-is/hail/pull/9209#issuecomment-669255320,3,['allocate'],"['allocated', 'allocates']"
Energy Efficiency,"> Ah, I thought I said I was happy to fix the optimized version rather than revert. I do think it can be simplified, though, per my comments.; > ; > Cotton also had the suggestion of writing this function unstaged using two utility functions:; > ; > ```scala; > def findFirstNonZeroByte(addr: Long, n: Long): Long; > def allPresent(addr: Long, n: Long): Long // uses findFirstNonZeroByte; > ```. How do we convert from address: Code[Long] to Long without going through emit? I suppose you mean creating an unstated version, and then calling it through invokeScalaObject?. I'm happy to write this. edit: In this version, if we don't want to assume alignment, what is an easier way other than checking the alignment of the addr? I think we're assuming int alignment in much of our codebase, but this is also not guaranteed (since we only aim to guarantee power of 2, as discussed last week).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978:853,power,power,853,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561475978,1,['power'],['power']
Energy Efficiency,"> Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an allocateAndStoreString method, which takes care of both steps, potentially more efficiently, but also more ergonomically. PString.allocate probably shouldn't exist, then! The public interfaces to PString probably need to include the ability to copy value=>value (address) and to put a string in a region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719:115,allocate,allocate,115,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358719,6,"['allocate', 'efficient']","['allocate', 'allocateAndStoreString', 'efficiently']"
Energy Efficiency,"> Even this may be expensive. This would all be staged. The generated code would just directly allocate into the provided region, no overhead. But I agree we should benchmark first.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9106#issuecomment-661890102:95,allocate,allocate,95,https://hail.is,https://github.com/hail-is/hail/pull/9106#issuecomment-661890102,1,['allocate'],['allocate']
Energy Efficiency,"> For the tutorial, I'll just put a password. I think for Stanley Center stuff we should use GCP auth. Yes. I'm imagining we'll have a tutorial service for intermittent tutorials and a jupyter/Hail service, both, although maybe eventually the latter can be used for both?. > But yeah, I also saw timeouts if a pod can't be scheduled right away. I definitely saw this case (e.g. I refreshed and then got the notebook). > I think imagePullPolicy: Never is a bad idea. Agreed, too aggressive. > I think we should rely on k8s to pull the 5GB jupyter image in a reasonable time period. No. I'm going to be demanding about making our tools responsive with good feedback (not responsive in the sense of responsive web design, but responsive in the sense of fast). It has to be fast, and when can't be, it has to give clear feedback about what it's doing and how long it will take. We routinely see pulling a 5GB image take 1-2m. That's spin up a VM level nonsense. Kubernetes 1.6 had an SLO to schedule 99% of pre-pulled containers within 5s on a 5K node cluster (from the plots it looks like they were closer to 2s):. > Pod startup time: 99% of pods and their containers (with pre-pulled images) start within 5s. from http://webcache.googleusercontent.com/search?q=cache:Soglxt0kAI0J:blog.kubernetes.io/2017/03/scalability-updates-in-kubernetes-1.6.html+&cd=1&hl=en&ct=clnk&gl=us. When we have to pull an image, I want spinner and the estimated spin time. If we have to spin up a node, same. (I know this is a first cut. I'm just saying where I'd like to see us head.). > I just run make clean-jobs, but we could add a delete endpoint and a little web page. OK, here's my picture:; - first time, prompt for password,; - if no notebook is running launch one and go straight there,; - if notebook is running, get a page with a link to the notebook and a link to kill it. That might be considered strange web design (skip the console depending on the state), in which case I'd vote for the console always. (Wha",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659:323,schedul,scheduled,323,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431248659,1,['schedul'],['scheduled']
Energy Efficiency,"> I don't see what the test_instance database is buying you. CI needs the ability to create databases. In production, it gets this power from the default/database-server-config that has the database root credentials that was set up by hand with the cluster and the database. Now, that means for CI to run in the tests, the default namespace needs a database-server-config. We used to copy the production one, giving the tests root in the database. Not good. Instead what I do is create a new database, call test_instance, and use the test instance credentials to create database-server-config. Now, you can't create databases within this database, but you can create tables and do other happy database stuff. So test_instance is the database that the test CI is going to give out when it needs to create databases. That means CI roughly needs to know if it can create databases (it is root) or it has a database but can't create ones (e.g. test CI), in which the create database step needs to do slightly different things. Does that help?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944:131,power,power,131,https://hail.is,https://github.com/hail-is/hail/pull/7683#issuecomment-562888944,1,['power'],['power']
Energy Efficiency,"> I don't see where the instance version gets checked. I assume if you're scheduling a v2 batch, you should check it is a v2 instance, no? Just to confirm the migration plan: we'll merge this, then once all the instances have flipped to v2, then we make another PR that changes the format to v2?. I thought we discussed this and decided we'll merge, wait for instances to flip to 2, and then merge the second pr to flip the batch format to 2. > Want to confirm the testing plan. You've tested this with batch format v1 (this PR), batch format v2, and tested the migration (that is, switching from master to this PR with batch format v1). I tested it from v1 -> v2. Forgot to test switching from master to v1. Will do once you're happy with the code, and will repeat the other tests.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332:74,schedul,scheduling,74,https://hail.is,https://github.com/hail-is/hail/pull/7949#issuecomment-578182332,1,['schedul'],['scheduling']
Energy Efficiency,"> I still generally find nginx and rules about trailing slashes confusing, but as far as I can tell this seems fine. What do you find confusing?. edit:; Not sure if this helps (if not, let me know, I should be able to clearly answer any questions you have):. Normal location blocks (meaning `location /path/to` with no modifiers or regex) match on prefix, which means that Nginx checks whether the specified `/path/to` exists entirely as a word in the longer url, starting from the beginning of that url (after the domain). A `location = /path/to` (with `=` modifier), is similar in that it is checked against the beginning of the url, with the change that the matched prefix must match exactly (there cannot be anything after `/path/to`). The proxy_pass rule with regard to slashes: Say I have `location /path/to/foo { proxy_pass http://127.0.0.1; }`. Upon matching, Nginx will redirect the request to `127.0.0.1/path/to/foo`, because in the absence of a trailing slash, the entire url (after the domain) is appended. If the proxy_pass directive has a trailing slash, the entire matched prefix is dropped, and only the uri after the prefix is appended. Ex: if the url was `http://domain.com/path/to/foo`, the redirect would be `http://127.0.0.1/`. If the url was `http://domain.com/path/to/foo/bar` the redirect would be to `http://127.0.0.1/bar`. In this PR, when no trailing slash is provided to `https://internal.hail.is/monitoring/service`, the exact location block matches, since the inexact match block has a trailing slash, and therefore doesn't match the slash-less url (moot point anyway: if we made both the inexact and exact location blocks have no slashes, the exact one would take precedence). Then we specify that the proxy_pass has a trailing slash for each block, so that the root of the internal website is `/` rather than `/monitoring/{service}`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983:1425,monitor,monitoring,1425,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-555152983,2,['monitor'],['monitoring']
Energy Efficiency,> I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job. This is a good idea. My current thinking for the autoscaler is to utilize multiple input parameters such as the ones you list to determine the optimal number of instances to create.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899:176,schedul,scheduling,176,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-592059899,1,['schedul'],['scheduling']
Energy Efficiency,"> I think we should change the taints on the node pools before we merge this PR. The GCP UI has changed and I don't see where we can change the taints. I think that's backwards. This PR shouldn't change the scheduling (it just adds tolerations to non-existent taints), so it should go in first, then we should add the taint and let the preemptible workload on non-preemptible nodes get rescheduled to preemptible nodes. However, I think what we should do is create a new tainted non-preemptible pool, merge this PR, and then delete the old pool. Yeah, looks like you can't edit labels or taints from the UI. Maybe you can from the command line? ; Anyway, with the above strategy which seems upgrade safe, it doesn't matter.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039:207,schedul,scheduling,207,https://hail.is,https://github.com/hail-is/hail/pull/7636#issuecomment-565690039,1,['schedul'],['scheduling']
Energy Efficiency,"> Introducing an image with the wheel already installed isn't worthwhile, it adds 2.5 min latency. Agreed. I think our best path for speed is keeping these images totally cacheable so basically dependencies (nothing that will have to change on every commit, e.g. the wheel). Installing the wheels in the image is just adding more latency and work of localization. > The large number of splits often requires default Hail to scale up adding a 2min delay (It would be great to get that down).; I'm gonna revert the change that added images and maybe try to reduce service backend parallelism a bit. 36 minutes is an improvement. We should probably focus on making Hail faster rather than trying to squeeze lower latency out of parallelism. Totally agree.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182:555,reduce,reduce,555,https://hail.is,https://github.com/hail-is/hail/pull/13076#issuecomment-1561101182,1,['reduce'],['reduce']
Energy Efficiency,"> Just to be clear, the other things you tried besides the nginx config shouldn't be in the PR?. I'm not sure. When I was debugging the issue, I tried a bunch of things, and in the end I'm not sure which ones actually worked. I didn't like timing out the scheduler because that can lead to double-schedule. I think ultimately we should control the work entering the driver by some metric of its performance: like CPU load or latency of handling job_complete messages or latency of scheduling per job.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256:255,schedul,scheduler,255,https://hail.is,https://github.com/hail-is/hail/pull/8149#issuecomment-591935256,3,['schedul'],"['schedule', 'scheduler', 'scheduling']"
Energy Efficiency,"> Oops, sorry, your PR was for the monitoring namespace. I see the problem, CI doesn't have a route for `''`. I'll push a fix. Yeah, monitoring, thought you meant it was a similar issue (exact route not matching). Sounds good, I wasn't looking for the """", such a weird route.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245:35,monitor,monitoring,35,https://hail.is,https://github.com/hail-is/hail/pull/7145#issuecomment-536246245,2,['monitor'],['monitoring']
Energy Efficiency,"> Relatedly, the auth system and the front end are not in this pull request (and AFACIT aren't in master yet?), which makes it harder to reason about the overall system. The auth system make sense as an independent PR (is that what #5162 is?). The changes that expose / use this new API (i.e. the UI component) should be a part of this PR so we can reason about the entire proposed change. I'm not sure how to really avoid this, some of it is the nature of our pull request goal (small, single-principle), and the other is the tradeoff of decoupling. This is also why I spend more time writing comments about the intended consumption of the notebook updates. Use those comments to reason about the overall system, and if that doesn't help, ask me to write more helpful comments.; ; The auth system is part of the Greenfield web pull request. That will be split up into something like 10-20 pull requests once the system is fully working, as mentioned in that repo. The auth-gateway will be in 2 of those (one for package-lock, one for the business logic). I've added the gateway changes to this particular pull request; that effectively shows the interface for authorization. I have mixed feelings about mixing that with the rest of this PR, happy to remove and issue separate PR.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641:622,consumption,consumption,622,https://hail.is,https://github.com/hail-is/hail/pull/5215#issuecomment-460065641,2,"['Green', 'consumption']","['Greenfield', 'consumption']"
Energy Efficiency,"> So I'm going to insist on the classical loop interface I described above, since it is strictly more powerful than the interfaces you've proposed. I agree that the tail-recursion interface seems like the right primitive to expose in python, on top of which we could implement convenience methods for building while/for loops if we decide it's worth it. > Giving each loop a name seems natural. Apart from the wrapping issue (the greatest existential threat our generation faces) I don't see any problem calling an outer loop from an inner loop. Also agree. This will require either adding another context of loops/continuations in the environment (valid places to jump to, and their argument types), or keeping them in the normal value context by adding a new continuation type. > Is Patrick's proposal for extra types written up anywhere?. My proposal has two main differences. In; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), x),; 0, 0); ```; the point that jumps back to the top of the loop is explicit, but the point that jumps out of the loop is not. I suggested making this something like; ```; hl.loop(; lambda i, x:; hl.cond(i < 10, hl.recur(i + 1, x + i), hl.break(x)),; 0, 0); ```; or, if we're giving names to loops, it might be simpler to pass the break and recur functions to the lambda:; ```; hl.loop(; lambda sum, ret, i, x:; hl.cond(i < 10, sum(i + 1, x + i), ret(x)),; 0, 0); ```. The second difference is in the typing. In this PR, the `hl.recur` expression is given the type of the entire loop. I would add a single new type `Bottom`, and give all expressions which jump (both the recur and the break expressions) the type `Bottom`. `Bottom` is the empty type, so there can be no closed expressions of type `Bottom`. In the type checker, `Bottom` is only allowed to appear in tail positions, and for `If`, we keep the rule that both branches must have the same type, so either both branches are `Bottom` or neither are. This keeps the semantics simple: an i",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407:102,power,powerful,102,https://hail.is,https://github.com/hail-is/hail/pull/7614#issuecomment-559072407,1,['power'],['powerful']
Energy Efficiency,"> So error propagation from CI back to hailctl isn't great right now. (Something worth fixing!) If it an error in what you're trying to deploy (e.g. branch not found, syntax error in build.yaml, etc.) you can find it in the CI log. I should fix it!. edit: Yeah, I could see it in the logs, I just didn't understand some of the syntax, so dug around CI and deploy codebase, figured it out. Monitoring wasn't expected, thanks.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930:389,Monitor,Monitoring,389,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540028930,1,['Monitor'],['Monitoring']
Energy Efficiency,"> Struct decoding (the fourth generated code one, with 399 own time and 229 samples) is pretty branchy: it checks a bit for each field. I'm not sure how to speed this up. Consider a struct of 8 optional fields. There are 2^8 possible missingness pattern. Each pattern corresponds to a different sequence of field-decoders. I suppose we could generate 256 different patterns and jump to them? That seems excessive. We could maybe generate 16 patterns but that only saves 3/4 of the branches. Maybe that's enough for a substantial speedup?. With the array decoding, I suspect a lot of the speedup wasn't from avoiding branches, but from avoiding a bunch of extra operations handling missing bits one at a time, especially computing the address of the containing byte and loading it from memory every time. We should be able to do something similar for structs, though it will be more complicated. I think that's worth trying independently of trying to reduce branches.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107:950,reduce,reduce,950,https://hail.is,https://github.com/hail-is/hail/issues/13792#issuecomment-1761652107,1,['reduce'],['reduce']
Energy Efficiency,"> The end we're taking the max (in our favor). I'm not totally comfortable with that, e.g. a preemption happens after a job is finished, but the preemption event gets recorded first, you'll charge the user up to the preemption (max) even tho the job already finished. That doesn't seem right. So I see two options:. I intended to always take the min (user's favor) for the end time and the start time should never change but if it does I take it to be earlier in our favor. If the trigger isn't doing that then I got it wrong. Basically, the way I understood the trigger is as follows:. ```; IF OLD.end_time IS NOT NULL AND (NEW.end_time IS NULL OR NEW.end_time > OLD.end_time) THEN; SET NEW.end_time = OLD.end_time;; SET NEW.reason = OLD.reason;; END IF;; ```. This will update the record to have the new end time unless the old end time is not NULL and either the new end time is null (don't want to overwrite the existing value with a null value) or new end time > old end time (don't want to update the record with an end time that is greater than the existing end time in the database). To not override the values, then we need to set the new end time and reason to the old end time and reason to avoid updating the record. Does this make sense? . > Use the reason to update the end time. Completed time should be taken first, then I think whatever is earliest (in the user's favor) between deletion/preemption and cancellation. I can do this if you think it's clearer. I think the answer will be the same. > Alternatively, we just take the earliest time (in the user's favor) always which I think can only give up a small amount of compute on deletion/preemption or cancellation (where we see the event, and while it's being processed, the job completes). This is what I'm doing (or I think I'm doing). See comment above.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375:190,charge,charge,190,https://hail.is,https://github.com/hail-is/hail/pull/7600#issuecomment-558726375,1,['charge'],['charge']
Energy Efficiency,> The error is here:; > `Invalid value for field \'resource.scheduling.instanceTerminationAction\': \'DELETE\'. You cannot specify a termination action for a VM instance that has the standard provisioning model (default).`. Thanks very much for finding that and sorry for not seeing this issue earlier! This is fixed now.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11878#issuecomment-1144803938:60,schedul,scheduling,60,https://hail.is,https://github.com/hail-is/hail/pull/11878#issuecomment-1144803938,1,['schedul'],['scheduling']
Energy Efficiency,"> The phenotypes would need to annotated the imported bgen mt every time. This is very cheap, especially compared to the extra IO/decoding burden. I should note, though, that in the next year we'll start to develop new types of file encodings that should let us represent this data as efficiently as the BGEN in a faster way (using a faster compression codec than zlib)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4780#issuecomment-439415482:285,efficient,efficiently,285,https://hail.is,https://github.com/hail-is/hail/issues/4780#issuecomment-439415482,1,['efficient'],['efficiently']
Energy Efficiency,"> The reason we didn't expose the other parameters was what if we had 16 core jobs waiting to be scheduled and then we changed the worker pool size to 4 cores. Yep. Let's call that admin operator error and let's not do that. The other reason was we had hardcoded the billing computation in the code, but that's fixed now. But it is hardcoded in the documentation, so we still shouldn't really be changing any of these settings (I see this mainly for the second instance at this point). Separately, we should decouple the billing from the details of the implementation so we get a bit more flexibility on the backend in the main instance, as we've discussed. I'm OK with this.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109:97,schedul,scheduled,97,https://hail.is,https://github.com/hail-is/hail/pull/9285#issuecomment-674998109,1,['schedul'],['scheduled']
Energy Efficiency,"> This isn't a browser thing,. It is a browser thing as well as a monitor thing, as far as I can see. I linked to a chromium bug that specifically identified the issue as a browser thing. There is also some difference that I cannot find a bug report for Safari that makes line width unequal between Safari and Chromium browsers, unless I set Safari's line width to <1.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8964#issuecomment-645707340:66,monitor,monitor,66,https://hail.is,https://github.com/hail-is/hail/pull/8964#issuecomment-645707340,1,['monitor'],['monitor']
Energy Efficiency,"> Use os.environ in the test (and remember to set it back to its old value it afterward). This won't work. Hail has already been initialized. We definitely want to set these for every benchmark, at least for now -- this could be responsible for docker blowing limits (we only allocate 1.5 cores, I think)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8050#issuecomment-583462187:276,allocate,allocate,276,https://hail.is,https://github.com/hail-is/hail/pull/8050#issuecomment-583462187,1,['allocate'],['allocate']
Energy Efficiency,"> What's the pressing reason for this to join the mainline?. Ben has wanted this for a while, and there's interest from the group. > I'd prefer to stick to the usual process on services team where thorough code review of code and test-code is done for stuff under our purview (as things in hailtop/batch do). Sure. Regarding testing, in my experience regenie (the C++ program) crashes when given improper inputs, rather than writing anything out, so there is a reduced testing need (we should. verify the expected number of non-empty outputs are created I think, and some basic tests that we parse inputs). My plan for getting regenie in is roughly 4 part, of which this is 1. ; 1) Get the most basic / canonical version of regenie up, on the local backend. This version should take all of the arguments demonstrated in the [regenie tutorial / example](https://rgcgithub.github.io/regenie/options/), pass them to step 1 and step 2, and write an output. ; * Here I'm mostly interested in making sure I'm using Batch Resource classes correctly.; 2) Get the same working for the ServiceBackend.; 3) Expand the service offering to run the paper's example (may require some tweaks to the input handling code), and test that we can run this at scale; 4) Introduce a per-phenotype parallelism mode, which loses a bit of per-core efficiency in favor of greater scale out, benchmark that.; 5) Handle all other combinations of inputs.; 6) Optimize performance (local RAID-0 SSDs up to 9TB, larger instances). So, if at all possible, I would like to stay focused on the first task, within the bounds of what you need to accomplish as leader of the services team; I'd really appreciate your help.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082:461,reduce,reduced,461,https://hail.is,https://github.com/hail-is/hail/pull/9194#issuecomment-667434082,1,['reduce'],['reduced']
Energy Efficiency,"> Why is writing the hail table first more efficient than just directly exporting from the grouped matrixtable?. We take special care to ensure our system is as efficient as possible when reading or writing to this native format. So, it's partly a sociological thing. On the practical end of things, Hail's native formats (for Tables and Matrix Tables) are a partitioned binary format. The partitioned part means Hail can use many cores in parallel to process and write the dataset. The binary part means that Hail need not use unnecessarily large (in terms of bytes) representations of values. These three things together make writing the native formats use less time, use less memory, and be more reliable. ---. > One thing I noticed is the mt_hwe_vals variable in my code below is a MatrixTable and not a GroupedMatrixTable. Is this correct?. Yes, after you aggregate you get back an MT with a different column key. ---. The `entries` method converts your matrix table from a compact and efficient matrix into a ""long"" and inefficient table. I generally recommend avoiding it if you can. However, if you only have a handful of ancestries, I wouldn't expect this to be *that* bad. You can just write the MT itself:. ```python3; ancestry_table = hl.Table.from_pandas(ancestry.astype({""person_id"":str}), key='person_id'); mt = mt.annotate_cols(ancestry = ancestry_table[mt.s].ancestry); mt_hwe_vals = mt.group_cols_by(mt.ancestry).aggregate(hwe = hl.agg.hardy_weinberg_test(mt.GT)); mt_hwe_vals = mt_hwe_vals.select_rows().select_cols() # drop irrelevant row and column fields; mt_hwe_vals.write(bucket + '/hwe.ht'); ```. ---. > I tried modifying the code to what is shown below but I'm still having the same issue. Just to be clear it's the exact same error ""Container exited with a non-zero exit code 137. ""? This makes me think we have an issue with `entries`, because, even though it's not great, it shouldn't be blowing RAM here. Can you share the log file from your previous or next attempt?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636:43,efficient,efficient,43,https://hail.is,https://github.com/hail-is/hail/issues/13287#issuecomment-1679755636,3,['efficient'],['efficient']
Energy Efficiency,> You should leave the optional/required classes -- those are easy ways to intern a ptype so it never gets allocated more than once.; > ; > > by adding the final class modifier to PCanonicalString; > ; > You can remove the `final` modifier here. done,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7750#issuecomment-567091607:107,allocate,allocated,107,https://hail.is,https://github.com/hail-is/hail/pull/7750#issuecomment-567091607,1,['allocate'],['allocated']
Energy Efficiency,"> buildSkip does not need a ptype. This method is used to skip encoded data, which is never getting decoded into a physical representation. So in EArray:. ```scala; def _buildSkip(mb: EmitMethodBuilder, r: Code[Region], in: Code[InputBuffer]): Code[Unit] = {; val len = mb.newLocal[Int](""len""); val i = mb.newLocal[Int](""i""); val skip = elementType.buildSkip(mb). if (elementType.required) {; Code(; len := in.readInt(),; i := 0,; Code.whileLoop(i < len,; Code(; skip(r, in),; i := i + const(1)))); } else {; val mbytes = mb.newLocal[Long](""mbytes""); val nMissing = mb.newLocal[Int](""nMissing""); Code(; len := in.readInt(),; nMissing := PCanonicalArray.nMissingBytes(len),; mbytes := r.allocate(const(1), nMissing.toL),; in.readBytes(r, mbytes, nMissing),; i := 0,; Code.whileLoop(i < len,; Region.loadBit(mbytes, i.toL).mux(; Code._empty,; skip(r, in)),; i := i + const(1))); }; }; ```. Do you want to just code (len + 7) >>> 3 in EArray (say `val nMissingBytes = (len+7) >>> 3` in the constructor). This would be a fast way to delete `PCanonicalArray.nMissingBytes`. The actual missing ness encoding scheme doesn't seem tied to PArrays (notable the allocation is using a different alignment altogether).",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/7701#issuecomment-564225592:686,allocate,allocate,686,https://hail.is,https://github.com/hail-is/hail/issues/7701#issuecomment-564225592,1,['allocate'],['allocate']
Energy Efficiency,"> nb, from [Authorization Overview](https://kubernetes.io/docs/reference/access-authn-authz/authorization/):; > ; > > Caution: System administrators, use care when granting access to pod creation. A user granted permission to create pods (or controllers that create pods) in the namespace can: read all secrets in the namespace; read all config maps in the namespace; and impersonate any service account in the namespace and take any action the account could take. This applies regardless of authorization mode.; > ; > Permission to create a pod gives you permission to mount any secrets in said namespace. Pod creation is a dangerous and powerful permission.; > ; > See this [recently closed ticket on k8s](https://github.com/kubernetes/kubernetes/issues/4957).; > ; > [An issue from June 2018](https://github.com/kubernetes/community/pull/1604) notes this is an issue for multi-tenant clusters. The k8s maintainers don't have bandwidth to iterate on a solution right now. Thanks, yeah, I shared this with Cotton yesterday. We need to be careful seems to be the conclusion.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942:639,power,powerful,639,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479640942,1,['power'],['powerful']
Energy Efficiency,"> that seems fine. These aren't used outside of PString, right?. No longer. TakeByAggregatorSuite.scala and Functions.scala now use allocateAndStoreString",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576362658:132,allocate,allocateAndStoreString,132,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576362658,1,['allocate'],['allocateAndStoreString']
Energy Efficiency,">> the store methods on PString should take a Code[String], not a Code[Array[Byte]], I think. Done, however this is a bit awkward, and I suspect a source of future error, because in order to pStringInstance.allocate you need to know the byte length, not the string/code point length. So I thought to make allocate take a string instead of length. However, if you have the string at the time of allocation, and pass it to allocate, you probably intend to store it, in which case you pay the cost of 2x the number of calls to getBytes. Therefore I made an `allocateAndStoreString` method, which takes care of both steps, potentially more efficiently, but also more ergonomically. I left the allocate(region: Region, length: Int) and store(addr: Long, str: String) methods, though we have no current use for them so I worry will wind up as bloat.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168:207,allocate,allocate,207,https://hail.is,https://github.com/hail-is/hail/pull/7904#issuecomment-576358168,6,"['allocate', 'efficient']","['allocate', 'allocateAndStoreString', 'efficiently']"
Energy Efficiency,"@catoverdrive FYI, I had to switch two newLocal to newField in StagedRegionValueBuilder. This is because if you try to allocate in a new method builder (inside wrapToMethod, say), the local will go into the method builder the SRVB was originally constructed with which is the wrong one. Probably SRVB should not have a method builder but a class builder and allocate should take the MethodBuilder we're emitting the allocation into.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3669#issuecomment-392413962:119,allocate,allocate,119,https://hail.is,https://github.com/hail-is/hail/pull/3669#issuecomment-392413962,2,['allocate'],['allocate']
Energy Efficiency,"@catoverdrive I dismissed your review because I added more changes to address your comment on the FIXME. I think addRegionValue now does a minimal amount of work. In particular, if you write add a region value at the top level to the same region (rvb.start(t); rvb.addRegionValue(rv); rvb.end), it doesn't modify the region but simply sets start = rv.offset. This means that rvb.start can't actually do anything, and some other add routines need to check if they need to allocate. This adds some overhead that should get compiled away in the staged version. I also improved the tests to test adding to the same as well as a different region.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902:471,allocate,allocate,471,https://hail.is,https://github.com/hail-is/hail/pull/2299#issuecomment-336900902,1,['allocate'],['allocate']
Energy Efficiency,"@cseed can you take a look at this for rough structural comments? Happy to back out the `key` business and leave that in Python, but this will make it more efficient",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3974#issuecomment-408702565:156,efficient,efficient,156,https://hail.is,https://github.com/hail-is/hail/pull/3974#issuecomment-408702565,1,['efficient'],['efficient']
Energy Efficiency,@daniel-goldstein Can you take a quick look at this and make sure none of the deleted log statements are used in your monitoring infrastructure? Thanks!,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/11904#issuecomment-1164536883:118,monitor,monitoring,118,https://hail.is,https://github.com/hail-is/hail/pull/11904#issuecomment-1164536883,1,['monitor'],['monitoring']
Energy Efficiency,"@daniel-goldstein sure, functors.reduce with a backwards iterator does the trick. I also added some tests, including one that ensures we do not evaluate arguments after the non-null one.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5560#issuecomment-471116569:33,reduce,reduce,33,https://hail.is,https://github.com/hail-is/hail/pull/5560#issuecomment-471116569,1,['reduce'],['reduce']
Energy Efficiency,"@danking Can you take another look? The only thing I didn't address is the `Phenotypes -> List[Phenotype], VariantChunks -> List[VariantChunk]`. I don't want to rip it out yet in case Wei comes out with the new SAIGE implementation with phenotype groupings. Again, just looking for a green light to start testing it. We can figure out the question about the CLI later on.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13804#issuecomment-1771696190:284,green,green,284,https://hail.is,https://github.com/hail-is/hail/pull/13804#issuecomment-1771696190,1,['green'],['green']
Energy Efficiency,"@danking IIUC the TeamCity build is now working with spark-2.1.0 but not spark-2.0.2; (even though running `./gradlew shadowJar archiveZip` on my laptop with spark-2.0.2 works fine.). From looking at the Maven repo; https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.11; and the elasticsearch-spark connector docs; https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html; there's no indication that some versions only support v2.1, though it does say; ```; elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0. Spark 2.0 is supported in elasticsearch-hadoop since version 5.0; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234:643,Reduce,Reduce,643,https://hail.is,https://github.com/hail-is/hail/pull/2049#issuecomment-320274234,1,['Reduce'],['Reduce']
Energy Efficiency,"@danking Set auth0 callback based on window (this turns out to be cleanest), cleaned up styles, further reduced bundle size a bit by removing use of a state management tool (that wraps any arbitrary object in an observable that can be watched), and added a basic header menu to allow logout. I think the most challenging part of using this web architecture will be managing actions on server vs browser. That is probably the only piece that isn't obvious. I added a few comments that may help; namely _app.js 's constructor runs before everything else (as it wraps all other components), but lifecycle functions (componentDidMount) run from the inner child out to the parent. Constructor runs both on server and client (since the class contains the needed functions that are translated into HTML). getInitialProps is the only lifecycle event that runs on both server and client. All other events are client only. To require something to only run on the server or browser, from getInitialProps, or constructor, check for (typeof window === 'undefined'). Not incredibly elegant, but not terribly problematic either.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562:104,reduce,reduced,104,https://hail.is,https://github.com/hail-is/hail/pull/4931#issuecomment-454606562,1,['reduce'],['reduced']
Energy Efficiency,"@danking Sorry to keep making you break things out, but it is really helpful for me and the changes will go in faster. Can you make a separate PR with the following changes that don't relate to passing the indices and the new index code? Specifically, the following items from your list:. ```; added row_fields which prevents reading and allocation of LID and RSID (also improved python-type-checking for row_fields and entry_fields). I changed several asserts to if's with fatals, so as not to allocate strings. We no longer copy the genotype data into a buffer in the block reader. This was forcing the fastKeys to do an unnecessary data copy. I changed the contract on BgenRecord to require that getValue is called to ""consume"" the record before the next record is taken. getValue(null) just skips bytes (no copy, no decompression). I added RegionValueBuilder.unsafeAdvance which can be used when you're creating an array of empty structs but don't want to do all the unnecessary RVB bookkeeping work. I use RegionValueBuilder.unsafeAdvance to make loading a BGEN without entry fields very fast. I fixed Table.index to not trigger a partition key info gathering; ```",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018:495,allocate,allocate,495,https://hail.is,https://github.com/hail-is/hail/pull/3727#issuecomment-397281018,1,['allocate'],['allocate']
Energy Efficiency,"@danking img: https://github.com/genuinetools/img. ""Standalone, daemon-less, unprivileged Dockerfile and OCI compatible container image builder. img is more cache-efficient than Docker and can also execute multiple build stages concurrently, as it internally uses BuildKit's DAG solver. The commands/UX are the same as docker {build,tag,push,pull,login,logout,save} so all you have to do is replace docker with img in your scripts, command line, and/or life."". Oops, seems it doesn't quite work unprivileged yet, see: https://github.com/genuinetools/img#running-with-docker. Waiting on an upstream docker change, no movement in two months. Hrm.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290:163,efficient,efficient,163,https://hail.is,https://github.com/hail-is/hail/pull/5623#issuecomment-474189290,1,['efficient'],['efficient']
Energy Efficiency,"@danking, @cseed An alternative: [as mentioned in the ticket Dan linked] the acl boundary for pod creation is a namespace. If we scope all user resources to their namespace, and during user resource creation give notebook service account 'create-pod' permissions in the user's namespace, and also remove create pod permissions in the default namespace, we reduce the likelihood that a compromised notebook leader could expose user secrets and other data.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/5753#issuecomment-479643234:356,reduce,reduce,356,https://hail.is,https://github.com/hail-is/hail/pull/5753#issuecomment-479643234,1,['reduce'],['reduce']
Energy Efficiency,@jbloom22 I realized that updating the length of the array after creating it doesn't work because of the variable size of the missingness bits that get allocated.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3151#issuecomment-373180986:152,allocate,allocated,152,https://hail.is,https://github.com/hail-is/hail/pull/3151#issuecomment-373180986,1,['allocate'],['allocated']
Energy Efficiency,@jbloom22 That would be great. We have made Nirvana even faster recently. Also we are working on reduce the overhead (i.e. time used to load the Cache) for each Nirvana process. I will test on the best blockSize again when this is done.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3266#issuecomment-391058985:97,reduce,reduce,97,https://hail.is,https://github.com/hail-is/hail/pull/3266#issuecomment-391058985,1,['reduce'],['reduce']
Energy Efficiency,"@jigold addressed those changes. . Regarding the margin of the `div.wy-nav-content` element, I'm reducing the padding on the right rather than increasing the max-width, I think that should keep the left margin aligned. Though I think that it might not be a bad idea to reduce the left margin across all of the doc pages. I changed one of the treeview parameters as well, hopefully will help with selection issue you were experiencing. Though it is still a bit finicky in certain situations, usually when selecting/unselecting some combination of parent and child nodes (such as in gnomad.exomes). Selecting child nodes on selection of the parent isn't a basic option in the treeview class unfortunately, and I haven't figured out a way to do it that is completely to my satisfaction yet. The clear selections button seems to reset everything appropriately, at least.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1936#issuecomment-315459328:269,reduce,reduce,269,https://hail.is,https://github.com/hail-is/hail/pull/1936#issuecomment-315459328,1,['reduce'],['reduce']
Energy Efficiency,@jigold this is a minimal adaptation of #3466 which avoids exposing RowMatrix by putting an export command on BlockMatrix.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838:26,adapt,adaptation,26,https://hail.is,https://github.com/hail-is/hail/pull/3500#issuecomment-386445838,1,['adapt'],['adaptation']
Energy Efficiency,"@liameabbott I think you should go ahead and merge #3859. Once this is in, you can then use `locus_windows` to simplify, reduce memory req, and be more robust to catching out-of-order loci.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022:121,reduce,reduce,121,https://hail.is,https://github.com/hail-is/hail/pull/3873#issuecomment-401440022,1,['reduce'],['reduce']
Energy Efficiency,"@patrick-schultz have you thought about how to wrap MakeStream (split up to avoid JVM bytecode limits) in the old or new infrastructure? This PR gets ride of MakeArray but until MakeStream, I don't think this is viable. Also, ToArray(MakeStream(...)) seems seems like it will be less efficient since it switches of the index while MakeArray inlines the array construction. When the stream consumer is smaller, we might consider inlining it.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567:284,efficient,efficient,284,https://hail.is,https://github.com/hail-is/hail/pull/8148#issuecomment-590660567,1,['efficient'],['efficient']
Energy Efficiency,"@patrick-schultz knows what i mean. if you have non-overlapping partitions, you just need to order them correctly and then no intra-partition merging is needed, so should be much more efficient (like an un-keyed union)",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/8110#issuecomment-594592285:184,efficient,efficient,184,https://hail.is,https://github.com/hail-is/hail/issues/8110#issuecomment-594592285,1,['efficient'],['efficient']
Energy Efficiency,"@patrick-schultz, sorry, I missed this in-line comment. For the sake of unifying the discussion, I'll reply in PR comment so we can continue in one thread for both discussions (which I think are intimately related). > Right, but this is on the generic key_by path, and this is no longer an obvious optimization in all cases. I think my real question is: what is the new semantics for key_by? If I want to change my key from [A, B] to [B], then probably it will shuffle and choose balanced partitions, keeping roughly the same amount of parallelism, but if the existing partitioner satisfies a somewhat obscure condition that I don't have much control over, it will instead coalesce partitions.; >; > What if we gate this behavior behind a flag on TableKeyBy, and expose a way to opt in to the optimization in python?. This behavior is only accessible when TableKeyBy isSorted=true. If you've used a hidden field (only accessible through a) my newly exposed `_key_by_assert_sorted` or b) writing IR yourself) to assert that your dataset is already in the order of the new key, I'm certain you would *not* want to shuffle. Moreover, switching from `[A, B]` to `[B]` could very well reduce your effective parallelism even after a shuffle because keys are not permitted to be split across partitions. Consider a dataset keyed by `[Locus, Alleles, Gene]` with 10k partitions. If we re-key to `[Gene]` and we only have 1000 genes annotated, we'll lose partitions. In fact, in the 1:1 partition case (the case we're optimizing here) you *must* lose parallelism because each partition contains exactly one value for the key `B`. Indeed, each partition contains only one record at all!",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344:1180,reduce,reduce,1180,https://hail.is,https://github.com/hail-is/hail/pull/8864#issuecomment-637177344,1,['reduce'],['reduce']
Energy Efficiency,@tpoterba Copy suggestions expected. I'm also confused by the test failures... Different tests are failing each time. Is master green? Random tests fail for me when I run unit tests against master locally as well.,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374:128,green,green,128,https://hail.is,https://github.com/hail-is/hail/pull/4068#issuecomment-411199374,1,['green'],['green']
Energy Efficiency,"@tpoterba Done. A few differences from your suggestion: ; 1) No real need to check at the byte level, because modulus will be at most 3 for non-4-divisible lengths (at the byte level...and we only need to check (nBits - (m1*32)) / 8 bytes anyway), and so we would test a max of 3 bytes, and usually less than that in practice.; 2) Cotton had suggested the unstated function live on Memory...but since Memory appears to only call into unsafe, Region calls its own functions, and this code relied on Region, I put the unstaged function in Region rather than Memory. . The tests live in PContainerTest. I can move them to Region, but as we need to allocate some memory, and the easiest way to do that is through ScalaToRegionValue, which requires a ptype, and we're interested in the missing bytes at the moment, the easiest translation would have the Region test calling into PContainer, so it didn't seem to matter much. Let me know if you see it differently.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839:645,allocate,allocate,645,https://hail.is,https://github.com/hail-is/hail/pull/7646#issuecomment-561757839,1,['allocate'],['allocate']
Energy Efficiency,"@tpoterba I apologize for replying late. . I ran the code, and it does not seem to work. Hail seems to upload everything but the Jupyter notebook cell does not stop running, or it is just taking time. It seems to be stuck on: `Running Spark job 1: reduce at TextTableReader.scala:147`",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/3922#issuecomment-411533395:248,reduce,reduce,248,https://hail.is,https://github.com/hail-is/hail/issues/3922#issuecomment-411533395,1,['reduce'],['reduce']
Energy Efficiency,"@tpoterba fixed the config issue and changed n_partitions to ensure workers are scheduled for the FASTA reading. I tested this on a single batch worker so the jobs overlapped and flexed the shared mount code, but we don't really have a guarantee in our test setup because batch has no way to force collocation of jobs (and even so we can't exactly force that the runtimes will overlap). I suppose if there's an issue here it will bubble up as a nondeterministic failure. Not great but perhaps good enough for now?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688:80,schedul,scheduled,80,https://hail.is,https://github.com/hail-is/hail/pull/12736#issuecomment-1499151688,1,['schedul'],['scheduled']
Energy Efficiency,@tpoterba this is ready and green,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/12295#issuecomment-1428565793:28,green,green,28,https://hail.is,https://github.com/hail-is/hail/pull/12295#issuecomment-1428565793,1,['green'],['green']
Energy Efficiency,"@ttbek, thanks for the comment and concern,. > ""the implementations should rely directly on java.util.Random"" Umm, why? From my outsiders perspective I would have assumed that high quality software worked on by the Broad Institute would use a half decent Random Number Generator (RNG). The phrase ""should rely directly on `java.util.Random`"" was referring to not accepting a source of Randomness as a parameter. It was unnecessarily specific, we're sorry that lead to your confusion. We would be happy to accept a pull request that resolves this issue by building an RNG on more theoretically sound primitives as we have done for [hash functions](https://github.com/hail-is/hail/blob/master/src/main/scala/is/hail/utils/HashMethods.scala) or by using an existing efficient random number generator, such as the ones provided by Apache.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/2314#issuecomment-384139281:763,efficient,efficient,763,https://hail.is,https://github.com/hail-is/hail/issues/2314#issuecomment-384139281,1,['efficient'],['efficient']
Energy Efficiency,"A cluster monitoring terminal command also seems useful since you might forget to start your batch with the detailed information. I think a question for us is why this shouldn't be in the web UI? I think there's a clear benefit to having information directly in the CLI when you're using ipython or python or submit, but if you're starting a new terminal window to monitor a running job, why not start a browser window?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13063#issuecomment-1572346942:10,monitor,monitoring,10,https://hail.is,https://github.com/hail-is/hail/issues/13063#issuecomment-1572346942,2,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,A read_filter=[] intervals=[/seq/references/HybSelOligos/whole_exome_agilent_1.1_refseq_plus_3_boosters/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.targets.interval_list] excludeIntervals=null interval_set_rule=UNION interval_merging=ALL interval_padding=50 reference_sequence=/seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta nonDeterministicRandomSeed=false disableRandomization=false maxRuntime=-1 maxRuntimeUnits=MINUTES downsampling_type=BY_SAMPLE downsample_to_fraction=null downsample_to_coverage=1000 use_legacy_downsampler=false baq=OFF baqGapOpenPenalty=40.0 fix_misencoded_quality_scores=false allow_potentially_misencoded_quality_scores=false performanceLog=null useOriginalQualities=false BQSR=null quantize_quals=0 disable_indel_quals=false emit_original_quals=false preserve_qscores_less_than=6 defaultBaseQualities=-1 validation_strictness=SILENT remove_program_records=false keep_program_records=false unsafe=null num_threads=1 num_cpu_threads_per_data_thread=1 num_io_threads=0 monitorThreadEfficiency=false num_bam_file_handles=null read_group_black_list=null pedigree=[] pedigreeString=[] pedigreeValidationType=STRICT allow_intervals_with_unindexed_bam=false generateShadowBCF=false logging_level=INFO log_to_file=null help=false variant=(RodBinding name=variant source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.unannotated.vcf) snpEffFile=(RodBinding name=snpEffFile source=/seq/dax/all_1kg_exomes/v1/all_1kg_exomes.snpeff.vcf) dbsnp=(RodBinding name= source=UNBOUND) comp=[] resource=[] out=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub no_cmdline_in_header=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub sites_only=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub bcf=org.broadinstitute.sting.gatk.io.stubs.VariantContextWriterStub annotation=[SnpEff] excludeAnnotation=[] group=[] expression=[] useAllAnnotations=false list=false alwaysAppendDbsnpId=false MendelViolationGeno,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658:17885,monitor,monitorThreadEfficiency,17885,https://hail.is,https://github.com/hail-is/hail/issues/1822#issuecomment-301916658,1,['monitor'],['monitorThreadEfficiency']
Energy Efficiency,AbstractStringBuilder.java:448); at java.lang.StringBuffer.append(StringBuffer.java:270); at org.apache.log4j.helpers.PatternParser$LiteralPatternConverter.format(PatternParser.java:419); at org.apache.log4j.PatternLayout.format(PatternLayout.java:506); at org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:310); at org.apache.log4j.WriterAppender.append(WriterAppender.java:162); at org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251); at org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66); at org.apache.log4j.Category.callAppenders(Category.java:206); at org.apache.log4j.Category.forcedLog(Category.java:391); at org.apache.log4j.Category.log(Category.java:856); at org.slf4j.impl.Log4jLoggerAdapter.warn(Log4jLoggerAdapter.java:400); at org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66); at org.apache.spark.scheduler.TaskSetManager.logWarning(TaskSetManager.scala:52); at org.apache.spark.scheduler.TaskSetManager.handleFailedTask(TaskSetManager.scala:693); at org.apache.spark.scheduler.TaskSchedulerImpl.handleFailedTask(TaskSchedulerImpl.scala:421); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply$mcV$sp(TaskResultGetter.scala:139); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.scheduler.TaskResultGetter$$anon$3$$anonfun$run$2.apply(TaskResultGetter.scala:124); at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1953); at org.apache.spark.scheduler.TaskResultGetter$$anon$3.run(TaskResultGetter.scala:124); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); ```; `pyhail-submit`:; ```bash; #!/bin/bash. if [ $# -ne 2 ]; then; echo 'usage: gcp-pyhail-submit <cluster> <py-file>'; exit 1; fi. cluster=$1; scr,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027:1591,schedul,scheduler,1591,https://hail.is,https://github.com/hail-is/hail/issues/1186#issuecomment-267416027,1,['schedul'],['scheduler']
Energy Efficiency,"AbstractUnsafe$7.run(AbstractChannel.java:691); at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:367); at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:671); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:456); at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144); at java.lang.Thread.run(Thread.java:745); 2019-01-22 13:12:06 SparkContext: INFO: Successfully stopped SparkContext; 2019-01-22 13:12:06 NettyRpcEnv: WARN: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@115b6ba4 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@3f21bf73[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]; 2019-01-22 13:12:06 YarnSchedulerBackend$YarnSchedulerEndpoint: ERROR: Error requesting driver to remove executor 14 after disconnection.; org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:155); at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:132); at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:228); at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:515); at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSchedulerBackend.scala:253); at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.apply(YarnSch",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587:214058,Schedul,ScheduledThreadPoolExecutor,214058,https://hail.is,https://github.com/hail-is/hail/issues/4733#issuecomment-456534587,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"Added server blocks. @cseed. Added the proxy forwarding headers for consistency (and may provide more information in logs), although they're not strictly necessary. Prometheus doesn't seem to be working, but behavior is identical without move to server blocks (namely it redirects to a default Nginx page on internal.hail.is ; same behavior with and without this change). Behavior of redirecting to ""service"".internal if missing slash still occurs; this seems to occur without hitting the namespace monitoring router (meaning `k logs router-868b794f58-r49hr -n monitoring` shows nothing). So this appears to be happening upstream. Had surprising amount of trouble /monitoring from the routes, even with corresponding changes in monitoring.yaml, and trying to rewrite in a /monitoring block (meaning tried location / and location /monitoring/*, both with and without rewrite rule `rewrite /monitoring/grafana/ /` or similar with a capture group). Something I don't quite understand, insight appreciated because I would prefer not to spend more time experimenting with this. Also, would it be reaonsalbe to not propagate the /namespace/service to internal routes (so rewrite before sending)? It seems like internal server blocks receive the full url, which means that they would need to handle those subpaths when used internally, but not when used normally (for instance I'm not sure how notebook deployed to a namespace gets away with not having a special path for `akotlar/`. Does the last commit address the goal?",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516:499,monitor,monitoring,499,https://hail.is,https://github.com/hail-is/hail/pull/7015#issuecomment-540393516,7,['monitor'],['monitoring']
Energy Efficiency,"Agreed. We tend to idle at 5 n1-standard-8 non-preemptible machines. An additional pool of n1-standard-16's or 32's would be fine. Prometheus could take, say, 45 GB, and a bunch of compute-hungry but memory-lean tasks will get co-scheduled.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/6774#issuecomment-519249271:230,schedul,scheduled,230,https://hail.is,https://github.com/hail-is/hail/pull/6774#issuecomment-519249271,1,['schedul'],['scheduled']
Energy Efficiency,"Ah, figured out what's going on:; ```; ERROR	2020-01-15 18:17:49,022	batch.py	schedule_job:385	error while scheduling job (11, 3) on instance batch-worker-pr-7886-default-npqddriu0gh7-z20pv	Traceback (most recent call last):\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 375, in schedule_job\n raise e\n File ""/usr/local/lib/python3.6/dist-packages/batch/batch.py"", line 366, in schedule_job\n await session.post(url, json=body)\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client.py"", line 589, in _request\n resp.raise_for_status()\n File ""/usr/local/lib/python3.6/dist-packages/aiohttp/client_reqrep.py"", line 947, in raise_for_status\n headers=self.headers)\naiohttp.client_exceptions.ClientResponseError: 413, message='Request Entity Too Large', url='http://10.128.0.25:5000/api/v1alpha/batches/jobs/create; ```. This is causing an instance to be marked unhealthy. Somehow that's causing an always_run job to not run before a batch is considered finished.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815:107,schedul,scheduling,107,https://hail.is,https://github.com/hail-is/hail/pull/7886#issuecomment-574813815,1,['schedul'],['scheduling']
Energy Efficiency,"Ah, no, sorry, I just haven't written the other ones (which can be significantly more efficient) while working on the joint caller. Wrapping primitive types is an easy temporary fix.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/5345#issuecomment-466821036:86,efficient,efficient,86,https://hail.is,https://github.com/hail-is/hail/issues/5345#issuecomment-466821036,1,['efficient'],['efficient']
Energy Efficiency,"Ah, you're totally right, this is unnecessary. I'm looking at a pipeline: split_multi, sampleqc. There wasn't a clear indication in the WebUI Spark wasn't recomputing this (it isn't shown as a green dot like persist), but after the job is complete, the shuffle is marked as ""skipped"" and wasn't recomputed. I don't know how long intermediate shuffle results are kept around or if/when they are flushed.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601:193,green,green,193,https://hail.is,https://github.com/hail-is/hail/pull/1814#issuecomment-301566601,1,['green'],['green']
Energy Efficiency,"Also, I didn't want N bump loops all bumping the global events for scheduling and cancel events. I also have all schedulers share the same async worker pool.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/9774#issuecomment-738118480:67,schedul,scheduling,67,https://hail.is,https://github.com/hail-is/hail/pull/9774#issuecomment-738118480,2,['schedul'],"['schedulers', 'scheduling']"
Energy Efficiency,"Also, ignore getting the schedulable cores backwards.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/13943#issuecomment-1792916933:25,schedul,schedulable,25,https://hail.is,https://github.com/hail-is/hail/pull/13943#issuecomment-1792916933,1,['schedul'],['schedulable']
Energy Efficiency,"Am I strange in that I want to name something what it is (ci, batch, etc.) rather than give everything codenames? The purpose of codenames is to hide and obscure, you know. I think this should be called tutorial. And when it becomes a notebook service, notebook. And when it becomes the Hail service, it should just be the main website. The landing page should be password protected. We should think about whether we want to collect additional information there (e.g. email), although for now I don't think we need to, as everyone who signed up for the next tutorial filled out a questionnaire. I'm getting proxy timeouts. We need an ready endpoint and something on the client side to poll and redirect. Actually, awesome if it doesn't poll but uses, say, websockets, and the server watches the pod for a notification for k8s (or does this and also polls, which seems to be our standard pattern). Should we have an auto-scaling non-preemptible pool and schedule these there? If we do that, to optimize startup time, we should have imagePullPolicy: Never and then pull the image on startup and push it on update. When do you reap jupyter pods? jupyterhub has a simple management console that lets you shut down notebooks. > figure out how to teach flask url_for to use a root other than /. I don't think you can do this dynamically using headers. Blueprints seem to be the answer in Flask: https://stackoverflow.com/questions/18967441/add-a-prefix-to-all-flask-routes/18969161#18969161. Is there a reason you didn't make it a subdomain? I thought we decided we preferred that.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869:953,schedul,schedule,953,https://hail.is,https://github.com/hail-is/hail/pull/4576#issuecomment-431037869,1,['schedul'],['schedule']
Energy Efficiency,And a typical interaction for a current 2.0.2 user:. ```bash; dking@wmb16-359 # gradle compileScala . FAILURE: Build failed with an exception. * Where:; Build file '/Users/dking/projects/hail2/build.gradle' line: 39. * What went wrong:; A problem occurred evaluating root project 'hail'.; > Please generate a gradle.properties file first by executing ./configure. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. BUILD FAILED. Total time: 1.413 secs; 1 dking@wmb16-359 # ./configure; With what version of Spark will you run Hail? (default: 2.0.2); ; using default version: 2.0.2; dking@wmb16-359 # gradle compileScala; The Task.leftShift(Closure) method has been deprecated and is scheduled to be removed in Gradle 5.0. Please use Task.doLast(Action) instead.; at build_2mbp15794fq4sj14khxclz0wz.run(/Users/dking/projects/hail2/build.gradle:168); :compileJava UP-TO-DATE; :nativeLib; (cd libsimdpp-2.0-rc2 && cmake .); -- Configuring done; -- Generating done; -- Build files have been written to: /Users/dking/projects/hail2/src/main/c/libsimdpp-2.0-rc2; :compileScala UP-TO-DATE. BUILD SUCCESSFUL. Total time: 4.418 secs,MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637:751,schedul,scheduled,751,https://hail.is,https://github.com/hail-is/hail/pull/1613#issuecomment-290201637,1,['schedul'],['scheduled']
Energy Efficiency,"And to directly respond to this comment:. > If we are just using the bytes uploaded and downloaded that are tracked by the resource usage monitor, then I think we can do a first pass at adding this functionality. This sounds great! This would resolve question 1 and eliminate the risk. We should charge the highest possible price: 0.23 USD/GiB. Answering question 2 can proceed slowly and carefully knowing that we don't have a cost risk.",MatchSource.ISSUE_COMMENT,hail-is,hail,0.2.133,https://github.com/hail-is/hail/issues/13428#issuecomment-1692171657:138,monitor,monitor,138,https://hail.is,https://github.com/hail-is/hail/issues/13428#issuecomment-1692171657,2,"['charge', 'monitor']","['charge', 'monitor']"
