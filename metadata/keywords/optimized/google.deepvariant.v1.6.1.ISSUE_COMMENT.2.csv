quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,"ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7819,install,install,7819,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"lif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C com",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:1869,INSTALL,INSTALL,1869,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,1,['INSTALL'],['INSTALL']
Deployability,"ll dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16608,install,install,16608,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"lled. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3223,Install,Install,3223,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,['Install'],"['Install', 'Installing']"
Deployability,"load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16434,install,install,16434,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"loud-sdk:alpine; timestamp: '2018-11-08T14:28:05.897359Z'; - description: Stopped pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStoppedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:28:05.747135Z'; - description: Started pulling ""gcr.io/deepvariant-docker/deepvariant_runner:0.7.0""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.PullStartedEvent; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; timestamp: '2018-11-08T14:27:34.961215Z'; - description: Worker ""google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7""; assigned in ""us-west1-b""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerAssignedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:27:06.604193Z'; labels: {}; pipeline:; actions:; - commands:; - -c; - /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project valis-194104 --zones; us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs://canis/CNR-data/TLE_a_001.bam --bai; gs://canis/CNR-data/TLE_a_001.bam.bai --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz --ref_fai; gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi --gcsfuse; entrypoint: bash; environment: {}; flags: []; imageUri: gcr.io/deepvariant-docker/deepvariant_runner:0.7.0; labels: {}; mounts: []; name: ''; pidNamespace: ''; portMappings: {}; - commands:; - /bin/sh; - -c; - gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log; entrypoint: ''; envi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:7532,pipeline,pipeline,7532,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['pipeline'],['pipeline']
Deployability,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8662,install,installed,8662,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['install'],['installed']
Deployability,"ly uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:12804,Install,Installing,12804,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Installing']
Deployability,"ly uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/loc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:9979,Install,Installing,9979,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Installing']
Deployability,"m.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11158,update,updates,11158,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['update'],['updates']
Deployability,"m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permissions); ```bash; wget http://launchpadlibrarian.net/300780258/parallel_20161222-1_all.deb ; dpkg -x parallel_20161222-1_all.deb parallel; export PATH=$HOME/parallel/usr/bin:$PATH; ```. 5. Install TensorFlow MKL-DNN; ```bash; WHEEL_NAME=tensorflow-2.0.0-cp36-cp36m-linux_x86_64.whl; wget ""https://storage.googleapis.com/penporn-kokoro/tf-mkl-2.0-py36/${WHEEL_NAME}"" -O ""/tmp/${WHEEL_NAME}""; pip3 install --upgrade ""/tmp/${WHEEL_NAME}""; ```. 6. Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p $OUTPUT_DIR. export PYTHONPATH=./bazel-genfiles:$PYTHONPATH; python3 ./ba",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:1599,patch,patches,1599,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,1,['patch'],['patches']
Deployability,"machine. (Not required to run on GCP. I just use this to get a; > machine to test); >; > gcloud compute instances create ""${USER}-cpu"" --scopes; > ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts""; > --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072""; > --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel; > Skylake""; >; > ssh into the machine:; >; > gcloud compute ssh pichuan-cpu --zone us-west2-b; >; > Get the binaries and models:; >; > BUCKET=""gs://deepvariant""; > BIN_VERSION=""1.4.0""; > MODEL_VERSION=""1.4.0""; >; > BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; > MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard""; >; > mkdir -p bin; > # Download the DeepVariant binaries.; > gsutil -m cp ""${BIN_BUCKET}/*"" bin/; > chmod a+x bin/*; >; > Then, I ran:; >; > cd bin; bash run-prereq.sh; cd -; >; > The run-prereq.sh tends to be the most tricky one - it will require root; > permission, and it'll install a bunch of stuff on your machine. If you; > can't use Docker because of root permissions, you likely won't be able to; > run this as well.; >; > Download test data:; >; > INPUT_DIR=""${PWD}/quickstart-testdata""; > DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; >; > mkdir -p ${INPUT_DIR}; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; > wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; > wget -P ${I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:1522,install,install,1522,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['install'],['install']
Deployability,md line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4685,Pipeline,Pipeline,4685,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,"me/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1565,release,releases,1565,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['release'],['releases']
Deployability,"ment python; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; /opt/conda/envs/dv/bin/python3; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>; import dataclasses; ModuleNotFoundError: No module named 'dataclasses'; ```; Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc.; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version; Python 3.6.15; (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python; /opt/conda/envs/dv/bin/python; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python; Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_159361046425",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:1052,install,installed,1052,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,1,['install'],['installed']
Deployability,"met same issue when we were trying to run deep variant on GCP Centos7 vm. Error was raised by the function call of psutil.cpu_freq(); https://github.com/google/deepvariant/blob/r0.7/deepvariant/resources.py#L158 . psutil.cpu_freq function is only defined under the condition ; os.path.exists(""/sys/devices/system/cpu/cpufreq"") or os.path.exists(""/sys/devices/system/cpu/cpu0/cpufreq""); https://github.com/giampaolo/psutil/blob/release-5.4.2/psutil/_pslinux.py#L656-L657. So the issue is that the required paths do not exist in the CentOS 7 release. existence check of the paths before calling psutil.cpu_freq() should solve the issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-429700976:427,release,release-,427,,https://github.com/google/deepvariant/issues/104#issuecomment-429700976,2,['release'],"['release', 'release-']"
Deployability,"mmand>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2402,Install,Install,2402,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Install'],['Install']
Deployability,"mp"" directory created at my current working directory. Given that I'm not able to reproduce your issue, I'm not sure what's the best next step for me. If you're able to share more detailed setting for me so I can reproduce your issue, that'll be helpful. I'm not super familiar with Singularity, so I wonder if there are some other setting (other than the version) that could affect this. I'll share my steps below so you can take a look. . ----. Here are my steps trying to test Singularity on CentOS 8. Get a machine:; ```; gcloud compute instances create ""${USER}-centos8"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-8"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --boot-disk-size ""200G"" \; --zone ""us-west1-b""; ```. ssh into the machine:; ```; gcloud compute ssh ${USER}-centos8; ```. Check OS version:; ```; [pichuan@pichuan-centos8 ~]$ cat /etc/os-release; NAME=""CentOS Linux""; VERSION=""8""; ID=""centos""; ID_LIKE=""rhel fedora""; VERSION_ID=""8""; PLATFORM_ID=""platform:el8""; PRETTY_NAME=""CentOS Linux 8""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:centos:centos:8""; HOME_URL=""https://centos.org/""; BUG_REPORT_URL=""https://bugs.centos.org/""; CENTOS_MANTISBT_PROJECT=""CentOS-8""; CENTOS_MANTISBT_PROJECT_VERSION=""8""; ```. I installed singularity on the machine following instructions on https://sylabs.io/guides/3.7/admin-guide/installation.html#installation-on-linux. Confirmed the version:; ```; [pichuan@pichuan-centos8 ~]$ singularity --version; singularity version 3.7.0-1.el8; ```. I'm using the data in Quick Start to test. I downloaded data in the Quick Start: https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. Then, I ran:; ```; BIN_VERSION=1.1.0. singularity build deepvariant.sif docker://google/deepvariant:${BIN_VERSION}. singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittes",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-767294612:1170,release,release,1170,,https://github.com/google/deepvariant/issues/296#issuecomment-767294612,2,['release'],['release']
Deployability,"n (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9479,install,installed,9479,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['install'],['installed']
Deployability,"n -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_pr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8602,install,install,8602,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,n below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 20,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4947,Pipeline,Pipeline,4947,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,"n the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2281,Install,Install,2281,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Install'],['Install']
Deployability,"n't see any issues at the time. I repeated my steps, and still found that **I wasn't able to reproduce your issue, if I'm running in a directory where I have write access.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1098,install,install,1098,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['install'],['install']
Deployability,"n. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:; - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m; - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly the same BAM, just with different v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1061,release,releases,1061,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772,1,['release'],['releases']
Deployability,"n.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)"";",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5921,install,install,5921,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"n: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3731,Install,Install,3731,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,"['Install', 'install']","['Install', 'installed']"
Deployability,"ncompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5911,Install,Install,5911,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,3,"['Install', 'install']","['Install', 'installed']"
Deployability,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:1790,update,updates,1790,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,2,['update'],['updates']
Deployability,"nda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:1103,install,install,1103,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['install'],['install']
Deployability,"ndex.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1922,Install,Install,1922,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Install'],['Install']
Deployability,"ng, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam"" \; --examples ""quickstart-output/sing.make_examples.tfrecord.gz"" \; --gvcf ""quickstart-output/sing.gvcf.tfrecord.gz""; ```. Here is the log I got from my Singularity run:; ```; INFO: Using cached SIF image; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; [W::hts_idx_load3] The index file is older than the data file: quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; I0629 23:43:41.562350 139796154570496 genomics_reader.py:223] Reading quickstart-testdata/NA128",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14616,install,install,14616,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['install'],['install']
Deployability,"nstall advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5308,install,install,5308,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"nstall; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:15514,install,install,15514,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,['install'],['install']
Deployability,"nstructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEB",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12356,Pipeline,Pipeline,12356,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,"nt. ```bash; # Power8 environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1180,install,install,1180,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"nt/blob/r1.5/docs/deeptrio-quick-start.md#notes-on-singularity. https://github.com/google/deepvariant/blob/r1.5/scripts/install_singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1132,update,update,1132,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236,2,['update'],['update']
Deployability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3472,Install,Install,3472,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['Install', 'install']","['Install', 'installed']"
Deployability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7707,Install,Install,7707,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,3,"['Install', 'install']","['Install', 'installed']"
Deployability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2101,Install,Install,2101,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,"['Install', 'install']","['Install', 'installed']"
Deployability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2446,Install,Install,2446,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,3,"['Install', 'install']","['Install', 'installed']"
Deployability,"nv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; ++ PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10925,install,installed,10925,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['install'],['installed']
Deployability,"o -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory'; # # It's unclear whether we need this or not. Setting up to get rid of the errors.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # pip3 install ""${PIP_ARGS[@]}"" nvidia-tensorrt; # echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4566,install,install,4566,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['install'],['install']
Deployability,"o write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1072,Install,Install,1072,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['Install'],['Install']
Deployability,"o,; > ; > I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents,",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:1001,install,installed,1001,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,2,['install'],['installed']
Deployability,"oad Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2686,Install,Installing,2686,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,"['Install', 'install']","['Installing', 'installation']"
Deployability,oh~~~yeap.I have find the cause of this problem! ; These samples did go through different pipelines where docker mounted to different containers; but it looks right except the sample name .; Thanks very much,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/670#issuecomment-1608591412:90,pipeline,pipelines,90,,https://github.com/google/deepvariant/issues/670#issuecomment-1608591412,1,['pipeline'],['pipelines']
Deployability,"ok, here are my steps:. # Get a GPU machine. I used the command here: https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform. My machine:; ```; pichuan@pichuan-gpu:~$ uname -a; Linux pichuan-gpu 5.11.0-1029-gcp #33~20.04.3-Ubuntu SMP Tue Jan 18 12:03:29 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux; ```. # Install GPU driver and Singularity on the machine:; ```; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_nvidia_docker.sh | bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.3/scripts/install_singularity.sh | bash; ```. Singularity version:; ```; pichuan@pichuan-gpu:~$ singularity --version; singularity version 3.7.0; ```. # Got the test data from Quick Start. I followed the steps in https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-quick-start.md to get small test data. # Run Singularity. ```; # Pull the image.; BIN_VERSION=1.3.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run DeepVariant.; # Using ""--nv"" and ""${BIN_VERSION}-gpu"" is important.; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=$(nproc); ```. The command above worked, so I copy/pasted the command from the original post:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725:374,Install,Install,374,,https://github.com/google/deepvariant/issues/514#issuecomment-1035630725,1,['Install'],['Install']
Deployability,"ollowing dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5153,Update,Update,5153,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Update'],['Update']
Deployability,"ollowing dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang wi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6179,Update,Update,6179,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Update'],['Update']
Deployability,"om pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 28日 星期二 10:55:17 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:14422,upgrade,upgrade,14422,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['upgrade'],['upgrade']
Deployability,"ome/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6491,install,install,6491,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"omponents-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple ti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5221,Update,Update,5221,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['Update'],['Update']
Deployability,"on setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16552,install,installed,16552,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,['install'],"['install', 'installed']"
Deployability,"on-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVariant with Singularity. I followed Quick Start to get data. ```; singularity pull docker://google/deepvariant:1.4.0; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check numpy version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:1859,install,install,1859,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['install'],['install']
Deployability,"on3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0x38): undefined reference to `void absl::lts_20230802::log_internal::LogMessage::CopyToEncodedBuffer<(absl::lts_20230802::log_internal::LogMessage::StringType)0>(std::basic_string_view<char, std::char_traits<char> >)'; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <24>(char const (&) [24])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi24EEERS2_RAT__Kc[_ZN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:5026,INSTALL,INSTALL,5026,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['INSTALL'],['INSTALL']
Deployability,"on3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:3879,configurat,configurations,3879,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['configurat'],['configurations']
Deployability,"on?. I'm going to walk through what I've tested so far. Maybe you can check which step is different from your experience. ---. Just to make sure I try it myself, here is what I did:. Get a GPU machine to test with:. ```bash; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. I ssh to the machine `gcloud compute ssh pichuan-gpu --zone us-west1-b`. Because my machine doesn't have Nvidia driver installed, I used:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_nvidia_docker.sh; sudo bash -x install_nvidia_docker.sh ; ```; (the docker part is probably not necessary. I just need the driver). And then because I want to test Singularity, I install that with:. ```bash; wget https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/install_singularity.sh; sudo bash -x install_singularity.sh ; ```. Now, my machine has Singularity. I'll start following https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity. I ran:. ```bash; BIN_VERSION=1.6.0; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}-gpu""; ```. This created the file `deepvariant_deeptrio-1.6.0-gpu.sif` on my machine. I checked its size:. ```bash; pichuan@pichuan-gpu:~$ ls -lh deepvariant_deeptrio-1.6.0-gpu.sif ; -rwxrwxr-x 1 pichuan pichuan 12G Dec 5 07:38 deepvariant_deeptrio-1.6.0-gpu.sif; ```. ( @alanlamsiu , This is one thing I'd like you to double check. If you're converting from the 1.6.0 version, I don't think you should see `1.6.0rc2` in your .sif filename. Which is why I asked what command you used to that get that .sif file. You might be pulling a previous, unofficial Docke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:1117,install,install,1117,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,2,['install'],['install']
Deployability,"oogle/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:2812,install,install,2812,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['install'],['install']
Deployability,"ools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=64; ```. ```; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ```; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}"". gunzip ""${DATA_DIR}/ucsc_hg19.fa.gz""; ```. ```; sudo apt -y update; sudo apt -y install parallel; sudo apt -y install docker.io; ```. ```; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-000??.gz \; ${OUTPUT_BUCKET}; ```. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:2603,update,update,2603,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,3,"['install', 'update']","['install', 'update']"
Deployability,"orFlow ${DV_GCP_OPTIMIZED_TF_WHL_VERSION} wheel""; # pip3 install ""${PIP_ARGS[@]}"" --upgrade ""intel-tensorflow==${DV_GCP_OPTIMIZED_TF_WHL_VERSION}""; # else; echo ""Installing standard CPU-only TensorFlow ${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION} wheel""; pip3 install ""${PIP_ARGS[@]}"" --upgrade ""tensorflow==${DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION}""; # fi; # fi; # fi. # # A temporary fix.; # # Context: intel-tensorflow 2.7.0 will end up updating markupsafe to 2.1.1,; # # which caused the issue here: https://github.com/pallets/markupsafe/issues/286.; # # Specifically:; # # ImportError: cannot import name 'soft_unicode' from 'markupsafe'.; # # So, forcing a downgrade. This isn't the best solution, but we need it to get; # # our tests pass.; pip3 install ""${PIP_ARGS[@]}"" --upgrade 'markupsafe==2.0.1'. # ################################################################################; # # CUDA; # ################################################################################. # note_build_stage ""Install CUDA"". # # See https://www.tensorflow.org/install/source#gpu for versions required.; # if [[ ""${DV_GPU_BUILD}"" = ""1"" ]]; then; # if [[ ""${DV_INSTALL_GPU_DRIVERS}"" = ""1"" ]]; then; # # This script is only maintained for Ubuntu 20.04.; # UBUNTU_VERSION=""2004""; # # https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:2510,Install,Install,2510,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Install']
Deployability,"orks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free to skip it, but it's here if you are curious to dive deeper in the possible _de novo_ calls from DeepTrio/GLnexus. Basically the big idea is take it slow and have fun to get the most of out it, as with many moving parts (_programs + parameters_) and varied data you want to be confident in the calls - which can take a lot of finesse. With super-clean data, that's not such a big deal - but that's not why we use these tools :). Hope it helps,; Paul. #### References. [1] [RTG Tools Manual](https://github.com/RealTimeGenomics/rtg-tools/blob/master/installer/resources/tools/RTGOperationsManual.pdf); [2] [dv-trio: a family-based variant calling pipeline using DeepVariant](https://academic.oup.com/bioinformatics/article/36/11/3549/5823297?login=false); [3] [FamSeq: A Variant Calling Program for Family-Based Sequencing Data Using Graphics Processing Units](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003880); [4] [DeepTrio: Variant Calling in Families Using Deep Learning](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1); [5] [A unified haplotype-based method for accurate and comprehensive variant calling](https://pubmed.ncbi.nlm.nih.gov/33782612/) _(This is the Octopus paper.)_; [6] [DeNovoGear: de novo indel and point mutation discovery and phasing](https://pubmed.ncbi.nlm.nih.gov/23975140/)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:5479,install,installer,5479,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,4,"['install', 'pipeline']","['installer', 'pipeline']"
Deployability,"oud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4768,install,installed,4768,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['install'],['installed']
Deployability,"owing dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6328,Install,Install,6328,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Install'],['Install']
Deployability,"ownloads?target_os=Linux&target_arch=x86_64&Distribution=Ubuntu&target_version=20.04&target_type=deb_local; # echo ""Checking for CUDA...""; # if ! dpkg-query -W cuda-11-3; then; # echo ""Installing CUDA...""; # UBUNTU_VERSION=""2004""; # curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # #####################################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:3885,upgrade,upgrade,3885,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,4,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"p with the runtime increase question. There are a few reasons I can think of that might have increased overall runtime between v0.9.0 and v1.0.0, but the magnitude of increase you're observing is not quite what I'd expected. First, let's take a quick comparison on our GitHub documentation:; - https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-case-study.md : 82m+189m+36m = 307m; - https://github.com/google/deepvariant/blob/r1.0/docs/metrics.md : 89m+258m+84m = 431m. Before we talk about code changes, note that r0.9 was done on a 50x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r0.9/scripts/run_wgs_case_study_docker.sh)), and in r1.0 we used a 35x BAM ([see script](https://raw.githubusercontent.com/google/deepvariant/r1.0/scripts/run_wgs_case_study_docker.sh)). The lower coverage BAM is one main reason why the `postprocess_variants` took more time (84m vs 36m). There are a few known reasons why runtime would increase.; 1. In [v1.0.0 release notes](https://github.com/google/deepvariant/releases/tag/v1.0.0), we mentioned that `--min_mapping_quality` is now set to 5 (previously 10). This would increase the number of reads being considered, as a result, increase the number of candidates generated. Which is why `make_examples` runtime might increase. And if there are more candidates, `call_variants` runtime can also increase. 2. In [v0.10.0 release notes](https://github.com/google/deepvariant/releases/tag/v0.10.0), we mentioned that we have turned off `--ws_use_window_selector_model` to improve accuracy but increased runtime too. . All these being said, I am surprised by your observation ""v0.9.0 takes 10-14h while v1.0.0 takes more than 24h per sample"". If you can provide the following information, it'll be very helpful for us:. 1. What is the runtime breakdown of the 3 steps on v0.9.0 and v1.0.0? This will help us understand which step (make_examples, call_variants, postprocess_variants) might be increasing the most.; 2. On exactly",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-695126772:1008,release,release,1008,,https://github.com/google/deepvariant/issues/346#issuecomment-695126772,1,['release'],['release']
Deployability,"packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pip; Using cached pip-22.2.2-py3-none-any.whl (2.0 MB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:2899,install,installed,2899,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1255,Install,Install,1255,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Install'],['Install']
Deployability,"r (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3532,upgrade,upgraded,3532,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,3,"['install', 'upgrade']","['installed', 'upgraded']"
Deployability,"rallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/QJref.fa"" --reads ""/mnt/input.bam"" --examples ""/mnt/dpv/make_examples.tfrecord@3.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""/mnt/dpv/gvcf.tfrecord@3.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; [E::hts_open_format] Failed to open file ""/mnt/input.bam"" : No such file or directory; Traceback (most recent call last):; File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/path1/4_Test/qingjiang/dpv/tmp_dir/Bazel.runfiles_p4oo9k4b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886:2690,install,installed,2690,,https://github.com/google/deepvariant/issues/613#issuecomment-1431702886,1,['install'],['installed']
Deployability,"rap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8992,install,install,8992,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10043,install,installation,10043,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['install'],['installation']
Deployability,"recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting be",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:9566,install,installation,9566,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installation']
Deployability,"red/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSI",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11819,install,installed,11819,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['install'],['installed']
Deployability,"rg)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . 2023-06-05 01:38:40 (48.1 MB/s) - written to stdout [3145/3145]. OK. > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; Hit:1 https://download.docker.com/linux/ubuntu focal InRelease; Hit:2 https://download.sublimetext.com apt/stable/ InRelease; Hit:3 http://ports.ubuntu.com/ubuntu-ports focal InRelease; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:10843,update,update,10843,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['update'],['update']
Deployability,"ries either. ---. Here is what I did:. Get a machine. (Not required to run on GCP. I just use this to get a machine to test). `gcloud compute instances create ""${USER}-cpu"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ""ubuntu-2004-lts"" --image-project ""ubuntu-os-cloud"" --machine-type ""custom-64-131072"" --boot-disk-size ""300"" --zone ""us-west2-b"" --min-cpu-platform ""Intel Skylake""`. ssh into the machine:. ```; gcloud compute ssh pichuan-cpu --zone us-west2-b; ```. Get the binaries and models:. ```; BUCKET=""gs://deepvariant""; BIN_VERSION=""1.4.0""; MODEL_VERSION=""1.4.0"". BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wgs_standard"". mkdir -p bin; # Download the DeepVariant binaries.; gsutil -m cp ""${BIN_BUCKET}/*"" bin/; chmod a+x bin/*; ```. Then, I ran:; ```; cd bin; bash run-prereq.sh; cd -; ```. The `run-prereq.sh` tends to be the most tricky one - it will require root permission, and it'll install a bunch of stuff on your machine. If you can't use Docker because of root permissions, you likely won't be able to run this as well. Download test data:. ```; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241:1336,install,install,1336,,https://github.com/google/deepvariant/issues/590#issuecomment-1322695241,1,['install'],['install']
Deployability,"rl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-ubuntu2004.pin; # sudo mv cuda-ubuntu2004.pin /etc/apt/preferences.d/cuda-repository-pin-600; # # From https://forums.developer.nvidia.com/t/notice-cuda-linux-repository-key-rotation/212772; # sudo -H apt-key adv --fetch-keys ""http://developer.download.nvidia.com/compute/cuda/repos/ubuntu${UBUNTU_VERSION}/x86_64/3bf863cc.pub""; # sudo add-apt-repository -y ""deb https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/ /""; # sudo -H apt-get update ""${APT_ARGS[@]}""; # # From: https://superuser.com/a/1638789; # sudo -H DEBIAN_FRONTEND=noninteractive apt-get \; # -o Dpkg::Options::=--force-confold \; # -o Dpkg::Options::=--force-confdef \; # -y --allow-downgrades --allow-remove-essential --allow-change-held-packages \; # full-upgrade; # sudo -H apt-get install ""${APT_ARGS[@]}"" cuda-11-3; # fi; # echo ""Checking for CUDNN...""; # if [[ ! -e /usr/local/cuda-11/include/cudnn.h ]]; then; # echo ""Installing CUDNN...""; # CUDNN_TAR_FILE=""cudnn-11.3-linux-x64-v8.2.0.53.tgz""; # wget -q https://developer.download.nvidia.com/compute/redist/cudnn/v8.2.0/${CUDNN_TAR_FILE}; # tar -xzvf ${CUDNN_TAR_FILE}; # sudo cp -P cuda/include/cudnn.h /usr/local/cuda-11/include; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo cp -P cuda/lib64/libcudnn* /usr/local/cuda-11/lib64/; # sudo chmod a+r /usr/local/cuda-11/lib64/libcudnn*; # sudo ldconfig; # fi; # # Tensorflow says to do this.; # sudo -H apt-get install ""${APT_ARGS[@]}"" libcupti-dev > /dev/null; # fi. # # If we are doing a gpu-build, nvidia-smi should be install. Run it so we; # # can see what gpu is installed.; # nvidia-smi || :; # fi. # ################################################################################; # # TensorRT; # ################################################################################. # note_build_stage ""Install TensorRT"". # # Address the issue:; # # 'dlerror: libnvinfer.so.7: cannot open sh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:4052,Install,Installing,4052,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Installing']
Deployability,"rotocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/adv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2541,install,install-compile-source,2541,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install-compile-source']
Deployability,"rray numpy extension module failed. Most; > likely you are trying to import a failed build of numpy.; > Here is how to proceed:; > - If you're working with a numpy git repository, try `git clean -xdf`; > (removes all files not under version control) and rebuild numpy.; > - If you are simply trying to use the numpy version that you have installed:; > your installation is broken - please reinstall numpy.; > - If you have already reinstalled and that did not fix the problem, then:; > 1. Check that you are using the Python you expect (you're using /usr/bin/python),; > and that you have no directories in your PATH or PYTHONPATH that can; > interfere with the Python and numpy versions you're trying to use.; > 2. If (1) looks fine, you can open a new issue at; > https://github.com/numpy/numpy/issues. Please include details on:; > - how you installed Python; > - how you installed numpy; > - your operating system; > - whether or not you have multiple versions of Python installed; > - if you built from source, your compiler versions and ideally a build log; > ; > Note: this error has many possible causes, so please don't comment on; > an existing issue about this - open a new one instead.; > ; > Original error was: PyCapsule_Import could not import module ""datetime""; > ; > Traceback (most recent call last):; > File ""/usr/lib/python2.7/runpy.py"", line 174, in _run_module_as_main; > ""__main__"", fname, loader, pkg_name); > File ""/usr/lib/python2.7/runpy.py"", line 72, in _run_code; > exec code in run_globals; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 223, in <module>; > File ""/opt/deepvariant/bin/make_examples.zip/__main__.py"", line 204, in Main; > File ""/usr/lib/python2.7/subprocess.py"", line 523, in call; > return Popen(*popenargs, **kwargs).wait(); > File ""/usr/lib/python2.7/subprocess.py"", line 711, in __init__; > errread, errwrite); > File ""/usr/lib/python2.7/subprocess.py"", line 1235, in _execute_child; > self.pid = os.fork(); > OSError: [Errno 11] Re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-598179709:1810,install,installed,1810,,https://github.com/google/deepvariant/issues/274#issuecomment-598179709,6,['install'],['installed']
Deployability,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3848,install,installed,3848,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['install'],['installed']
Deployability,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4874,install,installed,4874,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['install'],['installed']
Deployability,"rself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. ![image](https://github.com/google/deepvariant/assets/6555937/059d6a98-7365-4e06-a3df-a32876042733). Then you use that equation (or your own) to update the learning rate with each iteration of model training. $`2)`$ For batch size, you can have a discrete range like this `batch_sizes = [16, 32, 64]` to select from. Then for each iteration, you look at the metrics and select what to tweak, given the model you want to start from. Meaning you run through all the batch sizes, and see which one performs best. Then you use that, and go through different learning rates based on the accuracy of the resulting models. If you have other parameters you want to play with, then you empirically determine how they interact with the tuning of the model for reaching optimal accuracy. What does this mean? This means you have to empirically try a lot of combinations, going through many iterations until you find the optimal model representing your data. Again keep in mind this generally is geared for diploid germline variant calling - which still requires some tuning - but you would need play with the tuning more if it varies a lot given your training and v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:2547,update,update,2547,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,2,['update'],['update']
Deployability,"rsion # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8263,install,install,8263,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"ry, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2692,Install,Install,2692,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,1,['Install'],['Install']
Deployability,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3712,Update,Update,3712,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,3,"['Update', 'configurat']","['Update', 'configuration']"
Deployability,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2580,configurat,configuration,2580,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100,1,['configurat'],['configuration']
Deployability,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5104,Install,Install,5104,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,3,"['Install', 'install']","['Install', 'installed']"
Deployability,"scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam"" --examples ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/make_examples.tfrecord@1.gz"" --emit_realigned_reads --gvcf ""/scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_output_intermediate/gvcf.tfrecord@1.gz"" --realigner_diagnostics ""/output/realigned_reads"" --regions ""chr15:41,132,484-42,007,831"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LANG = ""en_GB.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; I0807 12:32:36.932506 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader; W0807 12:32:36.932703 47023237326656 make_examples_core.py:276] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0807 12:32:36.943572 47023237326656 make_examples_core.py:239] Preparing inputs; I0807 12:32:36.953401 47023237326656 genomics_reader.py:222] Reading /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam with NativeSamReader; I0807 12:32:36.964995 47023237326656 make_examples_core.py:239] Common contigs are ['chr1', 'chr2', 'chr3', 'chr4",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:3589,install,installed,3589,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113,1,['install'],['installed']
Deployability,"sh; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:2483,release,releases,2483,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,['release'],['releases']
Deployability,"singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. This worked for me. Check numpy version:. ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; python -c 'import numpy as np; print(np.version.version)'; ```; Which shows:; ```; 1.19.2; ```. It seems like my machine doesn't already have numpy, though:. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ pip3.8 show numpy; WARNING: Package(s) not found: numpy; ```; doesn't show anything. ## Install numpy 1.23.0 to see it breaks things. I ran:; ```; pip3.8 install numpy==1.23.0; ```; (Because you mentioned your cluster has 1.23.0). Now this shows:. ```; [pichuan@pichuan-centos7 ~]$ pip3.8 show numpy; Name: numpy; Version: 1.23.0; Summary: NumPy is the fundamental package for array computing with Python.; Home-page: https://www.numpy.org; Author: Travis E. Oliphant et al.; Author-email: None; License: BSD; Location: /home/pichuan/.local/lib/python3.8/site-packages; Requires: ; Required-by: ; ```. Then I re-ran:. ```; # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. Which still seems to wor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:3129,install,install,3129,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['install'],['install']
Deployability,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1278,install,installed,1278,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236,6,"['configurat', 'install']","['configuration', 'installed']"
Deployability,sion UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session -,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:1939,Pipeline,Pipeline,1939,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,sion await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clini,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4358,Pipeline,Pipeline,4358,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,1,['Pipeline'],['Pipeline']
Deployability,"so, had been finding my ways around it but still. please can you give me more information on how to use the docker image, because the bazel installation is not just working out for me. thank you i so much appreciate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415959978:140,install,installation,140,,https://github.com/google/deepvariant/issues/89#issuecomment-415959978,1,['install'],['installation']
Deployability,"ss.** But, I'm able to see the same issue, if I start my command in a directory where I can't write to. I have two questions for you:; (1) When you run the command, do you have write permission to the directory you're in? (Based on the current code, that's where the converted model files are written to.); (2) What is your Singularity version?. I listed all my steps below in case it's useful. ---. # Worklog. ## Get a Ubuntu16.04 machine; I used the [command here](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform) to get a Ubuntu16.04 machine. ## Set up on the machine; After ssh into the machine, before start running the [PacBio tutorial](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md), I'll install things first:. ```; curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh; ```; After installing conda, I logged out and re-logged in. I install Singularity:; ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.1/scripts/install_singularity.sh; bash install_singularity.sh; ```. Here is my Singularity version:; ```; (base) pichuan@pichuan-cpu:~$ singularity --version; singularity version 3.3.0-1; ```. ## Run through PacBio tutorial; I follow the steps here:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md; and ran through all commands set up conda, and download all files. When I get to this step:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:1256,install,installing,1256,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['install'],['installing']
Deployability,"stall RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-pacbio-model-case-study.md to test. Download data:. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; cur",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:2753,install,installed,2753,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,['install'],['installed']
Deployability,"stead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prere",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8503,Install,Installing,8503,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Installing']
Deployability,"t all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4883,install,installed,4883,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['install'],['installed']
Deployability,"t and log file. code is running but neither output generating or error throwing just running.; please see below code and log file. ###### code #############; #!/usr/bin/env nextflow. nextflow.enable.dsl=2; params.outdir = '/home/deepak/integration/resu1'; params.data_dir = '/home/deepak/integration/resu1/4.markDupliM'; params.refhg38 = '/home/deepak/integration/hg381_22XYM'; params.bed = '/home/deepak/integration'. workflow {; // Define channels for input data; Channel; .fromPath(""${params.data_dir}/*_sorted_md.bam""); .map { file -> ; def sample_id = file.baseName.replace('_sorted_md', ''); return [sample_id, file]; }; .set { read_pairs }; /// Step 1. DeepVariant; DeepVariant(read_pairs, params.refhg38, params.bed); }. process DeepVariant {; tag ""deepavar on ${sample_id}""; publishDir ""${params.outdir}/5.finaleepvar"", mode: 'copy'; cpus 4; //BIN_VERSION 1.6.1. input:; tuple val(sample_id), path(read_files); val(params.refhg38); val(params.bed); ; output:; //tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), path(""${sample_id}_rawd.gvcf.gz""), emit: raw_vcfs; tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), emit: raw_vcfs. script:; """"""; docker run \; -v ""${params.data_dir}"":/opt/bam -v ""${params.refhg38}"":/opt/refhg38 -v ""${params.bed}"":/opt/bed \; google/deepvariant:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /opt/refhg38/Homo_sapiens_assembly38cleaned.fasta \; --reads /opt/bam/${read_files} \; --regions /opt/bed/hg38_exomeY.bed \; --output_vcf /opt/bam/${sample_id}_rawd.vcf.gz \; --num_shards ${task.cpus}; """"""; }. ######## code ################. terminal:; (base) deepak@ubuntu22:~/integration$ nextflow run final_deepvarian.nf . N E X T F L O W ~ version 24.04.4. Launching `final_deepvarian.nf` [hungry_stonebraker] DSL2 - revision: 4dab17f4f2. executor > local (1); [dd/64034b] DeepVariant (deepavar on SRR26512958) [ 0%] 0 of 2. log file attached; [nextflow.log](https://github.com/user-attachments/files/17008943/nextflow.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013:1733,integrat,integration,1733,,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013,1,['integrat'],['integration']
Deployability,"t be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:2278,a/b,a/build-prereq,2278,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['a/b'],['a/build-prereq']
Deployability,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1467,Configurat,Configuration,1467,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['Configurat'],['Configuration']
Deployability,"t you see:. ```C; #define _GNU_SOURCE; #include <pthread.h>; #include <stdio.h>; #include <stdlib.h>; #include <errno.h>. #define handle_error_en(en, msg) \; do { errno = en; perror(msg); exit(EXIT_FAILURE); } while (0). int; main(int argc, char *argv[]); {; int s;; cpu_set_t cpuset;; pthread_t thread;. thread = pthread_self();. /* Set affinity mask to include CPUs 0 to 7. */. CPU_ZERO(&cpuset);; for (int j = 0; j < 8; j++); CPU_SET(j, &cpuset);. s = pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_setaffinity_np"");. /* Check the actual affinity mask assigned to the thread. */. s = pthread_getaffinity_np(thread, sizeof(cpuset), &cpuset);; if (s != 0); handle_error_en(s, ""pthread_getaffinity_np"");. printf(""Set returned by pthread_getaffinity_np() contained:\n"");; for (int j = 0; j < CPU_SETSIZE; j++); if (CPU_ISSET(j, &cpuset)); printf("" CPU %d\n"", j);. exit(EXIT_SUCCESS);; }; ```. You should see something like this:. ```Bash; $ ./affinity; Set returned by pthread_getaffinity_np() contained:; CPU 0; CPU 1; CPU 2; CPU 3; CPU 4; CPU 5; CPU 6; CPU 7; $; ```. 2) Could you tell us what operating system and cpu are your running? If it is Linux could you the following commands and tell us what you see (these will tell us something about your CPU, Linux version, and total number of threads you can have):. Command 1: `cat /proc/sys/kernel/threads-max`; Command 2: `lscpu`; Command 3: `cat /etc/os-release` ; Command 4: `lsb_release -a`. 3) Extra credit: This will tell us about total number of threads used by the application, and if they might be hitting the maximum allowable thread count. And then just type for the process ID (PID) of the above:. ps -o thcount PID. or . watch ps -o thcount PID. where PID is the process ID of the process above. You can get process IDs via the `ps aux` command. Basically what this will do is tell us if you can actually set affinity on your CPU and the number of cores/threads available. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-993147588:1813,release,release,1813,,https://github.com/google/deepvariant/issues/497#issuecomment-993147588,1,['release'],['release']
Deployability,"t"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. #####",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:15942,install,install,15942,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,['install'],['install']
Deployability,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2680,Configurat,Configuration,2680,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['Configurat'],['Configuration']
Deployability,"t/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: /opt/software/GCCcore/6.4.0/lib64/libstdc++.so.6: version `CXXABI_1.3.11' not found (required by /mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so); ```. I upgraded to a higher version of GNU and re-ran but I got a nother error . ```; module load GNU/7.3.0-2.30. python $HOME/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/binaries/DeepVariant/0.7.2/DeepVariant-0.7.2+cl-225213413/make_examples.zip \; --mode training --reads ""${BAM}"" --ref ""${REF}"" --examples ""$training.tfrecord.gz"" \; --truth_variants ""${TRUTH_VCF}"" --confident_regions ""${TRUTH_BED}"" \; --exclude_regions ""chr20:14000000-15000000"" --sample_name ""train"" ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 41, in <module>; from deepvariant import pileup_image; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 42, in <module>; from third_party.nucleus.util import ranges; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 42, in <module>; from third_party.nucleus.io import bed; File ""/tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 79, in <module>; from third_party.nucleus.io.python import bed_reader; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /tmp/Bazel.runfiles_FlJ2h7/runfiles/com_google_deepvariant/third_party/nucleus/io/python/../../../../_solib_k8/libexternal_Shtslib_Slibhtslib.so); ```. I tried to install local GLIBC. I tried v2.25 ans v2.28 using conda but the installation failed.; Any suggestions to move forward?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453685106:3973,install,install,3973,,https://github.com/google/deepvariant/issues/137#issuecomment-453685106,2,['install'],"['install', 'installation']"
Deployability,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10062,Install,Installing,10062,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,3,"['Install', 'install']","['Installing', 'installed']"
Deployability,"tal_build_setting_api --java_runtime_version=remotejdk_11""; ; function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; ```; ```; diff --git a/build-prereq.sh b/build-prereq.sh; index ad34e285..1fc2d203 100755; --- a/build-prereq.sh; +++ b/build-prereq.sh; @@ -41,7 +41,7 @@ source settings.sh; ; note_build_stage ""Install the runtime packages""; ; -./run-prereq.sh; +#./run-prereq.sh; ; note_build_stage ""Update package list""; ; @@ -71,12 +71,17 @@ function ensure_wanted_bazel_version {; then; echo ""Bazel ${wanted_bazel_version} already installed on the machine, not reinstalling""; else; - pushd ~/bazel; - curl -L -O https://github.com/bazelbuild/bazel/releases/download/""${wanted_bazel_version}""/bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - chmod +x bazel-*.sh; - ./bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh --user > /dev/null; - rm bazel-""${wanted_bazel_version}""-installer-linux-x86_64.sh; - popd; + wget https://github.com/bazelbuild/bazel/releases/download/7.3.1/bazel-7.3.1-linux-arm64; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazel; + cp bazel-7.3.1-linux-arm64 /usr/local/bin/bazelisk; + chmod +x /usr/local/bin/bazel; + chmod +x /usr/local/bin/bazelisk; fi; }; ```; ```; diff --git a/tools/build_clif.sh b/tools/build_clif.sh; index c7c3378b..a08ab475 100755; --- a/tools/build_clif.sh; +++ b/tools/build_clif.sh; @@ -39,7 +39,7 @@ echo ========== Run this script in root mode.; CLIF_UBUNTU_VERSION=""${CLIF_UBUNTU_VERSION-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:3042,release,releases,3042,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['release'],['releases']
Deployability,"tc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9115,Install,Install,9115,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,1,['Install'],['Install']
Deployability,"tch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; export BAZEL_PYTHON=/home/qilibj/inst/bin/python; export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11""; # export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS --cxxopt=-std=gnu++11 --copt=-fsigned-char --cxxopt=-fsigned-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:18658,install,install,18658,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"terfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash; # Prerequisites; cmake --version #3.5+; protoc --version # 3.2.0+ build from source code for both C++ and Python; pip install virtualenv; pip install pyparsing; yum install subversion; yum install ocaml; pip install 'pyparsing>=2.2.0'; pkg-config --libs python # workable. # download source code; cd $HOMEPATH; git clone https://github.com/google/clif.git; cd clif. # set environment; export INSTALL_DIR=""$HOMEPATH/inst""; export CLIFSRC_DIR=""$HOMEPATH/clif""; export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend""; export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which proto",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:11814,install,install,11814,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,['install'],['install']
Deployability,"than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNIN",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:1009,Update,Update,1009,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Update'],['Update']
Deployability,"thank you for your kind support @pichuan . It seems I was running a really old and obsolete version of anaconda(4.2.1) in my SGE. It took some gymnastics to install it for a single user and run across the nodes but I updated my conda to current version. after that installation completed with no errors. . the ""bioconda/label/cf201901::deepvariant"" was provided in bioconda page, which is supposed to be another installation solution. I think this issue can be closed. For anyone in future running in a problem similar to me, I'll make a repo with the installation code in my profile!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2074662859:157,install,install,157,,https://github.com/google/deepvariant/issues/806#issuecomment-2074662859,5,"['install', 'update']","['install', 'installation', 'updated']"
Deployability,"thanks Pi-Chuan, decided to start building from your image with Ubuntu 20.04 to make sure that works before using the Databricks Runtime with Ubuntu 20.04, and got. 18 0.288 ========== [Tue Aug 10 21:03:43 UTC 2021] Stage 'Install bazel' starting; 18 0.297 ./build-prereq.sh: line 50: bazel: command not found; 18 0.298 ~/bazel /opt/deepvariant; 18 0.298 ./build-prereq.sh: line 56: curl: command not found; ------; executor failed running [/bin/sh -c ./build-prereq.sh]: exit code: 127. Assume there's a simple fix to add bazel and curl in, but I have had no time to test further since then, plan to get back on this next month",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-902138384:223,Install,Install,223,,https://github.com/google/deepvariant/issues/476#issuecomment-902138384,1,['Install'],['Install']
Deployability,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:574,install,install,574,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051,3,"['configurat', 'install']","['configurations', 'install']"
Deployability,"thanks! . Ah after some digging I see the newest version of the Databricks Runtime is supposed to be using Ubuntu 20.04, but the existing docker containers have not been upgraded to reflect that. ; Let me see if I can get this resolved...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896302178:170,upgrade,upgraded,170,,https://github.com/google/deepvariant/issues/476#issuecomment-896302178,1,['upgrade'],['upgraded']
Deployability,"the path forwards a lot clearer. Reading between the lines, is (3) the reason we are advised to shuffle the validation data as well, because it combines all the chr20-22 examples into one dataset?. And, I forgot to reply this part - for validation set, running the shuffling step is mainly for combing the examples into one dataset (and create a text file that describe it). Another reason is - if you specify `max_examples` in model_eval, it just might be safer to have the examples already pre-shuffled:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/model_eval.py#L118. ---. One more thing to clarify:; Unfortunately I might be using terminology that are a bit confusing in the doc:. In https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md , ; our **training** set are the labeled examples that our classifier actually learns from. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#training_set. **Validation** set is the labeled examples that our classifier never directly learns from, but is used to pick a best model checkpoint. ; This is the same as defined in: https://developers.google.com/machine-learning/glossary/#validation_set; When I wrote our training case study, I tried to not use another term ""**tuning** set"", which is the same thing as ""validation set"". But now I read it again, I think I did accidentally use the term ""tuning set"" at least once. I'll update that in a future release.; Just to clarify again: When I use the term ""tuning set"", I'm referring to the same thing as validation set. There is also **test** set, defined here: https://developers.google.com/machine-learning/glossary/#test_set; When doing machine learning, the best practice is to leave the **test** set alone as much as possible, and NOT to make any decisions (on model picking, paramater tuning) on it at all. Which is why we had a validation set held out in the first place.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638566733:1529,update,update,1529,,https://github.com/google/deepvariant/issues/312#issuecomment-638566733,4,"['release', 'update']","['release', 'update']"
Deployability,"this is everything that is in the vcf, so the deviations are not picked up at all. It is just one line with the 5' deviation. If I include other genomic region, than I do see more variations there, but for this region it is not recognized at all.; ![image](https://github.com/user-attachments/assets/cb092f42-5285-48f5-b2c8-dc1f9590e9c2). [output.vcf.txt](https://github.com/user-attachments/files/17303859/output.vcf.txt). However both positions (3948 and 3949) show up in the output.g.vcf; [output.g.vcf.txt](https://github.com/user-attachments/files/17303928/output.g.vcf.txt). ![image](https://github.com/user-attachments/assets/408893bd-27a6-48ac-9f6c-4260a96741c9). *update: I just tried the same data with clair3 and it also does not call these two positions. . I then aligned against only the transgen in the genomic context (~50 kb each side) and then the variants are called correctly, so it seemed to have something to do with the reads also mapping to the other region. So I checked the reads again in more detail and it turns out that at the 5' SNP only 2 reads are primary and at the two 3' SNPs there is only one primary alignment, the others are supplementary. Therefore vsc_min_count_snps=2 filters the 3' SNPs out, while the 5' barely passes. I then reduced vsc_min_count_snps to 1 and now I find also the two 3' SNPs in the vcf output, with one passing and the other being Refcall. Of course plenty of other SNPs show up as well, but with low QUAL values and being RefCall ... So this is an issue of the way that I make the reference FASTA, but I don't see any other way of doing this, because otherwise I would have to duplicate the surrounding genomic DNA postentially ending up with many supplementary alignments as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/892#issuecomment-2401535093:673,update,update,673,,https://github.com/google/deepvariant/issues/892#issuecomment-2401535093,1,['update'],['update']
Deployability,"thon"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; #############################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:14882,install,install,14882,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"thub.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8083,install,install,8083,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"tible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Count",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:5253,Install,Install,5253,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Install'],['Install']
Deployability,"tible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is spe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6279,Install,Install,6279,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Install'],['Install']
Deployability,"tobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7399,install,install,7399,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"tory: [https://github.com/google/clif](https://github.com/google/clif). ```bash; # Prerequisites; cmake --version #3.5+; protoc --version # 3.2.0+ build from source code for both C++ and Python; pip install virtualenv; pip install pyparsing; yum install subversion; yum install ocaml; pip install 'pyparsing>=2.2.0'; pkg-config --libs python # workable. # download source code; cd $HOMEPATH; git clone https://github.com/google/clif.git; cd clif. # set environment; export INSTALL_DIR=""$HOMEPATH/inst""; export CLIFSRC_DIR=""$HOMEPATH/clif""; export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend""; export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note to remove -DLLVM_TARGETS_TO_BUILD=X86; # ""rm CMakeCache.txt"" to remove cmake cache; cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \; -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_util",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:13004,install,install,13004,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,8,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"tp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4594,install,install,4594,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"tput:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how to proceed, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:3344,install,installation,3344,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['install'],['installation']
Deployability,"ty, as well as parameters specific to singularity v3.1.1. ```; # install docker; sudo yum check-update; curl -fsSL https://get.docker.com/ | sh. # install nvidia-docker; distribution=$(. /etc/os-release;echo $ID$VERSION_ID); curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | \; sudo tee /etc/yum.repos.d/nvidia-docker.repo; sudo yum install -y nvidia-docker2; semanage fcontext -a -f f -t container_runtime_exec_t -s system_u /usr/bin/nvidia-docker; sudo restorecon -v /usr/bin/nvidia-docker. # start docker; sudo systemctl start docker; sudo systemctl status docker; sudo systemctl enable docker. # install deps; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y epel-release && \; sudo yum install -y golang openssl-devel libuuid-devel libseccomp-devel squashfs-tools; echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \; echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \; source ~/.bashrc. # install singularity; mkdir -p ${GOPATH}/src/github.com/sylabs && \; cd ${GOPATH}/src/github.com/sylabs && \; git clone https://github.com/sylabs/singularity.git && \; cd singularity; git checkout v3.1.1; cd ${GOPATH}/src/github.com/sylabs/singularity && \; ./mconfig && \; cd ./builddir && \; make && \; sudo make install; ; DVVER=0.8.0; # make deepvariant CPU image; sudo docker pull gcr.io/deepvariant-docker/deepvariant:${DVVER}; sudo docker tag gcr.io/deepvariant-docker/deepvariant:${DVVER} localhost:5000/deepvariant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; singularity build --nohttps deepvariant.${DVVER}.simg docker://localhost:5000/deepvariant:latest; ; # make deepvariant GPU image; sudo nvidia-docker pull gcr.io/deepvariant-docker/deepvariant_gpu:${DVVER}; sudo nvidia-docker tag gcr.io/deepvariant-docker/deepvariant_gpu:${DVVER} localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --rest",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482761898:1342,install,install,1342,,https://github.com/google/deepvariant/issues/132#issuecomment-482761898,1,['install'],['install']
Deployability,"u have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3415,install,installed,3415,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['install'],['installed']
Deployability,"uery Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broke",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2183,Update,Update,2183,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Update'],['Update']
Deployability,"un_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1376,Install,Install,1376,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Install'],['Install']
Deployability,"under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2529,install,install,2529,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,2,['install'],['install']
Deployability,"untime_version=remotejdk_11'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:4183,install,install,4183,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,2,"['Install', 'install']","['Installs', 'install']"
Deployability,"urces.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################################################; # //deepvariant/labeler:haplotype_labeler_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant/labeler:haplotype_labeler_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ##########################################################################; # fail due to mock data, open an issue in github; https://github.com/google/deepvariant/issues/154. ##########################################################################; # //deepvariant:make_examples_test; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:make_examples_test --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; # internvaltree v3 has some API changes with v2; ##########################################################################; pip install 'intervaltree==2.1.0'; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:22927,install,install,22927,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"ut a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # Fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:1257,install,install,1257,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,2,['install'],['install']
Deployability,"ut_pattern_list=""${OUTPUT_BUCKET}""/validation_set.with_label.tfrecord-?????-of-00064.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:6257,install,install,6257,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,2,['install'],['install']
Deployability,"v -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note to remove -DLLVM_TARGETS_TO_BUILD=X86; # ""rm CMakeCache.txt"" to remove cmake cache; cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \; -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; pytho",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:14054,install,install,14054,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"val ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. It completed without any error messages. I see:. ```; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/; bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0; call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh; deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip; ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! Please let me know. By the way, ; I noticed you were using `bioconda/label/cf201901::deepvariant`. I actually don't know what that does. (Can you explain where that came from and why you needed that?); But to understand be behavior, I also tried something like:; `conda create -y -n dv-env bioconda/label/cf201901::deepvariant`. If I did that, then it still seems to be work, but it somehow had an older version `deepvariant-0.7.2-1` in /home/pichuan/miniconda/envs/dv-env/share/, which I don't understand why. I'd recommend that you run using my first command, so you can have a more recent version to try out!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405:2232,install,install,2232,,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405,1,['install'],['install']
Deployability,"vanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script.; > ; > I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated.; > ; > Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:1171,pipeline,pipeline,1171,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,2,['pipeline'],['pipeline']
Deployability,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13206,install,installed,13206,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,['install'],['installed']
Deployability,"w session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16635,install,install,16635,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,12,"['install', 'upgrade']","['install', 'upgrade']"
Deployability,"which will label exon regions like this (including their start and end sites):. ```; chr1 HAVANA exon 12613 12721 . + . gene_id ""ENSG00000290825.1""; transcript_id ""ENST00000456328.2""; gene_type ""lncRNA""; gene_name ""DDX11L2""; transcript_type ""lncRNA""; transcript_name ""DDX11L2-202""; exon_number 2; exon_id ""ENSE00003582793.1""; level 2; transcript_support_level ""1""; tag ""basic""; tag ""Ensembl_canonical""; havana_transcript ""OTTHUMT00000362751.1"";; ```. With this you can determine where in the exon your variant falls in, and if it is near the end or beginning. I will focus on the high quality one variant, as the low quality one can be problematic. Skin tissue should be fine based on this figure: . ![image](https://github.com/google/deepvariant/assets/6555937/fc7823e6-de5f-46ea-80b9-59c5913d79de). The only other thing I can think of is that given that your number of reads is large, DeepVariant would downsample them before going into the model. So your supporting reads are picked by an allele counter, and it uses them to generate a matrix (image) that gets fed into the model generating the GT and GQ values. The height of these matrices is usually 100 rows. If it is greater it will randomly downsample from these reads, and usually use 95 of them as 5 are used for representing the reference sequence. I'm assuming you've updated the model as denoted in the tutorial and not used the regular WGS one. I know it's obvious, but as noted in the paper there is a difference between a RNA-seq model versus the WGS/WES one provided by DeepVariant. Other than that is there anything special around this site in IGV? Do you see this as a singular variant without anything surrounding it? Is there anything special of the sequences surrounding the variant (i.e. repeats/etc.)? Does it align uniquely or are there other alignments it can occur at? Do you see anything problematic with the reference-representing reads? I'm assuming your sample is germline diploid, and in the recombinant regions. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1695957189:1460,update,updated,1460,,https://github.com/google/deepvariant/issues/701#issuecomment-1695957189,1,['update'],['updated']
Deployability,"x-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1055,install,install,1055,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['Install', 'install']","['Installs', 'install']"
Deployability,"yes, I think this is a real bug that still exists.; Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard').; You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355996061:350,configurat,configuration,350,,https://github.com/google/deepvariant/issues/27#issuecomment-355996061,1,['configurat'],['configuration']
Deployability,"yparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repos",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16520,install,install,16520,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"ython packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:17181,install,install,17181,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['install'],['install']
Deployability,"zation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8294,Configurat,Configuration,8294,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Configurat'],['Configuration']
Deployability,"{TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6516,Install,Install,6516,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Install'],['Install']
Deployability,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7590,install,installed,7590,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['install'],['installed']
Deployability,"}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took `5m25.905s`. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. # This parts starts shuffling... ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. This worked for me. It took `15m32.746s`. I didn't get the numpy issue. So I decided to run `pip3 freeze` to get my current versions so you can compare:. ```bash; $ pip3 freeze; absl-py==2.1.0; apache",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:4589,update,update,4589,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,7,"['install', 'update', 'upgrade']","['install', 'update', 'upgrade']"
Deployability,"~; | |; | PyObject * {aka struct _object *}; /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:139:62: note: in definition of macro ‘PyArray_GETPTR1’; 139 | (i)*PyArray_STRIDES(obj)[0])); | ^~~; In file included from /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:12,; from /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/arrayobject.h:5,; from pandas/_libs/src/ujson/python/JSONtoObj.c:42:; /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarraytypes.h:1526:38: note: expected ‘const PyArrayObject *’ {aka ‘const struct tagPyArrayObject_fields *’} but argument is of type ‘PyObject *’ {aka ‘struct _object *’}; 1526 | PyArray_STRIDES(const PyArrayObject *arr); | ~~~~~~~~~~~~~~~~~~~~~^~~; pandas/_libs/src/ujson/python/JSONtoObj.c:319:31: warning: passing argument 1 of ‘PyArray_SETITEM’ from incompatible pointer type [-Wincompatible-pointer-types]; 319 | PyArray_SETITEM(npyarr->ret, item, value) == -1) {; | ~~~~~~^~~~~; | |; | PyObject * {aka struct _object *}; In file included from /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/arrayobject.h:5,; from pandas/_libs/src/ujson/python/JSONtoObj.c:42:; /tmp/pip-build-env-ga9pnwrz/overlay/lib/python3.10/site-packages/numpy/_core/include/numpy/ndarrayobject.h:292:32: note: expected ‘PyArrayObject *’ {aka ‘struct tagPyArrayObject_fields *’} but argument is of type ‘PyObject *’ {aka ‘struct _object *’}; 292 | PyArray_SETITEM(PyArrayObject *arr, char *itemptr, PyObject *v); | ~~~~~~~~~~~~~~~^~~; error: command '/usr/bin/gcc' failed with exit code 1; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; ERROR: Failed building wheel for pandas; ERROR: Could not build wheels for pandas, which is required to install pyproject.toml-based projects",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859#issuecomment-2264852415:6835,install,install,6835,,https://github.com/google/deepvariant/issues/859#issuecomment-2264852415,1,['install'],['install']
Energy Efficiency," ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:20988,monitor,monitor,20988,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['monitor'],['monitor']
Energy Efficiency,"#########################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from sklearn.externals import joblib"". ##########################################",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21088,monitor,monitor,21088,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['monitor'],['monitor']
Energy Efficiency,"(assigning to myself so I can monitor activity of this issue, and close after a week of inactivity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/299#issuecomment-617891319:30,monitor,monitor,30,,https://github.com/google/deepvariant/issues/299#issuecomment-617891319,1,['monitor'],['monitor']
Energy Efficiency,", I changed these lines:; https://github.com/google/deepvariant/blob/r1.2/deepvariant/data_providers.py#L189-L194; to:; ```; parsed = tf.io.parse_example(serialized=tf_example,; features=self.feature_extraction_spec); image = parsed['image/encoded']; if self.tensor_shape:; image = tf.io.decode_raw(image, tf.uint8); image = tf.reshape(image, [-1]+self.tensor_shape); ```. to make it work. This doesn't fully work when `use_tpu` is True. But given that we can [measure runtime](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md) on CPU machines, I was able to test on CPU machines with that change. After the change above, I re-ran the 4 test in [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md). I observed that:; (1) Accuracy is the same for 4 models - this is expected.; (2) Looking at the `call_variants` runtime, I do not see an improvement. Here are the runtime from all 4 runs:. ## WGS (Illumina); ```; real 114m10.723s; real 193m19.324s; real 74m39.826s; ```; ## WES (Illumina); ```; real 7m26.571s; real 1m24.083s; real 1m3.679s; ```. ## PacBio (HiFi); ```; real 126m28.198s; real 175m40.960s; real 67m49.753s; ```; ## Hybrid (Illumina + PacBio HiFi); ```; real 161m32.681s; real 200m26.225s; real 63m20.731s; ```. Comparing these to [r1.2 metrics.md](https://github.com/google/deepvariant/blob/r1.2/docs/metrics.md), surprisingly it does not look to me that the `call_variants` step has significant improvement. . A few possibilities:; (a) All these runtime have some variance. For example, when I made the metrics.md, I ran a few times and average the runtime (and try to round them up). So it's possible it's mostly variance. But, if this is an improvement, I'd still expect a bit better numbers.; (b) It is possible that the new code is more efficient, but might need some tuning (e.g., batch size? other params?) @DLPerf If you have any thoughts on this, let me know. Right now, with this empirical result, I won't plan to update our code.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479#issuecomment-905684335:1935,efficient,efficient,1935,,https://github.com/google/deepvariant/issues/479#issuecomment-905684335,1,['efficient'],['efficient']
Energy Efficiency,/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462] Processed 150001 examples in 293 batches [0.083 sec per 100]; I0608 12:14:43.829042 139794368661312 call_variants.py:462] Processed 200001 examples in 391 batches [0.081 sec per 100]; I0608 12:15:22.101066 139794368661312 call_variants.py:462] Processed 250001 ex,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:5023,monitor,monitor,5023,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code); 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs); 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 10001 0.142 0.000 0.428 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:6491,reduce,reduced,6491,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['reduce'],['reduced']
Energy Efficiency,07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz --num_shards 40 --regions chr20. Command exit status:; 1. Command output:; I0608 12:13:28.741300 139794368661312 call_variants.py:462] Processed 100001 examples in 196 batches [0.087 sec per 100]; I0608 12:14:06.236101 139794368661312 call_variants.py:462,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4761,monitor,monitor,4761,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"1. No - this should not be necessary. This operation is handled by DeepVariant.; 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`; - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads.; - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel.; 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/635#issuecomment-1520135737:1061,reduce,reduced,1061,,https://github.com/google/deepvariant/issues/635#issuecomment-1520135737,1,['reduce'],['reduced']
Energy Efficiency,"1. my files are larger than the examples . 60G Dec 6 2019 19CT021737-19CT021737-20190619_64683_S7_346963.bam; 60G Dec 6 2019 19CT021740-19CT021740-20190619_64686_S8_346962.bam; 61G Dec 6 2019 DS187706-DS187706_39743_S1_261735.bam. 2. I am going to try it with --regions 20. **3. I am only interested in the PASS variants, so is it possible to filter output to only those variants? this will reduce the size of the output considerably.**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400#issuecomment-751781255:391,reduce,reduce,391,,https://github.com/google/deepvariant/issues/400#issuecomment-751781255,1,['reduce'],['reduce']
Energy Efficiency,"19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 mem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:1801,monitor,monitor,1801,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['monitor'],['monitor']
Energy Efficiency,"3085 140121429608256 make_examples_core.py:257] 62017 candidates (71070 examples) [121.51s elapsed]; I0514 01:48:23.145400 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7088.; I0514 01:52:54.732045 140121429608256 make_examples_core.py:257] 67303 candidates (76952 examples) [285.47s elapsed]; ...; I0514 02:28:55.286200 140121429608256 make_examples_core.py:257] 166834 candidates (181516 examples) [142.03s elapsed]; I0514 02:29:01.686342 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 5953.; I0514 02:31:24.326849 140121429608256 make_examples_core.py:257] 172413 candidates (187997 examples) [149.04s elapsed]; I0514 02:31:26.924692 140121429608256 make_examples_core.py:257] Skip phasing: len(candidates[main_sample]) is 7496.; I0514 02:35:53.029588 140121429608256 make_examples_core.py:257] 178012 candidates (194646 examples) [268.70s elapsed]; ....; ```. And there's also the relevant command information from the log below (formatted for human consumption). ```; time seq 0 0 | parallel -q --halt 2 \; --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/shuang_data/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" \; --reads ""/shuang_data/HG003.GRCh38.chr20.pFDA_truthv2.bam"" \; --examples ""/tmp/tmpouwynlb0/make_examples.tfrecord@1.gz"" \; --add_hp_channel \; --alt_aligned_pileup ""diff_channels"" \; --max_reads_per_partition ""600"" \; --min_mapping_quality ""1"" \; --parse_sam_aux_fields \; --partition_size ""25000"" \; --phase_reads \; --pileup_image_width ""199"" \; --norealign_reads \; --regions ""chr20"" \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels ""0.12"" \; --task {}; ...; ```. Here's a snapshot of the output. ```; chr20 64506 . C CTCCAT 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:35:44:20,20:0.454545:44,0,35; chr20 68303 . T C 45.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:42:21,21:0.5:45,0,59; chr20 72765 . T TA 21.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:22:32:19,12:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230:1565,consumption,consumption,1565,,https://github.com/google/deepvariant/issues/649#issuecomment-1546990230,1,['consumption'],['consumption']
Energy Efficiency,"8; BogoMIPS: 4000.35; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 1024K; L3 cache: 39424K; NUMA node0 CPU(s): 0-63; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; ```. ## `/proc/cpuinfo` has info for 64 of them. I'll just list the first one. ```; $ cat /proc/cpuinfo ; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 85; model name : Intel(R) Xeon(R) CPU @ 2.00GHz; stepping : 3; microcode : 0x1; cpu MHz : 2000.178; cache size : 39424 KB; physical id : 0; siblings : 64; core id : 0; cpu cores : 32; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 13; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat md_clear arch_capabilities; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa; bogomips : 4000.35; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276044:6984,power,power,6984,,https://github.com/google/deepvariant/pull/363#issuecomment-735276044,1,['power'],['power']
Energy Efficiency,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2513,power,power,2513,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['power'],['power']
Energy Efficiency,"; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. # docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/input/ucsc.hg19.chr20.unittest.fasta \; > ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1563,power,power,1563,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['power'],['power']
Energy Efficiency,=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.proces,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:3780,monitor,monitor,3780,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"=========; str | 223354 | 35.79 MB; dict | 88941 | 25.94 MB; code | 50107 | 8.54 MB; type | 6121 | 5.65 MB; tuple | 63884 | 3.62 MB; list | 30942 | 3.19 MB; set | 2864 | 1.51 MB; weakref | 14251 | 1002.02 KB; abc.ABCMeta | 784 | 826.05 KB; cell | 20911 | 816.84 KB; int | 25259 | 697.77 KB; builtin_function_or_method | 8801 | 618.82 KB; google.protobuf.pyext.cpp_message.GeneratedProtocolMessageType | 679 | 594.12 KB; frozenset | 1862 | 541.02 KB; function (__init__) | 3439 | 456.74 KB; ```. Based on the above, that made me curious as pass-by-reference could show benefits, but wanted to see what the functions with lots of calls might be. Those types of functions are quite heavy regarding stack setup and teardown -- with intermediate local memory allocation -- which could also be the culprit based on Steve's temporal data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}; 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}; 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller); 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode); 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__); 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}; 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__); 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}; 113550 0.334 0.000 0.646 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:1704,reduce,reduced,1704,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['reduce'],['reduced']
Energy Efficiency,"> @leorippel got it, thanks for the clarification! I'm not sure what the issue is in this case, but if you are able to use a machine with more memory, that would be the easiest option. Another option is to shard your data by chromosome and run DeepVariant separately for each chromosome (or groups of chromosomes). I took the intermediate results and ran into a bigger machine. It worked. The memory consumption rised to 435G !! by far the hungry step. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358#issuecomment-708002351:400,consumption,consumption,400,,https://github.com/google/deepvariant/issues/358#issuecomment-708002351,1,['consumption'],['consumption']
Energy Efficiency,> @williamrowell Can you check whether your CPU supports AVX instruction?. This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash; wrowell@mp0608-sge:~$ lscpu | grep Flags; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774782391:498,monitor,monitor,498,,https://github.com/google/deepvariant/issues/419#issuecomment-774782391,1,['monitor'],['monitor']
Energy Efficiency,"> Hi @ZuyaoLiu ,; > ; > Just to clarify, you meant that you're running multiple call_variants on the same machine at the same time, so they're all opening the same tmp file?; > ; > If that's the case, then I can see that being an issue. I'll file an internal issue to track, and will name the h5 separately. Currently our code is: https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L98 and I can see this being an issue when multiple call_variants are run. We'll make sure to create a more unique filepath in the future to avoid issue!; > ; > On the other hand, historically we don't recommend running multiple call_variants runs on the same machine. Because TensorFlow will parallelize and use multiple CPUs already.; > ; > @ZuyaoLiu @crazysummerW Just for my sanity check, can you confirm that if you run just one call_variants on the machine, then it worked? (I want to make sure there are no other issues). @pichuan ,. Yes, you get me correctly. So I run call_variants on a cluster where each node is in charge of a single job. As I set my private tmp dir, these jobs will target the same h5 file, and it will cause the issue. . Currently, I am running the program by setting a unique tmp path to different jobs so that they won't use the same h5 file simultaneously. And they all worked well and finished with no errors.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1823460228:1035,charge,charge,1035,,https://github.com/google/deepvariant/issues/725#issuecomment-1823460228,1,['charge'],['charge']
Energy Efficiency,"> Hi @pichuan Thank you for your response.; > ; > I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you.; > ; > 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least?. In our https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md documentation, all experiments were done with n1-standard-64 GCP machines, which has 240 GB memory, and 64 vCPUs. I have not specifically tried machines with smaller memory. Can you tell me what kind of memory constraints you're considering?. > 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data?. Please see:; https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#whole-exome-sequencing-illumina. > 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results?. If you're using different reference genome, the numbers are not directly comparable because the truth sets are also different. > 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?. I suppose you're comparing numbers like https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md#benchmark-output and https://github.com/google/deepvariant/blob/r1.6/docs/metrics-deeptrio.md#hg003-1.;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186:345,allocate,allocate,345,,https://github.com/google/deepvariant/issues/720#issuecomment-1781737186,1,['allocate'],['allocate']
Energy Efficiency,"> Hi @tinyfallen; > ; > @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144).; > ; > So the main resource use can be estimated from the single sample runtime multiplied by sample number.; > ; > If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls. Many thanks for your reply! I would have a try. best ~",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651#issuecomment-1576435077:53,efficient,efficient,53,,https://github.com/google/deepvariant/issues/651#issuecomment-1576435077,1,['efficient'],['efficient']
Energy Efficiency,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417185546:858,allocate,allocate,858,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546,5,"['allocate', 'efficient', 'schedul']","['allocate', 'efficient', 'scheduler', 'scheduling', 'scheduling-within-an-application']"
Energy Efficiency,"@AldoCP @melkerdawy although this was closed some time ago, I have an update related to this question. We have successfully trained and released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). . DeepVariant now has a flag (`--split_skip_reads`) that can be used in conjunction with training or inference with RNA-seq data. The flag is required to efficiently process RNA-seq data.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222:430,efficient,efficiently,430,,https://github.com/google/deepvariant/issues/115#issuecomment-1281192222,1,['efficient'],['efficiently']
Energy Efficiency,"@AndrewCarroll Thanks for sharing your thoughts. I understand regarding the complexities involved, and that this may not be a low-hanging fruit. Also, bigger gains may be had from better data I imagine. While GIAB and Platinum Genomes are great, it would be great if the Syndip dataset would be pulled into a similar effort such as GIAB. Regarding dynamic subsampling, there may also be a need to correct for some effects such as re-thresholding with the 0.12 allele fraction cut-off after subsampling etc. The effect of realignment at a lower coverage (coverage after subsampling) may come into picture as well though may be not that much. Dynamic subsampling could result in a different input data statistic overall. May be your original assessment that static subsampling reduces complexity is a good enough argument to just continue with it. Thanks for the discussion!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230#issuecomment-553717173:775,reduce,reduces,775,,https://github.com/google/deepvariant/issues/230#issuecomment-553717173,1,['reduce'],['reduces']
Energy Efficiency,@George-du The most efficient pre-built binaries would be the Docker/Singularity approach.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1602332965:20,efficient,efficient,20,,https://github.com/google/deepvariant/issues/590#issuecomment-1602332965,1,['efficient'],['efficient']
Energy Efficiency,"@PlatonB thanks for the suggestion, I will file this request internally to determine if this is an optional feature we may want to include in a future release. Currently, DeepVariant can produce both a VCF and GVCF (along with other output files), so we wanted to give users the choice of which file to use downstream. stdout has been reserved for just logging outputs so users can monitor the status of the run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/253#issuecomment-567578821:382,monitor,monitor,382,,https://github.com/google/deepvariant/issues/253#issuecomment-567578821,1,['monitor'],['monitor']
Energy Efficiency,"@Rofidagamal I'm a bit stumped. The error suggests there is an issue with the bed file provided. ```; ValueError: OUT_OF_RANGE: EOF; ```. Maybe we can look a little closer at that and see if there is any sign of an issue there. Can you run:. ```; cut -f 1 data/chr20_CDS_3x.bed | wc; cut -f 1 data/chr20_CDS_3x.bed | uniq -c; ```. I wonder if it could also be related to the amount of space being allocated / avail. Can you also provide the output from the following:. ```; sudo docker run; -v ""$(pwd):$(pwd)""; -w $(pwd); google/deepvariant:""${BIN_VERSION}"" /bin/bash. # Once the image is running, run:; df -h; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581#issuecomment-1307848624:397,allocate,allocated,397,,https://github.com/google/deepvariant/issues/581#issuecomment-1307848624,1,['allocate'],['allocated']
Energy Efficiency,"@danielecook Thanks for the swift response. I ran this on an HPC cluster using the PBS scheduler. I allocated 200 GB for this job, and according to ""qstat"", this job only uses 20GB at maximum. I'm preferred to believe that this issue is not due to the lack of sufficient memory for DeepVariant or DeepTrio. . Here is a screenshot of the finished job status in the qstat table:; ![image](https://github.com/google/deepvariant/assets/40780228/9b7fe09d-fb20-4f8a-9ff9-972e539aa6a1)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1547109235:87,schedul,scheduler,87,,https://github.com/google/deepvariant/issues/646#issuecomment-1547109235,2,"['allocate', 'schedul']","['allocated', 'scheduler']"
Energy Efficiency,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563:794,adapt,adapt,794,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563,2,['adapt'],['adapt']
Energy Efficiency,"@husamia . 1. All the intermediate stages (make_examples, call_variants) output some files. For example, when we run our [DeepTrio WGS case study](https://github.com/google/deepvariant/blob/r1.1/docs/deeptrio-wgs-case-study.md), the `intermediate_results_dir` directory contains about 260G of data:; ```; TOTAL: 387 objects, 277723436467 bytes (258.65 GiB); ```. 2. Let me know if just chromosome 20 works or not. With that, the intermediate files should be smaller. 3. You can use the `--only_keep_pass` flag in postprocess_variants. But this won't significantly reduce the amount of output, because this filtering is only done right before writing to VCF files. ; https://github.com/google/deepvariant/blob/2dbebb4d97e15d0d5fcf303a4466314b1f313208/deepvariant/postprocess_variants.py#L124; If you're using run_deepvariant, you can add `--postprocess_variants_extra_args=""only_keep_pass=true""`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400#issuecomment-751790870:564,reduce,reduce,564,,https://github.com/google/deepvariant/issues/400#issuecomment-751790870,1,['reduce'],['reduce']
Energy Efficiency,"@husamia although this was closed some time ago, we have just released an Illumina RNA-seq model and [case study](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md). If you are still interested in calling variants from RNA-seq data using DeepVariant, this should work for you. We have also updated the DeepVariant code base to be more memory efficient with RNA-seq data. This involves passing a new flag (`--split_skip_reads`), that allows for reads containing large SKIP to be processed efficiently.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289:378,efficient,efficient,378,,https://github.com/google/deepvariant/issues/412#issuecomment-1281200289,2,['efficient'],"['efficient', 'efficiently']"
Energy Efficiency,"@pgrosu Thank you for the prompt response. Correct me if I'm wrong. ; taskset need to specify the specific cpu cores the process wants to occupy. Since my Spark cluster is multi-tenant, some processes may be running in the cluster and occupy some CPU cores. In addition, the dynamic resource allocation is enabled in my Spark cluster, so I can't assume all of my tasks can be equally assigned to each computing node. If my data are stored in 200 partitions, it mean that my program will launch 200 tasks by using pipe() to call taskset. I can't make sure which partition will be assigned to which computing node. Round-Robin assignment is a way, but it's violated the policy of the resource management (like Spark standalone or YARN). For example, I have 8 computing node with 4 cores per each. My Spark process might allocate 24 cores. It might be 8 computing nodes with 3 cores per each or 6 computing nodes with 4 cores per each. Furthermore, there is no information to let me know which task is done or which cpu core is available in my Spark program. the resource allocation might be unbalance and performance might be impacted severely, especially when several iterations of task assignment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-416891480:818,allocate,allocate,818,,https://github.com/google/deepvariant/issues/90#issuecomment-416891480,1,['allocate'],['allocate']
Energy Efficiency,"@pichuan I just stumbled upon the same thing, and it took me quite a while to figure out what's going on there. I am running deepvariant v0.9.0 (docker container), and I found that there is quite a lot of files left behind for failed jobs under /tmp on the execution host. Can you comment on deepvariant's behavior when two (or more) DV jobs are scheduled to the same execution host in a cluster setup? Should that be avoided?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/175#issuecomment-560473436:346,schedul,scheduled,346,,https://github.com/google/deepvariant/issues/175#issuecomment-560473436,1,['schedul'],['scheduled']
Energy Efficiency,"@pichuan Thank you very much. That was very detailed. This post can help a lot of people.; This project is a bit complicated to debug in this way. DeepVariant is too coupled to Nucleus and hope future versions to be more generic. ; I have another idea, and here's how it works：; make-example-->file-->callvariant-->file-->postprocess; Can we consider this approach，Reduce the I/O read and write and data serialization：; make_examples-->Internal memory--> call_variants-->Internal memory--> postprocess_variants",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/756#issuecomment-1865425784:365,Reduce,Reduce,365,,https://github.com/google/deepvariant/issues/756#issuecomment-1865425784,1,['Reduce'],['Reduce']
Energy Efficiency,"@pichuan Thanks for confirming!. I tried GPU-based training, but since the codebase currently doesn't support multi-GPU runs, it may not be efficient for me to use GPU-based training. Hence I am looking into using TPUs for the same. I will gladly provide feedback once I am able to do it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/376#issuecomment-720216342:140,efficient,efficient,140,,https://github.com/google/deepvariant/issues/376#issuecomment-720216342,2,['efficient'],['efficient']
Energy Efficiency,"@pichuan, I got it, good point! `.pb` file is intermediate and is removed after OpenVINO conversion:. ```; rm model.pb;; ```; However there is a way to generate `.xml` + `.bin` in runtime but not to keep in in the image. Also I can reduce a size of OpenVINO installation removing some components.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735606528:232,reduce,reduce,232,,https://github.com/google/deepvariant/pull/363#issuecomment-735606528,1,['reduce'],['reduce']
Energy Efficiency,"@sophienguyen01 - from the log file it looks like everything worked. Here are all the tune/categorical accuracies from your training data. ```; tune/categorical_accuracy=0.9944317936897278; tune/categorical_accuracy=0.9909400343894958; tune/categorical_accuracy=0.9915463924407959; tune/categorical_accuracy=0.9925118088722229; tune/categorical_accuracy=0.9921825528144836; tune/categorical_accuracy=0.9924613237380981; tune/categorical_accuracy=0.9926846623420715; tune/categorical_accuracy=0.9929667711257935; tune/categorical_accuracy=0.9925829172134399; tune/categorical_accuracy=0.9926416277885437; tune/categorical_accuracy=0.9923893213272095; tune/categorical_accuracy=0.9925225377082825; ```. The first number represents accuracy direct from the pretrained model. Since none of the subsequent tuning evaluations outperformed the original, no checkpoints were created. One thing you could try: reduce the learning rate, and see if that helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603:901,reduce,reduce,901,,https://github.com/google/deepvariant/issues/802#issuecomment-2042819603,2,['reduce'],['reduce']
Energy Efficiency,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033440565:456,reduce,reduce,456,,https://github.com/google/deepvariant/issues/802#issuecomment-2033440565,1,['reduce'],['reduce']
Energy Efficiency,Adding an acknowledgement that this discussion eventually resulted in improvements incorporated to DeepVariant described in the TensorFlow blog (https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344). Mark's proposal for the method to incorporate AVX improvements is a quite accurate description of what was implemented.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-488573130:179,power,power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-,179,,https://github.com/google/deepvariant/issues/21#issuecomment-488573130,1,['power'],['power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-']
Energy Efficiency,"Ahh, I see. Thank you @pichuan, now it makes total sense! I will rerun, adapting the command from https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md#run-make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428677704:72,adapt,adapting,72,,https://github.com/google/deepvariant/issues/99#issuecomment-428677704,1,['adapt'],['adapting']
Energy Efficiency,"And an follow up on gVCF file size, (thanks to @tedyun for reminding me of the flag):. There is a flag in `gvcf_gq_binsize` in make_examples that you can set to reduce the size of gVCF file, the tradeoff being the gVCF file size & runtime vs. more accurate representation of GQ in each hom. ref. site.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-700196941:161,reduce,reduce,161,,https://github.com/google/deepvariant/issues/346#issuecomment-700196941,1,['reduce'],['reduce']
Energy Efficiency,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:554,efficient,efficient,554,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484,2,['efficient'],"['efficient', 'efficiently']"
Energy Efficiency,"E_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I0327 13:32:07.160181 47175299967680 make_examples.py:386] ReadRequirements are: min_m apping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I0327 13:32:07.206463 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.218669 47175299967680 make_examples.py:535] Preparing inputs; I0327 13:32:07.293092 47175299967680 genomics_reader.py:223] Reading /scratch/moldach/ bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0327 13:32:07.294783 47175299967680 make_examples.py:535] Common contigs are ['chr20' ]; I0327 13:32:07.296233 47175299967680 make_examples.py:535] Writing ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:2101,Power,Power,2101,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['Power'],['Power']
Energy Efficiency,"Given these confusions, I feel like some explanations of what ""direct phasing"" means, other than in this ticket, would reduce users' questions down the road.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1550118274:119,reduce,reduce,119,,https://github.com/google/deepvariant/issues/649#issuecomment-1550118274,1,['reduce'],['reduce']
Energy Efficiency,"Hello @mosh305 . I would like to learn more about what you would like to do with this sample, and, if possible, propose some alternatives that are more likely to succeed. I don't believe that the NA12878 Mt.Sinai set here can be reliably processed. This is a non-CCS PacBio dataset, so there will be far too many candidate examples generated to process efficiently. Also, the DeepVariant models are not trained for non-CCS PacBio reads. May I recommend that instead you consider the CCS dataset for HG002 that was submitted to genome in a bottle:. ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/PacBio_CCS_15kb/. If this does sound interesting to you, we can provide to you a model trained for the CCS data type. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/138#issuecomment-458681829:353,efficient,efficiently,353,,https://github.com/google/deepvariant/issues/138#issuecomment-458681829,2,['efficient'],['efficiently']
Energy Efficiency,"Hello Kishwar (@kishwarshafin),. I followed your advice and have emailed you my review report. It took a little bit longer than I thought, as I also rewrote the algorithm section besides suggested corrections. If you think it would be more efficient, feel free to email me the LaTeX documents and figures. Let me know over email what you think of the report. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/709#issuecomment-1732470613:240,efficient,efficient,240,,https://github.com/google/deepvariant/issues/709#issuecomment-1732470613,1,['efficient'],['efficient']
Energy Efficiency,"Hello jessieshen97,. In order to train a good model you need to run make_examples on multiple sets on different coverages. Internally we use Google scheduler with thousands of hosts. You may try to do it on one machine but it may be challenging. ; To answer you question, yes you can train a DeepTrio model of the diploid organism. Although, we don't have a document with detailed steps on training a DeepTrio model you can try following the steps for DeepVariant toy model training. The only difference is that you need to use DeepTrio make_examples. All other sets would be identical.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/532#issuecomment-1086245019:148,schedul,scheduler,148,,https://github.com/google/deepvariant/issues/532#issuecomment-1086245019,1,['schedul'],['scheduler']
Energy Efficiency,"Hello, very great answer. I have other data including from an artificial evolution experiment. But I can't tell you more here without green light from my PIs. But definitely this thread is one of the most useful ever. Again, thank you so much all. I really really appreciate it. @AndrewCarroll did you get the file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650862825:134,green,green,134,,https://github.com/google/deepvariant/issues/682#issuecomment-1650862825,1,['green'],['green']
Energy Efficiency,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB; #SBATCH --qos=maxjobs100. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs; HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed; OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717#issuecomment-1770571154:212,adapt,adapt,212,,https://github.com/google/deepvariant/issues/717#issuecomment-1770571154,2,"['adapt', 'efficient']","['adapt', 'efficient']"
Energy Efficiency,"Hi @ASLeonard here is a table from the [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1). We stratify the F1-score across different region types. The published RNA-seq model is `DV RNA-seq [GTEx]`:. <img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200906677-4b6e2f11-8b29-44e3-871e-299f46d1cd64.png"">. The model has no problem running genome wide, but accuracy will vary by region type due to the nature of RNA-seq data. We observe the highest accuracy in CDS regions which is why the case study is limited to these regions. Users should filter variants depending on their use case. This might mean filtering by region, but you can also consider filtering by genotype quality (or both). We show how you can reduce the false-discovery rate in figure 5 of the preprint by filtering on genotype quality:. <img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200907763-1d21cc44-daff-47d2-87c6-e7917ea62a32.png"">. > Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?. The model is trained on exonic regions. We found this to give the best performance in our evaluations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398:764,reduce,reduce,764,,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398,1,['reduce'],['reduce']
Energy Efficiency,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:298,efficient,efficient,298,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,4,['efficient'],['efficient']
Energy Efficiency,"Hi @BowenKwan . I dug into this a bit and I think it is working as expected. Here are some take aways:. There are 57238 total examples in the exome. * On a batch_size 32 on 64 cores runs this runs at 0.67 sec per 100 [from pichuan] with total runtime 6m 21s. Expected runtime is 57238 / 100 * 0.67 seconds = 6m so this is all matching. * With a batch_size 32 on a GCE 8 cores instance DV 0.6 runs at 2.53 sec per 100, so expected runtime is 57238 / 100 * 2.53 seconds = 24 min. * The expected run rate (assuming perfect scaling from 64 => 8) is 0.67 * 8 = 5.36 sec per 100. This is 2x larger than the observed rate [in the positive direction] because 64 cores isn't as well utilized as 8 cores. So we are in fact running more efficiently on the 8 cores, again as expected. * I ran the 8 core call_variants end2end on the GCE instance and in fact it takes only 24 m. * You are reporting a run rate of 2.68 sec per 100 in one comment, which is very close to my observed GCE rate. * My guess: did you leave out the --regions ${CAPTURE_BED} argument? This changes how many examples are generated and could lead to a 8 hr runtime of call_variants even at 2.53 secs per 100. Leaving out that flag will result in candidates being generated genome-wide, so you'll have many more candidates and a concomitantly longer runtime.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391839227:726,efficient,efficiently,726,,https://github.com/google/deepvariant/issues/74#issuecomment-391839227,1,['efficient'],['efficiently']
Energy Efficiency,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-441822330:216,power,powerpc,216,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330,1,['power'],['powerpc']
Energy Efficiency,"Hi @JakeHagen . I will take a look at running a similar analysis on our exome samples. I suppose one remaining possibility is that the truncation of the reads reduces how far beyond the capture region the sequencing is getting. The edges of the capture region tend to both have less coverage and it's harder to sample both alleles. That's just a guess, I don't have a clear answer and will still try to collect more data. When you run DeepVariant for the exome, do you restrict to the capture regions only and do you add any padding to those?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338:159,reduce,reduces,159,,https://github.com/google/deepvariant/issues/586#issuecomment-1341790338,2,['reduce'],['reduces']
Energy Efficiency,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:535,reduce,reduced,535,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466,2,"['efficient', 'reduce']","['efficient', 'reduced']"
Energy Efficiency,"Hi @PengJia6 . Please see ; ![This IGV screenshot](https://github.com/google/deepvariant/assets/583711/1a448812-6b85-4614-bbda-6504595a50af). which shows the reads after rolling back to FASTQ and mapping with different parameters (top row) versus the original alignments (bottom row) for one of the samples. You can see the top row has a consistent representation in the reads. I mapped these with pbmm2 using `--preset HiFi`. However, you should be able to replicate this effect in minimap using the flag `-ax map-hifi`. If remapping is an option, this will be the cleanest way to improve these cases. If you are considering remapping, I want to mention that we recommend mapping to a version of GRCh38 without ALT contigs (for example [GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz](https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta.gz). We observe this to have higher accuracy across technologies, but it makes an especially large difference for long read technologies. I also note that you are using DeepVariant v1.1, but your BAM files don't have HaplotypePhase labels from running WhatsHap. Before version v1.4, we recommend a 2 step process of call-phase-call with WhatsHap in the middle for phasing. At version v1.4 and later, we started to do the phasing within DeepVariant, eliminating the need for this. If I could convince you to update to DeepVariant v1.5, you should see substantially better Indel accuracy as a result. Finally, I don't know of any ways of efficiently doing a realignment on PacBio reads after mapping, but if you find one, let me know. We are working on some interesting ideas that will be able to handle this case even without remapping, and potentially other cases which are even more complex, but that is still under development.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602:1561,efficient,efficiently,1561,,https://github.com/google/deepvariant/issues/660#issuecomment-1590741602,1,['efficient'],['efficiently']
Energy Efficiency,"Hi @SHuang-Broad,. Thank you both for the detailed statistics background information. For PEPPER - we're now not using the code from that approach and instead are using the make_examples logic in DeepVariant. However, you conclusion will still be correct. Within make_examples and even call_variants we're still not using the GPU to its fullest capability. Improving GPU utilization is an area we could make progress on. However, the skill profile of the team will allow us to do the work, but not as quickly as for groups that have more experience optimizing GPU performance. As a result, we don't feel that our team's leverage is as high for that work as opposed to other projects. We may get to it eventually, but it will probably remain lower priority. @pgrosu Thank you for the I/O profile. We have also flagged I/O as a bottleneck and area for improvement in DeepVariant in general. At this time, for speed improvements, we are looking into optimizations which reduce the amount of data being passed back and forth for various function calls. This seems to us to be the most engineer-time efficient way to speed up DeepVariant currently. I am hopeful that this will result in runtime improvements in the next 1 or 2 releases. This should benefit both CPU and GPU ways of running. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050:967,reduce,reduce,967,,https://github.com/google/deepvariant/issues/650#issuecomment-1551809050,2,"['efficient', 'reduce']","['efficient', 'reduce']"
Energy Efficiency,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308#issuecomment-628304654:112,adapt,adapt,112,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654,2,['adapt'],['adapt']
Energy Efficiency,"Hi @aardes . Yes, this is exactly what GLnexus was designed for, to allow for efficient merging of the N+1 sample in a manner analogous to genomicsDB (the [GLnexus paper](https://www.biorxiv.org/content/10.1101/343970v1) is informative for understanding this. The generation of a gVCF from DeepVariant is independent of its subsequent analysis by GLnexus, so you can incrementally add more sample, and later run GLnexus to merge these gVCFs. Hopefully this answered your question. Please let me know if there is something that remains unclear. Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/405#issuecomment-761936653:78,efficient,efficient,78,,https://github.com/google/deepvariant/issues/405#issuecomment-761936653,1,['efficient'],['efficient']
Energy Efficiency,"Hi @anands-repo . You are correct, subsampling is static. A nice effect of this is that it reduces the complexity in terms of training reproducibility. We have not deeply investigated whether dynamic resampling (making a different image per epoch) would benefit training. It's an interesting question. It could potentially reduce overfitting that might occur at the read level and therefore allow training to progress through more epochs before a model is selected. . I think it is unlikely that this would improve the current production training setup, but it is not impossible. For the WGS training curves, there is little overfitting apparent in training graphs over a large number of epochs. For the WES training curves, some overfitting is apparent, but we suspect this is less due to signal from the read level and more due to the smaller number of regions represented in the exome. This is one reason that we currently train the exome model by warmstarting from the WGS model. It is probably worth us taking a look at some point, but likely isn't the lowest hanging fruit for us to improve performance. Thank you for the suggestion and discussion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230#issuecomment-553653477:91,reduce,reduces,91,,https://github.com/google/deepvariant/issues/230#issuecomment-553653477,2,['reduce'],"['reduce', 'reduces']"
Energy Efficiency,"Hi @anands-repo,. Thanks for the question. You are correct that in these cases we are semantically treating GT=0 to be ""non-ALT""; the goal of the haplotypes.py library is to try to resolve cases where the raw calls indicate more than two alternate alleles at a single base pair. This can occur because DeepVariant processes each candidate variant independently, and sometimes candidates overlap the same stretch of the reference genome. As you observed in the code, the haplotypes.py library is even still just a ""best effort"" and there are cases where > 2 alternate alleles will still be emitted by the algorithm. Regarding your prevalence comment, empirically on the Genome in a Bottle HG002 sample we observed this library reduce the number of regions with > 2 alternate alleles from ~3000 to ~300 when it was first introduced. (The numbers may be slightly different now due to subsequent improvements in variant discovery and calling). regards,; Cory",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/247#issuecomment-564333529:726,reduce,reduce,726,,https://github.com/google/deepvariant/issues/247#issuecomment-564333529,1,['reduce'],['reduce']
Energy Efficiency,"Hi @claudiologiudice ,; You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115; Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/283#issuecomment-599859404:188,adapt,adapted,188,,https://github.com/google/deepvariant/issues/283#issuecomment-599859404,1,['adapt'],['adapted']
Energy Efficiency,"Hi @depristo . Thanks for the clarification and the resource!. Right now, my goal is to get the flow working, so I may need to use an Intel machine for the time being. I will need to revisit this later. I wonder whether this affects only make_examples. If so, it may be good to separate the build into two parts, one for make_examples, and one for the rest. I am not sure whether this is worth supporting from your side. If I am constrained to use Power machines for my subsequent runs, I will be able to look into this aspect.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-441838626:448,Power,Power,448,,https://github.com/google/deepvariant/issues/123#issuecomment-441838626,1,['Power'],['Power']
Energy Efficiency,"Hi @dkurt ,. I've tested your change, and compare to without using OpenVINO (we internally are now using `intel-tensorflow==2.5.0`). Here are the call_variants runtime comparison on our latest code:; * WGS:; * Without OpenVINO: ~166m (this was an average from 3 runs); * With OpenVINO: 151m26.692s (this was from one run. I can do 2 more runs to reduce the variance for comparison). My PacBio and Hybrid runs are still going. I can report them when they're done. I want to check whether this is roughly inline with your expectation, or if there are things I need to tweak further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/472#issuecomment-878550413:346,reduce,reduce,346,,https://github.com/google/deepvariant/pull/472#issuecomment-878550413,1,['reduce'],['reduce']
Energy Efficiency,"Hi @gunjanbaid . Unfortunately I am not compiling for x86, but for IBM power, so most of the installation scripts need to be discarded, and packages need to be manually compiled from source using IBM's Advance Toolchain gcc compilers. I have finally gotten all bazel tests to complete as well as the build to complete. I was wondering whether you could explain one piece of the build files though - this is just out of curiosity. In build_release_binaries, there is a function that starts as follows - which seems to be performing a hack to fix something:; ```; # Bazel's --build_python_zip replaces our carefully engineered symbolic links; # with copies. This function puts the symbolic links back.; function fix_zip_file {; orig_zip_file=$1. # Step 1: Copy the zip file to a temporary place.; TMPDIR=$(mktemp -d -t tmp.XXXXXXXXXXX); # The .zip version of the binary doesn't have the header that makes it; # self-executable. We use that version because otherwise unzip would; # complain and raise an error code.; cp ""${orig_zip_file}.zip"" ""${TMPDIR}""; ```. Would you be able to give a quick explanation of what the problem is? I understand what it does, but I do not understand why it is needed, or whether it is just for convenience. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356#issuecomment-699007209:71,power,power,71,,https://github.com/google/deepvariant/issues/356#issuecomment-699007209,1,['power'],['power']
Energy Efficiency,"Hi @husamia . Thank you for the question. The answer is a big complex. To summarize your question it is roughly. ```; The biggest issue that we have in calling de novos is managing ""false positive"" de novo events - not necessarily where the proband call is incorrect, but where a 0/1-0/0-0/0 call should be (for example 0/1-0/1-0/0). Will DeepTrio reduce the number of these calls.; ```. Calls in DeepVariant which are 0/1-0/0-0/0 where a human inspection in IGV shows evidence in the parent are generally caused by:. 1. Reads which have a MAPQ below the threshold that DeepVariant sees (MAPQ < 5). ; 2. Regions which DeepVariant seems to think may represent a segmental duplication where reads are mapped from a different region. In these cases, DeepVariant can all a position as 0/0. In terms of overall precision, DeepTrio has higher precision on de novo calls (0/1-0/0-0/0) than DeepVariant [tables for this are in the manuscript](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). In the cases that you mention, DeepTrio will more often designate the parent as a nocall (0/1-./.-0/0) for example. The genotype quality in the parents in DeepTrio can be useful for reducing the Type II error that you mention. If you want to further reduce these positions, I expect it would be possible to post-filter using the coverage values for the parent calls in the VCF (further filtering those that have a reasonable number of AD for ALT reads). We'll also consider whether including that during the VCF generation could make sense. Please let me know if this didn't answer your question.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/450#issuecomment-829516306:348,reduce,reduce,348,,https://github.com/google/deepvariant/issues/450#issuecomment-829516306,2,['reduce'],['reduce']
Energy Efficiency,"Hi @marchoeppner . Converting the representation from adjacent lines to single events is something we will look at. It won't necessarily be an easy change. I think the next thing I will do is to look at why both DeepVariant and GATK are making errors on this set and see if it looks like changing the representation is likely to help with those errors, or if something else is going on. If the errors can also be reduced, then this looks more interesting as something to do sooner rather than later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520#issuecomment-1559853651:413,reduce,reduced,413,,https://github.com/google/deepvariant/issues/520#issuecomment-1559853651,1,['reduce'],['reduced']
Energy Efficiency,"Hi @mclaugsf,. Let me give you a bit more context here on the runtime of call_variants. call_variants is the deep learning component of DeepVariant, so it relies on TensorFlow to execute the inception_v3 model used to evaluate our genotype likelihoods. In the 0.7 case study, make_examples creates 5,847,041 genomic tensors that need to be evaluated. When executing using CPUs, TensorFlow by default uses all of the available cores on the machine. So in our case study, which runs on a 64 core machine, we are using all 64 cores to evaluate these tensors. . So a rough estimate of the core-hours needed for the DeepVariant WGS case is:. 64 cores * 205 minutes of runtime ~= 219 core hours ~= 9 days. So if you are running on a machine with a single core, you should see call_variants take ~9 days. This is a bit of an over-estimate because 64 cores isn't 64x more efficient than 1 core. . Based on your 1 day turn around I'd guess you are running on a machine with 8 cores. Note these numbers assume you are using a modern CPU with AVX etc instruction sets. Not having those can increase the runtime by ~4x or so. Also I want to ask - in your original post are you processing exomes? If so, are you providing a capture regions bed to make_examples? Normally an exome produces < 100k examples (contrast that with 5.8M in a whole genome) so the runtime should be 60x less on an exome. That means instead of 9 days on a single core you are looking at 3.5 hours.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430736386:864,efficient,efficient,864,,https://github.com/google/deepvariant/issues/105#issuecomment-430736386,2,['efficient'],['efficient']
Energy Efficiency,"Hi @mdriller . DeepVariant itself does not have this option. . However, as I understand you really just want to convert a single sample gVCF to a reference expanded VCF. bcftools may allow you to efficiently do this with the option:. `bcftools convert --gvcf2vcf ${VCF} --fasta-ref ${REF}`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571#issuecomment-1284818258:196,efficient,efficiently,196,,https://github.com/google/deepvariant/issues/571#issuecomment-1284818258,1,['efficient'],['efficiently']
Energy Efficiency,"Hi @melkerdawy ,; currently our code doesn't support training with multiple GPUs. I haven't really tried training with CPUs, so I don't know whether model_train automatically utilize multiple CPUs or not. (We know that `call_variants` does. But I've never used CPU for training.); If you want, you can try running it and see how many CPUs it utilizes. However, I don't recommend training with CPUs becaues I think it'll be really slow. And, using GPU parallel with training also won't work. In the case if training, it's not easily parallelizable like inference. The most practical path forward is probably to get a really powerful GPU and see how well that works for you. I've been hoping to benchmark for training on GPU as well, but haven't got time to do so. If you have some results you want to share, that will be great.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/192#issuecomment-507880713:623,power,powerful,623,,https://github.com/google/deepvariant/issues/192#issuecomment-507880713,1,['power'],['powerful']
Energy Efficiency,"Hi @melkerdawy ; are you using CPUs only? There is a reason why our current training case study uses TPU. If you can't run with TPU, at least considering using GPUs to train. Fundamentally you need a powerful enough processing unit to train an inception v3 model.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/192#issuecomment-507491998:200,power,powerful,200,,https://github.com/google/deepvariant/issues/192#issuecomment-507491998,1,['power'],['powerful']
Energy Efficiency,"Hi @melop,. Great suggestion! As luck would have it, this will be a feature in our next release (`1.7.0`). We have parallelized/sharded `postprocess_variants` across multiple CPUs, which helps to reduce its maximum RAM footprint. It also takes in a `--regions` flag directly so you can easily split up the process further if that's necessary (although it shouldn't be). . Until `1.7.0` is released, your only option is to follow @pichuan's suggestion. You can use `bcftools concat` to join the region-specific VCFs back together. I hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868#issuecomment-2293775698:196,reduce,reduce,196,,https://github.com/google/deepvariant/issues/868#issuecomment-2293775698,1,['reduce'],['reduce']
Energy Efficiency,"Hi @olechnwin ,. The thresholds are usually chosen empirically. Based on what tasks we're trying to achieve, we choose it to find the best tradeoff between sensitivity and the amount of noises we bring in. This is more a research problem, especially that you're trying to adapt DeepVariant code to a different problem. So we won't be able to easily share a recipe here for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/578#issuecomment-1317450563:272,adapt,adapt,272,,https://github.com/google/deepvariant/issues/578#issuecomment-1317450563,1,['adapt'],['adapt']
Energy Efficiency,"Hi @pgrosu . The default of `--pileup_image_height=100` was initially determined by the most common use case of Illumina WGS data. In terms of tweaking pileup_image_height, there are 2 past experiments that I can recall right now:. ## Illumina WES:; Before we released an Illumina WES model, I experimented with training a WES model with `--pileup_image_height=200`. At the time (end of 2017), I observed that the Indel F1 dropped quite a bit when increasing the height, but helped SNP F1 a bit. This investigationwas why we didn’t change this default for the Illumina WES model release. ## PacBio:; In Nov 2019, I trained a model with `--pileup_image_height=75` to see if we can decrease runtime without too much accuracy tradeoff. At the time, my results were:. * Reducing height to 75 (from 100) reduces calling time to ~85% (from 3h20m to 2h50m); * Accuracy (on case study chr20); - Indel F1: 0.983872 —> 0.982728; - SNP F1: 0.998913 —> 0.998867. Based on feedback, we decided it’s not a high priority to continue. Because:; 1. Amplicon will likely use the height=100; 2. 30min is not a big bottleneck now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338#issuecomment-681126260:799,reduce,reduces,799,,https://github.com/google/deepvariant/issues/338#issuecomment-681126260,2,['reduce'],['reduces']
Energy Efficiency,"Hi @pichuan ; Thank you for your response. . I have completed the testing of NGS-WES data and Pacbio-WGS data on deeptrio1.6, and performed single sample analysis using deepvariant1.6. I have some questions that I would like to confirm with you. . 1. If I want to test 3 Pacbio WGS datasets using deeptrio1.6, how much memory should I allocate at least? . 2. Regarding NGS-deeptrio1.6 analysis, I only saw the benchmark comparison results for WGS-chr20. Do you have any results (Recall, Precision, F1_Score) to share for WES data? . 3. For Pacbio-deeptrio1.6 analysis, I tested the official WGS HiFi data (HG002, HG003, HG004). The reference genome used for alignment was hs37d5.fa. The bam was hifi_reads_aligned.haplotagged.bam (pbmm2+whatshap haplotag). The region is chr20. However, the benchmark comparison results were worse than the data you published. Does the choice of reference genome affect the precision of the results? . 4. Whether it is NGS-WES or Pacbio-WGS, the results from using deeptrio1.6 for analysis are slightly less precision than using deepvariant1.6. Is this normal? In theory, should the results from deeptrio1.6 be better than those from deepvariant1.6?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720#issuecomment-1780570310:335,allocate,allocate,335,,https://github.com/google/deepvariant/issues/720#issuecomment-1780570310,1,['allocate'],['allocate']
Energy Efficiency,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243#issuecomment-579406829:1025,Power,Power,1025,,https://github.com/google/deepvariant/issues/243#issuecomment-579406829,1,['Power'],['Power']
Energy Efficiency,"Hi @rabiafidan . We have found the following:. 1. The use of GLnexus preset DeepVariant_unfiltered is preferred for retaining True de novo calls, and we have updated the documentation for this.; 2. We also observe a reduction in 0/1 child, 0/0 parent calls when post-processing the final VCF to set a parent to ./. when that parent is 0/0, the child is 0/1 or 1/1, and that parent has either less than 8 reads covering the variant position, or and allele fraction of > 0.15. . The second filter seems to help reduces cases where there is not enough confidence to clearly call a de novo. Does this filtering strategy seem like it might further help refine your calls? We are considering whether to recommend postprocessing of this nature via a script in the future. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-899967692:509,reduce,reduces,509,,https://github.com/google/deepvariant/issues/440#issuecomment-899967692,2,['reduce'],['reduces']
Energy Efficiency,"Hi @sophienguyen01 ; You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py; This is more experimental and not officially documented yet. But you can find in our Docker:; https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/808#issuecomment-2062163191:556,adapt,adapt,556,,https://github.com/google/deepvariant/issues/808#issuecomment-2062163191,1,['adapt'],['adapt']
Energy Efficiency,"Hi @tinyfallen . @pgrosu is correct, the most efficient way is to run DeepVariant individually and then merge with GLnexus. One point to make, GLnexus runs quite quickly relative to the variant calling step. The joint calling operation won't add much cost relative to DeepVariant (scaling benchmarks for GLnexus can be found in Figure 7C of [the DeepVariant-GLnexus paper](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). So the main resource use can be estimated from the single sample runtime multiplied by sample number. If you are curious for a comparison, it might make sense to run a single sample and compare the calls for that sample with the extracted set of calls from the GATK joint calls.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651#issuecomment-1576131534:46,efficient,efficient,46,,https://github.com/google/deepvariant/issues/651#issuecomment-1576131534,1,['efficient'],['efficient']
Energy Efficiency,"Hi Bowen, just an FYI that I'm looking into this a bit. I'm going to try running call_variants on a 8 core machine on GCE to see how the timing looks. Can you send us the details of the CPU you are trying to run on? For example:. $ head -n 26 /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 63; model name	: Intel(R) Xeon(R) CPU E5-2690 v3 @ 2.60GHz; stepping	: 2; microcode	: 0x3c; cpu MHz		: 1200.024; cache size	: 30720 KB; physical id	: 0; siblings	: 24; core id		: 0; cpu cores	: 12; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 15; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc ibpb ibrs stibp dtherm ida arat pln pts; bugs		: cpu_meltdown spectre_v1 spectre_v2; bogomips	: 5187.99; clflush size	: 64; cache_alignment	: 64; address sizes	: 46 bits physical, 48 bits virtual; power management:. Also, just to confirm - you are using DV 0.6.1 with our gcp optimized TF wheel (this is downloaded by run-prereqs.sh)?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391790774:887,monitor,monitor,887,,https://github.com/google/deepvariant/issues/74#issuecomment-391790774,2,"['monitor', 'power']","['monitor', 'power']"
Energy Efficiency,"Hi Charles,. Thank you for the machine information. For the AWS instance, the 8-core machine has 8-fold fewer CPUs than the 64-cores used in the case study. Given this, the total number of core-hours is 144. This is actually quite efficient for a DeepVariant run. For more similar time to the case study, you could consider the m5.12xlarge or m5.24xlarge, with 48 and 96 cores respectively. The 96-core instance will likely be faster than the case study, though with worse economics. In all cases, the use of spot instances will be far more cost-favorable. For the GCP run posted, this looks to be the GCP cost-optimized execution framework. This framework is built by the Google Cloud, so although I cannot speak as directly to this, I can refer your question to the right place. That GitHub repository is here: https://github.com/googlegenomics/gcp-deepvariant-runner/issues. For your Docker question, I believe that this error reflects that you are not mounting the volumes in the proper way in your script. In your .sh script, you may want to try to more closely align the way your script mounts and creates input and output directories with the case study. I don't feel that I can clearly communicate the details of mounting the Docker volumes. It may be valuable to read Docker's description of this: https://docs.docker.com/storage/volumes/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483500506:231,efficient,efficient,231,,https://github.com/google/deepvariant/issues/171#issuecomment-483500506,2,['efficient'],['efficient']
Energy Efficiency,"Hi Cory (@cmclean),. Thank you for the new release, but if we look at the new timings with the `0.5.1` release, they seem to have gotten longer than with the previous version:. [Commit v0.5.1](https://github.com/google/deepvariant/commit/38c17ed0f8b8b03da8daf68d24226c4c1957e116). #### Timings: Whole Genome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![whole-genome-case-study-timing](https://user-images.githubusercontent.com/6555937/36058742-9f1916de-0df6-11e8-9431-a19a677612ef.png). #### Timings: Exome Case Study - [`0.5 (pink) vs. 0.5.1 (green)`]. ![exome-case-study-timings](https://user-images.githubusercontent.com/6555937/36058748-bff057a0-0df6-11e8-9941-dc89084572b8.png). What is the cause of the additional delay in version `0.5.1` as compared to the previous one?. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-364627307:345,green,green,345,,https://github.com/google/deepvariant/issues/27#issuecomment-364627307,2,['green'],['green']
Energy Efficiency,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:560,schedul,scheduler,560,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896,1,['schedul'],['scheduler']
Energy Efficiency,"Hi Fra,. That seems a bit small compared to other WES BAM files, which are between 5 - 10 GB:. https://ega-archive.org/datasets/EGAD00001005247/files. The disk space seems okay, though you have some memory-mapped file-systems, which should be okay if you have a good amount of memory. . Memory on Ubuntu can be checked with `cat /proc/meminfo` or (`top`). For example, when I ran the variant candidate selection, on just a small region of a chromosome and monitored memory usage, I saw the following trend:. ![image](https://github.com/google/deepvariant/assets/6555937/7e200851-3fde-4ad6-bf75-477ccefb9e32). You'll notice that it can be resource intensive. This is the reason I was opting we troubleshoot by first trying with a smaller region of a chromosome using `--regions`, ideally one you know variants should be present. A preliminary check with `samtools view -c BAM REGION` would ensure you have mapped reads there for DeepVariant to find, and with `samtools flagstat` to check on that subregion for the number of quality reads. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1631059899:456,monitor,monitored,456,,https://github.com/google/deepvariant/issues/675#issuecomment-1631059899,1,['monitor'],['monitored']
Energy Efficiency,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366745899:450,schedul,scheduler,450,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899,2,['schedul'],['scheduler']
Energy Efficiency,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219:240,power,power,240,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219,2,['power'],['power']
Energy Efficiency,"Hi Steve,. Sounds good. You can also try use the `df` command if you want to monitor disk usage just in case, though might want to use it in conjunction with `grep` to filter for just the disk that affected :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960186921:77,monitor,monitor,77,,https://github.com/google/deepvariant/issues/491#issuecomment-960186921,1,['monitor'],['monitor']
Energy Efficiency,"Hi!I did some PCR and Sanger sequencing.; The following mutation can be verified successfully：; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:238,Power,PowerPoint,238,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628,2,['Power'],['PowerPoint']
Energy Efficiency,"Hi,. As reported above I can see a bunch of small files (30-40 Mb total) written in TMPDIR. If I look in the folder when deepvariant is running I can see files like these; `Bazel.runfiles_6nvtcv_j __pycache__ tmp8rz89h3g.py tmpglc9d5x3.py tmph9ntzkbx`. I will try another run to monitor when exactly they are created, but the job fails at very early stage when I submit it to the cluster, so I assume these are written during make_examples which is the first step in run deepvariant I think. Thanks for support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524#issuecomment-1067583182:279,monitor,monitor,279,,https://github.com/google/deepvariant/issues/524#issuecomment-1067583182,1,['monitor'],['monitor']
Energy Efficiency,"Hi,. I see, that actually makes a lot more sense, thank you for helping clear that up. As for the training, I understand that there isn't really any official documentation on adding classes, however I was wondering if I could be pointed towards some of the files/functions in charge of calculating the result of the CNN, to see what needs to be changed for adding additional classes. Another option I was considering that is easier to perform, however is much more computationally heavy, is to train a new model from scratch for each of the classes I wish to add, where I would modify the VCF file given to the 'make_examples' script to make the labels be : 0/0 = undefined, 0/1 = yes, 0/2 = no, and to do this for every new class I wish to add. So I would run the vanilla DeepVariant on my input BAM, then do further runs with each model to further categorize variants that would fall within each of the classes I am examining (can possibly even process the initial output VCF such that the additional runs will only run for within those regions that were initially classified as a variant). Is such an approach possible? If all that is being done is converting each input into 6 channels and have it run through the Inception Pipeline then I believe my approach should be possible. I guess my question specifically is, is there anything within the DeepVariant pipeline that will prevent me from training it to identify completely new classes instead of the default HOM-REF, HET, and HOM-ALT?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383347052:276,charge,charge,276,,https://github.com/google/deepvariant/issues/67#issuecomment-383347052,2,['charge'],['charge']
Energy Efficiency,"Hi,. I tried `--select_variant_types='indels multi-allelics'`, the vcf includes all the INDELS from vcf output that is not included with `--select_variant_types`. However, the output vcf using `--select_variant_types='indels multi-allelics'` also includes SNPs variants. . My purpose is to create INDELs training examples only and `--select_variant_types='indels multi-allelics'` still contains SNPs examples. Is there a way to filter examples (from tfrecord???.gz files) after `make_examples` step but before `shuffling` step. I also tried `--truth_variants` (using the truth vcf that only contains INDEL variants) and `-variant_caller=vcf_candidate_importer` parameter, but the number of examples are reduced significantly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/813#issuecomment-2091255112:703,reduce,reduced,703,,https://github.com/google/deepvariant/issues/813#issuecomment-2091255112,1,['reduce'],['reduced']
Energy Efficiency,"Hi,; Thanks for your question! In our [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) you mentioned, we showed an example of further training a model to perform better on mosquito data, which has higher variant density than human. Since then, internally we have continued to investigate what properties of the human genome and population structure DeepVariant learns during training. We’re hoping to come up with a suggested non-human model soon, but do not yet have a specific timeframe for releasing such a model. In terms of having a ""universal"" model, that is a good question too! We are currently investigating whether we can reduce the number of models we release while maintaining the high accuracy. Ideally if we can train one model that works well for all scenarios, we will certainly do that. Currently we’re optimizing our model accuracy for each common application, while keeping the number of released models as low as we can.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/204#issuecomment-518518311:728,reduce,reduce,728,,https://github.com/google/deepvariant/issues/204#issuecomment-518518311,2,['reduce'],['reduce']
Energy Efficiency,"I FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711. Tip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line; Jun-08 12:17:16.749 [Task monitor] DEBUG nextflow.Session - Session aborted -- Cause: Process `pbc_varicall (1)` terminated with an error exit status (1); Jun-08 12:17:16.752 [main] DEBUG nextflow.Session - Session await > all processes finished; Jun-08 12:17:16.764 [main] DEBUG nextflow.Session - Session await > all barriers passed; Jun-08 12:17:16.776 [main] DEBUG nextflow.trace.WorkflowStatsObserver - Workflow completed > WorkflowStats[succeededCount=0; failedCount=1; ignoredCount=0; cachedCount=0; pendingCount=0; submittedCount=0; runningCount=0; retriesCount=0; abortedCount=0; succeedDuration=0ms; failedDuration=15m 11s; cachedDuration=0ms;loadCpus=0; loadMemory=0; peakRunning=1; peakCpus=1; peakMemory=0; ]; Jun-08 12:17:16.977 [main] DEBUG nextflow.cache.CacheDB - Closing CacheDB done; Jun-08 12:17:16.991 [main] DEBUG nextflow.script.ScriptRunner - > Execution complete -- Goodbye; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:12552,monitor,monitor,12552,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"I consulted with the team to get an idea of the effort required, and we came to the conclusion that the DeepVariant code is full of diploid assumptions, so this would likely take a lot of effort, and unfortunately it isn't something that we have enough bandwidth to advise anyone else through either. You're of course welcome to fork DeepVariant and play around with it, but it will likely require many changes to the code to make it work. It is definitely possible though. You could also consider whether other ML-based variant callers are easier to adapt to polyploid, or if any polyploid callers exist already, which could be fun to add ML on top of as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/519#issuecomment-1054575852:551,adapt,adapt,551,,https://github.com/google/deepvariant/issues/519#issuecomment-1054575852,1,['adapt'],['adapt']
Energy Efficiency,"I re-ran DV+glnexus on a cohort of ~150 WGS trios. Using `--make_examples_extra_args ""ws_use_window_selector_model=false""`; does reduce the apparent mendelian violation rate. Here is the plot of that before (left) and with that argument (right). This is after some sane filtering on GQ and rare in gnomad. ![original](https://user-images.githubusercontent.com/1739/74752329-bb706c80-522b-11ea-8976-f357e4172663.png). The right side is lower; good. But, then I further filtered that set of variants on your recently released thousand genomes DV VCF, requiring an allele frequency less than 1% there. Now, even though the RHS as dropped by ~20 DNs per trio, the ""before"" LHS, has a lower number of putative de novos and the end result is that it is worse to run with `ws_use_window_selector_model=false`. ![filtering](https://user-images.githubusercontent.com/1739/74752590-26ba3e80-522c-11ea-8cc9-ee229ac95328.png). This must be because you ran the thousand genomes samples without turning off the window selector and I am annotating on exact REF and ALT allele matches, not using haplotype enumeration as does hap.py, but AFAIK, this is how all (allele frequency) annotation software will work. . Any recommendations on how to proceed? I did bcftools norm on my set and on the deep variant thousand G calls and I suspect something like [vcfallelelicprimitives](https://github.com/vcflib/vcflib#vcfallelicprimitives) will not help here either. ; A thousand G callset run without the window selector would help, but is only a band-aid. Is there an annotation software that does haplotype matching? Presumably this will be more of a problem as the GATK juggernaut fades.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-587538179:129,reduce,reduce,129,,https://github.com/google/deepvariant/issues/272#issuecomment-587538179,1,['reduce'],['reduce']
Energy Efficiency,"I see the same issue, but with Ubuntu-16. (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1: //deepvariant/protos:deepvariant_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (02:53:00) ERROR: /root/variant_calling/deepvariant/deepvariant/protos/BUILD:56:1 1 input file(s) do not exist. This is running on IBM Power 8 in a docker container, so I built pyclif from source. However, the location of pyclif isn't the same as what is in build-prereq.sh, so I commented that section out because `which pyclif` works. Would appreciate any help. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-440897520:363,Power,Power,363,,https://github.com/google/deepvariant/issues/6#issuecomment-440897520,1,['Power'],['Power']
Energy Efficiency,"I tried and was unsuccessful to reproduce the problem with the case study input data. I could try to reproduce the problem with your data if you can attach your BAM (just the 100,000 bases of it) and the reference.; As a work around you could run without --num_shards flag, or you could also try to reduce the number of shards.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-573900023:299,reduce,reduce,299,,https://github.com/google/deepvariant/issues/249#issuecomment-573900023,1,['reduce'],['reduce']
Energy Efficiency,"I want to give you an update on the situation. Following your suggestions, I've discussed with the system administrators and I found out that some older nodes on the cluster do not have AVX2 or AVX512 instructions. I've now completed a test running on nodes supporting those and the running time on the same BAM file is ~14h (290+326+134). So I've discussed with them and found a way to allocate all deepvar jobs to the new nodes supporting the AVX instruction. . Thanks for your support!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346#issuecomment-704106130:387,allocate,allocate,387,,https://github.com/google/deepvariant/issues/346#issuecomment-704106130,1,['allocate'],['allocate']
Energy Efficiency,I'm closing this issue because we aren't likely to provide prebuilt binaries *without* AVX instructions. One reason is that the AVX instructions are critical to efficiently evaluate our deep learning model. Another is that TensorFlow itself will soon provide prebuilt binaries with AVX instructions (https://github.com/tensorflow/tensorflow/releases). . Users who need to run DeepVariant on pre-AVX instruction chipsets should build DeepVariant from sources.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355389148:161,efficient,efficiently,161,,https://github.com/google/deepvariant/issues/21#issuecomment-355389148,2,['efficient'],['efficiently']
Energy Efficiency,"ILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note to remove -DLLVM_TARGETS_TO_BUILD=X86; # ""rm CMakeCache.txt"" to remove cmake cache; cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \; -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:13719,Power,PowerPC,13719,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['Power'],['PowerPC']
Energy Efficiency,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366748047:622,schedul,scheduler,622,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047,2,['schedul'],['scheduler']
Energy Efficiency,"It took about 8 hours, but I could run the **postprocess_variants** step on my local computer (using the commands [specified above](https://github.com/google/deepvariant/issues/167#issuecomment-480640009)). If Official Amazon Support doesn't have a solution for running this program on AWS, I might cross-post this on StackExchange (to see if I can figure out if there is some sort of configuration issue on AWS, and/or if I am not using ECS efficiently/correctly). However, I realize you have a lot of support to provide, so I will close this ticket and provide the successful output from my local computer:. ```; 2019-04-07 21:06:30.591035: I deepvariant/postprocess_variants.cc:88] Read from: Genos_Provided/call_variants_output.tfrecord.gz; 2019-04-07 21:06:33.504711: I deepvariant/postprocess_variants.cc:97] Done reading: Genos_Provided/call_variants_output.tfrecord.gz. #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505181: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505270: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-04-07 21:06:34.914308: I deepvariant/postprocess_variants.cc:105] Done SortSingleSiteCalls; I0407 21:06:36.217032 139687245461248 postprocess_variants.py:596] Writing output to VCF file: Genos_Provided/output.vcf.gz; I0407 21:06:36.221911 139687245461248 genomics_writer.py:163] Writing Genos_Provided/output.vcf.gz with NativeVcfWriter; I0407 21:06:36.231071 139687245461248 postprocess_variants.py:601] 1 variants written.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480702286:442,efficient,efficiently,442,,https://github.com/google/deepvariant/issues/167#issuecomment-480702286,1,['efficient'],['efficiently']
Energy Efficiency,"Looks like I was impatient. Now that it's been going for several hours (And reduced learning rate) tensorboard is giving better results. TPs dropped to 0, and everything dropped to 0, but now it's back up. I thought starting with the pre-trained wgs model would let it just improve but I guess it had to re-learn. Planning to scale up to the entire genome next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251#issuecomment-563508582:76,reduce,reduced,76,,https://github.com/google/deepvariant/issues/251#issuecomment-563508582,2,['reduce'],['reduced']
Energy Efficiency,"Now I have filtered the HiFi reads based on mapq, to check if we see the same patter of strange peak of GQ 30 and VAF 0,17; I wanted to do the other things as well but we are out of computing power",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1662524118:192,power,power,192,,https://github.com/google/deepvariant/issues/682#issuecomment-1662524118,1,['power'],['power']
Energy Efficiency,"OK, thanks, you could close the ticket now. I meant to ask whether `make_examples` could be parallelized with or without GPU. Your previous link and #81 only show GPU use `call_variants`. From my understanding call_variant loads the Inception model and do NN forward computation to do prediction, so it makes sense it leverages the parallel power from GPU. But make_examples just convert BAM into images. Also, hanging for > 4hr doesn't seem to be caused only by deep pileup. I am currently rerunning make_examples, and will report in a new issue if it hangs again.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428649613:341,power,power,341,,https://github.com/google/deepvariant/issues/99#issuecomment-428649613,1,['power'],['power']
Energy Efficiency,Part of this PR is included in the latest release: https://github.com/google/deepvariant/releases/tag/v0.8.0; Thank you for your contribution!. @fo40225 I'll try to schedule a separate discussion about the async writer with you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-482391198:165,schedul,schedule,165,,https://github.com/google/deepvariant/pull/152#issuecomment-482391198,1,['schedul'],['schedule']
Energy Efficiency,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385485505:249,reduce,reduced,249,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505,1,['reduce'],['reduced']
Energy Efficiency,"Posting an answer for @akolesnikov : . ---. Hi @chrisfleisch ,. Your observations are very inline with our performance evaluations. We had a great improvement in performance of call_variants when we enabled Intel MKL library (see [blog](https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/). As far as I understand AMD does not support MKL and therefore cannot take advantage of this optimization (I found this [discussion](https://www.reddit.com/r/Amd/comments/9rx0rj/is_amd_working_on_an_alternative_to_intel_mkl_or/) on that topic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-604195181:295,power,power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,295,,https://github.com/google/deepvariant/issues/274#issuecomment-604195181,1,['power'],['power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-']
Energy Efficiency,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used?; You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2116222843:132,monitor,monitor,132,,https://github.com/google/deepvariant/issues/820#issuecomment-2116222843,1,['monitor'],['monitor']
Energy Efficiency,So it should charge same amount of money right (compare running one by one with in parallel)?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/33#issuecomment-355563693:13,charge,charge,13,,https://github.com/google/deepvariant/issues/33#issuecomment-355563693,1,['charge'],['charge']
Energy Efficiency,"Thank you Lucas for your answer. I understand it is for compression. I think I was looking for a similar function to GATK BP resolution. When you use targets, the output would be reduced by a lot. The gvcf becomes rather useless, since you cannot look at or filter individual sites on depth. Some sites might have a depth well over my threshold, but is filtered because the minimum depth is well under my threshold. I am thinking the median DP will have similar issues.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/826#issuecomment-2151667306:179,reduce,reduced,179,,https://github.com/google/deepvariant/issues/826#issuecomment-2151667306,1,['reduce'],['reduced']
Energy Efficiency,"Thank you both for your inputs. Regarding the old, anecdotal observation that GPU isn't efficiently used, I didn't collect any GPU resources like this time. And here are the CPU/memory/disk usages for the same input data, under CPU vs under GPU modes, back then. Note these are collected from running the PEPPER pipeline (release 0.4.0). Note how using the GPU didn't help in terms wallclock time.; ![pepper shard-0 high-cpu resources](https://github.com/google/deepvariant/assets/16310888/e7970a06-b38d-4972-8a8d-6a5cf0595ef1); ![pepper shard-0 NVDA-P100 resources](https://github.com/google/deepvariant/assets/16310888/78b9da04-d9c7-4c5b-a730-ae99b8ee4908). And here's the cost of the GPU mode, broken down by SKU (we use Google cloud). ![pepper NVDA-P100 cost](https://github.com/google/deepvariant/assets/16310888/64316bdd-3ceb-47cb-a130-dd6da34dc8fc). In terms of running DV in ""production"", we did optimize the CPU pipeline down to below $20/sample. We didn't optimize the GPU pipeline because of the wall-clock issue. Based on the samples I've analyzed with 1.5.0 so far, the cost indeed dropped. Parabricks isn't really an option for us, as we build pipelines using WDL and run them on Terra. With all these said, I'm definitely not saying that this is a blocker for us. It is more of a curiosity/optimization question. Thank you!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017:88,efficient,efficiently,88,,https://github.com/google/deepvariant/issues/650#issuecomment-1550151017,1,['efficient'],['efficiently']
Energy Efficiency,"Thank you for clearing my doubts despite your extremely busy schedule, I hereby present to you my most sincere appreciation",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/713#issuecomment-1746323445:61,schedul,schedule,61,,https://github.com/google/deepvariant/issues/713#issuecomment-1746323445,2,['schedul'],['schedule']
Energy Efficiency,"Thank you for the reply. . Following are the cpuinfo of my machine. processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391801518:689,monitor,monitor,689,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518,1,['monitor'],['monitor']
Energy Efficiency,"Thanks @Asppagh . Sorry it took me a while to get back to this. From your response above, you're talking about the `call_variants` step.; The issue (https://github.com/google/deepvariant/issues/74) you cited above was from a while ago, so the numbers might have changed. But I think it's still a good reference point. Even with 8 cores, going from < 1sec to 20-60 sec does seem extreme. Let me try to get a 8-core, 64 GB machine and test it with Singularity and report back, so we can have a better comparison. Other thing that come into mind is that `call_variants` step uses TensorFlow, which relies on CPU optimization to be faster. See: https://google.github.io/deepvariant/posts/2019-04-30-the-power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-512-optimizations/; Can you check what type of CPU you have? (What's in your ` /proc/cpuinfo`?). One more note on num_shards:; num_shards affects the way that `make_examples` step is parallelized. On the high level it shouldn't affect call_variants step. But, setting `num_shards` to 8 means the output from `make_examples` will be in 8 shards (8 files), so it's possible that the input into call_variants become slower, but I don't expect a big difference. And, I would expect setting `num_shards=1` would make `make_examples` step much slower.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866205653:699,power,power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-,699,,https://github.com/google/deepvariant/issues/463#issuecomment-866205653,1,['power'],['power-of-building-on-an-accelerating-platform-how-deepVariant-uses-intels-avx-']
Energy Efficiency,"Thanks @pichuan :). Finally I got the root cause of this issue, it seem the return number of min on Ubuntu and RHEL are different. I tried to run the following commands on both python2:. The output of **Ubuntu** (on X86) is:; ```; >>> import mock; >>>; >>> expected_start=9; >>> expected_end=21; >>> bufsize=0; >>> expected_bases = 'A' * (expected_end - expected_start); >>>; >>> start=10; >>> end=21; >>> contig='20'; >>> ref_reader = mock.MagicMock(); >>> ref_reader.query.return_value = expected_bases; >>> contig_nbp = ref_reader.contig(contig).n_bases; >>> res = min(end + bufsize, contig_nbp); >>> res; 21; ```. And then I tried the same code on **RHEL 7.5 (both on X86 and Power**, the output is:. ```; >>> import mock; >>>; >>> expected_start=9; >>> expected_end=21; >>> bufsize=0; >>> expected_bases = 'A' * (expected_end - expected_start); >>>; >>> start=10; >>> end=21; >>> contig='20'; >>> ref_reader = mock.MagicMock(); >>> ref_reader.query.return_value = expected_bases; >>> contig_nbp = ref_reader.contig(contig).n_bases; >>> res = min(end + bufsize, contig_nbp); >>> res; <MagicMock name='mock.contig().n_bases' id='140373647899088'>; >>> int(res); 1; ```. And I accidentally run the above command with Python3, and it throw the following error:. ```; >>> res = min(end + bufsize, contig_nbp); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; TypeError: '<' not supported between instances of 'MagicMock' and 'int'; ``` . So propose to fix the above type conversion if you have plan to support RHEL or upgrade to Python 3 in the future :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464961152:680,Power,Power,680,,https://github.com/google/deepvariant/issues/154#issuecomment-464961152,1,['Power'],['Power']
Energy Efficiency,"Thanks @pichuan,; I run on a cluster with different machines, which are pretty similar for example one of the machine that I use has 8 cores, each with the below cpu info:. cat /proc/cpuinfo; processor	: 0; vendor_id	: GenuineIntel; cpu family	: 6; model		: 85; model name	: Intel(R) Xeon(R) Silver 4116 CPU @ 2.10GHz; stepping	: 4; microcode	: 0x200004d; cpu MHz		: 2095.078; cache size	: 16896 KB; physical id	: 0; siblings	: 1; core id		: 0; cpu cores	: 1; apicid		: 0; initial apicid	: 0; fpu		: yes; fpu_exception	: yes; cpuid level	: 13; wp		: yes; flags		: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts mmx fxsr sse sse2 ss syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts nopl tsc_reliable nonstop_tsc eagerfpu pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx hypervisor lahf_lm 3dnowprefetch arat; bogomips	: 4190.15; clflush size	: 64; cache_alignment	: 64; address sizes	: 40 bits physical, 48 bits virtual; power management:. What I compared was not only call_variant it was make example step too. To make it clear I enclose a part of the log here, however it uses slightly different setting and different example but it shows what I said in my previous comment.; In this case I did not specify any number of core for the cpu and the result are slightly better than if I specify the cpu cores equal to 8. stdout of the process:; input file S-001701867.markdup.bam; I0622 13:05:17.760246 47710258629632 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0622 13:05:17.867540 47710258629632 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0622 13:05:17.933148 47710258629632 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-866226252:985,power,power,985,,https://github.com/google/deepvariant/issues/463#issuecomment-866226252,2,['power'],['power']
Energy Efficiency,"Thanks you!. An update: I've allocated 500GB disk and am still seeing PAPI 10, so it would be great if can get an understanding of this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960053107:29,allocate,allocated,29,,https://github.com/google/deepvariant/issues/491#issuecomment-960053107,1,['allocate'],['allocated']
Energy Efficiency,"Thanks, @pichuan. Always happy to contribute to the open source community.; I ran twice the command with the new argument and in bot cases it failed :( the external hard drive where I have allocated the bam file got ejected and a; [output2.log](https://github.com/google/deepvariant/files/15167067/output2.log); lso I noticed that the syslog and kern.log became insanely big (~200GB), leaving me with no extra disk space.; Any ideas what might be going on?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/810#issuecomment-2085656965:189,allocate,allocated,189,,https://github.com/google/deepvariant/issues/810#issuecomment-2085656965,1,['allocate'],['allocated']
Energy Efficiency,"Thanks, @pichuan. Any suggestion about the long hangs, please? My BAM size is 8.5G. I confirm the machine has been running overnight based on the GCE Monitoring tab.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428623317:150,Monitor,Monitoring,150,,https://github.com/google/deepvariant/issues/99#issuecomment-428623317,1,['Monitor'],['Monitoring']
Energy Efficiency,"Thanks, Andrew!; I totally understand the priority setting. In the meantime, I've collected some DV-1.5.0 resource usage data on a particular sample (note the scattering schemes aren't the same between the two references). Overall it's making really good use of the resources available (**_maybe_** except the memory, but it's more likely I over-allocated). Hopefully, it can help your team with further tuning.; [30X.human.GRCh38.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501324/30X.human.GRCh38.deepvariant.regular-resources-usage.pdf); [30X.human.T2T.deepvariant.regular-resources-usage.pdf](https://github.com/google/deepvariant/files/11501325/30X.human.T2T.deepvariant.regular-resources-usage.pdf). Thanks,; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1551851074:346,allocate,allocated,346,,https://github.com/google/deepvariant/issues/650#issuecomment-1551851074,1,['allocate'],['allocated']
Energy Efficiency,"Thanks, Paul!. I do indeed sometimes see out-of-memory errors (e.g. for an even crazier sample with 140X coverage), but usually I get an PAPI error code 9 from that (based on experience, error code 9 indicates memory issues and error code 10 indicates disk issue, PAPI 10 is sort of an catch-all error code so it's not very informative). Never the less, the procedure you described is very helpful and I'll ; * try with increased the memory; * incorporate that into our workflow (conditionally, e.g. for high coverage samples), and ; * also use that for collecting data in tuning the resources allocated to DV. Thanks!; Steve",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-960166365:594,allocate,allocated,594,,https://github.com/google/deepvariant/issues/491#issuecomment-960166365,1,['allocate'],['allocated']
Energy Efficiency,"The Honourable Andrew and Paul.; Thank you both very much for your kind answers and advice, it will be very helpful for me in my next endeavours, Deepvariant is a very efficient and useful piece of software, thank you for all your hard work in making it available to us. I will follow your advice and read some other people's research papers. Sincere thanks again.; Cheng",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/680#issuecomment-1641247564:168,efficient,efficient,168,,https://github.com/google/deepvariant/issues/680#issuecomment-1641247564,1,['efficient'],['efficient']
Energy Efficiency,The above issue was due to a problem with numpy compilation. Newer versions of numpy need to be compiled from source for POWER platforms.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-695338068:121,POWER,POWER,121,,https://github.com/google/deepvariant/issues/123#issuecomment-695338068,1,['POWER'],['POWER']
Energy Efficiency,"The code for the current, released version of DeepVariant does not use PairHMM to score haplotypes. Since the submission of the DeepVariant manuscript, there have been 4 releases which have improved various aspects of the code, training regime, and training data for models. The DeepVariant paper does validly describe the methods used in a working version, both the original PrecisionFDA submission and the improvements made for the first open source release (v0.4). However, there are further improvements which are not captured in that publication, and which are instead represented either in other joint publications (e.g. https://www.biorxiv.org/content/10.1101/519025v2) or in blogs produced by our team or close partners (e.g. https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html), (e.g. https://medium.com/tensorflow/the-power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-c8f0acb62344).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488762439:853,power,power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-,853,,https://github.com/google/deepvariant/issues/180#issuecomment-488762439,1,['power'],['power-of-building-on-an-accelerating-platform-how-deepvariant-uses-intels-avx-optimizations-']
Energy Efficiency,"The instructions at https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md run ok. . I have no idea how I ended up with only 2 cores in that VM, especially after having to email support to even get the 64 requested cores allocated, since they quotaed me at 32. Also, I realized that the docker install command involving ""buster"" referred to a Debian release, which would never have worked with the Google Cloud instructions, since they specifically indicate you should use Ubuntu 16.04. Have someone fix that page! It is still an important page for anyone wanting to deploy on google, if they are not experienced with that cloud platform. Thnks,; Ariel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-751141693:246,allocate,allocated,246,,https://github.com/google/deepvariant/issues/399#issuecomment-751141693,1,['allocate'],['allocated']
Energy Efficiency,"The short answer is yes. That's because that parameter gets fed to `prepare_inputs()`, which generates a `DeepVariantInput` object that generates a `tf.data.Dataset`, which gets fed in your case into an [`Estimator`](https://github.com/google/deepvariant/blob/r0.7/deepvariant/modeling.py#L415-L419) it constructs for you, that will then will be processed via `predict()` which sets up a [`MonitoredSession`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L595) which by default each Session is thread-safe. I believe the rest get used to monitor the sessions, but I would need to double-check on that. Turtles all the way down :). Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105#issuecomment-430634265:390,Monitor,MonitoredSession,390,,https://github.com/google/deepvariant/issues/105#issuecomment-430634265,2,"['Monitor', 'monitor']","['MonitoredSession', 'monitor']"
Energy Efficiency,"Unfortunately we don't have any documentation on the method except release notes. The main motivation was to reduce the runtime and simplify the pipeline for PacBio data. The phasing is generated from DeepVariant proposed candidates. Proposed candidates are generated by counting alleles at each position and applying heuristics to reduce the number of proposed candidates. The main differences between one-step phasing and WhatsHap are:. * One-step phasing uses a greedy algorithm that processes intervals of 25000 bases long at a time. Using a greedy algorithm makes it inferior to WhatsHap. Although, experiments showed that final DeepVariant accuracy only slightly suffers. * Another big difference is that one-step phasing uses ""noisy"" proposed candidates when WhatsHap is run on genotyped variants produced by running DeepVariant on unphased data. The code which performs the phasing operation is in https://github.com/google/deepvariant/blob/r1.4/deepvariant/direct_phasing.cc. . Please note that our the DeepVariant model in v1.4 is able to run on candidates phased by WhatsHap, and the model has similar performance. This can be done by adding flags to the step of make_examples if run separately. If it is of interest for you to run DeepVariant v1.4 using the WhatsHap flags instead of the direct phasing, we can provide you with instructions to do so.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407:109,reduce,reduce,109,,https://github.com/google/deepvariant/issues/557#issuecomment-1228968407,4,['reduce'],['reduce']
Energy Efficiency,"We have previously highlighted two examples where DeepVariant has been applied in non-human organisms:. * [Rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant) - are diploid and have similar variant density to humans with short-read data. The existing model worked well.; * [Mosquitos](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) - Used a retrained model to account for high variant density with short-read data. With mosquitos we found that our human-trained model had high numbers of false positive calls. This is likely because in humans increased variant density is often caused by mismapping of similar genomic regions, leading DeepVariant to not call what are likely artifactual variants (in humans). But in mosquitos, we variant density is much higher, so DV falsely discards what are real variants. Fortunately, zebrafish appear to have similar SNP densities with humans, and the fact you are using PacBio HiFi reads may also reduce variant-density issues. However, there are a number of factors that may contribute to poor performance: The specific strains being experimented with, regions of the genome being evaluated, differences in structural variation, coverage, etc. We would expect the DV human model to perform well in zebrafish, but it would be a good idea to estimate the error rate through a mendelian analysis, Sanger sequencing, or other methods to determine whether retraining is needed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/415#issuecomment-771074189:1088,reduce,reduce,1088,,https://github.com/google/deepvariant/issues/415#issuecomment-771074189,1,['reduce'],['reduce']
Energy Efficiency,"ah good catch sorry I missed the image!. It has been a while since I have used slurm, but you could try:. ```; #SBATCH --cpus-per-task=28; #SBATCH --ntasks-per-node=1 # Or don't set this.; ```. See the explanation for `--ntasks-per-node`:. `--ntasks-per-node=<ntasks>`; > Request that ntasks be invoked on each node. If used with the --ntasks option, the --ntasks option will take precedence and the --ntasks-per-node will be treated as a maximum count of tasks per node. Meant to be used with the --nodes option. This is related to --cpus-per-task=ncpus, but does not require knowledge of the actual number of cpus on each node. In some cases, it is more convenient to be able to request that no more than a specific number of tasks be invoked on each node. Examples of this include submitting a hybrid MPI/OpenMP app where only one MPI ""task/rank"" should be assigned to each node while allowing the OpenMP portion to utilize all of the parallelism present in the node, or submitting a single setup/cleanup/monitoring job to each node of a pre-existing allocation as one step in a larger job script. I think the issue is that `--ntasks-per-node` is not allocating CPUs",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/546#issuecomment-1180880714:1008,monitor,monitoring,1008,,https://github.com/google/deepvariant/issues/546#issuecomment-1180880714,1,['monitor'],['monitoring']
Energy Efficiency,"ariant.simg /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=output.vcf.gz --output_gvcf=output.g.vcf.gz. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/mnt/gs18/scratch/users/mansourt/Tamer2/kerdawy/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2019-04-14 11:40:35.568961: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real 0m2.359s; user 0m0.767s; sys 0m0.949s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482956117:1274,Power,Power,1274,,https://github.com/google/deepvariant/issues/132#issuecomment-482956117,1,['Power'],['Power']
Energy Efficiency,"arser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in fina",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9621,Monitor,MonitoredSession,9621,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['Monitor'],['MonitoredSession']
Energy Efficiency,"at might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:3761,power,power,3761,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['power'],['power']
Energy Efficiency,"command which @pichuan provided but it still print nothing on terminal. ; And the docker version of this machine is `Docker version 19.03.3, build a872fc2`. So I switch to another machine which supported AVX instruction and run the test again by @tedyun 's advice.; Then it run normally and output the files as same as the tutorial. Here is the environment & the command:. - Host CPU info; ```text; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 63; model name : Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz; stepping : 2; microcode : 0x43; cpu MHz : 1199.975; cache size : 25600 KB; physical id : 0; siblings : 20; core id : 0; cpu cores : 10; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 15; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d; bogomips : 4595.05; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```; - OS ,kernel & docker version; ```sh; # uname -a; Linux CoreS 3.10.0-1062.12.1.el7.x86_64 #1 SMP Tue Feb 4 23:02:59 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux. # cat /etc/centos-release; CentOS Linux release 7.7.1908 (Core). # docker -v; Docker version 19.03.12, build 48a66213fe; ```. - Test run command; ```sh; # BIN_VERSION=""1.0.0""; # ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:1058,monitor,monitor,1058,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['monitor'],['monitor']
Energy Efficiency,"d how the preservation of viability through repair via homologs (preserving consensus) or other DNA repair mechanisms is significant to this model organism. Below are a few excerpts denoting this:. - _*""The signatures of IHR [interhomolog recombination] found in specific regions of A. vaga genome (14) or in natural A. vaga populations (8) thus likely result from mechanisms such as crossing over (CO) and/or gene conversion that take place during the meiotic pairing of homologs (Fig. 4B).""*_. - _*""[T]he nonreductional meiotic process in bdelloid rotifers was likely evolutionary maintained to serve primarily for DNA repair to safeguard the genetic information of the species, especially when thriving in semiterrestrial environments where DNA DSBs [double-strand breaks] do accumulate during prolonged periods of desiccation.""*_. - _*""Regardless of the origin of DNA DSBs (programmed DSBs during the meiotic-derived oogenesis or accidental DSBs due to genotoxic stresses), IHR in the germ line can efficiently and accurately reconstruct broken chromosomes while shuffling the allelic content and creating offspring that are genetically diverse from their mother.""*_. - This shows preservation of function by complete correction through homologs, or through DNA repair pathways reconstituting viable function. $`\underline{In \; Nature \; (from \; the \; paper \; and \; supplementary \; materials)}`$. - In this paper the authors were trying to show the randomization of variation. This is a very different goal than trying to show the preservation of gene function under variation. Their focus was more on the linkage between SNPs, and chose to carefully look at how differences progress across lineages. If you look at one of the gene specific analysis was done for the COX1 gene as a phylogenetic tree -- presented in the Supplementary Materials -- it illustrated high similarity among the lineages:. ![image](https://github.com/google/deepvariant/assets/6555937/a119536e-cdc4-4ea3-a06a-fbf32b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342:1556,efficient,efficiently,1556,,https://github.com/google/deepvariant/issues/682#issuecomment-1656474342,1,['efficient'],['efficiently']
Energy Efficiency,"endor_id : GenuineIntel; cpu family : 6; model : 94; model name : Intel(R) Core(TM) i7-6700 CPU @ 3.40GHz; stepping : 3; microcode : 0xc2; cpu MHz : 1013.093; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 22; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb invpcid_single intel_pt rsb_ctxsw spec_ctrl retpoline kaiser tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xgetbv1 dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp; bugs : cpu_meltdown spectre_v1 spectre_v2; bogomips : 6816.62; clflush size : 64; cache_alignment : 64; address sizes : 39 bits physical, 48 bits virtual; power management:. I have used the preliminaries set in the exome case study, namely. ```; BASE=""/HD_disk/exome-case-study""; BUCKET=""gs://deepvariant""; BIN_VERSION=""0.6.1""; MODEL_VERSION=""0.6.0""; MODEL_CL=""191676894"". # Note that we don't specify the CL number for the binary, only the bin version.; BIN_BUCKET=""${BUCKET}/binaries/DeepVariant/${BIN_VERSION}/DeepVariant-${BIN_VERSION}+cl-*""; MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${MODEL_VERSION}/DeepVariant-inception_v3-${MODEL_VERSION}+cl-${MODEL_CL}.data-wes_standard""; DATA_BUCKET=""${BUCKET}/exome-case-study-testdata""; ```. I have also run the run_prereq.sh, however, I am not entirely sure if I have the gcp optimized TF wheel. Can you show me which one is the gcp optimized TF wheel and how can I install that if I have not?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391801518:1297,power,power,1297,,https://github.com/google/deepvariant/issues/74#issuecomment-391801518,1,['power'],['power']
Energy Efficiency,g process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varical,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4107,monitor,monitor,4107,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"g section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX and ChrY in the hemizygous case, and we have a few internal ideas around how to use this to mak",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1971,reduce,reduced,1971,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025,2,['reduce'],['reduced']
Energy Efficiency,"h test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-16T02:45:46Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; sudo docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARG",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:4204,Power,Power,4204,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['Power'],['Power']
Energy Efficiency,"hi，; I used DeepVariant1.4 and GATK.4.2.0.0. I want to reduce the running time of the process overall. So I want to confirm, is there no problem with data preprocessing without deduplication and BQSR for DeepVariant?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/638#issuecomment-1526875856:55,reduce,reduce,55,,https://github.com/google/deepvariant/issues/638#issuecomment-1526875856,1,['reduce'],['reduce']
Energy Efficiency,"iant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I believe does not work since you need sudo permission to create this directory. You will want to modify this command to ensure that `/home/cwarden/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM` exists locally prior to mounting. We also recommend updating your script to explicitly use the 0.8.0 image at ` gcr.io/deepvariant-docker/deepvariant:0.8.0`, which is now tagged as `latest`. Models are included in this image. The WES model checkpoint for this image is at `/opt/models/wes/model.ckpt`. To run the v0.8.0 container in interactive mode, you will need to modify the entrypoint as below. ```; docker run -it --entrypoint bash -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant:0.8.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483448362:2172,efficient,efficient,2172,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362,1,['efficient'],['efficient']
Energy Efficiency,"ib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:9478,Monitor,MonitoredSession,9478,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['Monitor'],['MonitoredSession']
Energy Efficiency,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1871,adapt,adapting,1871,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838,2,['adapt'],['adapting']
Energy Efficiency,"lp with troubleshooting. . 1. affinity.c compilation run into error: . ```; ./affinity; pthread_setaffinity_np: Invalid argument.; ```. ```; 2. cat /proc/sys/kernel/threads-max; 2061146; lscpu; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 32; On-line CPU(s) list: 0-31; Thread(s) per core: 1; Core(s) per socket: 16; Socket(s): 2; NUMA node(s): 2; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU E5-2698 v3 @ 2.30GHz; Stepping: 2; CPU MHz: 2798.211; CPU max MHz: 3600.0000; CPU min MHz: 1200.0000; BogoMIPS: 4600.13; Virtualization: VT-x; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 40960K; NUMA node0 CPU(s): 0-15; NUMA node1 CPU(s): 16-31; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb invpcid_single intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm xsaveopt cqm_llc cqm_occup_llc dtherm ida arat pln pts md_clear spec_ctrl intel_stibp flush_l1d. cat /etc/os-release; NAME=""Red Hat Enterprise Linux Server""; VERSION=""7.9 (Maipo)""; ID=""rhel""; ID_LIKE=""fedora""; VARIANT=""Server""; VARIANT_ID=""server""; VERSION_ID=""7.9""; PRETTY_NAME=""Red Hat Enterprise Linux""; ANSI_COLOR=""0;31""; CPE_NAME=""cpe:/o:redhat:enterprise_linux:7.9:GA:server""; HOME_URL=""https://www.redhat.com/""; BUG_REPORT_URL=""https://bugzilla.redhat.com/"". REDHAT_BUGZILLA_PRODUCT=""Red Hat Enterprise Linux 7""; REDHAT_BUGZILLA_PRODUCT_VERSION=7.9; REDHAT_SUPPORT_PRODUCT=""Red Hat Enterprise Linux""; REDHAT_SUPPORT_PRODUCT_VERSION=""7.9"". lsb_release -a; LSB Version: :core-4.1-amd6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-993155459:1043,monitor,monitor,1043,,https://github.com/google/deepvariant/issues/497#issuecomment-993155459,1,['monitor'],['monitor']
Energy Efficiency,"much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.475 0.000 {built-in method marshal.loads}; 3136 0.422 0.000 0.422 0.000 {built-in method io.open_code}; 15152 0.183 0.000 0.183 0.000 {method 'reduce' of 'numpy.ufunc' objects}; 3136 0.164 0.000 2.265 0.001 <frozen importlib._bootstrap_external>:914(get_code); 5151 0.146 0.000 0.436 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py:196(normalize_log10_probs); 1 0.146 0.146 0.146 0.146 {method 'summary_counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 10001 0.142 0.000 0.428 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:249(with_gq_and_likelihoods); 5151 0.134 0.000 0.610 0.000 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant_caller.py:194(_calc_reference_confidence); 5268/5203 0.119 0.000 0.562 0.000 {built-in method builtins.__build_class__}; 1339 0.119 0.000 0.119 0.000 {method 'write' of 'third_party.nucleus.io.python.tfrecord_writer.TFRecordWriter' objects}; 1276 0.116 0.000 0.676 0.001 /tmp/Bazel.runfiles_t7ewtxb0/runfiles/com_google_deepvariant/deepvariant/variant",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:7085,reduce,reduce,7085,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['reduce'],['reduce']
Energy Efficiency,"n by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1906,adapt,adapt,1906,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['adapt'],['adapt']
Energy Efficiency,"nd other forums. I don't see any issues with that. The only thing which is important is to add the changes as a single commit separately from other changes so it can be properly tracked in git history. > Can you help me understand the expected benefit of the changes? It looks like this should improve runtime for call_variants. Do you have any high-level information from benchmark runs for us to understand the percentage improvement to expect. You're right - this PR about efficiency of deep learning part only (call_variants). We need some time to collect some benchmark numbers and check what's is OK to share publicly. Probably, extra optimizations could be applied. > If we are to incorporate these changes, we would want to make sure this would perform well across various hardware. For example, we would want our Docker images to gracefully fallback to working code if it is on a machine with incompatible hardware. Do you expect OpenVINO to have this property (for example, if someone is running on an AMD machine). Proposed changes are very smooth - by default, OpenVINO is not used. We can test other Intel hardware such iGPU and HDDL-R. For non-Intel HW users can always use default implementation. > We will also have to think about how these changes interact with any updates we would make to our use of TensorFlow. We're not directly planning anything in the near future, but it's good for us to consider. I think it won't be an issue. It make sense to isolate some OpenVINO related logic to separate Python script to reduce conflicts during development. Additionally, I just wanted to ask if you're interested in using GitHub Actions so you can perform initial tests for pull requests. In example, https://github.com/dkurt/deepvariant/blob/master_openvino/.github/workflows/main.yml does Docker build and then runs WGS on getting-started data with TensorFlow and OpenVINO and compares the outputs ([logs](https://github.com/dkurt/deepvariant/runs/1259964899?check_suite_focus=true)).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-709941019:2041,reduce,reduce,2041,,https://github.com/google/deepvariant/pull/363#issuecomment-709941019,2,['reduce'],['reduce']
Energy Efficiency,"nt/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15624,Monitor,MonitoredSession,15624,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['Monitor'],['MonitoredSession']
Energy Efficiency,nt:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1); Jun-08 12:07:05.943 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:12:06.012 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:06.076 [Task monitor] DEBUG n.processor.TaskPollingMonitor - !! executor local > tasks to be completed: 1 -- submitted tasks are shown below; ~> TaskHandler[id: 1; name: pbc_varicall (1); status: RUNNING; exit: -; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.724 [Task monitor] DEBUG n.processor.TaskPollingMonitor - Task completed > TaskHandler[id: 1; name: pbc_varicall (1); status: COMPLETED; exit: 1; error: -; workDir: /data/shared/clinical/LongRead/Pipeline/work/55/335c478f0f7e93d6d5e06cd4d99711]; Jun-08 12:17:16.741 [Task monitor] ERROR nextflow.processor.TaskProcessor - Error executing process > 'pbc_varicall (1)'. Caused by:; Process `pbc_varicall (1)` terminated with an error exit status (1); Command executed:. run_deepvariant --model_type PACBIO --ref /data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta --reads /data/shared/clinical/LongRead/Data//m84011_220902_175841_Aln.bam --output_vcf /data/shared/c,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:4434,monitor,monitor,4434,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"om_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_2gnuyvf0/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:15768,Monitor,MonitoredSession,15768,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['Monitor'],['MonitoredSession']
Energy Efficiency,red/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; Jun-08 12:02:05.810 [main] DEBUG nextflow.Session - Session await; Jun-08 12:02:05.895 [Actor Thread 5] DEBUG nextflow.container.SingularityCache - Singularity found local store for image=docker://google/deepvariant:1.5.0; path=/data/shared/clinical/LongRead/cache/google-deepvariant-1.5.0.img; Jun-08 12:02:06.011 [Task submitter] DEBUG n.executor.local.LocalTaskHandler - Launch cmd line: /bin/bash -ue .command.run; Jun-08 12:02:06.012 [Task submitter] INFO nextflow.Session - [55/335c47] Submitted process > pbc_varicall (1);,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2748,monitor,monitor,2748,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['monitor'],['monitor']
Energy Efficiency,"rsing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data cond",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1627,adapt,adapts,1627,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['adapt'],['adapts']
Energy Efficiency,"s to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozygous`$:. ![image](https://github.com/google/deepvariant/assets/6555937/1e43b84e-17ae-40ea-8c82-b7e87d0cf3d6). Finally we test for $`homozygous`$ $`alternate`$:. ![image](https://github.com/google/deepvariant/assets/6555937/cedce40f-4fc0-45fe-843b-e2652b31c0af). Now we can see there is a significant correlation with a heterozygous variant call. So to call a variant site's genotype you now have the power to traverse the whole neural network -- through a set of transformations (you can tweak) to identify hidden patterns in the data -- as it is all linked from the start (read encoding) to finish (genotype identification). This ability is enabled through the preservation of the propagation of information (which is deeply and directly linked) in order to help one optimize upon. That's why you don't want to throw anything away, as @AndrewCarroll and @pichuan mentioned as it helps you inspect and tweak the selection power and criteria as it is all interconnected. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:5196,power,power,5196,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,4,['power'],['power']
Energy Efficiency,"t cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard instance class should consume 288 CPU-hours, which on a pre-emptible GCP machine should cost $0.01 per core-hour at current pricing, for $2.88. This is also not the most cost-efficient way to run DeepVariant. 2\); Your script runs `mkdir -p ${OUTPUT_DIR}`, which I b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483448362:1304,efficient,efficiently,1304,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362,1,['efficient'],['efficiently']
Energy Efficiency,"t;; 	padding-right:7.2pt;}; .oa3; 	{border:1.0pt solid white;; 	background:#E9EBF5;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 | 43,0,50; 38 | 38 | 91,0.41 | 38 | 38,0,50; 48 | 48 | 136,0.51 | 48 | 48,0,62; 43 | 43 | 31,0.42 | 19 | 19,0,46; 43 | 43 | 33,0.45 | 41 | 41,0,55; 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->; </body>. </html>. The following mutation validation failed:; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt so",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:2406,Power,PowerPoint,2406,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628,1,['Power'],['PowerPoint']
Energy Efficiency,"tance so it must be a local issue. I did run into a different performance issue with AMD during the call_variants.py step in my testing. I setup an Intel 48 core instance and an AMD 48 core instance. Both in AWS. > intel:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 1; > Vendor ID: GenuineIntel; > CPU family: 6; > Model: 85; > Model name: Intel(R) Xeon(R) Platinum 8175M CPU @ 2.50GHz; > Stepping: 4; > CPU MHz: 1520.299; > BogoMIPS: 4999.99; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 32K; > L2 cache: 1024K; > L3 cache: 33792K; > NUMA node0 CPU(s): 0-47; > Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke. > amd:~$ lscpu; > Architecture: x86_64; > CPU op-mode(s): 32-bit, 64-bit; > Byte Order: Little Endian; > CPU(s): 48; > On-line CPU(s) list: 0-47; > Thread(s) per core: 2; > Core(s) per socket: 24; > Socket(s): 1; > NUMA node(s): 3; > Vendor ID: AuthenticAMD; > CPU family: 23; > Model: 1; > Model name: AMD EPYC 7571; > Stepping: 2; > CPU MHz: 2524.374; > BogoMIPS: 4399.90; > Hypervisor vendor: KVM; > Virtualization type: full; > L1d cache: 32K; > L1i cache: 64K; > L2 cache: 512K; > L3 cache: 8192K; > NUMA node0 CPU(s): 0-7,24-31; > NUMA node1 CPU(s): 8-15,32-39; > NUMA node2 CPU(s): 16-23,40-47; > Flags: fpu vm",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-603439134:1093,monitor,monitor,1093,,https://github.com/google/deepvariant/issues/274#issuecomment-603439134,1,['monitor'],['monitor']
Energy Efficiency,"test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; pip install scikit-learn --force-reinstall --no-deos; # build from source; wget https://github.com/scikit-learn/scikit-learn/archive/0.20.2.tar.gz; tar zxvf 0.20.2.tar.gz; cd scikit-learn-0.20.2; python setup.py bdist_wheel; # verify; python -c ""from skle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:21016,monitor,monitor,21016,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['monitor'],['monitor']
Energy Efficiency,"this is everything that is in the vcf, so the deviations are not picked up at all. It is just one line with the 5' deviation. If I include other genomic region, than I do see more variations there, but for this region it is not recognized at all.; ![image](https://github.com/user-attachments/assets/cb092f42-5285-48f5-b2c8-dc1f9590e9c2). [output.vcf.txt](https://github.com/user-attachments/files/17303859/output.vcf.txt). However both positions (3948 and 3949) show up in the output.g.vcf; [output.g.vcf.txt](https://github.com/user-attachments/files/17303928/output.g.vcf.txt). ![image](https://github.com/user-attachments/assets/408893bd-27a6-48ac-9f6c-4260a96741c9). *update: I just tried the same data with clair3 and it also does not call these two positions. . I then aligned against only the transgen in the genomic context (~50 kb each side) and then the variants are called correctly, so it seemed to have something to do with the reads also mapping to the other region. So I checked the reads again in more detail and it turns out that at the 5' SNP only 2 reads are primary and at the two 3' SNPs there is only one primary alignment, the others are supplementary. Therefore vsc_min_count_snps=2 filters the 3' SNPs out, while the 5' barely passes. I then reduced vsc_min_count_snps to 1 and now I find also the two 3' SNPs in the vcf output, with one passing and the other being Refcall. Of course plenty of other SNPs show up as well, but with low QUAL values and being RefCall ... So this is an issue of the way that I make the reference FASTA, but I don't see any other way of doing this, because otherwise I would have to duplicate the surrounding genomic DNA postentially ending up with many supplementary alignments as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/892#issuecomment-2401535093:1268,reduce,reduced,1268,,https://github.com/google/deepvariant/issues/892#issuecomment-2401535093,1,['reduce'],['reduced']
Energy Efficiency,"to none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa4; 	{border:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; -->; </style>; </head>. <body>; <!--StartFragment-->. QUAL | AQ | DP,alt ratio | GQ | PL; -- | -- | -- | -- | --; 43 | 43 | 47,0.57 | 42 | 43,0,50; 38 | 38 | 91,0.41 | 38 | 38,0,50; 48 | 48 | 136,0.51 | 48 | 48,0,62; 43 | 43 | 31,0.42 | 19 | 19,0,46; 43 | 43 | 33,0.45 | 41 | 41,0,55; 43 | 43 | 41,0.20 | 17 | 16,0,49. <!--EndFragment-->; </body>. </html>. The following mutation validation failed:; <html xmlns:m=""http://schemas.microsoft.com/office/2004/12/omml""; xmlns=""http://www.w3.org/TR/REC-html40"">. <head>. <meta name=ProgId content=PowerPoint.Slide>; <meta name=Generator content=""Microsoft PowerPoint 15"">; <style>; <!--tr; 	{mso-height-source:auto;}; col; 	{mso-width-source:auto;}; td; 	{padding-top:1.0px;; 	padding-right:1.0px;; 	padding-left:1.0px;; 	mso-ignore:padding;; 	color:windowtext;; 	font-size:18.0pt;; 	font-weight:400;; 	font-style:normal;; 	text-decoration:none;; 	font-family:Arial;; 	mso-generic-font-family:auto;; 	mso-font-charset:0;; 	text-align:general;; 	vertical-align:bottom;; 	border:none;; 	mso-background-source:auto;; 	mso-pattern:auto;}; .oa1; 	{border-top:1.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:3.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#4472C4;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:middle;; 	padding-bottom:3.6pt;; 	padding-left:7.2pt;; 	padding-top:3.6pt;; 	padding-right:7.2pt;}; .oa2; 	{border-top:3.0pt solid white;; 	border-right:1.0pt solid white;; 	border-bottom:1.0pt solid white;; 	border-left:1.0pt solid white;; 	background:#CFD5EA;; 	mso-pattern:auto none;; 	text-align:center;; 	vertical-align:midd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628:2465,Power,PowerPoint,2465,,https://github.com/google/deepvariant/issues/503#issuecomment-1018275628,1,['Power'],['PowerPoint']
Energy Efficiency,"yeap, it's caused by empty shards. I was able to reproduce this by using 64 shards with the quickstart test data. @depristo should I file a separate issue for this as it's not really a docker issue?. @chenshan03: thanks for the report. As a workaround until this bug is fixed, you may reduce the number of shards to avoid having empty ones.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355036534:285,reduce,reduce,285,,https://github.com/google/deepvariant/issues/27#issuecomment-355036534,1,['reduce'],['reduce']
Energy Efficiency,"you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1147,adapt,adapted,1147,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['adapt'],['adapted']
Integrability," 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9491,depend,dependency,9491,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability," 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:9014,depend,dependency,9014,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability," > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10594,depend,dependency,10594,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability," Best,. Brad Thomas. ________________________________; From: Ryan Poplin <notifications@github.com>; Sent: Friday, February 9, 2018 12:54 PM; To: google/deepvariant; Cc: Brad Thomas; Author; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Great to hear. There are minimum allele fractions that are used to generate candidate variants here (https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L165), you'll likely want to lower those thresholds for your use case. For the model itself, there is no explicit allele fraction, but keep in mind that the model was trained to predict the diploid genotype states of {hom ref, het, and hom var} so things that are lower allele fraction will likely be classified as 0/0 by the model. We don't currently have a version that does somatic variant calling. -; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-364524990>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqVYdivUTLfEIl26QitFq5k-svzBeks5tTJSBgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364650047:1380,message,message,1380,,https://github.com/google/deepvariant/issues/47#issuecomment-364650047,2,['message'],['message']
Integrability," I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3634,depend,dependency,3634,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['depend'],['dependency']
Integrability," dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what DeepVariant designed for.; DeepVariant is wrapped around TensorFlow, which is a much more general purpose ML tool. If there are functionalities that we don't provide, please also look into TensorFlow to see if they have something useful for you. I'm closing this issue now because this is not really a DeepVariant issue. But if you believe this actually reveals some bugs in our codebase, I'm happy to discuss further if you can show a reproducible example that demonstrate the error.; Closing this issue now. But happy to follow up on this issue if you have more thoughts/questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203#issuecomment-518462111:2516,wrap,wrapped,2516,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111,2,['wrap'],['wrapped']
Integrability," etc. Juan Pablo Aguilar. ________________________________; From: Pi-Chuan Chang ***@***.***>; Sent: Friday, September 20, 2024 6:52:36 PM; To: google/deepvariant ***@***.***>; Cc: Aguilar Cabezas, Juan Pablo ***@***.***>; Mention ***@***.***>; Subject: [External] Re: [google/deepvariant] Retraining DeepVariant without trios data? (Issue #878). Use caution with links and attachments. Hi @desmodus1984<https://github.com/desmodus1984> ,. Fundamentally, training a DeepVariant requires truth data (truth variants and confident regions).; The core question here is: Would you be able to get truth data for the bats you're studying?. I quickly looked through your recent discussion with @kishwarshafin<https://github.com/kishwarshafin> .; I believe @kishwarshafin<https://github.com/kishwarshafin> has been trying to give you some tips on some ways to construct truth. Note that this is an advanced topic. We don't expect most of our users to train DeepVariant models, or to construct truth data. However, if you do have truth data (truth variants and confident regions), you should be able to follow the documentation https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md to try to train a model. From your description above, I still think the best way to proceed is to directly use DeepVariant release models. Once you have the callsets, try to evaluate the calls first. Even if you plan to train a model, it'll be good to have those baseline metrics available, so you know whether your trained model is working or not. Does this help? If I'm misunderstanding your question, let me know. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/878#issuecomment-2364726373>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AJWD2VJ75MSKTWY7ILOFVM3ZXSRLBAVCNFSM6AAAAABNXT6B26VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDGNRUG4ZDMMZXGM>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195:2493,Message,Message,2493,,https://github.com/google/deepvariant/issues/878#issuecomment-2364755195,1,['Message'],['Message']
Integrability," for your kind and prompt response. For _point 1_ (and part of _point 2_), thank you very much for that additional information. I think my question was a simpler one: you can see multiple submissions for some groups, and I was trying to see if I understood all the submissions that used DeepVariant. Since there is only one ""rpoplin"" label (and no other entries from Verily Life Sciences), I'll assume that is the only DeepVariant benchmarks (in contrast to the there being multiple groups using GATK, in pipelines that gave varying results). I am also assuming that no one else was using DeepVariant at that time. However, please correct me if I am wrong. For _point 2_, I apologize: it is bad form to critique something without having tested it yourself. I sometimes worry that frequent use of deep learning may represent something that is popular (where many applications may not remain in common use in the long-term), but I need to assess each situation individually. So, I am very sorry about my tone in my initial message. Because of this post, I am now using DeepVariant as a way to practice learning some new skills in my free-time (such as using cloud computing options), but that makes it harder for me to provide a timely response. While the practice is something that I would like to gain on my own (I believe that I will lose some intuition about the results if I don't run the analysis myself), you are certainly welcome to work with any of the underlying data that I have uploaded to my [PGP page](https://my.pgp-hms.org/profile/hu832966). For _point 3_, I apologize that I need to take more time to read other papers carefully before citing them. For example, I have pretty much always seen a drop in accuracy for indels versus SNPs. However, if filtering for regions expected to have good concordance, I the loss in indel accuracy wasn't as bad as you might expect from [Figure 4](https://www.nature.com/articles/s41587-019-0054-x/tables/4) in that paper (however, the concordance in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476428247:1052,message,message,1052,,https://github.com/google/deepvariant/issues/165#issuecomment-476428247,2,['message'],['message']
Integrability," packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest version (2.7.12-1~16.04).; python-pip is already the newest version (8.1.1-2ubuntu0.4).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 28日 星期二 10:55:12 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:9206,depend,dependency,9206,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['depend'],['dependency']
Integrability," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4391,depend,dependency,4391,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability," tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5417,depend,dependency,5417,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability," while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `docker ps -a` after the script stopped from that error message. So, that puts me back where I started, and I actually have that extra error message (over trying to use Docker from the instance launched via ECS). Nevertheless, if I make any additional progress, I will let you know. **Update (4/10/2019)**: FYI, if anybody else has a similar problem, I eventually remembered that I needed to add myself to the Docker group using `sudo usermod -a -G docker ec2-user`, exiting, and then starting a new ssh session (a similar command was useful when I wanted to use gcsfuse along with Docker on Google Cloud, which is what I am currently testing...).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480642492:1976,message,message,1976,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492,3,['message'],['message']
Integrability,"""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9053,depend,dependecy,9053,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['dependecy']
Integrability,"## When there isn't an error message at all:; Is it possible that it could be running out of memory? That could be why running on an individual region succeeds while it fails on the whole bed file. Out of memory issues are hard to catch because they don't result in error messages. I ran the exome case study myself just now and it worked fine. In terms of compute resources, our testing system goes as low as ""n1-standard-8"" on Google Cloud, which is 8 vCPUs and 30 GB of memory. If your setup is less than 30GB of memory that might be the cause of the issue. Reducing the number of shards to 1 might help, but if it doesn't, you might just need to use a machine with more resources. ## When faidx also fails; Check that the fasta looks okay and perhaps redo the `samtools faidx` that created the `.fai` in the first place just in case. Is the ""chr1"" the exact sequence name of one the sequences in the fasta?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917501282:29,message,message,29,,https://github.com/google/deepvariant/issues/483#issuecomment-917501282,2,['message'],"['message', 'messages']"
Integrability,"############################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:16829,depend,depend,16829,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['depend']
Integrability,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10142,integrat,integrate,10142,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['integrat'],['integrate']
Integrability,"* Please make sure that contig names are consistent in both BAM file and reference. May be you could paste couple of lines (10 lines) from BAM and from your reference?; * Having ""Failed to retrieve block: unexpected end of file"" error message may mean that BAM index does not match the BAM file. Could you try to reindex your BAM?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367#issuecomment-716198032:235,message,message,235,,https://github.com/google/deepvariant/issues/367#issuecomment-716198032,1,['message'],['message']
Integrability,"-char"". # for GPU enabled; # fix ""ImportError: No module named google.protobuf"" by install protobuf from source; bazel clean; bazel shutdown; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant/... \; --action_env=LD_LIBRARY_PATH --test_env=LD_LIBRARY_PATH \; --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH \; --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH \; --action_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH --test_env=PYTHON_LIB_PATH=$PYTHON_LIB_PATH \; --action_env=BAZEL_PYTHON=$BAZEL_PYTHON --test_env=BAZEL_PYTHON=$BAZEL_PYTHON >& output.log 2>&1 &; bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" deepvariant:gpu_tests --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH. # for CPU only; bazel test -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant/... # build binary; bazel build -c opt ${DV_COPT_FLAGS} ""$@"" deepvariant:binaries --build_python_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; echo 'Expect a usage message:'; (bazel-bin/deepvariant/call_variants --help || : ) | grep '/call_variants.py:'. bazel build :licenses_zip --action_env=PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:19628,message,message,19628,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['message'],['message']
Integrability,"-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qili",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:17058,depend,depend,17058,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['depend']
Integrability,".10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m3.261s; user 0m5.770s; sys 0m5.974s; I0910 00:06:33.584306 140309422569216 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1.; ```. However, in your case, it seems like there's nothing useful in the log. I think I've heard about similar cases before (where there was no useful error message showing up). I previously wondered if it's possible that `parallel` somehow hid away some message. But I can't immediately think of a reproducible setting that I can debug. But speaking of ""returned non-zero exit status 252"", I did a quick search in our GitHub issues and found: https://github.com/google/deepvariant/issues/325; Can you see if this might be relevant to your issue?. If so, I'd still like to find a way to reproduce on my side, so I can see if we can make a more meaningful error message... 🤔",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-689891360:4050,message,message,4050,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360,3,['message'],['message']
Integrability,".3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## TensorFlow 1.12.0. > Note: Tensorflow 1.12.0 can be built with AT 11.0. Build instructions: [https://www.tensorflow.org/install/source](https://www.tensorflow.org/install/source). Edit instructions: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). Floatn.h error: [https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:8640,depend,dependency,8640,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['dependency']
Integrability,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8170,depend,dependency,8170,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2909,depend,dependency,2909,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3935,depend,dependency,3935,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,".local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2564,depend,dependency,2564,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; > -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; > --ref; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; > --report_title MITO60_Stats --sample_name MITO60 --output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > --model_type ONT_R104; >; >; > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> And, just in case the documentation isn't clear:; >>; >> This part:; >>; >> sudo docker run \; >> -v ""${INPUT_DIR}"":""/input"" \; >> -v ""${OUTPUT_DIR}"":""/output"" \; >> google/deepvariant:""${BIN_VERSION}"" \; >> ...; >>; >> The variable BIN_VERSION was specified in earlier in the steps:; >>; >> BIN_VERSION=""1.6.1""; >>; >> So, in Unix command it's equivalent to:; >>; >> google/deepvariant:""1.6.1"" \; >>; >> —; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; >> .; >> You are receiving this because you were mentioned.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508:4974,Message,Message,4974,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508,1,['Message'],['Message']
Integrability,"/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3*; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:{BIN_VERSION=""1.6.1""} python; /media/manish/Data/Jyoti_Mridha_AIC/Program/deepvariant-1.6.1/scripts/run_deepvariant.py; *--reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; *--report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result/; --model_type ONT_R104. On Wed, Jun 12, 2024 at 11:20 AM Pi-Chuan Chang ***@***.***>; wrote:. > Can you check your FASTA file has corresponding index files?; >; > What do you see what you run:; >; > ls /media/manish/Data/Jyoti_Mridha_AIC/My_S/Nanopore_Mito/Mito_Genome_hg38/zip_ref_docker/hg38_chrM.fa.gz*; >; > And, just as a check, can you confirm whether you were able to run; > https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md; > ?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/829#issuecomment-2162157549>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BDQL2ZEKDU5WYOAUIP24CJLZG7OSVAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGE2TONJUHE>; > .; > You are receiving this because you authored the thread.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162176427:3174,Message,Message,3174,,https://github.com/google/deepvariant/issues/829#issuecomment-2162176427,1,['Message'],['Message']
Integrability,"/analysis/deepvariant/data:/data -v; XXXXXXXXXXXXXXXXXX/bed:/bed google/deepvariant:0.9.0; /opt/deepvariant/bin/run_deepvariant --model_type=WES; --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12. Results:. ls -ltrh deep_variant_id80429g20/; drwxr-sr-x 2 root root 4.0K Aug 4 12:15 xGENIDTn2_DeepVariant. And I've set the command dynamically:. command:. deep_dir=deep_variant_dynamic1b; mkdir -p /XXXXXXXXXXXXXXXXXXXXX/$deep_dir; docker pull google/deepvariant:0.9.0; # this was ran, some directories censored by XXXXXXXXXX for security reasons; LINE='docker run -it -u `id -u`:`id -g` -v; /XXXXXXXXXXXXXXXXXXXXX/gatk_align_metrics_t/:/input -v; /XXXXXXXXXXXXXXXXXXXXX/$deep_dir/xGENIDTn2_DeepVariant:/output -v; /XXXXXXXXX/deepvariant/data:/data -v /XXXXXXXXXXXXXXXXXXXXX/bed:/bed; google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant; --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam; --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed; --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz; --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12'; echo ""$LINE""; eval $LINE. Results:. ls -ltrh deep_variant_dynamic1b; drwxr-sr-x 2 root root 4.0K Aug 4 12:24 xGENIDTn2_DeepVariant. On Thu, Aug 4, 2022 at 1:59 PM Kishwar Shafin ***@***.***>; wrote:. > hi @IndyHouseGuy <https://github.com/IndyHouseGuy> ,; >; > You can add; >; > docker run -it -v /data:/data \; > -u `id -u`:`id -g`; >; > to your docker command to avoid this issue.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/550#issuecomment-1205591500>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/A2LCPRQWWLAYOZXICW5LXSDVXQAGNANCNFSM55QXIB6A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158:2515,Message,Message,2515,,https://github.com/google/deepvariant/issues/550#issuecomment-1205685158,1,['Message'],['Message']
Integrability,"/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. T",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4281,depend,dependency,4281,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"0f](https://gist.github.com/nonchip/2c93ff2d9bc1bf2cd12bc6e76010da0f). ```bash; # development packages; yum install python-devel python-pip -y. # dependency of numpy 1.14.6; OPT=""-D_XLOCALE_H=1 -O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9460,depend,dependencies,9460,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['dependencies']
Integrability,"1. I haven't seen the `n_elements is zero` message before, but googling it, it looks like a memory allocation problem.; 2. No, 200GB should absolutely be enough for DeepVariant with any setting, including PacBio HiFi. I'm not sure what is going on here, but in case it helps unblock you for now, have you tried doing the haplotagging with whatshap? See the pacbio case study for exactly how we run this workflow: https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-pacbio-model-case-study.md.; I would be curious if you are able to at least follow that case study on your compute setup and see whether that works. Our published PacBio model is based on that workflow with whatshap. @kishwarshafin do you have any ideas about what is going on here?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/490#issuecomment-948755113:43,message,message,43,,https://github.com/google/deepvariant/issues/490#issuecomment-948755113,1,['message'],['message']
Integrability,"1. No - this should not be necessary. This operation is handled by DeepVariant.; 2. There are two arguments being passed here: `--make_examples_extra_args=""split_skip_reads=true, channels=''`; - `--make_examples_extra_args=""split_skip_reads=true` - Turns on operation that splits reads. This is the equivalent of GATK SplitNCigarReads.; - `channels=''` - Disables additional channels from being added. Currently we add an insert_size channel by default, but the RNA-seq model does not currently use this channel.; 3. The RNAseq model can be applied in any region. However, if you look at our [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1.full), you will see that we restrict much of our analysis to CDS regions. The reason for this is that variant calling in CDS regions is a desirable use case where we can maintain relatively high accuracy in RNA-seq data. However, you can use a larger set of regions depending on what the goals of your analysis are. You can also apply filtering to increase precision in these regions (at a cost of reduced sensitivity). See the section `Selecting a cut-off for practical implementation` in our preprint for details on this idea. <img width=""633"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/234003967-3aa5abd4-f8f3-494d-8ada-79eef723548b.png"">. I recommend following along with our [case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-rnaseq-case-study.md) to get started.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/635#issuecomment-1520135737:929,depend,depending,929,,https://github.com/google/deepvariant/issues/635#issuecomment-1520135737,1,['depend'],['depending']
Integrability,"6_64-linux-gnu/libdl.so.2 (0x0000155553032000); 	librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x0000155553028000); 	libcublas.so.12 => not found; 	libcublasLt.so.12 => not found; 	libcudnn.so.8 => /lib/x86_64-linux-gnu/libcudnn.so.8 (0x0000155552dfc000); 	libnvinfer.so.8 => /usr/local/nvidia/lib/libnvinfer.so.8 (0x000015554454d000); 	libstdc++.so.6 => /lib/x86_64-linux-gnu/libstdc++.so.6 (0x000015554436b000); 	libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x000015554421c000); 	libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x0000155544201000); 	libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x000015554400f000); 	/lib64/ld-linux-x86-64.so.2 (0x0000155555526000); ```. When I grep for `libcublas` in the container:; ```stdout; [walid@a100: 1.6.1]$ singularity exec --nv deepvariant_1.6.1-gpu.sif /usr/bin/grep -r -i 'libcublas' /; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE_NAME=""${NV_LIBCUBLAS_PACKAGE_NAME:-""libcublas-11-3""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_VERSION=""${NV_LIBCUBLAS_VERSION:-""11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_PACKAGE=""${NV_LIBCUBLAS_PACKAGE:-""libcublas-11-3=11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_VERSION=""${NV_LIBCUBLAS_DEV_VERSION:-""11.5.1.109-1""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE_NAME=""${NV_LIBCUBLAS_DEV_PACKAGE_NAME:-""libcublas-dev-11-3""}""; /.singularity.d/env/10-docker2singularity.sh:export NV_LIBCUBLAS_DEV_PACKAGE=""${NV_LIBCUBLAS_DEV_PACKAGE:-""libcublas-dev-11-3=11.5.1.109-1""}""; *** bunch more omitted output. I just wanted to show above versions ***; ```. I tried to pull and bind a local install of cublas, but I did not want to make my dependencies too complicated. . TL;DR: please test if `libcublas.so.12` is properly linked. Thank you!. Thank you all for looking into this. Please let me know if I can test or be of help. :smile:. Cheers,; Walid",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060:3619,depend,dependencies,3619,,https://github.com/google/deepvariant/issues/844#issuecomment-2397203060,1,['depend'],['dependencies']
Integrability,"8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WA",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10298,depend,dependency,10298,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,86286081/work; pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work; pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work; requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work; requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work; rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work; scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work; six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work; sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work; tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl; tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl; tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl; tensorflow==2.0.0; tensorflow-estimator==2.0.0; termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work; tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d; toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work; typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work; urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work; Werkzeug==0.16.1; wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work; yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work; zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:7621,wrap,wrapt,7621,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,1,['wrap'],['wrapt']
Integrability,":/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1354,protocol,protocolbuffers,1354,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,":?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated read lengths to generate a sequencer error model -- you don't need to do that, as that cost a lot of money to do properly. You can probably do it with known samples and compare it to golden truth datasets, but again that will cost. The other way to do it is to try other assays to confirm your results. . Understanding all the moving parts takes time. Sure it is very rewarding once you have a complete understanding of a model, but it all depends on how much time is reasonable for you. You can always keep learning after getting the PhD, as that just teaches you how to independently think about big problems and how to systematically break them down (defending your approach). Once you confidently have that thought process down, you're virtually unbeatable and don't need hone that skill as it already is innate. You can always practice that during your postdoc :) . Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:2441,depend,depends,2441,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491,2,['depend'],['depends']
Integrability,"; >; >; > From: Pi-Chuan Chang [mailto:notifications@github.com]; > Sent: Tuesday, April 10, 2018 1:04 PM; > To: google/deepvariant <deepvariant@noreply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380935943:1838,message,message,1838,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943,1,['message'],['message']
Integrability,"; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6011,protocol,protocolbuffers,6011,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6269,protocol,protocolbuffers,6269,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"> . Yes. I didn't get unexpected results when only warning messages are shown. I just wonder whether the warning message would affect the results in some hidden way that I'm not aware of. So I tried to remove the warning message and ended up finding out it's somehow due to the lack of a dependency, locale, in the docker image.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566#issuecomment-1253261845:59,message,messages,59,,https://github.com/google/deepvariant/issues/566#issuecomment-1253261845,4,"['depend', 'message']","['dependency', 'message', 'messages']"
Integrability,"> @aderzelle DeepVariant performs local realignment by default (can be disabled with `--norealign_reads` for the `make_examples` step). Realignment may cause this region to look a bit different than what you see in the original BAM, explaining the candidates that are generated.; > ; > By default, the realigned reads do not get written out, but you can use the flags below to output the realigned reads to a file called `realigned_reads.bam`. These flags can be added to the `make_examples` step so that you can inspect the realigned data.; > ; > `--emit_realigned_reads` - enables writing out of realigned reads; > `--realigner_diagnostics=/path/to/dir` - path to which the reads will be written. Hello, how are we supposed to pass those arguments with the one-step wrapper script? . I tried. ```; docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/${SAMPLE}.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=/output/${SAMPLE}.vcf.gz --output_gvcf=/output/${SAMPLE}.g.vcf.gz --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt-97700"" --make_examples_extra_args=emit_realigned_reads,realigner_diagnostics=/media/urbe/MyADrive/12-03-2020_DeepVariant_NDPD/bam_diagnostics; ```. but runs into. ```; Traceback (most recent call last): ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module> ; app.run(main) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run ; _run_main(main, args) ; File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main ; sys.exit(main(argv)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 304, in main ; commands = create_all_commands() ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 276, in create_all_commands ; sample_name=FLAGS.sample_name)) ; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 182, in make",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/280#issuecomment-598767292:768,wrap,wrapper,768,,https://github.com/google/deepvariant/issues/280#issuecomment-598767292,1,['wrap'],['wrapper']
Integrability,"> @dkurt A quick update:; > ; > I just noticed that the outputs of the multiple runs with OpenVINO are **not** deterministic. (I confirmed by running the same command 10 times on a WES BAM file with use_openvino on); > I actually wonder if there's something weird with the threading code that you added to make the logging more smooth.; > ; > (I have confirmed that without OpenVINO, the results are deterministic. I ran another 10 to make sure all VCFs are exactly the same - which is what I expected).; > ; > I will go ahead and see if I can make OpenVINO runs deterministic by removing the threading code. If you have some ideas why (or why I shouldn't expect it to be deterministic), please let me know. I confirmed that by reverting the changes in https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 , my new 10 runs with OpenVINO are now producing the exactly same VCFs! 🎉; (Still different from without openvino, but that is expected.). @dkurt For this upcoming release, I will just print out a message to warn the users that all the logging information will come out towards the end. We can look into improving the logging in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-737655909:1046,message,message,1046,,https://github.com/google/deepvariant/pull/363#issuecomment-737655909,1,['message'],['message']
Integrability,> @husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?. I closed the container. . > @husamia Basically what you are seeing is that you're running out of memory. Try it on a machine with more memory. that's a good point. is there an option to specify the temp folder? this would solve my problem.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767600297:132,message,messages,132,,https://github.com/google/deepvariant/issues/412#issuecomment-767600297,1,['message'],['messages']
Integrability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. Sorry, I am a beginner of Docker. I've learned something about docker technology in the past two days, figuring out what you mean. Now I can build an iamge that does not depend on operating system of my device. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761:133,rout,route,133,,https://github.com/google/deepvariant/issues/591#issuecomment-1334703761,4,"['depend', 'rout']","['depend', 'route']"
Integrability,"> @leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that?; > ; > This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04. thanks@danielecook. Yes, I am able to run DV using Docker, which did work.; I need to debug some parts of this project to better understand it, so I have to build it from source.; Do you mean there is a route to build DV using Docker or Singularity?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277:133,rout,route,133,,https://github.com/google/deepvariant/issues/591#issuecomment-1329988277,2,['rout'],['route']
Integrability,> @williamrowell Can you check whether your CPU supports AVX instruction?. This is the likely cause. No AVX support on this node. Seems odd that there's a core dump with no error message. ```bash; wrowell@mp0608-sge:~$ lscpu | grep Flags; Flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 popcnt aes lahf_lm tpr_shadow vnmi flexpriority ept vpid dtherm ida arat; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774782391:179,message,message,179,,https://github.com/google/deepvariant/issues/419#issuecomment-774782391,1,['message'],['message']
Integrability,"> Could be the stupidest advice in the universe but she you looking st your vcf file.or thinking if the necwr comment to send back to me, Literally just thought talking through the problem might help, fellow human, and no I didn't check the organism, you could have told me and I would gkne the ncbi datanae downloaded the genomes aligned them checked your region if interest don't worry if I see your name on the email thread on this public github repository I won't reply and I'll loom forward your paper on bioarvix hopefully, Honestly all the best, And if you don't care about prokaryotes then fair enough, Joe; > […](#); > On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote: The name of the organism has been said in this thread, that you are unable to find it, and believe we deal with a prokaryote is pathetic, really. Why would we bother with your stupid advice when you didn't even take the time to read the thread? — Reply to this email directly, view it on GitHub <[#682 (comment)](https://github.com/google/deepvariant/issues/682#issuecomment-1658768123)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ> . You are receiving this because you commented.Message ID: ***@***.***>. There are so many trolls it's difficult to know when someone is just clumsy. I am willing to give you the benefit of the doubt. Buit really you could have read the messages above, or message me to know what we are talking about? If this discussion is in public, it's indeed to attract interest of others. But just read 2 minutes without suggesting the first spontaneous idea you have. Which is not idiot in itself but that's something we though of if ... 2014 in my memory serves me well ^^. Allez, useless to have petty fight and it's not a good look. I might have overreacted to a genuine sympathetic comment.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658794438:1239,Message,Message,1239,,https://github.com/google/deepvariant/issues/682#issuecomment-1658794438,3,"['Message', 'message']","['Message', 'message', 'messages']"
Integrability,"> Hello Ferdinand,; > ; > I have a few questions which may help us to understand whether this would be expected or not. First, can you tell me how many variants are Refcall in this sample (zcat Sample.final.vcf.gz | grep RefCall | wc -l) and how many variants are PASS in this sample (zcat Sample.final.vcf.gz | grep PASS | wc -l); > ; > Second, is it possible for you to point me to the capture regions that you used (the S07604514 BED file) or, if that is not possible, for you to tell me how many bases it covers.; > ; > Knowing this information will help understand whether the number of variants are within expectations, whether they are a function of something about the sample, or whether there is some other issue to address. Generally, the commands do not seem in error.; > ; > Thanks,; > Andrew. Thanks for your quick response. I have 1191 RefCall variants and 10972 PASS variants in the final VCF file. We sequenced the sample NA12878 from the HapMap project, for benchmarking it with hap.py against the GIAB reference data.; As you mentioned we used the Agilent SureSelect Human All Exon V6 r2 Bedfile (S07604514 BED file), which is also attached to this message. [S07604514_Regions.txt.gz](https://github.com/google/deepvariant/files/2914627/S07604514_Regions.txt.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/158#issuecomment-468228631:1167,message,message,1167,,https://github.com/google/deepvariant/issues/158#issuecomment-468228631,1,['message'],['message']
Integrability,"> Hi @Axze-rgb,; > ; > Yes, you want to only look at your `./.` and `0/0` genotypes, because the subclonal variants will likely not look like true germline ones, and probably fall into the category of not being called or a reference call. Keep in mind the model was created with a specific type of variation in the dataset as it was minimizing the loss function to optimizes the model's weights, in order to have the variant caller's GQ values correlate maximally with the GQ scores derived empirically from GIAB ground truth. So your tensor images might not look like optimal ones that the model would recognize as a true germline variant, but that discrepancy might have significant statistical stability above the noise region, of which you can take advantage of. That is what your are trying to test for and eventually model, with the hope to show that the PacBio model can be generalized for more dataset types − that's the basic hypothesis here.; > ; > So here's a quick way you can fix the multiallelic issue above:; > ; > 1) First split the multiallelic sites into biallelic records like this:; > ; > ```; > bcftools norm -m - multi_allelic.vcf > biallelic.vcf; > ```; > ; > 2) Then parse for the `0/0` and `./.` genotypes − I'm assuming your genotypes are not phased:; > ; > ```; > bcftools query -f '[%GT,%GQ,%VAF]' biallelic.vcf | grep '\./\.\|0/0' | cut -f2,3 -d',' > gq_vaf.csv; > ```; > ; > Then just load it up in R with `read.csv(""gq_vaf.csv"")`, and plot it. There is a lot more error modeling you will need to do to generate the confidence intervals, which would probably require more data.; > ; > Hope it helps, ~p. For the record there is an issue with your second code, only the first is generated by cut, or it depends on the unix system. ; It's easy to fix by asking bcftools itself to make the new line; > bcftools query -f '[%GT,%GQ,%VAF\n]' biallelic.vcf | grep '\./\.\|0/0' > gq_vaf.csv. I know you probably did it from memory, but in case someone else finds the thread ;)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762:1732,depend,depends,1732,,https://github.com/google/deepvariant/issues/682#issuecomment-1651199762,1,['depend'],['depends']
Integrability,"> Hi @ZuyaoLiu; > ; > When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized.; > ; > We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu; > ; > and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well.; > ; > @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not?; > ; > And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)?; > ; > Thank you!. Hi @pichuan ,. I finished testing with a trio from a non-model species, and found a huge difference. Basically, under the default human model, V 1.6 produces more sites violating mendelian rules than V1.5. The post is here #726 .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1793557479:74,message,message,74,,https://github.com/google/deepvariant/issues/722#issuecomment-1793557479,3,['message'],"['message', 'messages']"
Integrability,"> Hi @aderzelle; > ; > I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position.; > ; > Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md); > ; > In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure. Hello, in my case, which is not humans, I find the following. First, the reads I used are the same used during the reference genome assembly process. Therefore, any new homozygous variant with a vaf of ~ 1 is either the reflect of assembly errors or mapping errors. I do find such variants. But I am more intrigued by the peak of Hom (x/x) at a vaf of ~ 0.5 . am I right to assume this is not typical and might reflect a problem? ; Interestingly for the reference calls, there also seems to be 2 peaks, one with a vaf around 0.2 which I guess is all right and one with a vaf around 0.5 which I guess also indicates a potential problem. ![genotypes_deepvariant](https://user-images.githubusercontent.com/23341393/72908229-7db81b00-3d35-11ea-99f9-e3dfa126a127.png). As I am working with an asexual diploid, I can't replicate the methodology of the mosquito to retrain deepvariant. I, however, have an ancestral population an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-577247342:1018,depend,depending,1018,,https://github.com/google/deepvariant/issues/257#issuecomment-577247342,1,['depend'],['depending']
Integrability,"> Hi @aditya-88, thanks for filing this bug! We will look into both 1) using disutils.spawn and 2) improving the error message. Thank you!; It'll make things easier for the prospective users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160#issuecomment-470924522:119,message,message,119,,https://github.com/google/deepvariant/issues/160#issuecomment-470924522,1,['message'],['message']
Integrability,"> Hi @husamia; > ; > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine.; > ; > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia; > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400#issuecomment-749549925:45,message,message,45,,https://github.com/google/deepvariant/issues/400#issuecomment-749549925,1,['message'],['message']
Integrability,"> Hi @husamia; > ; > The openVINO acceleration occurs at the call_variants stage, not during make_examples. If you are running from the Docker image, each stage should report its runtime in standard error. I would use that reported time for the call_variants stage to assess whether there is any improvement. Is performance at the make_examples stage is dependent on the disk read/write?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/408#issuecomment-766349948:354,depend,dependent,354,,https://github.com/google/deepvariant/issues/408#issuecomment-766349948,1,['depend'],['dependent']
Integrability,"> Hi @yangyxt , can you try a command like: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity ?. Thanks for the response. I tried a command without --env argument in the very beginning and the warning logs were still what is in the screenshot above. Then I started to alter the env variable with --env argument and still got the same warning messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584:393,message,messages,393,,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584,1,['message'],['messages']
Integrability,"> I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; > Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; > (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); > (2) Is this reliably reproducible on the same input?; > ; > Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it. Dear pichuan,. It is reliably reproducible with the same input (4x) (16x CPU, 30GB RAM).; When I run on a server with 32x CPU and 60GB RAM I got similar error:. ```; I1030 20:48:33.115523 140410104575744 make_examples.py:1167] Found 487 candidate variants; I1030 20:48:33.116027 140410104575744 make_examples.py:1168] Created 499 examples; real	3m0.610s; user	74m46.176s; sys	3m15.360s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 63 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/dnanexus/ref.fa"" --reads ""/home/dnanexus/input.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --regions ""/home/dnanexus/regions.bed"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@64.gz"" --task {}' returned non-zero e",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232#issuecomment-548709854:357,message,message,357,,https://github.com/google/deepvariant/issues/232#issuecomment-548709854,2,['message'],"['message', 'messages']"
Integrability,"> I'm happy to hear you have enjoyed my YouTube videos :); > ; > Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names.; > ; > I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :); > ; > Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?. for your last question here I've tried to run with region ""chr1:450739-451678"" from the `idt_capture_novogene.grch38.bed` file and it produced vcf files both for the test bam file in the WES run and for my personal bam file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917356380:111,message,message,111,,https://github.com/google/deepvariant/issues/483#issuecomment-917356380,1,['message'],['message']
Integrability,"> Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. @williambrandler You can try v1.1.0, which builds on 18.04: https://github.com/google/deepvariant/tree/r1.1. In v1.2.0, we updated to Python3.8. When I tested this, I was trying to get Python3.8 to work on Ubuntu18.04, but didn't quite get it to work. Not saying it's impossible, but we think it was easiest to update the whole Dockerfile setup to run on Ubuntu20.04 rather than trying to get it to also work on 18.04. > ; > Would also like some clarification on this statement to help me figure out what is going on,; > ; > `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`. For the build-prereq.sh statement -- what it means is: As part of that script, it runs https://github.com/google/deepvariant/blob/r1.2/tools/build_clif.sh which builds [CLIF](https://github.com/google/clif) from scratch. CLIF is required when you build DeepVariant code. The build_clif.sh script installs a bunch of stuff on your machine. If you directly run ./build-prereq.sh on your machine, you just need to be aware that those things are installed as a side effect. But, if you're using our Dockerfile and using docker build, we only carry over the built binaries to the next stage. So the extra things installed by CLIF wasn't carried over. Hopefully that's more clear?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896293199:39,depend,dependencies,39,,https://github.com/google/deepvariant/issues/476#issuecomment-896293199,4,['depend'],['dependencies']
Integrability,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417185546:173,depend,dependencies,173,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546,2,['depend'],['dependencies']
Integrability,"@AndrewCarroll 2 was exactly what I meant, thank you!. About accessing pre-logit layer, I understand that is how you do under the hood, yet, is there any user interface through CLI or Python Module that I could use. As far as I know, in order to use this `endpoint['PreLogits']` approach, I would need to fork the DeepVariant and change the output, am I missing something?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371#issuecomment-717204430:159,interface,interface,159,,https://github.com/google/deepvariant/issues/371#issuecomment-717204430,1,['interface'],['interface']
Integrability,"@AndrewCarroll Thanks for the thorough response. This was extremely helpful. Also re the haplotype caller comparison: I thought DeepVariant also uses a PairHMM to score haplotypes to re-align reads to, and then feeds that to the CNN. . From the paper:; > The likelihood function used to score haplotypes is a traditional pair HMM with fixed parameters that do not depend on base quality scores. This likelihood function assumes that each read is independent. Finally, each read is then realigned to its most likely haplotype using a Smith–Waterman-like algorithm with an additional affine gap penalty score for homopolymer indels. So both methods use PairHMM to score haplotypes, and assist in re-aligning reads, ya? After that, the similarity is nil between the methods as you mentioned. . Sorry if that was implicit in your response, but wanted to double check I understand.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488154062:364,depend,depend,364,,https://github.com/google/deepvariant/issues/180#issuecomment-488154062,1,['depend'],['depend']
Integrability,"@Dani-kolbe how you filter will be dependent on the type of analysis and how strict you want to be. Can you clarify - are all of these variants listed considered false positives or just the first two (low coverage) sites?. `GQ` assesses genotype quality. It will be correlated with depth, but is also impacted by other factors, and as you see the `GQ` can still be high even for some variants with low depth. Can you apply filteres that target low depth or missingness?. For example:. * Filter sites with high missing rates (e.g. > 20% of samples with a `./.`).; * Filter sites where the average depth per sample is < 5.; * Do you expect higher coverage at these sites based on your WES setup? You can also consider filtering for sites that your WES assay is expected to capture.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645#issuecomment-1542581479:35,depend,dependent,35,,https://github.com/google/deepvariant/issues/645#issuecomment-1542581479,1,['depend'],['dependent']
Integrability,"@ErinKinghorn somewhat confusingly, shard specifies are 0-based for the first number (shard index) and 1-indexed for the second (count). So your first shard is `00000-of-00032` and your last shard is `00031-of-00032`. Can you confirm that you indeed are observing only 31 output files?. The message you report here looks normal - and the warning should not make a difference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776#issuecomment-1967496024:291,message,message,291,,https://github.com/google/deepvariant/issues/776#issuecomment-1967496024,1,['message'],['message']
Integrability,"@HagenC I want to clarify a bit more. Even though you cannot currently modify the value of `min_base_quality`, you CAN modify the existing flags, depending on how you are running DeepVariant. I was not sure what you meant by the GCP cloud shell. Are you running on one machine yourself or following [this tutorial](https://cloud.google.com/genomics/docs/tutorials/deepvariant)? . If you are running on one machine, you can take a look at [the WGS case study](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-case-study.md). This is an older version of the document that shows how to pass different flags to each step (`make_examples`, `call_variants`, `postprocess_variants`). You can find the current version of the document [here](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md), but this uses Docker and does not show how to run each step with additional flags.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/135#issuecomment-453232080:146,depend,depending,146,,https://github.com/google/deepvariant/issues/135#issuecomment-453232080,1,['depend'],['depending']
Integrability,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:59,depend,depends,59,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106,2,['depend'],['depends']
Integrability,"@MorganHow does your reference match the BAM file you are using? From looking at this error message, my first thought is that your BAM file is mapped to a different reference.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/372#issuecomment-717644098:92,message,message,92,,https://github.com/google/deepvariant/issues/372#issuecomment-717644098,1,['message'],['message']
Integrability,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414:536,synchroniz,synchronization,536,,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414,1,['synchroniz'],['synchronization']
Integrability,"@akolesnikov ; Thank you for the answer. All paths (both directories and fiels) that I included in my script do exist. Still, it shows me an error as file doesn't exist as I wrote in my first message above",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/577#issuecomment-1284441913:192,message,message,192,,https://github.com/google/deepvariant/issues/577#issuecomment-1284441913,1,['message'],['message']
Integrability,@andrewrech Thanks for reporting this! How are you running DeepVariant? Could you post the full output of the error message you saw because of this issue? Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/146#issuecomment-461153551:116,message,message,116,,https://github.com/google/deepvariant/issues/146#issuecomment-461153551,1,['message'],['message']
Integrability,"@ashraf123456789 I don't think the commands should vary. Could you check that you have set `${BIN_VERSION}`? I have seen this error message when the variable is not set, causing the image name to be incorrect. You could try running the following:. ```; BIN_VERSION=""0.8.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=4; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/181#issuecomment-489226839:132,message,message,132,,https://github.com/google/deepvariant/issues/181#issuecomment-489226839,1,['message'],['message']
Integrability,"@brentp Thanks for the suggestion! As @AndrewCarroll mentioned, we'll look into adding it.; Adding it to our Docker image should be quite straightforward. I have one more question:; Can you provide a specific example usage you have in mind? ; You mentioned bam -> cram. I wonder if you're expecting being able to call samtools manually from our docker image, or do you actually expect something integrated into the one-step run_deepvariant.py script.; In addition to adding it to our Docker image, it'll be nice for us to add corresponding documentation as well. So I'm trying to think through how our users would like to use this. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/414#issuecomment-768599474:395,integrat,integrated,395,,https://github.com/google/deepvariant/issues/414#issuecomment-768599474,1,['integrat'],['integrated']
Integrability,"@crazysummerW . As Pi-Chuan mentioned, DeepConsensus is integrated into the Revio system; so, you will get DeepConsensus reads directly from that system. The `n1000.subreads.bam` demo dataset being discussed here is from Sequel II. It is a small number of reads from the human genome. You should be able to push it through the mechanical steps of alignment and variant calling, but the results will be limited by coverage. To figure out which mechanical step is broken here, I would recommend to pass the FASTQ directly rather than through a `fofn` to be more explicit. The pbmm2 alignment should have input of HiFi reads, not subreads. So, a typical aligned file name would be `REF.hifi_reads.bam` without a mention of subreads. > pbmm2 align hs37d5.fasta ${shard_id}.output.fastq aligned.bam --sort",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672#issuecomment-1616079783:56,integrat,integrated,56,,https://github.com/google/deepvariant/issues/672#issuecomment-1616079783,1,['integrat'],['integrated']
Integrability,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:186,message,messages,186,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,"['depend', 'message']","['dependencies', 'messages']"
Integrability,"@ekofman Currently, the case studies (and corresponding scripts) are used to show an example of how to run DeepVariant. We showed an example of how to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:1004,depend,depending,1004,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,2,['depend'],['depending']
Integrability,"@f-ferraro,. multiprocessing is the standard Python module which DeepVariant depends on. Without a proper Python environment I'm afraid there is no way to make it work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1819531780:77,depend,depends,77,,https://github.com/google/deepvariant/issues/733#issuecomment-1819531780,1,['depend'],['depends']
Integrability,@husamia Can you see if any earlier logs have more information? See if you can find another Traceback with more informative error messages?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767302975:130,message,messages,130,,https://github.com/google/deepvariant/issues/412#issuecomment-767302975,1,['message'],['messages']
Integrability,"@kalexiou . `call_variants` is a binary in our Docker image too. So you can run:; ```; docker run google/deepvariant:1.1.0 /opt/deepvariant/bin/call_variants --help; ```; to look at all relevant flags. Your command should be probably something like:; ```; ...; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@16.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt""; ```. The checkpoint should be either wgs, wes, or pacbio depending on which type you were using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/427#issuecomment-790376285:558,depend,depending,558,,https://github.com/google/deepvariant/issues/427#issuecomment-790376285,1,['depend'],['depending']
Integrability,@leedchou is it possible for you to install Docker and/or singularity and run DeepVariant using that? . This may be an easier route than enabling builds on Ubuntu16.04. Currently we only build/test on Ubuntu 20.04.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517:126,rout,route,126,,https://github.com/google/deepvariant/issues/591#issuecomment-1329920517,1,['rout'],['route']
Integrability,"@mvelinder I noticed that you're running on t2 micro. And it seems like you're not running the Quick Start, but actually running on our WES BAM file. Given that t2 micro only has 1G RAM, make_examples likely ran out of memory. ; DeepVariant's error message when running OOM can be a bit confusing. This is something we still hope to improve. Can you try a machine with more RAM? If you're just running a small example like the Quick Start:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. I've confirmed that **t2.medium** worked for just the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md). After copying all the data, I ran:; ```; time sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; In my test run on a **t2.medium** instance, this took: ; ```; real0m23.790s; user0m0.032s; sys0m0.028s; ```; to complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462#issuecomment-866404518:249,message,message,249,,https://github.com/google/deepvariant/issues/462#issuecomment-866404518,1,['message'],['message']
Integrability,"@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363230217:1622,message,message,1622,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217,2,['message'],['message']
Integrability,@oschwengers DeepVariant was included in [this bacterial variant calling benchmark](https://doi.org/10.1093%2Fgigascience%2Fgiaa007). tl;dr results were quite varied depending on the species and reference genome distance from the sample.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183#issuecomment-1542901689:166,depend,depending,166,,https://github.com/google/deepvariant/issues/183#issuecomment-1542901689,1,['depend'],['depending']
Integrability,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:162,message,messages,162,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,2,['message'],"['message', 'messages']"
Integrability,"@pgrosu Thank you for the reply! I wouldn't have thought to install qemu and colima before. I tried your instructions and added `brew install docker` as well. Successfully started colima.; ```; > colima start --arch x86_64 --cpu 4 --memory 8 --disk 20 --cpu-type Broadwell-v4; INFO[0000] starting colima ; INFO[0000] runtime: docker ; INFO[0000] preparing network ... context=vm; INFO[0000] starting ... context=vm; INFO[0073] provisioning ... context=docker; INFO[0074] starting ... context=docker; INFO[0092] done ; ```; The docker run `docker run --platform linux/amd64 google/deepvariant:1.5.0` seems to be incomplete.; The first installation, it gave this message `Unable to find image 'google/deepvariant:1.5.0' locally; 1.5.0: Pulling from google/deepvariant` and after the pull complete message and the `Status: Downloaded newer image for google/deepvariant:1.5.0`, it was stucked: ; ```; 2023-06-04 16:15:06.784293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; ```; and finally gave `ERRO[1898] error waiting for container: `; I stopped colima and rerun `docker run --platform linux/amd64 google/deepvariant:1.5.0`. it has been stuck like this for over 30 minutes with the same message:; ```; 2023-06-04 16:36:19.453896: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags. ERRO[3173] error waiting for container:; ```; Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623:661,message,message,661,,https://github.com/google/deepvariant/issues/657#issuecomment-1575644623,3,['message'],['message']
Integrability,"@pgrosu Thanks Paul, we appreciate the close eye you've given to the codebase. Unfortunately managing all of the dependencies is challenging, and we aren't super happy with how complex the build/run prereqs scripts are. And we agree with you that there's a lot of opportunity to explore extensions, simplifications, and optimizations of DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361436703:113,depend,dependencies,113,,https://github.com/google/deepvariant/issues/41#issuecomment-361436703,2,['depend'],['dependencies']
Integrability,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:1150,depend,dependency,1150,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461,1,['depend'],['dependency']
Integrability,"@pichuan My goal is to use **long reads** of E.Coli in order to evaluate the variant calling from such type of reads. Most of my knowledge on bioinformatics came from reading the [quick-start](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md) and [model training](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-model-training.md) of DeepVariant, and a little on the internet. From what you wrote to me earlier I understand there's a problem with my `truth_variant` files, since it doesn't have GT fields. In that case what I need to do next so I could run the training successfully?; Do I need to find a different source for the VCF and BED files? Do I need to find new reads too? Do the `truth_variants` and `reads` depend on each other in any way?. I hope this explains my situation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128#issuecomment-450850276:766,depend,depend,766,,https://github.com/google/deepvariant/issues/128#issuecomment-450850276,1,['depend'],['depend']
Integrability,"@pichuan Thank you so much. I have no root in my local machine, so I start a deepvariant:1.5.0 docker container, and in this container, I git the source code and try to build it from source. The reason why I need to build it from source is that I want to change some source code of deepvariant(like pileup image code). ; I run the code:; ```; cd /root/clif/build; cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; ```; and the problem is also:; ```; -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```; I guess if I don't prebuilt protobuf before that? However, I found the build-prereq.sh have installed protobuf, I don't know the reason.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823759004:598,message,message,598,,https://github.com/google/deepvariant/issues/739#issuecomment-1823759004,1,['message'],['message']
Integrability,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error ; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazel; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'. And i tried the installation and got this:. solokopi@solokopi-All-Series:~$ sudo apt-get install bazel; Reading package lists... Done; Building dependency tree ; Reading state information... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; E: Unable to locate package bazel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415936477:428,depend,dependency,428,,https://github.com/google/deepvariant/issues/89#issuecomment-415936477,1,['depend'],['dependency']
Integrability,"@pichuan thank you for your prompt response, i ran bazel as you instructed and and got the following error solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ bazelUnexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; And i tried the installation and got this:; solokopi@solokopi-All-Series:~$ sudo apt-get install bazelReading package lists... DoneBuilding dependency tree       Reading state information... DoneN: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extensionE: Unable to locate package bazel. On Friday, August 24, 2018, 8:40:23 AM PDT, Pi-Chuan Chang <notifications@github.com> wrote: ; ; ; From the log, it seems like the issue is that bazel was not installed.; After you run build-prereq.sh, can you try typing in bazel as a command and see if it exits?; And, if install bazel failed for you, can you paste the part of log of how the installation failed for you?. By the way, is there a reason why you want to build your own binaries? In the latest instructions, we moved to using docker directly. I'm curious whether that worked for you or not. Would love to know if there's a reason to prefer building your own binaries. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-415936563:420,depend,dependency,420,,https://github.com/google/deepvariant/issues/89#issuecomment-415936563,1,['depend'],['dependency']
Integrability,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-643473196:154,inject,injects,154,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196,1,['inject'],['injects']
Integrability,"@richard-nm Can you paste your command too?. I wonder if you're using older code with newer models. In 1.3.0, we used to have this file that specifies the shape in 3 integers:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt.input_shape; 100 221 6; ```. In the later moment, we changed the format. For example, 1.4.0:. ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.5.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]}; ```; (We also added one more channel, which is why the shape is now 100 221 7.). From the error messages you're getting, it seems like you're using code version older than 1.4.0.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627#issuecomment-1512358132:708,message,messages,708,,https://github.com/google/deepvariant/issues/627#issuecomment-1512358132,1,['message'],['messages']
Integrability,"@scott7z Is it possible to add an error message for this? Seems to be a common issue among users. @holgerbrandl as mentioned in #16, you can build the image from source that conforms to your environment. We prefer to provide 1 image with reasonable optimization settings. You may also consider using [Container-optimized OS](https://cloud.google.com/container-optimized-os/docs/) on Google Cloud to easily explore the tool.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/39#issuecomment-358337849:40,message,message,40,,https://github.com/google/deepvariant/issues/39#issuecomment-358337849,1,['message'],['message']
Integrability,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353510712:310,depend,dependencies,310,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712,6,['depend'],"['dependencies', 'dependency']"
Integrability,"@simoncchu can you provide more details: your singularity version and operating system version, the actual error message you're seeing, etc.; Anything that will help us reproduce or understand your issue will be helpful.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-766990007:113,message,message,113,,https://github.com/google/deepvariant/issues/296#issuecomment-766990007,1,['message'],['message']
Integrability,"@sophienguyen01 , . If you are looking for INDEL purity then `--select_variant_types='indels'` should work well I believe? You are losing some INDELs but ultimately you are creating a training set that contains only INDELs. You can also generate more training samples by downsampling the bam and re-running `make_examples`. On the other hand, if you think about inference pipeline, when you are in prediction mode, you can't switch your `--select_variant_types` to something else than your training data as the model will now have to deal with data it has never seen before. There can be two scenarios:. 1) You train a model with `--select_variant_types='indels'` and after the model is trained, you are running inference with `--select_variant_types='indels multi-allelics'`. Then the model will have to deal with mult-allelic variants it has never seen before so the prediction on those would expected to be poor. 2) You train a model with `--select_variant_types='indels multi-allelics'` and allow a few snps to be present in the training. In which case during your inference or prediction, the model will be confident on all indel cases. It depends on what exactly you are trying to do, but, my suggestion would be to use `--select_variant_types='indels multi-allelics'` for your purpose.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/813#issuecomment-2091275266:1145,depend,depends,1145,,https://github.com/google/deepvariant/issues/813#issuecomment-2091275266,1,['depend'],['depends']
Integrability,"@yangyxt was this resolved?; From the original error message, it seems to me that the input to call_variants was truncated. Which means that your make_examples run might have not been fully succeeded. Another possible issue is: If you happen to have multiple make_examples running and overwriting the same files, you also might have corrupted output from make_examples (which will cause the call_variants step to err out.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564#issuecomment-1251331616:53,message,message,53,,https://github.com/google/deepvariant/issues/564#issuecomment-1251331616,1,['message'],['message']
Integrability,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-354748344:182,wrap,wrapper,182,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344,4,['wrap'],"['wrapper', 'wrapping']"
Integrability,A final update. We've just improved the error messages for these cases in the internal version. The next release of DeepVariant should have this fix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28#issuecomment-354390647:46,message,messages,46,,https://github.com/google/deepvariant/issues/28#issuecomment-354390647,1,['message'],['messages']
Integrability,"A summary of last trials:; 1. Downloaded fasta file again from repository (since it had an error with chr1:4655405-4655474); 2. Ran again the WES run with the original files ; 3. Got again the ""parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameter",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917442547:605,message,message,605,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547,1,['message'],['message']
Integrability,"A%2F%2Fgithub.com%2Famyhouseman&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=r2WZK5XXX5pgimGQX1ckdqp3N6Cyk1wXH92kGK00jKA%3D&reserved=0> ,. Please use: GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz. You can read further explanation of why this is the best version to use in this blog by Heng Li: https://lh3.github.io/2017/11/13/which-human-reference-genome-to-use<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Flh3.github.io%2F2017%2F11%2F13%2Fwhich-human-reference-genome-to-use&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=eEiHxUnxYv8doc%2F5aVYE%2BH0tGcKjhXbXSsKtMkSBbUQ%3D&reserved=0>. —; Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fgoogle%2Fdeepvariant%2Fissues%2F549%23issuecomment-1200473679&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=oDUjWabQjt0eM86LLzt%2BsJ5ERoJ0BBRm5863uX0qH4w%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FANUP4ZC4YG3AONVCKJ7RT53VW27C7ANCNFSM55E3G3QA&data=05%7C01%7C%7C2692a76d3d554223df0208da73209c7f%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C637948881139900777%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=Ci6wL9MBJUn9xtmb4eLWiMAZi%2BFEFI03EvFd2jZp0rc%3D&reserved=0>.; You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320:2397,Message,Message,2397,,https://github.com/google/deepvariant/issues/549#issuecomment-1200474320,1,['Message'],['Message']
Integrability,"ARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT'",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2698,depend,dependency,2698,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"Actually I have a suggestion:; Can you directly run the failed command. `./opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210124.deduped.bam --examples /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/make_examples.tfrecord@6.gz --sample_name A210124 --sample_name_parent1 A210126 --channels insert_size --gvcf /paedyl01/disk1/yangyxt/test_tmp/singularity_inter_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/gvcf.tfrecord@6.gz --normalize_reads --pileup_image_height_child 100 --pileup_image_height_parent 100 --regions /paedyl01/disk1/yangyxt/indexed_genome/hg19/ucsc.hg19.no_dad.bed --task 0`. Without using the one-step run_deeptrio. I now remember that the way I wrote that script can make the underlying error unclear, and as a result hard to debug when an error occurs. If you directly run the failed job, you should get more helpful error message. Sorry for the inconvenience. I believe I improved that for run_deepvariant.py a while ago but haven't done the same for run_deeptrio.py.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1547153842:1121,message,message,1121,,https://github.com/google/deepvariant/issues/646#issuecomment-1547153842,1,['message'],['message']
Integrability,"Actually those messages might not be a warning, ""The above is not a warning and is just a point of information.""; https://discuss.tensorflow.org/t/tensorflow-with-proper-compiler-flag-error-message/12393/3. Now I will try with the test quickstart run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575652409:15,message,messages,15,,https://github.com/google/deepvariant/issues/657#issuecomment-1575652409,2,['message'],"['message', 'messages']"
Integrability,"Ah I see. Sorry I missed that part in your original message.; And, I think I understand your question better now. . --intermediate_results_dir isn't designed to capture all temp files from DeepVariant. It's for capturing the intermediate outputs (from make_examples, call_variants) in case that users need to re-use them later on. In your case, using your workaround of setting TMPDIR actually makes sense to me. From your description, it also seems like it's related to your system setting. If you think this is going to be a common issue, please share your command and I'm happy to add it to our documentation as a workaround for other users.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524#issuecomment-1067587398:52,message,message,52,,https://github.com/google/deepvariant/issues/524#issuecomment-1067587398,1,['message'],['message']
Integrability,"Also, if I try to follow your instructions for an EC2 instance, I can't actually create a _t1.micro_ EC2 instance, but I can create a **t2.micro** EC2 instance (with 1 vCPU and 1 GB of RAM). I remember spending some effort to try and install Docker on the EC2 instance. That installation is quite quick, and I almost wondered if I hadn't somehow didn't test `sudo yum install docker`. However, there was slow-down after pulling the docker image (possibly due to the type of instance?). If I sign into another terminal and try to run `docker images` (to confirm that everything worked OK), I get the following error message:. `Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.38/images/json: dial unix /var/run/docker.sock: connect: permission denied`. Strictly speaking, I was running the script rather than just the pull command. So, when the 1st terminal stopped running (in approximately 5-10 minutes), I still got the same error message that I started with:. ```; sudo sh run_deepvariant.sh; Unable to find image 'gcr.io/deepvariant-docker/deepvariant:latest' locally; latest: Pulling from deepvariant-docker/deepvariant; 18d680d61657: Pull complete; 0addb6fece63: Pull complete; 78e58219b215: Pull complete; eb6959a66df2: Pull complete; 54de1d38bbd7: Pull complete; d17c3563217d: Pull complete; ba1bdbdefce9: Pull complete; 94eba53c4ad9: Pull complete; 413f494b0501: Pull complete; 4d89363e7fb4: Pull complete; e9213d1ccf36: Pull complete; fb6121657d6b: Pull complete; Digest: sha256:705523e7bc241e0d3e1e57d4d338c83e289d51a8231c09b0d2dee03c015cee0f; Status: Downloaded newer image for gcr.io/deepvariant-docker/deepvariant:latest; docker images; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. _[I entered ""docker images"" when I didn't see anything, so that is input, not output]_. However, I then get that ""docker.sock"" error message if I try to run `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480642492:615,message,message,615,,https://github.com/google/deepvariant/issues/167#issuecomment-480642492,2,['message'],['message']
Integrability,"Also, if you can show us the output of; cat /proc/cpuinfo; that will help. It's possible the pre-compiled binaries require a newer CPU than you are using,; which could explain the illegal instruction message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-351842327:200,message,message,200,,https://github.com/google/deepvariant/issues/16#issuecomment-351842327,1,['message'],['message']
Integrability,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-482393946:139,message,message,139,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946,6,['message'],"['message', 'messages']"
Integrability,"And @melop , the most recent time I tried to set up a GPU machine was using these steps:. https://github.com/google/deepvariant/issues/745#issuecomment-1840177389. I don't think I've seen the `failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected` error message you saw. Based on that error message I did a bit internet search. I wonder if this is relevant: https://stackoverflow.com/a/48715413 Specifically , try setting CUDA_VISIBLE_DEVICES to 0, by running `export CUDA_VISIBLE_DEVICES=0` and see if that works for you?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761#issuecomment-1890184989:280,message,message,280,,https://github.com/google/deepvariant/issues/761#issuecomment-1890184989,2,['message'],['message']
Integrability,"And there's bacteria with 4 chromosomes.... On Mon, 31 Jul 2023, 17:50 Joe, ***@***.***> wrote:. > You said it was random clonal organism, il have a look when the reads are; > on the sra database, was just trying to help,; >; > And yes I don't care about SNPs sorry.; >; > (Google people) I'll be here to offer random advice to other people if I'm; > still allowed,; >; > Joe; >; > On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:; >; >> Ok so you have no idea what we are talking about, this is not a; >> prokaryote. I would suggest you avoid hopping into conversations for making; >> completely ignorant comments, not even bothering to check about what; >> organisms we are dealing with.; >>; >> —; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; >> .; >> You are receiving this because you commented.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658765720:1021,Message,Message,1021,,https://github.com/google/deepvariant/issues/682#issuecomment-1658765720,1,['Message'],['Message']
Integrability,"Andrea;; Thanks for following up and for all the additional details. It's a bit strange, as all the file paths in your error messages look like it got installed under python 3.6 rather than 2.7. It's hard to tell if that's just coming from the conda machinery or indicative of a different problem. . You could try installing in a separate environment as a first pass to avoid any conflicts:; ```; conda create -y -n deepvariant python=2.7 -c bioconda -c conda-forge deepvariant 'google-cloud-sdk<243.0.0'; ```; If that still has the same issue, then we'd need to dig more into the `post-link.sh` errors you're seeing. In this step deepvariant is downloading the trained model files from GCP, which can sometimes have internet issues or other problems. This previous post has some suggestions for debugging it:. https://github.com/google/deepvariant/issues/177#issuecomment-504940567. Hope this helps get it resolved and get deepvariant running for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566496481:125,message,messages,125,,https://github.com/google/deepvariant/issues/252#issuecomment-566496481,1,['message'],['messages']
Integrability,Anna;; Apologies about the issues. If you also include the conda-forge channel in your install it should resolve cleanly:; ```; conda create -n deepvariant -c conda-forge -c bioconda python=2.7 deepvariant; ```; bioconda is heavily dependent on conda-forge packages so you'll want to include that whenever installing anything from bioconda to ensure all the dependencies are available. Hope this helps get it running for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584441591:232,depend,dependent,232,,https://github.com/google/deepvariant/issues/177#issuecomment-584441591,2,['depend'],"['dependencies', 'dependent']"
Integrability,"Another update on CLIF dependency:; @chapmanb , as you noticed, CLIF is an issue here. We pre-built our own CLIF and directly used it in the DeepVariant build. I also just realized that we didn't release the script that we used to build CLIF, which should totally be released. I was planning to push out a 0.6.1 today, but now it's late so I'm going to wait until Monday for my own sanity and not breaking things over the weekend. However, if it's helpful I'll paste the content here right now. Note that this is used to build for Ubuntu. I did start looking into whether we can modify it for CentOS 6, but stuck at how to get `protoc` and hasn't resumed my work yet. I'll just paste our script for Ubuntu and hopefully that could be helpful if you want to look into building a CentOS compatible CLIF. Next week I'll push a 0.6.1 that has this under the tools/ directory. And I'll also see if I can figure out how to build it for CentOS6. ```; # Builds OSS CLIF binary for DeepVariant.; #; # This script should be run on a cloud VM. Known to work on some versions of; # Linux OS.; #; # OSS CLIF takes a very long time to build (10+ minutes) since it needs to; # compile parts of clang and LLVM. To save this build time, we use this script; # to build CLIF, install it in /usr/local/clif, and then packages up; # /usr/local/clif and shared protobuf libraries from /usr/local/lib into a tgz; # called oss_clif.latest.tgz.; #; # This oss_clif.latest.tgz is used by build-prereq.sh to build DeepVariant.; # Various versions that we built and released can be found under:; # https://console.cloud.google.com/storage/browser/deepvariant/packages/oss_clif; #; # We do recognize that this should be temporary, and will update when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:23,depend,dependency,23,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,2,['depend'],['dependency']
Integrability,"By the way, can you let me know what documentation page you were working from for the original command?. I changed the warning message but want to make sure we're looking at the whole user flow up to that point to make sure nothing confusing is still there. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-840889768:127,message,message,127,,https://github.com/google/deepvariant/issues/457#issuecomment-840889768,1,['message'],['message']
Integrability,"CUDA 8.0, cudnn 6, and tensorflow-gpu 1.4. I ran the WES example. That went fine. Questions: what is the minimum allele frequency deepvariant will call? Is there a version contemplated that will do matched tumor/normal pairs? Unmatched pairs (as with a “pooled” normal)?. Thanks,; Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 5:10 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Unfortunately DeepVariant isn't yet compatible with TensorFlow version 1.5 so I think you'll need to install tensorflow-gpu version 1.4 for this to work. In our build script we do this with; sudo -H pip install --upgrade 'tensorflow-gpu==1.4'. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363252951>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqcP3vxIqOIV_Q6VUU-5cueBwPpQiks5tR4plgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-364172639:1222,message,message,1222,,https://github.com/google/deepvariant/issues/47#issuecomment-364172639,2,['message'],['message']
Integrability,"C] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8710,depend,dependency,8710,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,Can you share the error message? Also cc'ing @kishwarshafin who might be better able to help with the PEPPER step.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-992114837:24,message,message,24,,https://github.com/google/deepvariant/issues/497#issuecomment-992114837,1,['message'],['message']
Integrability,"Could be the stupidest advice in the universe but are you looking at your; vcf file :). Literally just thought talking through the problem might help, fellow; human, and no I didn't check the organism, you could have told me and I; would gkne the ncbi datanae downloaded the genomes aligned them checked; your region if interest don't worry if I see your name on the email thread; on this public github repository I won't reply and I'll loom forward your; paper on bioarvix hopefully,. Honestly all the best,. And if you don't care about prokaryotes then fair enough,. Joe. On Mon, 31 Jul 2023, 17:54 Axze-rgb, ***@***.***> wrote:. > The name of the organism has been said in this thread, that you are unable; > to find it, and believe we deal with a prokaryote is pathetic, really. Why; > would we bother with your stupid advice when you didn't even take the time; > to read the thread?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658773430:1203,Message,Message,1203,,https://github.com/google/deepvariant/issues/682#issuecomment-1658773430,1,['Message'],['Message']
Integrability,"D_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1763,depend,dependencies,1763,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,['depend'],['dependencies']
Integrability,"Do you see any other error messages higher up in the logs? The CalledProcessError is just the wrapper, so it doesn't tell us what went wrong inside make_examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774270943:27,message,messages,27,,https://github.com/google/deepvariant/issues/419#issuecomment-774270943,2,"['message', 'wrap']","['messages', 'wrapper']"
Integrability,"Does the RNA-Seq model work with BAMs created with HISAT2?. On Mon, Oct 17, 2022 at 1:22 PM Daniel E Cook ***@***.***>; wrote:. > @husamia <https://github.com/husamia> although this was closed some time; > ago, we have just released an Illumina RNA-seq model and case study; > <https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md>.; > If you are still interested in calling variants from RNA-seq data using; > DeepVariant, this should work for you.; >; > We have also updated the DeepVariant code base to be more memory efficient; > with RNA-seq data. This involves passing a new flag (--split_skip_reads),; > that allows for reads containing large SKIP to be processed efficiently.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/412#issuecomment-1281200289>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ADGRHUEWLJCA6FHYWYMDT4TWDWDLTANCNFSM4WRXVE4A>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484:1031,Message,Message,1031,,https://github.com/google/deepvariant/issues/412#issuecomment-1286287484,1,['Message'],['Message']
Integrability,Does the message still appear to indicate that it is using libraries installed on your machine rather than those present in the container?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580#issuecomment-1304596419:9,message,message,9,,https://github.com/google/deepvariant/issues/580#issuecomment-1304596419,1,['message'],['message']
Integrability,"E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:4140,message,message,4140,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,2,['message'],['message']
Integrability,"FYI, I'm not sure if this is the best solution, but I noticed that running in my home directory avoids the need to use _sudo_. While I am still encountering the same result (and I thought I encountered some issue with running Docker interactively at another step), I do have the ability to launch Docker in my home directory in interactive mode:. `docker run -it -v /mnt/efs-genome:/mnt/efs-genome gcr.io/deepvariant-docker/deepvariant`. and then run the commands for :. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Again, I am neither seeing an error message nor a result file (and the command stops running within seconds). However, if this provides a useful option for troubleshooting, I thought I should mention it. Also, I am starting the Google Cloud testing, but I am currently only at the file upload stage (I want to make sure I can run the 1st two steps, before checking that as a solution for the 3rd step). Thank you very much for your help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480628311:893,message,message,893,,https://github.com/google/deepvariant/issues/167#issuecomment-480628311,1,['message'],['message']
Integrability,"Fantastic information, thank you. On Thu, May 26, 2022 at 9:01 AM Andrew Carroll ***@***.***>; wrote:. > Hi @avilella <https://github.com/avilella>; >; > DeepVariant has been used on MGI datasets, both using the standard; > Illumina model, as well as retrained models. There is some complexity that; > the MGI/BGI technologies have evolved over time, so some demonstrations may; > not reflect the newest methods.; >; > The general finding is that the Illumina models tend to work well for MGI; > data, though we find examples of retraining for certain datasets improve; > further.; >; > Our advanced training tutorial; > <https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md>; > walks through retraining an Illumina model for data from BGISEQ 500 and this; > comparison; > <https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/>; > was conducted several years ago using the out-of-the-box Illumina model.; >; > If you know of any genome in a bottle sequencing datasets that are; > available from more recent MGI platforms, I'd be interested in pointers to; > those locations. I would be quite curious to see how the technology has; > evolved over the last several years.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/538#issuecomment-1138272184>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABGSN3EDTSIAXWBYLGQ3PDVL4VV7ANCNFSM5W4SRYCA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160:1554,Message,Message,1554,,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160,1,['Message'],['Message']
Integrability,"Frustrating. We are blocked from using Google Drive or DropBox. I will send the file from home. Thanks,; Brad Thomas. From: Paul Grosu [mailto:notifications@github.com]; Sent: Tuesday, May 1, 2018 10:50 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi Brad,. Sometimes smtp (email) servers block zip files. Just put it on Google Drive or DropBox and share the link to it. ~p. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-385705660>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWj7l8lWX5469lRFaED45lcY1l0Kks5tuIQogaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-385706487:1014,message,message,1014,,https://github.com/google/deepvariant/issues/62#issuecomment-385706487,2,['message'],['message']
Integrability,"HI @pichuan,. Please check this screenshot out and see if it is more clear. ![image](https://github.com/google/deepvariant/assets/34832128/82ed1379-29b4-403f-aa07-75002c1d831e). What I did was I first shelled into the container. Then I checked the files in `/opt/deepvariant/bin/` in the container because I tried to look for the `run_deeptrio` file, which could not be found when I followed steps in https://github.com/google/deepvariant/blob/r1.6/docs/deeptrio-quick-start.md#notes-on-singularity and gave the error message as in the title of this issue. Was it possible that I missed anything when running Deeptrio?. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054:518,message,message,518,,https://github.com/google/deepvariant/issues/745#issuecomment-1840119054,2,['message'],['message']
Integrability,Haha thanks for catching that warGning message! I'll take care of that :). Sorry it looks like the warnings about these HP-related flags are confusing!. First let me try to understand what you are trying to do:; 1) Is your BAM phased? e.g. using whatshap.; 2) Which documentation page/example are you following that gave you these errors originally?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-838736936:39,message,message,39,,https://github.com/google/deepvariant/issues/457#issuecomment-838736936,1,['message'],['message']
Integrability,"Happy to help!; For your question, it depends on how low the coverage is. You can see this blog post for how coverage impacts accuracy: https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-840635414:38,depend,depends,38,,https://github.com/google/deepvariant/issues/457#issuecomment-840635414,1,['depend'],['depends']
Integrability,"Hello @golubnikova . The current release of DeepVariant does use Python2.7. We anticipate that our next release will migrate to python3, alongside some other updates in the TensorFlow libraries used. Using DeepVariant from the Docker image distributed should work regardless of your current python dependencies. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/258#issuecomment-572913563:298,depend,dependencies,298,,https://github.com/google/deepvariant/issues/258#issuecomment-572913563,1,['depend'],['dependencies']
Integrability,"Hello @pichuan, I ran the full deepvariant pipeline after deleting all output directories from the previous run. It seems call_variants outputs only 16 files to the intermediate dir, whereas make_examples outputs 19 (with --num_shards 19). Here's the full command:. `podman run -it --rm -e LD_LIBRARY_PATH=/usr/bin:/usr/lib/nvidia:/usr/local/nvidia/:/usr/local/cuda-12.3/lib64:/usr/local/cuda-12.3/bin:/usr/local/lib/python3.8/dist-packages/tensorrt_libs/ --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/run_deepvariant --model_type=WGS --regions 'chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM' --num_shards 19 --ref=/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz --reads=/data/bamfiles/sample1.E250013.L1.hg38.rg.bam --output_vcf=/data/variants/sample1.vcf.gz --output_gvcf=/data/variants/sample1.g.vcf.gz --intermediate_results_dir=/data/variants/sample1.intermediate --logging_dir=/data/variants/sample1.logs`. Adding the ld_library_path -argument gets rid of the error messages about libvinfer, however I still get the cuda error:. `2024-07-16 14:14:08.323907: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error`. Although call_variants did use gpu and ran in about half an hour. Then postprocess_variants halts with:; `ValueError: ptrue must be between zero and one: nan`. (Full error log in the first message) I'll try to play around with --num_shards next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849#issuecomment-2232682994:1216,message,messages,1216,,https://github.com/google/deepvariant/issues/849#issuecomment-2232682994,2,['message'],"['message', 'messages']"
Integrability,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:633,Message,Message,633,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772,2,['Message'],['Message']
Integrability,"Hello Ryan,. I built on an Ubuntu 16 system with CUDA-9.0, CUDNN version 7, tensorflow-gpu installed via tf-nightly-GPU using the last build available of 1.5 (it depends on CUDA-9.0). As I mentioned, build_and_test.sh showed success, running all tests successfully. I can install CUDA 8, CUDNN 6 and try again. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Monday, February 5, 2018 3:08 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). I wonder if there is an issue somehow with how you are building with tensorflow-gpu. What commands did you use to build it? We think this should work with tensorflow-gpu==1.4. Here is what I did to try it out:. git clone https://github.com/google/deepvariant.git. #edit settings.sh so that DV_GPU_BUILD=1 and DV_INSTALL_GPU_DRIVERS=1. ./build-prereq.sh; ./build_release_binaries.sh. then ran a test command with the quickstart testdata:. python deepvariant/bazel-bin/deepvariant/make_examples.zip; --mode calling; --ref ""${REF}""; --reads ""${BAM}""; --regions ""chr20:10,000,000-10,010,000""; --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"". which executes as expected. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-363221616>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqV3UwUysMDiEktsAe-3kindiR3myks5tR22hgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-363230217:162,depend,depends,162,,https://github.com/google/deepvariant/issues/47#issuecomment-363230217,1,['depend'],['depends']
Integrability,"Hello, as you can see the error message is ""samtools: command not found"". Can you please see if there's a way you can install samtools for your environment?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608:32,message,message,32,,https://github.com/google/deepvariant/issues/579#issuecomment-1292297608,1,['message'],['message']
Integrability,"Hello,. Did you receive the attachment I resent on 4/16? Also, any thoughts on the error I was seeing?. Thank you and best regards,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-385701252:957,message,message,957,,https://github.com/google/deepvariant/issues/62#issuecomment-385701252,2,['message'],['message']
Integrability,"Hello,. I tried installing deepvariant using `conda install deepvariant` .; With this, conda successfully;; -Collects package metadata; -Solves environment; -prepares and verifies the transactions (i.e required packages to download/install/update). Unfortunately, executing the transaction fails with the error below,. ```; Preparing transaction: done; Verifying transaction: done; Executing transaction: | ERROR conda.core.link:_execute_post_link_actions(658): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back.; failed; ERROR conda.core.link:_execute(568): An error occurred while installing package 'bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1'.; LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>. Attempting to roll back. Rolling back transaction: done. LinkError: post-link script failed for package bioconda/label/cf201901::deepvariant-0.7.2-py27h5d9141f_1; running your command again with `-v` will provide additional information; location of failed script: /root/miniconda3/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```. I had a similar error installing with ; `conda install -c bioconda deepvariant` and `conda install -c bioconda/label/cf201901 deepvariant` and `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Any idea and solution to what could be causing the error, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-504749118:833,message,messages,833,,https://github.com/google/deepvariant/issues/177#issuecomment-504749118,3,['message'],['messages']
Integrability,"Here is an attempt to deliberately mess up the BAM file in the case study, so I can get the make_examples step to fail, and observe the logs.; However, in my test runs below, I'm seeing useful error messages. @Asppagh if you have suggestions on how I can reproduce an error type like yours. It'll be really helpful! Otherwise I'm currently stuck on how to help you debug this. I'll share my test runs below so you can take a look:. ---. I followed steps in:; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md; to get data. ## I deliberately messed up the BAM, and ran `run_deepvariant`; ```; ls -l quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; -rw-rw-r-- 1 pichuan pichuan 3925783 Nov 27 2017 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; ```; ```; head -c 3000000 quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam > quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam; cp quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai quickstart-testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam.bai; ```. I ran:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.truncated.bam \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```. for the sake of completeness, I'll paste the log below up to the stack trace in make_examples:. ```; I0629 23:08:46.468520 139667868600064 run_deepvariant.py:317] Re-using the directory for intermediate results in /tmp/tmpj5fx0phm. ***** Intermediate results will be written to /tmp/tmpj5fx0phm in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.truncated.bam"" ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:199,message,messages,199,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['message'],['messages']
Integrability,"Here is the zip file. Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Saturday, April 14, 2018 12:09 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi,; I'm not seeing the zip file. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381304103>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqT_omHfFPRVmhBNx0mJ-jQQyMRMXks5toYR4gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381621757:846,message,message,846,,https://github.com/google/deepvariant/issues/62#issuecomment-381621757,2,['message'],['message']
Integrability,"Heureka, I suppose - so this is the story:. I have previously installed a conda environment on the host system (has nothing to do with singularity or Deepvariant); and as part of that environment, I also installed ortools via pip (pip install ortools). One of the depencies is protobuf; which will not install the c++ version by default in this constellation (see: https://github.com/protocolbuffers/protobuf/issues/539). . The issue: The pip command installs the protobuf module in $HOME/.local/lib/python3.8/site-packages and for some reason, Deepvariant INSIDE the container sees this and tries to use it (my guess). . Any idea of how to prevent this from happening? I think binding $HOME into the container is pretty standard Singularity (and Docker?) behavior, but these kinds of library clashes shouldn't happen - maybe the container could be set up to not see these host-system libraries when loading modules in python?. Cheers,; Marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056:384,protocol,protocolbuffers,384,,https://github.com/google/deepvariant/issues/499#issuecomment-1012930056,1,['protocol'],['protocolbuffers']
Integrability,"Hi @ASLeonard here is a table from the [preprint](https://www.biorxiv.org/content/10.1101/2022.10.16.512451v1). We stratify the F1-score across different region types. The published RNA-seq model is `DV RNA-seq [GTEx]`:. <img width=""795"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200906677-4b6e2f11-8b29-44e3-871e-299f46d1cd64.png"">. The model has no problem running genome wide, but accuracy will vary by region type due to the nature of RNA-seq data. We observe the highest accuracy in CDS regions which is why the case study is limited to these regions. Users should filter variants depending on their use case. This might mean filtering by region, but you can also consider filtering by genotype quality (or both). We show how you can reduce the false-discovery rate in figure 5 of the preprint by filtering on genotype quality:. <img width=""648"" alt=""image"" src=""https://user-images.githubusercontent.com/1536935/200907763-1d21cc44-daff-47d2-87c6-e7917ea62a32.png"">. > Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?. The model is trained on exonic regions. We found this to give the best performance in our evaluations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398:611,depend,depending,611,,https://github.com/google/deepvariant/issues/584#issuecomment-1309177398,1,['depend'],['depending']
Integrability,"Hi @Asppagh ; From the error above it wasn't very informative. This seems like it failed on the make_examples step already. We should have just stopped there, instead of proceeding into call_variants and next steps. --> This is now fixed in internal code, and will be fixed in the next release. Another question is -- why did the failed make_examples not produce any useful logs?. This one is a bit less clear to me. . With the same setting, instead of using /opt/deepvariant/bin/run_deepvariant (which is a convenient script that combines 3 steps), can you try directly running with . `/opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord.gz""`. This should allow you to just run 1 make_examples, without using GNU parallel as well. Hopefully whatever error messages will be more clear here.; Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870041849:1072,message,messages,1072,,https://github.com/google/deepvariant/issues/465#issuecomment-870041849,2,['message'],['messages']
Integrability,"Hi @Axze-rgb,. This clearly tells me that your organism is highly sensitive under selective pressure. So let's think what might happen if you used Clair3. Clair3 has both a full-alignment network (ResNet), and a pileup network (Bi-LSTM). . Bi-LSTM is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:710,depend,dependencies,710,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['depend'],['dependencies']
Integrability,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291#issuecomment-607407000:840,Depend,Depending,840,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000,1,['Depend'],['Depending']
Integrability,"Hi @DiableJambe ,; I noticed this issue is still open from a while ago. So I want to give a quick update and close this issue.; DeepVariant 0.8.0 is out yesterday. It might still not solve your original issue, but I just want to give you a heads up in case you want to try out the new version.; From @qili93 's last response, many versions of the dependencies will be different now. But hopefully it helped resolved your original problem.; I'll close this issue now. If you have more questions about the new version, please feel to open another issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-482634571:347,depend,dependencies,347,,https://github.com/google/deepvariant/issues/123#issuecomment-482634571,1,['depend'],['dependencies']
Integrability,"Hi @DiableJambe, . We use libssw (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library) for our local alignment. This code directly uses emmintrin.h in ssw.c so there's no easy way to remove it for the powerpc architecture. That said, there appears to be a pull request to add PPC64 support (https://github.com/mengyao/Complete-Striped-Smith-Waterman-Library/pull/35) but this has been pending for more than a year. We'd be more than happy to update libssw if you can get an implementation in there that doesn't require SSE intrinsics. Let us know if you want to go down that route.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-441822330:590,rout,route,590,,https://github.com/google/deepvariant/issues/123#issuecomment-441822330,1,['rout'],['route']
Integrability,"Hi @FarmOmics ,. The `--model_type=WES` is a shorthand specifically for the wrapper script ""run_deepvariant.py"". Specifically here; https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L239. In https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md , you'll see that the RNAseq run actually overwrites these arguments:; with:; ```; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; ```. which basically means: If you run `make_examples` on your own (without using the wrapper script ""run_deepvariant.py""), you'll want to provide `--split_skip_reads=true`, but not providing `channels`. And we also provided RNAseq model with:; ```; --customized_model=model/model.ckpt \; ```. So, https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-rnaseq-case-study.md isn't actually using WES model at all. (In the future we'll think about how to make this less confusing.). In terms of preprocessing for RNAseq data, the important flag to add is `--split_skip_reads` to make_examples. Let me know if I can help clarifying with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/572#issuecomment-1281335694:76,wrap,wrapper,76,,https://github.com/google/deepvariant/issues/572#issuecomment-1281335694,2,['wrap'],['wrapper']
Integrability,"Hi @Fred-07,. Since the [Element AVITI System](https://www.elementbiosciences.com/blog/whole-exome-sequencing-101-cost-effective-dna-sequencing-to-understand-genetic-disease) seems to be dependent on external exome enrichment solutions, the answer would be it depends. Element seems to [prefer Roche for library preparation](https://www.elementbiosciences.com/news/elements-new-aviti-system-shows-seamless-compatibility-and-high-performance-with-kapa-library-preparation-kits-in-multiple-ngs-applications) - also used in the paper - and which has its [own enrichment solution](https://sequencing.roche.com/us/en/products/group/kapa-hyperexome.html). Now if the exome selection is optimal, and coverage passes the Fold-80 base penalty (i.e. how much more required sequencing is necessary for 80% of the target bases to achieve desired mean coverage among samples), then the WES model should work given some in-house validation - as the reads have a higher quality (as shown below), and have worked for WGS:. ![image](https://github.com/google/deepvariant/assets/6555937/daa31daa-7abe-46fa-8037-f6ed49112c6f). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/703#issuecomment-1705578032:187,depend,dependent,187,,https://github.com/google/deepvariant/issues/703#issuecomment-1705578032,2,['depend'],"['dependent', 'depends']"
Integrability,"Hi @GaianX39 . I wanted to add just a few things. . First, in our next release we're planning to improve the de novo detection aspects of DeepTrio, so if that's of interest to you, please stay tuned for this. . Using GIAB to validate performance is only something that you can do when sequencing the known samples (e.g. HG002-HG003-HG004). If you have those, then please follow the ""Running Hap.py"" steps at the end of most quick starts (e.g. [Hap.py section of WGS case study](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md#perform-analysis-with-happy-against-421-truth-set). To do this with a joint called VCF, we use BCFtools to subset the VCF to individual samples (e.g. `bcftools -s ${SAMPLE_ID}`). For runtime, we have benchmarks in the Figure 6 of the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1). Here, we see DeepTrio takes about 1.5x the time that running DeepVariant on all 3 samples does. The cost should be a similar multiple as this is run on the same hardware. What this translates to in cost depends on how you run it (local, which cloud provider and with which deals, etc...)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491:1074,depend,depends,1074,,https://github.com/google/deepvariant/issues/704#issuecomment-1719944491,1,['depend'],['depends']
Integrability,"Hi @JosephLalli . At the time we were putting together the vg-giraffe paper, we noticed that Indel errors would occur because the positioning of read indel events in the CIGAR string were not always left-normalized. ABRA both fixed that issue and also seems to give some additional advantage by standardizing the representation before running the candidate generation in DeepVariant. Subsequent to that finding, both ourselves and the vg team built methods which left-normalize Indel CIGAR events during processing. We found that this reduced almost all, but not quite all, of the Indel performance difference with ABRA processed reads. Because we have built the DeepVariant vg pipeline to include read normalization with the --normalize_reads=true tag, we don't routinely benchmark the two methods against each other. Our last head-to-head benchmark on ABRA on and off on 35x HG003 was no_ABRA Indel F1=0.9953, with ABRA Indel F1=0.9958. @pichuan Has made [this linked gist](https://gist.github.com/pichuan/eedab4cf2e06fa7ceb2fad0f9b3f8066) that we use for efficient, single machine processing of vg giraffe+DeepVariant. We opted not to include the use of ABRA realignment in that pipeline. With the benchmark numbers available here, I hope this is sufficient information for you to decide whether you would like to include it or not. We continue to look at improvements in the Indel realignment/reassembly process to see if we can further improve in some of the manners that ABRA helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466:763,rout,routinely,763,,https://github.com/google/deepvariant/issues/629#issuecomment-1503848466,1,['rout'],['routinely']
Integrability,"Hi @NagaComBio. Sorry for the delay! I don't have a clear solution to this problem just from looking at the error message, but if you can share the data, e.g. with just a small slice of the bam, then I can try to reproduce the issue. If that's possible, you can email me at marianattestad@google.com. For now I can tell you that `--group_variants=false` is only applicable when using `vcf_candidate_importer`, which is the most common way that this error occurs, since the input VCF for that can have multiple candidate variants in the same position, which isn't supposed to be possible when the candidates are generated by make_examples without `vcf_candidate_importer`. Thanks,; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301:114,message,message,114,,https://github.com/google/deepvariant/issues/517#issuecomment-1050344301,2,['message'],['message']
Integrability,"Hi @Tonitsk8264 . PacBio CLR and ONT R9.4 have higher base error rates. It is possible to call variants on this, but you will need to use [PEPPER-Margin-DeepVariant](https://github.com/kishwarshafin/pepper) instead. Please see that GitHub link for details. The preset there to call variants with R9.4 data should be either `--ont_r9_guppy5_sup` or `--ont_r9_guppy4_hac` depending on how it is basecalled. The preset for CLR should be `--clr`. For GLnexus outputs, you can use DeepVariantWGS for merging general sequencing results.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/713#issuecomment-1741830142:370,depend,depending,370,,https://github.com/google/deepvariant/issues/713#issuecomment-1741830142,1,['depend'],['depending']
Integrability,"Hi @ZuyaoLiu ,; I was out so I didn't follow up in the past few weeks.; Reading back in the previous comments, is my understanding correct that 1) GPU was being used despite the messages, 2) you have a separate question bout Mendelian violations which is in another GitHub issue. . If the remaining question is covered in another issue, we can close this issue. Otherwise, please remind me again what the current issue related to GPU is. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1823352377:178,message,messages,178,,https://github.com/google/deepvariant/issues/722#issuecomment-1823352377,1,['message'],['messages']
Integrability,"Hi @ZuyaoLiu . When I tested calling and training, I also saw that message. But in both of my calling and training, the GPU was utialized. We added an entry in FAQ: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-am-i-seeing-cuda_error_not_initialized-initialization-error-while-running-on-gpu. and I mentioned that message in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md#test-the-model as well. @ZuyaoLiu , can you help check whether the results of calling is reasonable on your side, and whether GPU is utilized or not?. And, similarly in the training case, some of the warning messages you have might not affect the results. Can you also check whether you can run through the steps (and whether GPU is utilized or not)?. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1787602575:67,message,message,67,,https://github.com/google/deepvariant/issues/722#issuecomment-1787602575,3,['message'],"['message', 'messages']"
Integrability,"Hi @aderzelle . I have two recommendations: First, to tell whether the CNN made a RefCall classification, you can look for an output line in the VCF itself. DeepVariant will write a RefCall for every candidate considered. If a line is not present in the output VCF, it means that the reference and non-reference read counts instead generated the reference call because a candidate was not made at that position. Second, I would recommend that you look at the visual report that @MariaNattestad created in the most recent DeepVariant release. There is a way to run this on previous VCF files, see: [This page](https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-vcf-stats-report.md). In that visualization, take a look at the VAF support for each call. The nearby variant phenomenon manifests as a higher number of REF calls with a VAF close to 1.0. In humans, this seems to be DeepVariant avoiding false calls in LINE elements and segmental duplications, but this could be undesirable depending on your reference genome and population structure.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/257#issuecomment-569131100:998,depend,depending,998,,https://github.com/google/deepvariant/issues/257#issuecomment-569131100,1,['depend'],['depending']
Integrability,"Hi @aditya-88, thanks for filing this bug! We will look into both 1) using disutils.spawn and 2) improving the error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/160#issuecomment-470633217:117,message,message,117,,https://github.com/google/deepvariant/issues/160#issuecomment-470633217,1,['message'],['message']
Integrability,"Hi @aedavids, the below messages are expected when running DeepVariant. You will see these and many other logging outputs when you run the software. ```; I1219 00:20:56.935748 140093586556672 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. I do notice a possible issue with your Docker command, pasted below. ```; sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4; ```. Since you are mounting your local `/data/input` and `/data/output` directories to `/input` and `/output` in the container, you will want to change the file paths used in the command to reference the container directories. For example, in the below command, you can use: ; `--ref=/input/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/254#issuecomment-567577141:24,message,messages,24,,https://github.com/google/deepvariant/issues/254#issuecomment-567577141,1,['message'],['messages']
Integrability,"Hi @akolesnikov , thank you for your response here i attached code, terminal output and log file. code is running but neither output generating or error throwing just running.; please see below code and log file. ###### code #############; #!/usr/bin/env nextflow. nextflow.enable.dsl=2; params.outdir = '/home/deepak/integration/resu1'; params.data_dir = '/home/deepak/integration/resu1/4.markDupliM'; params.refhg38 = '/home/deepak/integration/hg381_22XYM'; params.bed = '/home/deepak/integration'. workflow {; // Define channels for input data; Channel; .fromPath(""${params.data_dir}/*_sorted_md.bam""); .map { file -> ; def sample_id = file.baseName.replace('_sorted_md', ''); return [sample_id, file]; }; .set { read_pairs }; /// Step 1. DeepVariant; DeepVariant(read_pairs, params.refhg38, params.bed); }. process DeepVariant {; tag ""deepavar on ${sample_id}""; publishDir ""${params.outdir}/5.finaleepvar"", mode: 'copy'; cpus 4; //BIN_VERSION 1.6.1. input:; tuple val(sample_id), path(read_files); val(params.refhg38); val(params.bed); ; output:; //tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), path(""${sample_id}_rawd.gvcf.gz""), emit: raw_vcfs; tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), emit: raw_vcfs. script:; """"""; docker run \; -v ""${params.data_dir}"":/opt/bam -v ""${params.refhg38}"":/opt/refhg38 -v ""${params.bed}"":/opt/bed \; google/deepvariant:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /opt/refhg38/Homo_sapiens_assembly38cleaned.fasta \; --reads /opt/bam/${read_files} \; --regions /opt/bed/hg38_exomeY.bed \; --output_vcf /opt/bam/${sample_id}_rawd.vcf.gz \; --num_shards ${task.cpus}; """"""; }. ######## code ################. terminal:; (base) deepak@ubuntu22:~/integration$ nextflow run final_deepvarian.nf . N E X T F L O W ~ version 24.04.4. Launching `final_deepvarian.nf` [hungry_stonebraker] DSL2 - revision: 4dab17f4f2. executor > local (1); [dd/64034b] DeepVariant (deepavar on SRR26512958) [ 0%] 0 of 2. log file attached",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013:318,integrat,integration,318,,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013,4,['integrat'],['integration']
Integrability,"Hi @alisamatisse,. I'm not sure what `samtools view -h -F 2048` does. DeepVariant does not do any special processing for chimeric reads. What reads are used from the input BAM is controlled by the following flags: ; * `--keep_duplicates` default to False; * `--keep_supplementary_alignments` default to False; * `--keep_secondary_alignments` default to False; * `--min_mapping_quality` depends on the data, for PacBio it is set to 1. Variants are created for all positions where there are at least two reads support an alt allele.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/863#issuecomment-2278553367:386,depend,depends,386,,https://github.com/google/deepvariant/issues/863#issuecomment-2278553367,1,['depend'],['depends']
Integrability,"Hi @anitagh ; From my experience it's not too uncommon for BAM files in the wild to have either no sample name in the BAM file, or multiple sample names (which will crash DeepVariant right now). We can easily add `--sample_name` to the run_deepvariant.py script, so you can still run that script once. We'll do this so it'll come out in the next release. So far, we want users to explicitly add this --sample_name this flag because we want to make sure users are aware that their BAM file has more than one (or 0) sample names. But it seems like all the cases I've seen so far, none of them actually intended for them to be different sample names anyway. So I might consider just removing this constraint and just make it a warning message instead. Either way, in our next release, you should be expecting to have `--sample_name` flag in the run_deepvariant.py script which you can do in one step. Thanks for reporting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535102260:732,message,message,732,,https://github.com/google/deepvariant/issues/222#issuecomment-535102260,1,['message'],['message']
Integrability,"Hi @anitagh and @PlatonB , ; to give you an update on this issue, we have made a change internally that:; 1) Added `--sample_name` to run_deepvariant.py; 2) When input BAM files has no sample names or more than one sample names, instead of crashing with the error message you reported, we now use a default string as the sample name (or pick one from the multiple names) and prints out a warning. This behavior should be less cumbersome to our users, and shouldn't cause any issues for most use cases. Our team is working towards a next release. Once the release is out, I can post another update to this issue to let you know. For now, please bear with us and use the solution in https://github.com/google/deepvariant/issues/222#issuecomment-534768468. We'll keep you posted!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222#issuecomment-535311937:264,message,message,264,,https://github.com/google/deepvariant/issues/222#issuecomment-535311937,1,['message'],['message']
Integrability,"Hi @ankurc17 ; Can you tell us more about what the issues are?; For example, what OS are you using, what command did you run and what error messages you've seen.; It'll be great if we can assist you here, because then other users can learn from our conversation too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161:140,message,messages,140,,https://github.com/google/deepvariant/issues/574#issuecomment-1276221161,2,['message'],['messages']
Integrability,"Hi @annabeldekker ,; one more thing to point out -- depending on which version you're using, we actually changed (improved) the way PacBio flags works between version 1.0.0 and 1.1.0. In the older v1.0.0, we asked users to set two flags `sort_by_haplotypes` and `parse_sam_aux_fields`:; https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-haplotagged-chromosome-20-alignments. We simplified in the current v1.1.0 by creating one new flag `use_hp_information`: so you only need to set `--use_hp_information` if your BAM has HP tags. You no longer need to set `sort_by_haplotypes` and `parse_sam_aux_fields` separately, and in fact, please just use `use_hp_information` instead of setting the other flags directly to avoid confusion, because you're using v1.1.0. To summarize, for v1.1.0, please set `--use_hp_information=true` if your BAM has HP tags. If your BAM doesn't have HP tags, set `--use_hp_information=false` (or don't specify it - false is the default). Thanks for reporting this. In the future we're looking into whether we can make this simpler by building some phasing functionality into DeepVariant, so we don't have to our users to run the two-step process for PacBio. But for now, it's best to follow the [Quick Start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md) of the corresponding version, and make sure you use the flags as recommended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844209552:52,depend,depending,52,,https://github.com/google/deepvariant/issues/457#issuecomment-844209552,2,['depend'],['depending']
Integrability,"Hi @bopohdr, if it's possible, can you send me to full log?. In order to experience an error, I tried our example in Quick Start, but I deliberately make the FASTA file incompatible with the BAM file by removing the `chr` prefix. The error I got looks like this:; ![error](https://user-images.githubusercontent.com/471813/68056060-6f66df80-fcaf-11e9-875b-c56db1303c40.png). Note that the bottom error message is similar to yours, but it's not the most informative one, because it was just telling you that the wrapper script [run_deepvariant.py](https://github.com/google/deepvariant/blob/r0.8/scripts/run_deepvariant.py) is unable to run make_examples successfully. I'm hoping that somewhere above this last error message, you might be able to see more what have gone wrong. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232#issuecomment-548948642:401,message,message,401,,https://github.com/google/deepvariant/issues/232#issuecomment-548948642,3,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"Hi @crazysummerW ,. Looking at your error, it seems like this might be relevant:. ```; File ""/usr/local/lib/python3.8/dist-packages/h5py/_hl/files.py"", line 241, in make_fid; fid = h5f.create(name, h5f.ACC_TRUNC, fapl=fapl, fcpl=fcpl); File ""h5py/_objects.pyx"", line 54, in h5py._objects.with_phil.wrapper; File ""h5py/_objects.pyx"", line 55, in h5py._objects.with_phil.wrapper; File ""h5py/h5f.pyx"", line 122, in h5py.h5f.create; OSError: [Errno 5] Unable to synchronously create file (unable to lock file, errno = 5, error message = 'Input/output error'); ```. This is because this logic in our code writes a temp file:; https://github.com/google/deepvariant/blob/r1.6/deepvariant/keras_modeling.py#L97-L99. ```; tmp_weights_dir = tempfile.gettempdir(); tmp_weights_path = os.path.join(tmp_weights_dir, 'tmp_weights.h5'); model.save_weights(tmp_weights_path); ```. Can you check your setting, and see if somehow your run wasn't able to create a temp file?. I reran our set up in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md (using a GCP machine as an example) and wasn't able to reproduce that error. So, it'll be very helpful for me to understand your machine setup, and try to make our code more robust in the future. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725#issuecomment-1799132190:298,wrap,wrapper,298,,https://github.com/google/deepvariant/issues/725#issuecomment-1799132190,3,"['message', 'wrap']","['message', 'wrapper']"
Integrability,"Hi @crazysummerW . The message here is most consistent with reads that do not have quality values. Inspection of the GIAB file indicates that this is the case here - the reads have bases and not quality values. These GIAB files appear to be deposited in 2018. It seems likely to me that they are PacBio continuous long read (CLR) sequencing instead of Circular Consensus Sequencing (CCS). PacBio CLR sequencing has a higher base error rate and DeepVariant is not designed to process this older type of sequence data. . Mechanically, the reason this input file fails is that it lacks quality values, which DeepVariant expects. CLR sequencing does not generate quality values. For test data, I recommend using a more recent set of sequencing which use the CCS prep, e.g. the contents of: . https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_CCS_15kb_20kb_chemistry2/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336:23,message,message,23,,https://github.com/google/deepvariant/issues/631#issuecomment-1507742336,1,['message'],['message']
Integrability,"Hi @dbrami ,. Thanks for the question. To make sure you can restart from each of the steps, you're on the right track : using `--dry_run=true` to find out what to run is useful. The other part that's missing is: when you want to rerun the next steps (say `call_variants`, after `make_examples` are done), you'll want to be able to access the outputs of the previous step. If you're using the one step `run_deepvariant` or `run_deeptrio` wrapper, the way you can do it is to make sure you specify `--intermediate_results_dir` and point it to your own output directory that you can access later. (The default is temp directory in /tmp, which would be inside Docker and might not be accessible if the run dies). And actually, if you use `--dry_run=true` just for the sake of finding out the commands, and then run each of the steps manually (still through docker), that actually gives you the best control, because you can see where all the intermediate files go. As your point about using a sentinel file after each of the steps finishing - I totally understand the point, and agree that such solution can make resuming the run easier. I can file an internal issue to track this suggestion. However, the feature for resuming runs might not be able to prioritized soon based on our workload. And overall, if you're putting things in a production pipeline, it might be better for you to have more control (by running the steps separately and have full control). I understand this is not ideal. But this is currently a trade-off for my team.; If you would be interested in helping us improving run_deepvariant.py and run_deeptrio.py, I'm also open to reviewing pull requests!. Thank you for reporting. I'll file a bug to track internally and can update here if we make more progress.; If you have any questions about the workaround I suggested, please feel free to follow up!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749#issuecomment-1848067102:437,wrap,wrapper,437,,https://github.com/google/deepvariant/issues/749#issuecomment-1848067102,1,['wrap'],['wrapper']
Integrability,"Hi @dbrami . With respect to the python version, this software is contained within Docker, so you don't need to worry about what version is on your system. They python versions that we can use are a function of the requirements in the dependencies of DeepVariant. In this case, we need to use Python 3.8 as a result of those dependencies. You are right that updating to Python 3.11 might have speed advantages, but this is something we will have to look at over time as underlying libraries are updated. . With respect to GLnexus, we are expecting that there may be further updates to GLnexus, and will look at updates to that when those might become available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/755#issuecomment-1865191737:235,depend,dependencies,235,,https://github.com/google/deepvariant/issues/755#issuecomment-1865191737,2,['depend'],['dependencies']
Integrability,"Hi @duceppemo , can you clarify what error message you were seeing when you try with a BAM file with `.csi` indices?. I just tested with data from https://github.com/google/deepvariant/blob/r1.2/docs/deepvariant-quick-start.md. And I deliberately deleted the `.bai` index file and created a `.csi` instead:. ```; $ rm -f quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.bai; $ samtools index -c quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; $ ls quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam*; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam.csi; ```. After that, it seems like I was still able to go through the Quick Start steps without any issues. DeepVariant is using htslib to read BAM files, and it seems like `.csi` is already supported there. Can you give me an reproducible example, if you're seeing any issues?. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/481#issuecomment-919633316:43,message,message,43,,https://github.com/google/deepvariant/issues/481#issuecomment-919633316,1,['message'],['message']
Integrability,"Hi @elcortegano . Thank you for the file, it's very informative, and I have a few observations that might help. First, all of the reads in this file which have sequence bases do have the correct number of quality values. The reads are not malformatted. However, there are reads with the SAM flag 256, secondary alignment. This occurs when the mapper finds a place which is almost or as good to map the reads to. Not all mappers report secondary alignments, and this is often controlled by a parameter. These reads have neither sequence bases, nor quality values, but do have a CIGAR string. I believe (but am not certain) that DeepVariant is attempting to parse these reads, and this is causing the error. I was not able to confirm, since I wasn't able to find the reference genome used to map. These reads can be validly ignored by DeepVariant. If this is the case, there are a few options to proceed. First, I believe that if you perform the command: . ```; samtools view -bh -F 256 file.bam > new_file.bam; ```. The file should now work with DeepVariant. . One thing it might be good to consider, based on my inspection of the CIGAR strings, I think these are likely HiFi reads. The program tag for minimap indicates the parameter -ax map-pb. I think that parameter is optimized for CLR. I believe the parameter for CCS/HiFi is -ax asm20. We usually take mapped BAM files from pbmm2, which wraps minimap2 with parameters optimized for HiFi. This could be why we didn't notice this exact issue before. If this is the source of your problem, our team can fix this behavior in future releases by ignoring flag 256 reads. You may want to consider mapping with pbmm2 as well (https://github.com/PacificBiosciences/pbmm2). If this does not fix your issue, could you please point me to the reference genome which you used to map to, I would need to try running DeepVariant and walk through the error more closely. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815421572:1393,wrap,wraps,1393,,https://github.com/google/deepvariant/issues/434#issuecomment-815421572,1,['wrap'],['wraps']
Integrability,"Hi @esraaelmligy . The quality threshold for filtering will depend on your tolerance for false positives versus false negatives. We find the quality is well-calibrated with error, so Qual of 20 is ~1% false discovery probability, Qual of 10 is ~10% false discovery probability and so on. . So if you value precision, something between Qual 10 and Qual 20 as a threshold is probably a good place to start.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/881#issuecomment-2359393776:60,depend,depend,60,,https://github.com/google/deepvariant/issues/881#issuecomment-2359393776,1,['depend'],['depend']
Integrability,"Hi @forumsan, thanks for reporting this issue! Others users have reported seeing this message as well. When we looked into this issue internally, we found out that this was actually a problem with logging in TensorFlow, and AVX-512 instructions are being used correctly. If you try running DeepVariant on a CPU-only machine that does not support AVX-512, you should see a clear increase in the overall runtime. The logging issue has since been fixed in TensorFlow, but won't show up in our current Docker images. This is because we are installing a version of the intel-tensorflow package that does not contain the fix. I'll close this issue for now, but feel free to reopen if you still have other questions. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/301#issuecomment-617907163:86,message,message,86,,https://github.com/google/deepvariant/issues/301#issuecomment-617907163,2,['message'],['message']
Integrability,"Hi @forumsan, you are already getting the speedup from AVX2/AVX-512 if you are using Intel Skylake or later! :) The message you are seeing is an issue with logging in TensorFlow. Feel free to ignore that message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/301#issuecomment-618511477:116,message,message,116,,https://github.com/google/deepvariant/issues/301#issuecomment-618511477,2,['message'],['message']
Integrability,"Hi @geng-lee ,; DeepVariant's default should be good for achieving a good F1 metrics. Some of our users sometimes further filter the variants, but it depends on why you're doing it.; Can you tell us why you want filter the SNPs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/671#issuecomment-1609966288:150,depend,depends,150,,https://github.com/google/deepvariant/issues/671#issuecomment-1609966288,1,['depend'],['depends']
Integrability,"Hi @gneedle1 . It depends on the type of experiment. If the barcodes are the same sample and you are trying to get at some other specific property (e.g. cell type or preparation), then it's a question of sequencing coverage. If you will have enough coverage to make good quality calls within the reads of a single barcode (something like at least 15x-20x depending on your tolerance for errors), then subsetting by barcode could be reasonable. If you have less coverage, then the effects of reducing coverage will likely be much larger than whatever effect you are trying to detect. . If the barcodes separate different samples (i.e. those with different germline DNA), then the correct thing is to separate by barcode. I would need a little more information about the nature of the samples and what you are looking for to give you a more direct opinion.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/772#issuecomment-1954778964:18,depend,depends,18,,https://github.com/google/deepvariant/issues/772#issuecomment-1954778964,2,['depend'],"['depending', 'depends']"
Integrability,"Hi @hmkim - your BAM header looks okay to me. The first line `[E::hts_open_format] Failed to open file ...` indicates it may be an underlying samtools issue (since that seems to be an error message from htslib/samtools), are there any other lines from the error message, outputted earlier? Assuming that the path is correct, it should work. . The only thing I can think of with the provided info is to check that you have read permissions on the file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/141#issuecomment-455367612:190,message,message,190,,https://github.com/google/deepvariant/issues/141#issuecomment-455367612,2,['message'],['message']
Integrability,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400#issuecomment-749251835:38,message,message,38,,https://github.com/google/deepvariant/issues/400#issuecomment-749251835,1,['message'],['message']
Integrability,Hi @jdmontenegro . I am going to close this issue for now. I will make a note to send you a message if/when we can revisit the CLR model. Thank you for your perspective on what data the community has and what will be valuable to them.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/347#issuecomment-696512260:92,message,message,92,,https://github.com/google/deepvariant/issues/347#issuecomment-696512260,1,['message'],['message']
Integrability,"Hi @jumpyknight . Which GIAB sample are you using and which version of the truth set? For HG002, there is a new truth set (v4.1) - ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/analysis/NIST_v4.1_SmallVariantDraftBenchmark_12182019/. Which corrected a number of errors in the prior truth set. If you are using v3.3.2 for this sample, it would be interesting to know if those have been corrected. To you question about the type of variants that occur. Yes, it is known that repeat expansion and contraction is more likely to occur, and these tend to add to the full repeat content. So it is not unexpected to see changes in repeat number. . To your question about whether this may have caused DeepVariant to be more prone to call reference, I think the proportion of errors in Genome in a Bottle is quite small. DeepVariant is very accurate, so these may be a reasonable fraction of the total apparent errors, but they are not a large fraction of the total training sites. I don't think that the presence of errors in the training set will have a large impact on DeepVariant, but I it may make DeepVariant just a bit worse than it could otherwise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/317#issuecomment-645804554:510,contract,contraction,510,,https://github.com/google/deepvariant/issues/317#issuecomment-645804554,1,['contract'],['contraction']
Integrability,"Hi @kalexiou ,; I think what might have happened is that the make_examples stage ran out of memory.; I recently had a run on a different BAM, which had similar behavior as you described. A few suggestions:; (1) If you're using the same type of machine, try running with fewer number of num_shards which should allow each make_examples to have more RAM.; (2) If you can try a different machine type, try n1-highmem-16 (104GB RAM) to see if that helps. We're looking into how we can make our error messages informative if this is the key issue. Based on this observation, I suspect your intermediate make_examples output might not be complete.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/427#issuecomment-793438811:496,message,messages,496,,https://github.com/google/deepvariant/issues/427#issuecomment-793438811,1,['message'],['messages']
Integrability,"Hi @kishwarshafin,. Ok thanks for the explanation. . Unfortunately, I am still having trouble. For some reason when I use positional_labeler my make_examples jobs fail. But they complete successfully using haplotype_labeler. I can't figure out what the issue is as the error message isn't particularly informative, at least not to me. I attach two log files from the make_examples step. Both are for the same sample, the only difference is that one uses haplotype_labeler (job succeeded) and the other uses positional_labeler (job failed). . It would be great to get your opinion on what is going on. Note that I have tested positional labeler a few times and it does seem to work for one sample, but there is no reason this sample should be distinct from the others. . haplotype_labeler:; [MAKE_EX_TRAIN_NEW_4926611-3.err.gz](https://github.com/user-attachments/files/16974777/MAKE_EX_TRAIN_NEW_4926611-3.err.gz). positional_labeler:; [MAKE_EX_TRAIN_NEW_4930167-3.err.gz](https://github.com/user-attachments/files/16974781/MAKE_EX_TRAIN_NEW_4930167-3.err.gz). Thanks. Dan",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2345422270:275,message,message,275,,https://github.com/google/deepvariant/issues/876#issuecomment-2345422270,1,['message'],['message']
Integrability,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** ; ```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:127,message,message,127,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,1,['message'],['message']
Integrability,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-465225098:1178,interface,interface,1178,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098,2,['interface'],['interface']
Integrability,"Hi @kokyriakidis. Thank you for your question. There are a few possibilities for this. First, chrX and chrY do share regions of homology, the pseudo autosomal regions (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2435358/), which allows them to pair and segregate appropriately during meiosis. Because the reference sequence for both chrX and chrY contains these sequences, even in a female individual, reads will map to either and this can manifest as HET calls. It could be good to look at whether these calls occur in PAR regions. This will depend on which reference you are using, as some references mask this region. Can you tell us the reference build that you are using?. Second, reads may end up mismapped onto chrY from autosomes. When mismapping occurs, there is signal that will look like variation. The variant callers are not told whether a sample is male or female, and have to judge whether the signal is consistent with a variant or if the sample is female. Errors in this process can occur. How many variants are you seeing as PASS, it could be illustrative to compare this number to other benchmark human samples. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/196#issuecomment-511731809:544,depend,depend,544,,https://github.com/google/deepvariant/issues/196#issuecomment-511731809,1,['depend'],['depend']
Integrability,"Hi @kostasgalexiou , can you provide more information like:; - What type of machines are you working on (how many cores, how much RAM); - Any error messages that you can share?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/427#issuecomment-788314979:148,message,messages,148,,https://github.com/google/deepvariant/issues/427#issuecomment-788314979,1,['message'],['messages']
Integrability,"Hi @lucasbrambrink ,. I tried to recreate the building process, so here are some changes I made to the scripts:; ```; diff --git a/settings.sh b/settings.sh; index 9d5f58c0..649b1bb8 100755; --- a/settings.sh; +++ b/settings.sh; @@ -89,18 +89,18 @@ export DV_GPU_BUILD=""${DV_GPU_BUILD:-0}""; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; -export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; +export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-0}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow""; -export DV_TF_NUMPY_VERSION=""1.19.2"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; +export DV_TF_NUMPY_VERSION=""1.24.1"" # To match GCP_OPTIMIZED_TF_WHL_FILENAME; ; # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}""; ; -export PYTHON_VERSION=3.8; +export PYTHON_VERSION=3.9; # shellcheck disable=SC2155; export PYTHON_BIN_PATH=""$(which python${PYTHON_VERSION})""; export PYTHON_LIB_PATH=""/usr/local/lib/python${PYTHON_VERSION}/dist-packages""; @@ -112,7 +112,7 @@ export USE_DEFAULT_PYTHON_LIB_PATH=1; # --experimental_build_setting_api""; # Presumably it won't be needed at some later point when bazel_skylib is; # upgraded again.; -export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11""; +# export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:446,Bridg,Bridge,446,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['Bridg'],['Bridge']
Integrability,"Hi @lucasbrambrink ,. Thanks for your prompt response. I started with an empty docker container and ran each script manually. And after I built, I committed the container to create a new image: deepvbuild:latest. Because I made quite a few changes, I have lost track of all the changes I have made. . For example, I made quite some changes to the version numbers because pip has version conflicts. The main ones are that I had to use python3.9 and pandas 1.4.4. Also, I kind of have to switch between numpy 2.0.2 (to build) and 1.24.1 (to run). I also had to install tensorflow-addons from git repo because pip does not have that. In build-prereq.sh, I downloaded bazel 7.3.1 linux arm64 binary. The main difficulty lies in building pyclif. There were a lot of errors related to protobuf cmake modules or linking abseil but it worked in the end. I don't think they should cause problems if they build successfully. For `build_release_binaries.sh`, I had to change the folder name from `k8-opt` to `aarch64-opt`. I am not sure whether this would cause problems but I also commented out line 59: `find ""runfiles/com_google_deepvariant"" -name '*.so' -exec ln --force -s --relative ""runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so"" {} \;` because there were no such folders or .so files. Because all bazel tests are passed and the binaries are successfully built, I would think it is not an error related to the dependencies. I am not sure how to get more information on what may go wrong from the error trace. Please feel free to let me know if there is anything else I can run to generate more useful information. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334737190:1430,depend,dependencies,1430,,https://github.com/google/deepvariant/issues/879#issuecomment-2334737190,1,['depend'],['dependencies']
Integrability,"Hi @maryawood ,; The default values in make_examples.py are our recommendations.; We do adjust things a bit based on different datatypes. For example, for PacBio, we changed --vsc_min_fraction_indels to 0.12:; https://github.com/google/deepvariant/blob/r1.1/scripts/run_deepvariant.py#L238; ```; special_args['vsc_min_fraction_indels'] = 0.12; ```. For retraining, it will depend on your data. For example, using different mappers could end up with different expected distributions of the mapping quality, and you might want to adjust accordingly. Given that training is a more advanced topic and it highly depends on your data, I don't have a simple recipe for that. One general rule is that you want to adjust these thresholds so that the candidate generation step is sensitive enough to propose the true variants, but not over sensitive that it ends up proposing too much noise.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/464#issuecomment-867256204:373,depend,depend,373,,https://github.com/google/deepvariant/issues/464#issuecomment-867256204,4,['depend'],"['depend', 'depends']"
Integrability,"Hi @mattwood-codifiedgenomics , I have an internal tracker to track this, so I'll close this for now.; If I encounter any issues when adding the dependency, I'll let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-876548428:145,depend,dependency,145,,https://github.com/google/deepvariant/issues/469#issuecomment-876548428,1,['depend'],['dependency']
Integrability,"Hi @mdriller . My answer to your question will depend on what exactly you will need from Plink and what sort of cohort approach you have. . If you just want to be able to run Plink on the joint genotype results, I wonder if you can try following the process which was performed for UKBiobank to convert their DeepVariant exome joint calls into PLINK format. That is the section **Conversion of pVCF to PLINK and BGEN files** [from the UKBiobank WES Protocol](https://biobank.ctsu.ox.ac.uk/crystal/ukb/docs/UKB_WES_Protocol.pdf). I hope this will work, as it is not generally our preference to replicate the functionality of -ERC BP_RESOLUTION, and this is likely to make writing output much slower. If, instead, you want calls at specific sites (similar to a genotyping chip approach but with NGS data), I would say that is is possible to force genotyping at a given set of alleles with one of the modules of DeepVariant (VCF candidate importer). I suspect this isn't what you want though. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172:47,depend,depend,47,,https://github.com/google/deepvariant/issues/571#issuecomment-1274112172,2,"['Protocol', 'depend']","['Protocol', 'depend']"
Integrability,"Hi @moldach,. The error message indicates that the sequence contig names present in the reference genome don't match those in the BAM file. . It would be useful to know the names and lengths of the contigs in the BAM header. Are you able to provide the output of this command:. samtools view -H maddog_bam_trim_bwaMEM_sort_dedupped.bam | grep \@SQ. Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-607119273:24,message,message,24,,https://github.com/google/deepvariant/issues/292#issuecomment-607119273,1,['message'],['message']
Integrability,"Hi @njbernstein . Performance on STR will be a function of the size of the event. In Illumina data, DeepVariant will likely stop calling events as they start to reach 100bp in size and larger. DeepVariant will call STR events below this size (for example, here is a HET call repeat expansion in one allele and repeat contraction in the other from a DeepVariant HG002 WGS VCF):. 10	50527727	.	CTATATATATATATATATATATATATATATATATATATATA	C,CTATATATATATATATATATATATATATATATATATATATATA	37.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/2:17:56:2,22,29:0.392857,0.517857:37,20,52,20,0,40. I don't have stratified accuracy metrics for STR performance, nor do I have comparisons of this to dedicated STR tools. I would imagine that dedicated STR callers perform better for the long (100bp+) events, due to specific approaches for that class of problem, below 100bp, I do not have an intuition as to which approach will perform better. For complex variants, do the extent these are in a size range callable by DeepVariant, DeepVariant will represent the sequence-resolved candidates found for variation. Here is an example from a DeepVariant HG002 WGS VCF):. 1 67310873 . CAAAAAAAAAAAAAAAAAAAGAAAAATTAAA C,CAAAAAAAAAAAAAAAAAAAAAGAAAAATTAAA 45.4 PASS . GT:GQ:DP:AD:VAF:PL 1/2:18:36:2,26,6:0.722222,0.166667:28,4,35,4,0,2. The second ALT allele has insertions of A at multiple places, so that this doesn't cleanly fit into a single contiguous set of inserted or deleted bases. In practice, these complex events will be rare in the size range that DeepVariant is designed to address as a small variant caller. Accidentally (because we did not design or train DeepVariant to do so), DeepVariant will call much larger insertion events in PacBio CCS data. Finally, with respect to your haplotype-aware question. Conceptually the first two steps are quite similar. For the first step, identifying which regions to reassemble, DeepVariant employs a relatively simple model which identifies regions that will benefit from reassembly. The",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/180#issuecomment-488147736:317,contract,contraction,317,,https://github.com/google/deepvariant/issues/180#issuecomment-488147736,2,['contract'],['contraction']
Integrability,"Hi @obsh, I am still not sure what is causing the failure. I noticed that you are using an older release (v0.6.1), so I would suggest switching to the latest v0.8.0 release. I'm not sure that this will address the failures, but I would recommend it regardless since we have updated the code and models. . As a sanity check, you could try running the command above with different data. [This page](https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration) contains an example using Whole Exome Sequencing (WES) data. If the problem is fixed, perhaps there is something unexpected about your data, and we can further investigate. We also recommend that you try directly running DeepVariant through the Docker image, which may provide additional error messages that are helpful for debugging. You can refer to [this document](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) for an example. Please let us know if you run into the issue through this setup. One thing to note: our models have been trained using human data, and may not necessarily generalize well across species. We have trained a model for mosquito data, and while doing so, we noticed that retraining using data from this species significantly improved performance (more details in [this post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)). The mosquito model is not public, but we are happy to share it with you. Feel free to email me at gunjanbaid@google.com for more information.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207#issuecomment-524686835:789,message,messages,789,,https://github.com/google/deepvariant/issues/207#issuecomment-524686835,1,['message'],['messages']
Integrability,"Hi @pgrosu,. Thank you very much for looking into the paper. We always try to do our best to present the algorithm, code and experimental design to deliver the message most clearly. Any feedback on how to make it better and more understandable is always highly appreciated. However, as this is not a technical issue about DeepVariant, it would be helpful to do this over email. Please send an email to shafin@google.com at your convenience so we can discuss how to improve the manuscript.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/709#issuecomment-1724108680:160,message,message,160,,https://github.com/google/deepvariant/issues/709#issuecomment-1724108680,2,['message'],['message']
Integrability,"Hi @pichuan , I am planning to integrate deepvariant into a whole germline variant calling pipeline which is using python3.8 and built everything into a Docker. So I need to build every prereq relating deepvariant inside it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/441#issuecomment-821249453:31,integrat,integrate,31,,https://github.com/google/deepvariant/issues/441#issuecomment-821249453,1,['integrat'],['integrate']
Integrability,"Hi @pichuan ,. Yes, the error messages seem not affect the run and the GPU was utilized during the run. ; I will run another round of training step, and will check the GPU utilization and the messages. Also, I have two questions about the training step. 1. In the Blog (Improved non-human variant calling using species-specific DeepVariant models), in a trio, one offspring was used for establishing the ""silver dataset"", and five other progenies were used for training and evaluating. So the individuals used for generating the truth dataset and training are different.; However, in the deepvariant documents, it seems that the truth dataset, training and evaluating examples all come from HG001, the same individual. Does the individual used for generating truth dataset and training have to be different? If not, could you please explain why di d you use different individuals in the first case?. 2. If I have multiple trios, can I first call ""silver set"" with a child from each trio separately, and make examples using the same child from each trio with its silver set, and then shuffle all examples together to train. Thank you!. Zuyao",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1787688990:30,message,messages,30,,https://github.com/google/deepvariant/issues/722#issuecomment-1787688990,2,['message'],['messages']
Integrability,"Hi @pichuan Thanks for the advise. Will look into these possibilities. The coarse-grained shuffling would be easiest, however it is mentioned that shuffling is an important step in the document you mentioned as well as the training page. Technically stochastic/batch gradient descent does depend on random batches. I will look into spark, as well as other options like dask or torque (which would need a script hack). If I have a setup that works for local clusters, I will share it. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/360#issuecomment-707444280:289,depend,depend,289,,https://github.com/google/deepvariant/issues/360#issuecomment-707444280,1,['depend'],['depend']
Integrability,"Hi @pichuan ~. Thank you for this quickly reply~. I tried to add `--user root` into docker command like the solved issue #325 ; But it still didn't show any additional error message like the issue ; I also check the disk and it is still have about 9 Gb free space. Here is the code and message output. ```sh; $ BIN_VERSION=""1.0.0""; $ sudo docker pull google/deepvariant:""${BIN_VERSION}""; Digest: sha256:6ef8f3b4c4465e41ee7597cd2351a7c44dd8b62a849a47766316507a6234f5f8; Status: Downloaded newer image for google/deepvariant:1.0.0; docker.io/google/deepvariant:1.0.0. $ INPUT_DIR=""${PWD}/quickstart-testdata""; $ ls -1 ${INPUT_DIR}; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi. $ OUTPUT_DIR=""${PWD}/quickstart-output""; $ ls ${OUTPUT_DIR}; intermediate_results_dir. $ df -h; 檔案系統 容量 已用 可用 已用% 掛載點; udev 7.8G 0 7.8G 0% /dev; tmpfs 1.6G 11M 1.6G 1% /run; /dev/sda1 109G 95G 8.9G 92% /; tmpfs 7.9G 200K 7.9G 1% /dev/shm; tmpfs 5.0M 4.0K 5.0M 1% /run/lock; tmpfs 7.9G 0 7.9G 0% /sys/fs/cgroup; /dev/loop1 56M 56M 0 100% /snap/core18/1885; /dev/loop3 55M 55M 0 100% /snap/gtk-common-themes/1502; /dev/loop2 162M 162M 0 100% /snap/gnome-3-28-1804/128; /dev/loop4 161M 161M 0 100% /snap/gnome-3-28-1804/116; /dev/loop0 30M 30M 0 100% /snap/snapd/8790; /dev/loop5 63M 63M 0 100% /snap/gtk-common-themes/1506; /dev/loop6 55M 55M 0 100% /snap/core18/1880; /dev/loop7 30M 30M 0 100% /snap/snapd/8542; /dev/loop8 39M 39M 0 100% /snap/remmina/4309; /dev/loop9 40M 40M 0 100% /snap/remmina/4324; tmpfs 1.6G 40K 1.6G 1% /run/user/108; tmpfs 1.6G 12K 1.6G 1% /run/user/1000. $ sudo docker run \; > --user root \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-689932270:174,message,message,174,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270,2,['message'],['message']
Integrability,"Hi @pichuan, I have tried singularity as suggested at the link and it worked fine. But I got the message below. The deepvariant version is 1.6.0 as used in the case at the link. Are there other versions available? How can I list the available versions? Thank you very much for all your help. 2023-11-24 20:50:47.554286: I tensorflow/core/platform/cpu_feature_guard.cc:193]; This TensorFlow binary is optimized with oneAPI Deep Neural Network Library; (oneDNN) to use the following CPU instructions in performance-critical operations:; AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate comp; iler flags. > Hi @spz1st , Building on different systems can be tricky. Have you considered using solutions like Singularity (or directly using Docker if you have permission)? An example of running with Singularity can be found in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740#issuecomment-1826444464:97,message,message,97,,https://github.com/google/deepvariant/issues/740#issuecomment-1826444464,1,['message'],['message']
Integrability,"Hi @pichuan,. The ""intervaltree==2.1.0"" resolved my another failed case of ""//deepvariant:make_examples_test"". While ""//deepvariant/labeler:haplotype_labeler_test"" still failed. And the following is the error message that I got:. ```; FAIL: test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:209,message,message,209,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['message'],['message']
Integrability,"Hi @pichuan,. VM instance: n1-standard-16 (16 vCPUs, 60 GB memory). I don't get any error messages. The instance is still on and not giving any errors....or at least I haven't found any logs myself... Below is the structure of the contents in the VM instance:. ├── [drwxrwxr-x 4.0K] input; │ └── [drwxrwxr-x 4.0K] data; │ ├── [-rw-rw-r-- 2.9G] Annuum.v1.6.Total.fa; │ ├── [-rw-rw-r-- 1.6M] Annuum.v1.6.Total.fa.fai; │ ├── [-rw-rw-r-- 17G] FC85.sort.pcr_rem.RG.kept.bam; │ └── [-rw-rw-r-- 9.2M] FC85.sort.pcr_rem.RG.kept.bam.bai; └── [drwxrwxr-x 4.0K] output; └── [drwxr-xr-x 4.0K] intermediate_results_dir; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 337M] gvcf.tfrecord-00007-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00008-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00009-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00010-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00011-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00012-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00013-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00014-of-00016.gz; ├── [-rw-r--r-- 338M] gvcf.tfrecord-00015-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00000-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00001-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00002-of-00016.gz; ├── [-rw-r--r-- 9.9G] make_examples.tfrecord-00003-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00004-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00005-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00006-of-00016.gz; ├── [-rw-r--r-- 10.0G] make_examples.tfrecord-00007-of-00016",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/427#issuecomment-788690917:90,message,messages,90,,https://github.com/google/deepvariant/issues/427#issuecomment-788690917,1,['message'],['messages']
Integrability,"Hi @qili93 ,. This is a bug that the mock ref_reader does not have its 'contig' function return value populated. This has been fixed internally and will be part of the next release. In the meantime, if you want to modify your own version of the code you need to do the following:. In deepvariant/labeler/BUILD, in the ""haplotype_labeler_test"", add the following dependency:; ""//third_party/nucleus/protos:reference_py_pb2"",. In deepvariant/labeler/haplotype_labeler_test.py, include the following import and mock call:. ### Insert at line 42; from third_party.nucleus.protos import reference_pb2. ### Insert at line 346; labeler._ref_reader.contig.return_value = reference_pb2.ContigInfo(name='20', n_bases=50)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-465775663:362,depend,dependency,362,,https://github.com/google/deepvariant/issues/154#issuecomment-465775663,1,['depend'],['dependency']
Integrability,"Hi @qili93 ; can you paste your error messages here?. Many of the recent test errors reported by users are related to the fact that we didn't pin our intervaltree version in our last release, and intervaltree v3 came out and had some API changes... So, the first thing to try is to change this line:; https://github.com/google/deepvariant/blob/r0.7/run-prereq.sh#L86; to:; ```; pip install --user 'intervaltree==2.1.0'; ```; and see if the tests will pass? Please let me know. Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464599230:38,message,messages,38,,https://github.com/google/deepvariant/issues/154#issuecomment-464599230,1,['message'],['messages']
Integrability,"Hi @richard-nm , thanks for the update. Given that our team didn't build or maintain the bioconda version, I'm not familiar with many things here. One question and one comment for you:; 1. Question: Where can I find documentation of this tool? For example, ""dv_call_variants.py"" isn't part of our GitHub repo, so I'm not familiar with that at all.; 2. Comment: Purely based on the error message you posted:; ```; Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax; ```; The corresponding line would be this one:; https://github.com/google/deepvariant/blob/r1.4/deepvariant/call_variants.py#L374; I'm not exactly why that would cause a SyntaxError though. It's a bit hard for me to figure out given that it's wrapped in something that I'm not familiar with. I did try to follow your steps above, but didn't succeed in installing:; ```; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; ```; gave me:; ```; UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant=1.4.0 -> python[version='>=3.6,<3.7.0a0']. Your python: python=3.7.5; ```. @richard-nm One more question for you -- is it possible to use Docker or Singularity? Which our team can support more easily as well. Thank you!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948:387,message,message,387,,https://github.com/google/deepvariant/issues/627#issuecomment-1518213948,2,"['message', 'wrap']","['message', 'wrapped']"
Integrability,"Hi @ruolin ,; thanks for reporting this issue. I'll try running on your BAM and reference and see if we can reproduce the issue.; We have in the past seen cases where the jobs run out of memory, and our error messages in that situation isn't very clear. So @danielecook 's guess of OOM makes sense. But the memory you're reporting sounds like it should be enough. So let me see if I can reproduce this issue.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446#issuecomment-826519447:209,message,messages,209,,https://github.com/google/deepvariant/issues/446#issuecomment-826519447,2,['message'],['messages']
Integrability,"Hi @sh940202123 , it's not obvious to me why this might be the case. It's strange that no error message comes out at all. Can you try something even simpler, like:; ```; sudo docker run google/deepvariant:1.0.0 /opt/deepvariant/bin/make_examples --help; ```; And see if the information comes out correctly?. I'll also check with my team member tomorrow to see if anyone has other suggestions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690806128:96,message,message,96,,https://github.com/google/deepvariant/issues/345#issuecomment-690806128,2,['message'],['message']
Integrability,"Hi @sh940202123 ,; Sorry that our error messages are still quite cryptic at this time. The last traceback is actually not quite as useful. You're seeing it because we uses Python's subprocess.check_call to call each of the commands:; https://github.com/google/deepvariant/blob/7ed651ed54d8563530d58ba80187121337fd10f2/scripts/run_deepvariant.py#L362. Usually, I hope that whatever appeared right before that is useful, though. For example, if I deliberately deleted the bam file, I got:. ```; ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. [E::hts_open_format] Failed to open file ""/input/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2136, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2063, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 427, in default_options; with sam.SamReader(flags_obj.reads.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_1ip32q9b/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_1ip",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-689891360:40,message,messages,40,,https://github.com/google/deepvariant/issues/345#issuecomment-689891360,1,['message'],['messages']
Integrability,"Hi @sh940202123 it seems the CPU on the host `Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz` doesn't support AVX instruction ( https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#%22Lynnfield%22_(45_nm) ), which is required for the TensorFlow release version we use in the docker image. Please see this previous comment https://github.com/google/deepvariant/issues/217#issuecomment-530580878 for more details. That being said, I do find it strange that it's silently not producing outputs, instead of showing an error message about the unsupported instruction.. Is it possible for you to try the same command in a machine with newer CPU? For Intel CPUs, _Sandy Bridge_ (released in 2011) or later architectures should support AVX. https://en.wikipedia.org/wiki/List_of_Intel_Core_i7_microprocessors#Sandy_Bridge_microarchitecture_(2nd_generation)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690820723:527,message,message,527,,https://github.com/google/deepvariant/issues/345#issuecomment-690820723,1,['message'],['message']
Integrability,Hi @situssog ; From the error message it seems like there's something wrong with the base quality scores in your BAM. Is there more information you can provide? Are you able to use the same BAM file with other tools that reads the base quality scores?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270#issuecomment-587577840:30,message,message,30,,https://github.com/google/deepvariant/issues/270#issuecomment-587577840,1,['message'],['message']
Integrability,"Hi @sivianil . That is interesting. I don't think that this error message has something to do with the allele being multiallelic. We generate multiallelic outputs for human samples and hap.py works for those. . If you want to filter to only bi-allelic calls, you can post-process the VCF with bcftools . `bcftools view -m2 -M2`. From reading the error, it seems that the most likely explanation is that the reference genome used in the hap.py evaluation does not exactly match the sequence of the reference genome used to map the reads in the BAM file used in DeepVariant. Are you certain those two files are exactly the same?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/606#issuecomment-1405862600:66,message,message,66,,https://github.com/google/deepvariant/issues/606#issuecomment-1405862600,1,['message'],['message']
Integrability,"Hi @sophienguyen01,. Just a few additional notes. It's been awhile since I run things on AWS. . My instinct is that the most cost-effective instance will turn out to be m7i.4xlarge. m7i.8xlarge will likely be slightly more expensive, but should scale close to linear in speed. . If you have PacBio data and experience a failure, you might want to try R7iz.4xlarge or R7iz.8xlarge. The GPU instances will be faster, but I suspect not necessarily cost-optimal. So it depends on what you are looking for.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1681230125:465,depend,depends,465,,https://github.com/google/deepvariant/issues/696#issuecomment-1681230125,1,['depend'],['depends']
Integrability,"Hi @spz1st ,; Was that just a warning and you're still able to run DeepVariant, or does the program actually crashed after that?; I think the message just means that your machine might not have certain optimized ops, but I think you should still be able to run it. If the code did crash, please let me know if there are more error messages that you're seeing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/740#issuecomment-1828442518:142,message,message,142,,https://github.com/google/deepvariant/issues/740#issuecomment-1828442518,2,['message'],"['message', 'messages']"
Integrability,"Hi @yonatansc97 . Those imports you listed have something in common: they are in C++ (or protos) and therefore need to be compiled first.; Inside the docker container, we have these already compiled into the binaries like `/opt/deepvariant/bin/make_examples`, while `make_examples.py` is the source, but it won't work without compiling its C++ dependencies. I don't know how to make this work with your special setup, but perhaps the documentation on building from source will help get you started: https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md. I hope that helps at least point you in the right direction!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/359#issuecomment-701664438:344,depend,dependencies,344,,https://github.com/google/deepvariant/issues/359#issuecomment-701664438,1,['depend'],['dependencies']
Integrability,"Hi @zhoudreames ,; Can you read this section and see if it answers your question? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data. In this particular case, many of the reads seem to have low mapping quality. DeepVariant has a min_maping_quality flag which can be lowered. Depending on the sequencer and the mapping software, if you think lowering it from the default (5) to a lower number makes sense, you can try adding the --min_mapping_quality flag to your make_examples step.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/498#issuecomment-992798611:344,Depend,Depending,344,,https://github.com/google/deepvariant/issues/498#issuecomment-992798611,1,['Depend'],['Depending']
Integrability,"Hi @zihhuafang ,; The flag was added after 1.6.1 was released. Sorry about that. Note that `run_deeptrio` is a wrapper script that just runs the underlying binaries. So, for now you can run `run_deeptrio` with the `--dry_run` flag, which will print out all the commands it is going to run with each of the steps. From there, you can modify to make sure the postprocess_variants has the correct flag for the corresponding samples. Sorry for the inconvenience for now. The `postprocess_variants_parent1_extra_args` flag will be available in the next release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/816#issuecomment-2121705660:111,wrap,wrapper,111,,https://github.com/google/deepvariant/issues/816#issuecomment-2121705660,1,['wrap'],['wrapper']
Integrability,Hi @zjminglead; The error messages say the BAM file looks truncated. Does it give any errors when you run `samtools view <BAM> | tail` and look at the end of the BAM file?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455#issuecomment-836919821:26,message,messages,26,,https://github.com/google/deepvariant/issues/455#issuecomment-836919821,1,['message'],['messages']
Integrability,"Hi @zxy1555847 . @MariaNattestad is correct that there are only a small number of errors, so it is hard to definitively tell you what is going on. . However, one thing I want to point out is that in general for exome sequencing, we expect for all analysis methods, accuracy will start dropping outside of the capture ranges with an increasing amount the farther we go from the capture. We also expect Indel to be affected more than SNP. . The reasons for this is that sequence coverage begins to drop toward the boundaries of the capture (the amount of this drop depends on the particular capture and the sequence context around it, but on average it will be the case). In general, lower coverage will mean lower accuracy, but we observe that coverage has a larger effect on Indels than SNPs (this is detailed in our [Extensive sequence dataset](https://www.biorxiv.org/content/10.1101/2020.12.11.422022v1) paper. The reasons that are complex (though if you want me to further elaborate, I can try). . In short, Indel accuracy dropping outside of capture regions is expected to some extent, and this is a function of the underlying sequencing method as opposed to the analysis method.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616#issuecomment-1444172404:563,depend,depends,563,,https://github.com/google/deepvariant/issues/616#issuecomment-1444172404,1,['depend'],['depends']
Integrability,"Hi Andrew, thank you very much for the feedback. This is something new I have learnt about the BAM files. Using the filtered BAM file, the error message disappears. The number of variants called has also increased considerably (~x20 for variants with PASS tag). Our reads are in fact HiFi. We have been doing the alignment with `minimap2 -ax map-pb` because to our understanding `deepvariant` is designed for read alignments (and not assembly-to-reference alignments as achieved with `minimap2 -ax asm`). Is this a misunderstanding? Could `deepvariant` be safely used with BAMs for assembly-to-reference alignments?. Thank you again,; Eugenio",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434#issuecomment-815783472:145,message,message,145,,https://github.com/google/deepvariant/issues/434#issuecomment-815783472,2,['message'],['message']
Integrability,"Hi Attila,. Yeah it takes about a week to fully uncover how all the dependencies work together, which can be fun if you have the time. The side-benefit is that then you'll discover that DeepVariant is just a basic collections of tools (and data-structures) with many possibilities to tweak, simplify and expand on - which can be even more fun to play with :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361132599:68,depend,dependencies,68,,https://github.com/google/deepvariant/issues/41#issuecomment-361132599,2,['depend'],['dependencies']
Integrability,"Hi Charles,; the bad_alloc error could indicate that you're running out of memory on this machine.; It seemes like m5.4xlarge have 64 GB RAM? Currently postprocess_variants reads in everything in memory and sorts them, which can take quite a lot of memory depending on how many records the previous step generated.; I suggest trying this step on a machine with more RAM, or keep an an eye on whether the memory usage is an issue. Currently for our WGS Case Study , we recommend getting a machine with 128GB RAM.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479348077:256,depend,depending,256,,https://github.com/google/deepvariant/issues/167#issuecomment-479348077,1,['depend'],['depending']
Integrability,"Hi Fra,. As far as I can infer, there is something in the BAM and reference files that the variant caller is unable to call variants for. Can you please take a look in the chromosome 8 region with the [Integrative Genomics Viewer (IGV)](https://software.broadinstitute.org/software/igv/) using your BAM aligned to the same exact reference, to confirm that you see some variation and proper alignment. I am looking at the call variant code, and the allele counts are dependent on the reference and read support by position. You should also limit your analysis with the `--regions` flag so it is quicker as well. If your data is WES then you should use the WES model, in order for the results to be scientifically valid. Maybe it might help if you realign your FASTQ files to the reference used by DeepVariant to regenerate the BAM file and the subsequent BAI file, just to be sure nothing happened along the way. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458:202,Integrat,Integrative,202,,https://github.com/google/deepvariant/issues/675#issuecomment-1652409458,2,"['Integrat', 'depend']","['Integrative', 'dependent']"
Integrability,"Hi I'm the GLnexus maintainer. From that side we're currently lacking a motivating, medium/large scale study with DeepVariant gVCF files which would provide the context to chase down the gnarly corner cases that arise in gVCF merging / joint genotyping, completing the current ""experimental"" integrated configuration. There are a few candidates in the pipeline but more would be really welcome. Ping me if I can help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-465059695:292,integrat,integrated,292,,https://github.com/google/deepvariant/issues/142#issuecomment-465059695,1,['integrat'],['integrated']
Integrability,"Hi Maria! Thanks for your fast reply,; so the Bam I'm using is acutually generated a bit differently than usual (at least it's not actual HiFi ccs reads).. I performed LAA from pb software (v8) on targeted (one gene) HiFi amplicon data. LAA performs ccs and demultiplexing, and from that it immediately creates consensus 'clustered/phased' sequences (as the target is only one gene, the fastq consists of only one or two consensus seqs, depending on the found alleles). I have mapped those against our reference gene using minimap2 and this results in the Bam I'm using as input here. . This particular Bam contains only one read. My worry is that DeepVariant will not like the fact that there is only one read, but I believe this issue would be similar when performing lowcov sequencing. Maybe the model I'm using 'PACBIO' is considering actual PacBio HiFi ccs reads? Anyways, I didn't get to that conclusion because it didn't find the record at all. My guess was that it had to do with sam parsing. ; I'm curious for your opinion.. :). Annabel",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-839642012:437,depend,depending,437,,https://github.com/google/deepvariant/issues/457#issuecomment-839642012,1,['depend'],['depending']
Integrability,"Hi Maria,; I'm very glad and excited to get a response from you personally. I appreciate your work and get a lot of help from your youtube videos :); Thank you very much for the answer, this is exactly what I've thought and tried to do. For a specific region (for example: ""chr15:98707564-98707715"") it worked well and produced a vcf file. For the bed file I've used the command:; bedtools bamtobed [OPTIONS] -i <BAM>; It extracted the regions from the bam file and than I removed the last three columns (bedtools creates bed file with 6 columns). The problem is that when I run the command with the exome regions from the capture protocol it also not working for me; (""idt_capture_novogene.grch38.bed""). Again I get the error:; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/wes_deepvarfast_38.sorted.bam --examples /output/inter_res/make_examples.tfrecord@8.gz --gvcf /output/inter_res/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917336418:631,protocol,protocol,631,,https://github.com/google/deepvariant/issues/483#issuecomment-917336418,1,['protocol'],['protocol']
Integrability,"Hi Mark and Asha,; here's what I believe the current status is:; (1) If there is just an empty shard (a shard file that exist, but just contains 0 record) out of many, what happens is the code will move on to the next shard to attempt to read image/format. -- this is what Mark meant by the previously fixed empty shards bug.; (2) However, if all the shard files exist but all of them contains 0 records, the current code can fail with that error message above. In this case, if the actual error message observed is:; The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format 'None' (expected 'raw'). It seems like this call_variant run is specifically being done on on that one file. And if that file has 0 record, unfortunately it will currently fail with that error. :-(. So, I think this is a real bug that we should fix. Because we do expect the use case where users run 64 separate call_variants, and some of them might have complete empty single input file. Is that correct?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355805026:447,message,message,447,,https://github.com/google/deepvariant/issues/27#issuecomment-355805026,2,['message'],['message']
Integrability,"Hi Paul, thanks for mentioning this issue.; I looked at our documentation and noticed that an update was made to our README a few days ago with this extra description:. Pre-built binaries are available at [gs://deepvariant/](https://console.cloud.google.com/storage/browser/deepvariant).; These are compiled to use SSE4 and AVX instructions, so you'll need a CPU (such as Intel Sandy Bridge) that supports them. (The file /proc/cpuinfo lists these features under ""flags"".). But it seems like this new information to the doc hasn't be synced to the external GitHub yet. This should come out in the new year at the latest. I suspect we'd like to keep the pre-built binary having optimization. But we will at least add that line of disclaimer so that it's clear what the binaries are built for.; Would it be ok for you to build DeepVariant for your CPU by following [Building and testing; DeepVariant](docs/deepvariant-build-test.md), or do you need pre-built binaries without AVX from us?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353701703:384,Bridg,Bridge,384,,https://github.com/google/deepvariant/issues/21#issuecomment-353701703,1,['Bridg'],['Bridge']
Integrability,"Hi Paul,. Thanks for your kind comment.; I don't know how to resolve my issue (time-dependent increasing memory usage) by your analysis (CPU usage? I'm sorry for my beginner comment...). Best,; Masaru",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/147#issuecomment-460971745:84,depend,dependent,84,,https://github.com/google/deepvariant/issues/147#issuecomment-460971745,1,['depend'],['dependent']
Integrability,"Hi Paul,. thanks for the explanation. In the meantime I found out that Debian 9 stretch uses openssl v1.1 whereas Ubuntu 16.04 uses v1.0 and perhaps that's why I run into the issue. I symlinked `ln -s libcrypto.so.1.1.0 libcrypto.so.1.0.0` and this hack solved the issue. But then other issues emerged on Debian 9, which made me try to build everything from source. However, the large number of dependencies prevented me from succeeding. Anyhow, I'll try installation on Ubuntu 16.04.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-361015565:395,depend,dependencies,395,,https://github.com/google/deepvariant/issues/41#issuecomment-361015565,1,['depend'],['dependencies']
Integrability,"Hi Phil,; as you can see from the log you posted, the error actually came from:; ```; File ""/opt/conda/envs/nf-core-deepvariant-1.0/lib/python2.7/site-packages/psutil/_pslinux.py"", line 701, in cpu_freq; ""can't find current frequency file""); ```; If I read this correctly, ""frequency"" isn't referring to anything DeepVariant related, but is referring to ""cpu_freq"". I also did a search in the DeepVariant codebase for the string ""can't find current frequency file"" and I can confirm that error message does not come from us. It is still possible that this is related to some interaction with the DeepVariant code, but from the information you provide, it's not clear to me how.; And, given that this is at the system level, I don't think it's related to what organism you're providing as the input data. To understand whether your input data works on DeepVariant code or not, the best way is probably to try it with just the DeepVariant code. If that doesn't work, it'll be more clear whether there might be different issues. If the data you're running on is publicly accessible, I'm also happy to try running DeepVariant on it. Let me know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191#issuecomment-504481029:494,message,message,494,,https://github.com/google/deepvariant/issues/191#issuecomment-504481029,2,['message'],['message']
Integrability,"Hi Phil,; it doesn't seem like this is relevant to our codebase. I suspect this is something from the wrapper from nf-core. I see that you're also asking on the nf-core GitHub issues, which is likely the right place to ask. ; If you think there might still be some issues related to the DeepVariant codebase, please feel free to add more information and reopen.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191#issuecomment-503826212:102,wrap,wrapper,102,,https://github.com/google/deepvariant/issues/191#issuecomment-503826212,1,['wrap'],['wrapper']
Integrability,"Hi Pi-Chuan,. I'm still not back home yet (so, I apologize that I'm not 100% answering your question). However, I can say this:. Even thought I would expect the direct command-line analysis (in AWS or Google Cloud) should cost less, I did briefly look into DNAnexus and Seven Bridges. However, DNAnexus requires that you create an account with an institutional e-mail, and I was checking about what were options for people who want to re-analyze that that is directly provided to them (who may not be scientists). I also didn't see a way to create a Seven Bridges account with a G-mail address, but both companies gave me an initial reply about my account creation question within 24 hours. That said, I am also currently working on uploading my data to PrecisionFDA (which uses DNAnexus, but I can't use my PrecisionFDA account to sign into DNAnexus). For the user, I think this would be free, and it provides a way to test results provided by companies (**and** I could create an account with a Gmail address). However, PrecisionFDA doesn't currently have a DeepVariant App. I passed along an earlier version of this thread to them, but I will also now ask if it might be helpful that DNAnexus already has a DeepVariant app (to see how easily that can be applied within PrecisionFDA). I'll upload the call_variants file when I get home. Unless there is something obvious that you can see, I think it would probably be more fair for your time to start a Google Cloud test (although that means you may have to wait a little while before I get to the same step on another platform). Thank you again for all of your prompt replies!. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479700070:276,Bridg,Bridges,276,,https://github.com/google/deepvariant/issues/167#issuecomment-479700070,2,['Bridg'],['Bridges']
Integrability,"Hi Pichuan,. It's a team effort :) Time is on our side as it's still sub-1.0. I think you can make it much easier and fun for yourself, as there another level to NGS analysis that can be tapped here. Currently the code sort of a wrapper to TensorFlow, and much of the definition of the input feature-set changed to the [0,254]-ranged 7 `ImageRow` channels -- `[base, base_quality, ..., matches_ref, op_len]` -- which is not really a RGB image anymore, and then run through the model to emit the three classes of predicted GT probabilities (homozygous reference, heterozygous and homozygous alternative):. ```; genotype_probabilities: 0.9999428988; genotype_probabilities: 1.8287e-05; genotype_probabilities: 3.88142e-05; ```. The VCF saving is helpful, but the interesting part is abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:229,wrap,wrapper,229,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['wrap'],['wrapper']
Integrability,"Hi Pichuan. > Can you give us a bit more information on your BAM? Is it WGS or WES? Which Illumina sequencing machine is it from?. The am using WES. We assumed this would run faster. We used UC Berkeley HiSeq 4000 illumina machine . > If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email [pichuan@google.com](mailto:pichuan@google.com); > . I sent the headers to you in email. > > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!; > . The run that worked well used WGS. The library was created by a different Lab. Not sure if this is relevant or not. We are running on RNA. We got really good F-scores on our ""gold standard"" data set. > > p.s. I am running in AWS . not sure if that makes a difference or not; > ; > I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > . region: oregen; m5dn.8xlarge; 32 CPU; 2 x 600GB SSD; Deep Learning AMI (Ubuntu 16.04) Version 26.0 (ami-07728e9e2742b0662)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573841969:599,message,message,599,,https://github.com/google/deepvariant/issues/260#issuecomment-573841969,1,['message'],['message']
Integrability,"Hi Ram,. I see what's happening. You have protobuf 3.5.1 in your include paths, but this is trying to compile the protobuf 3.6 version. You will notice in this line:. ```; new (initial_block_) Block(options_.initial_block_size, NULL);; ```. Which is only part of `3.6.x`:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/src/google/protobuf/arena.cc#L77. Basically a bunch of conflicts between declarations and use. So to simplify things, could you remove your protobuf 3.5.1 from your paths. If you are on a university cluster, it's usually something like `module unload MODULE_NAME`. After that try rerunning it again. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-422619015:292,protocol,protocolbuffers,292,,https://github.com/google/deepvariant/issues/94#issuecomment-422619015,2,['protocol'],['protocolbuffers']
Integrability,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:57,depend,depends,57,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044,1,['depend'],['depends']
Integrability,"Hi all,. sorry about the late reply I was testing the UKBiobank WES Protocol provided by Andrew but unfortunately it does not seem fix our problem.; The general issue is that to identify runs of homozygosity(ROH) with plink you can also just provide a vcf file but this vcf file needs a base resolution e.g. an entry for each position, whether it is variable or not:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT PacBio_CCS; SUPER_1 1 . C . . . DP=49 GT:AD:DP:RGQ 0/0:49:49:99; SUPER_1 2 . C . . . DP=50 GT:AD:DP:RGQ 0/0:50:50:99; SUPER_1 3 . T . . . DP=54 GT:AD:DP:RGQ 0/0:54:54:99; SUPER_1 4 . A . . . DP=61 GT:AD:DP:RGQ 0/0:61:61:99; ... And we were just wondering if there is a possibility to generate such a vcf file using DeepVariant. Thanks again for all the replies and help. best regards,. Max",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277:68,Protocol,Protocol,68,,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277,1,['Protocol'],['Protocol']
Integrability,"Hi sorry to jump on this thread but how sure are you of genomic stability; in your organism?. Just thinking about, mutation rates, did you amplify the pacbio sample,; have you checked for methylation?. Joe. On Mon, 31 Jul 2023, 17:17 Axze-rgb, ***@***.***> wrote:. > And that doesn't bother you? We interpret this as low clonal presence,; > while we have actually massive bad mapping of the T at the same place?; > Coincidence? Or are the few ""good"" Ts an artefact?; >; > My advisor once told me if I stopped asking such question I probably would; > have a PhD already, but ... I am warry of coincidence.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658710664>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2UURSWJPUBSPQQJU3LXS7LBBANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658730637:940,Message,Message,940,,https://github.com/google/deepvariant/issues/682#issuecomment-1658730637,1,['Message'],['Message']
Integrability,"Hi! Thanks for the question. Since you got DeepVariant working on the command line, this might just be a Nextflow issue, but we can certainly take a look and see if we can help on our end. A few questions and suggestions:; 1. Did you get this Nextflow pipeline from somewhere online or write it yourself?; 2. Can you include more of the log messages? The message you included looks to be from Nextflow, not from DeepVariant, so we need more context to help you debug this.; 3. Can you try running this with a small region, e.g. `--regions chr20:100,000-110,000` in the `run_deepvariant` command? This should make it faster to debug so you don't need to wait multiple hours. You can get that tiny run working before starting a full run.; 4. I'm not an expert on Nextflow, but do you need to set the `output:` in `process pbc_varicall` to see the outputs?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748:341,message,messages,341,,https://github.com/google/deepvariant/issues/659#issuecomment-1581314748,2,['message'],"['message', 'messages']"
Integrability,"Hi, ; I got another error when I remove my local download ( /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2) and re-run as follows. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; ERROR conda.core.link:_execute(700): An error occurred while installing package 'bioconda::deepvariant-0.8.0-py27h7333d49_0'.; Rolling back transaction: done. LinkError: post-link script failed for package bioconda::deepvariant-0.8.0-py27h7333d49_0; location of failed script: /home/ydliu/anaconda3/envs/py2.7/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ==> script output <==; stdout: ; stderr: INFO 1101 14:33:31.774699 util.py] Retrying request, attempt #1...; INFO 1101 14:34:33.764798 util.py] Retrying request, attempt #2...; INFO 1101 14:35:36.852041 util.py] Retrying request, attempt #3...; INFO 1101 14:36:43.774008 util.py] Retrying request, attempt #4...; INFO 1101 14:37:59.406010 util.py] Retrying request, attempt #5...; INFO 1101 14:39:31.468027 util.py] Retrying request, attempt #6...; INFO 1101 14:41:03.574773 util.py] Retrying request, attempt #7...; INFO 1101 14:42:35.649608 util.py] Retrying request, attempt #8...; INFO 1101 14:44:07.752737 util.py] Retrying request, attempt #9...; INFO 1101 14:45:39.836059 util.py] Retrying request, attempt #10...; INFO 1101 14:47:11.942440 util.py] Retrying request, attempt #11...; INFO 1101 14:48:43.990016 util.py] Retrying request, attempt #12...; INFO 1101 14:50:16.078940 util.py] Retrying request, attempt #13...; INFO 1101 14:51:48.200379 util.py] Retrying request, attempt #14...; INFO 1101 14:53:20.307419 util.py] Retrying request, attempt #15...; INFO 1101 14:54:52.388456 util.py] Retrying request, attempt #16...; INFO 1101 14:56:24.486618 util.py] Retrying request, attempt #17...; INFO 1101 14:57:56.579738 util.py] Retrying request, attempt #18...; INFO 1101 14:59:28.640781 util.py] Retrying request, attempt #19...; INFO 1101 15:01:00.705409 util.py] Retrying r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549130970:591,message,messages,591,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970,1,['message'],['messages']
Integrability,"Hi, ; thank you both for the answers and suggestions. > The error comes from the line `output_queue = multiprocessing.Queue()` Could you try a simple test? Run docker in CLI model: `docker run -it <DeepVariant image> bash` Inside docker start Python3 and execute:; > ; > ```; > import multiprocessing; > q = multiprocessing.Queue(); > ```; > ; > Please let us know if that works. No, it doesn't work. I get the following error that parallels the one above (full disclosure: I run it again with udocker, not docker):; ```; Python 3.8.10 (default, May 26 2023, 14:05:08); [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import multiprocessing; >>> q = multiprocessing.Queue(); Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory; ```. Also the approach suggested by @kishwarshafin unfortunately didn't work for me. I thought that udocker could be a viable option considering what said in #669. Maybe I'll try to downgrade to 1.5.0 since it's the version that was mentioned in the orginal post. . I'm not really familiar with multiprocessing but I will have a look. If you have any additional pointers, I would be really grateful for them :) . Thank you! ; Federico . EDIT: I tried running DeepVariant v1.5.0 and indeed it works! So I guess it is an issue of the newer release.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916:1170,synchroniz,synchronize,1170,,https://github.com/google/deepvariant/issues/733#issuecomment-1818694916,4,['synchroniz'],['synchronize']
Integrability,"Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup. In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag). Please let me know if it works after you remove the openvino flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462:286,message,message,286,,https://github.com/google/deepvariant/issues/597#issuecomment-1350569462,2,['message'],['message']
Integrability,"Hi, up to the latest version (1.6), we've been excluding HG003 and chr20-22 from the training data. Building on that assumption, if you train a model (warmstarting our model) that also exclude those data from training, you can be sure that the model would never have seen those data. And you're correct that it's still good to be aware that HG002 is the son of HG003. So it depends on what hypothesis you're testing, you'll want to think about that factor too.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/788#issuecomment-1992359932:374,depend,depends,374,,https://github.com/google/deepvariant/issues/788#issuecomment-1992359932,1,['depend'],['depends']
Integrability,"Hi,. Thanks for providing all of that information, it is very helpful. The issue does actually lie in pyclif and protobuf. We use an older version of protobuf that is stable with pyclif; updating it will break the interoperability between C++ and Python (the source of that seg fault). Our tests in `build_and_test.sh` do not appear to catch this, of which I have made a note to update in the future. . Is there a reason you cannot use our Docker builds from `1.6.1`? Also, if docker does not work for you, are you able to use singularity? I am unsure that brute forcing a docker build will lead to a container that runs everything as intended.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334875005:214,interoperab,interoperability,214,,https://github.com/google/deepvariant/issues/879#issuecomment-2334875005,1,['interoperab'],['interoperability']
Integrability,"Hi,; as you can see in the message:; ```; I0514 22:42:53.306706 140678655026944 make_examples.py:946] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; ```; This shows that in the input files you gave to make_examples use the convention *without* the prefix ""chr"". Therefore, in this case you should use `--regions ""20""` in your argument. It could be a bit confusing, but both conventions exist. For example, depending on which version of the reference you're using they can also be different. Would it help if we add a bit more error message? ; Maybe in addition to ""Could not parse"", we can say something more verbose like:; `Could not parse chr20 : make sure if the regions you specify are in the contigs of your BAM and FASTA file. A common error is to use ""chr"" prefix on files that don't have it, or vice versa.`; Would that have helped?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/73#issuecomment-389394212:27,message,message,27,,https://github.com/google/deepvariant/issues/73#issuecomment-389394212,3,"['depend', 'message']","['depending', 'message']"
Integrability,"Hi,; can you tell us the operating system you're on? Did the step with run-prereq.sh finish without any error messages?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41#issuecomment-360677087:110,message,messages,110,,https://github.com/google/deepvariant/issues/41#issuecomment-360677087,1,['message'],['messages']
Integrability,"I also tested running _make_examples_ without _parallel_ (so, without the 4 threads), and the run-time was similar on the same instance (I believe a few hours). However, I am still getting the same error message that the **call_variant** step (with or without the threads / shards). Thank you again for your help! :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478394293:204,message,message,204,,https://github.com/google/deepvariant/issues/166#issuecomment-478394293,1,['message'],['message']
Integrability,I can confirm this problem on some of my samples. The program never produces the final outputs due to the post processing step ending prematurely (but no error message reported). . Normal log:; `; I0814 02:06:33.719291 140247404730176 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; 2024-08-14 02:06:33.734191: I deepvariant/postprocess_variants.cc:94] Read from: /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/bwa_hapbetter/wgs/deepvariant/tmp/tmp1gvo5vri/call_variants_output-00000-of-00001.tfrecord.gz; 2024-08-14 02:13:18.938389: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 71894602; I0814 02:35:15.649105 140247404730176 postprocess_variants.py:1313] CVO sorting took 28.698674070835114 minutes; I0814 02:35:15.649988 140247404730176 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0814 02:40:15.761767 140247404730176 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0814 05:02:43.606994 140247404730176 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 142.46444999376934 minutes; I0814 06:01:36.673851 140247404730176 postprocess_variants.py:1407] Finished writing VCF and gVCF in 58.884093316396076 minutes. real 235m30.029s; user 220m0.378s; sys 13m54.784s. `. Samples with problems:; `; I0814 16:35:25.856544 140492383778624 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; 2024-08-14 16:35:25.879399: I deepvariant/postprocess_variants.cc:94] Read from: /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/bwa_hapbetter/wgs/deepvariant/tmp/tmpfrusl15j/call_variants_output-00000-of-00001.tfrecord.gz; 2024-08-14 16:44:06.712469: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 92795573; I0814 17:09:30.584156 140492383778624 postprocess_variants.py:1313] CVO sorting took 34.078398688,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868#issuecomment-2290172397:160,message,message,160,,https://github.com/google/deepvariant/issues/868#issuecomment-2290172397,1,['message'],['message']
Integrability,"I concur this issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------; On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),; >; > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it.; >; > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions.; >; > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results.; >; > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: **",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599:317,Message,Message,317,,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599,1,['Message'],['Message']
Integrability,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:74,message,message,74,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['message'],['message']
Integrability,"I ran make_examples with dad.sam.gz and recorded output of the realigner (upper part is original BAM and lower part show realigned reads). You are right Brent, realinged reads are mapped to support 1 del and 1 ins. But in the end the haplotype would be the same with 1 DEL and 1 INS or 3 SNPs, it is just a different representation. This behavior depends on penalty scores that we use for Smith–Waterman alignment. . ![image](https://user-images.githubusercontent.com/1168691/74576052-33921600-4f3e-11ea-8aef-1e354bf2b466.png)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-586520046:347,depend,depends,347,,https://github.com/google/deepvariant/issues/272#issuecomment-586520046,1,['depend'],['depends']
Integrability,"I ran the above command as such, but the terminal didn't return any errors except `sudo: unable to resolve host smd` where `smd` is the OpenStack instance name. `sudo: unable to resolve host smd` message didn't cause any problems in another OpenStack instance where I have been successful with running deepvariant. . ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ilAriAges1.fasta.bgz"" \; --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" \; --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" \; --norealign_reads \; --vsc_min_fraction_indels ""0.12"" \; --task 54 --logtostderr; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/315#issuecomment-638428497:196,message,message,196,,https://github.com/google/deepvariant/issues/315#issuecomment-638428497,1,['message'],['message']
Integrability,"I think you are getting the wrong ""gsutil"". There is an Ubuntu package with that name and matching error messages, but what you need is the ""Google Cloud SDK"" (see https://cloud.google.com/sdk/downloads).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5#issuecomment-349832933:105,message,messages,105,,https://github.com/google/deepvariant/issues/5#issuecomment-349832933,1,['message'],['messages']
Integrability,"I tried to look for the non-zero exit status 16 status, but wasn't quite able to figure out what it was.; Given that you said the same script was used the same script and completed on other things without errors, I have a few questions for you:; (1) Is your machine somehow overloaded when this failed? (This doesn't make sense to have such a cryptic message... but just trying to figure out what might have happened.); (2) Is this reliably reproducible on the same input?. Thanks for reporting. I'd like for DeepVariant to have more meaningful error messages in general. If I can figure out how to reproduce this, I'd like to improve it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/232#issuecomment-548613339:351,message,message,351,,https://github.com/google/deepvariant/issues/232#issuecomment-548613339,2,['message'],"['message', 'messages']"
Integrability,"I'm currently testing on a low-pass WGS bam (~2GB, from blood biopsy) for quicker debugging turnaround time, and am successfully getting 16 shards and cores to run when GCP has the appropriate machine type available, here is output of lscpu:. ```; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 16; On-line CPU(s) list: 0-15; Thread(s) per core: 2; Core(s) per socket: 8; Socket(s): 1; NUMA node(s): 1; Vendor ID: GenuineIntel; CPU family: 6; Model: 63; Model name: Intel(R) Xeon(R) CPU @ 2.30GHz; Stepping: 0; CPU MHz: 2300.000; BogoMIPS: 4600.00; Hypervisor vendor: KVM; Virtualization type: full; L1d cache: 32K; L1i cache: 32K; L2 cache: 256K; L3 cache: 46080K; NUMA node0 CPU(s): 0-15; ```. I then get 16 messages like this:; `; Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']`. Then the candidate site lines begin printing out. . How much memory per shard/core/worker (assuming one worker per shard and core if I'm not mistaken) is recommended?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-461090328:752,message,messages,752,,https://github.com/google/deepvariant/issues/150#issuecomment-461090328,1,['message'],['messages']
Integrability,"I'm happy to hear you have enjoyed my YouTube videos :). Hmm, there should be a more informative error message above the one that says ""parallel: This job failed"". If you don't see it, try running it without parallel and without the ""@"" sharding in the output file names. I wouldn't recommend using `bedtools bamtobed` to generate the bed file, even if other ways aren't working. That is because it would generate a region for every read, which is definitely not the --regions DeepVariant is expecting! Since you do have the exome capture bed file, then let's get that working instead :). Also, does it work for one of the regions in the `idt_capture_novogene.grch38.bed` file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917346497:103,message,message,103,,https://github.com/google/deepvariant/issues/483#issuecomment-917346497,1,['message'],['message']
Integrability,"I'm working on a project related to T2T variant calling. @AndrewCarroll, has your group examined this question in more detail? I am currently documenting the impact of T2T vs GRCh38 alignment on variant calling. I can tell you already that it has a [large effect](https://www.waisman.wisc.edu/2024/06/07/werling-slide-of-the-week-2024/) on alignment quality. Documenting the impact is one thing, but retraining DeepVariant is another. I *could* do it, but I don't look forward to it. If interested, I can provide my cram files. I have your published HG002-HG007 illumina reads at 20X and 30X depth aligned to:. - GRCh38 (w/ BWA-mem); - T2Tv2.0 (w/ BWA-mem; - GRCh38 (aligned to HPRCv1.1 w/ vg giraffe, surjected to GRCh38); - T2T (aligned to HPRCv1.1 w/ vg giraffe, surjected to T2T); - GRCh38 (aligned to personalized graph* created from HPRCv1.1, surjected to GRCh38); - T2T (aligned to personalized graph* created from HPRCv1.1, surjected to T2T). * per the protocol outlined in this paper: https://www.biorxiv.org/content/10.1101/2023.12.13.571553v2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-2261084300:961,protocol,protocol,961,,https://github.com/google/deepvariant/issues/534#issuecomment-2261084300,1,['protocol'],['protocol']
Integrability,"I've attached the output from the _call_variants_ step. **I'm not sure how much may have to do with having a total of ~2 weeks AWS experience**, but the current situation may be kind of interesting:. **1)** I believe I started running **postprocess_variants** this morning; **2)** After an 1-2 hours I would have expected to see that error message with the previous runs.; **3)** Using _r4.8xlarge_ EC2 instance launched from the ECS cluster (listed as having have 32 cores and 244 GB of RAM), **postprocess_variants** step is still running. I was admittedly surprised with I had problems with 64 GB of RAM (and initiated this issue). So, **using 244 GB of RAM seems to have gotten around the memory issue (_at least after having been running for ~12 hours_)**, but the run-time seems extraordinarily long (if I correctly understand that this step is supposed to be just file reformatting, for an Exome dataset). I kind of want to see if the job is capable of finishing running (and giving the expected result). However, I think I need to either figure out what is going on or test other options (since I want to test an alternative .bam alignment for that Exome file, as well as testing 2 alignments for my WGS data). In other words, if the job completes and produces a .vcf file, I will close this issue. If not, I will keep looking into things, but I think it may be a little while before I follow-up and get to the point where I would close the issue. [call_variants_output.tfrecord.gz](https://github.com/google/deepvariant/files/3041631/call_variants_output.tfrecord.gz)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479738587:340,message,message,340,,https://github.com/google/deepvariant/issues/167#issuecomment-479738587,1,['message'],['message']
Integrability,"If it helps, this is what the model folder looks like:. ```; total 401316; -rw-rw-r-- 1 ec2-user ec2-user 348681272 Dec 11 23:41 model.ckpt.data-00000-of-00001; -rw-rw-r-- 1 ec2-user ec2-user 18473 Dec 11 23:42 model.ckpt.index; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta; -rw-rw-r-- 1 ec2-user ec2-user 31118992 Dec 11 23:41 model.ckpt.meta.1; ```; (I think there are two meta files because I had to change the HTTPS permissions and re-download the files). and these were the commands to download those files:. ```; MODEL_VERSION=""0.7.2""; MODEL_NAME=""DeepVariant-inception_v3-${MODEL_VERSION}+data-wes_standard""; MODEL_HTTP_DIR=""https://storage.googleapis.com/deepvariant/models/DeepVariant/${MODEL_VERSION}/${MODEL_NAME}"". mkdir -p ${MODEL_NAME}; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.data-00000-of-00001; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.index; wget -P ${MODEL_NAME} ${MODEL_HTTP_DIR}/model.ckpt.meta; ```. I also tested deleting those files and re-downloading them (but I got the same error message).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166#issuecomment-478393809:1052,message,message,1052,,https://github.com/google/deepvariant/issues/166#issuecomment-478393809,1,['message'],['message']
Integrability,"If the actual depth in a particular region is greater than the pileup image height, DeepVariant randomly downsamples reads until the image has been filled up. For the default DeepVariant models (height 100), an image can accommodate at most 95 reads in a given region (5 rows are reserved for the reference sequence). . You may be able to successfully run our pretrained models with a different pileup image height (via `--pileup_image_height` in `make_examples.py`), depending on the new height. However, we generally do not recommend using different image heights at training and inference time. If you wish to use a different pileup image height, we recommend retraining a new model with images of that height. . If you are working with extremely high coverage sequencing data for applications such as somatic sequencing, we recommend using a somatic caller instead of DeepVariant, which is a germline caller.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/425#issuecomment-782444906:468,depend,depending,468,,https://github.com/google/deepvariant/issues/425#issuecomment-782444906,1,['depend'],['depending']
Integrability,"If you are okay with sharing the BAM header, it'll help too. If it's easier to share via email, you can email pichuan@google.com. > 2. Any idea of how I do a better job sizing compute resources?. On the GCP DeepVariant tutorial page:; https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; You can see some numbers of runtime and cost estimates on GCP. This might help give you some sense of what type of machine you can choose. Since you're running on AWS, the cost might change based on pricing. > 3. How do I know if docker is making progress or not?. Currently, our one-step script should be outputting some logs as it goes. If make_examples is still running, you should see lines of logs coming out, showing there's progress. There's likely room for improvement on how we show this progress. If you have suggestions or bug reports, please let us know. > I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. Sorry to hear that the logs are not coming out promptly. If it's possible, can you tell us where did the log get stuck?. > 4. I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. I haven't tried using nohub. I'll have to try and respond to this later.; > ; > thanks; > ; > Andy; > ; > p.s. I am running in AWS . not sure if that makes a difference or not. I don't expect it to make a difference. But if you do observe any issues, feel free to let us know what kind of AWS instances you're running on, and what's the unexpected behavior, so we can reproduce the issue.; > ; > p.p.s. Is",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260#issuecomment-573487475:1595,message,message,1595,,https://github.com/google/deepvariant/issues/260#issuecomment-573487475,2,['message'],['message']
Integrability,"Interesting. Could you share the command you ran and the error message?. On Tue, Apr 28, 2020 at 1:45 PM WeiweiBian <notifications@github.com> wrote:. > Thank a lot for your reply. The 'single quotes encased by double quotes'; > works for this case, but when I added more regions, it couldn't give me an; > available output and showed an exit status 247.; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/305#issuecomment-620845064>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AB4W4PIOBHJ5DAXPKER4A63RO457FANCNFSM4MQ4ZT2Q>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/305#issuecomment-620891124:63,message,message,63,,https://github.com/google/deepvariant/issues/305#issuecomment-620891124,1,['message'],['message']
Integrability,It all depends on how big your data is.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-464885119:7,depend,depends,7,,https://github.com/google/deepvariant/issues/157#issuecomment-464885119,1,['depend'],['depends']
Integrability,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13#issuecomment-351172185:3,depend,depends,3,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185,2,['depend'],['depends']
Integrability,"It looks like you're not mounting any directories in your Singularity command. See the FAQ for how to debug that:; https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open -- ""Why can't it find one of the input files? E.g., ""Could not open"""". If that doesn't work, can you include the error messages too?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474#issuecomment-885791994:352,message,messages,352,,https://github.com/google/deepvariant/issues/474#issuecomment-885791994,1,['message'],['messages']
Integrability,"It might have something to do with the `--regions`. Can you try just running on just one small region, something like '--regions=""chr20:10000000-10010000""`?; What was the bedtools command you used for making that bed file?; The bed file to use for `--regions` in a WES run should be the exome regions from the capture protocol.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917319405:318,protocol,protocol,318,,https://github.com/google/deepvariant/issues/483#issuecomment-917319405,1,['protocol'],['protocol']
Integrability,"It might not hurt to install a [Jenkins server](https://jenkins-ci.org/) - similar how the protobuf team does it - for continuous integration, as it also plays well with Github PRs and alleviates these simple headaches.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353393763:130,integrat,integration,130,,https://github.com/google/deepvariant/issues/19#issuecomment-353393763,2,['integrat'],['integration']
Integrability,"It was run with 256G RAM node and all other samples finish in the same RAM nodes. I will send you data later; ________________________________; From: Lucas Brambrink ***@***.***>; Sent: Tuesday, March 26, 2024 6:46:34 PM; To: google/deepvariant ***@***.***>; Cc: Zhigui Bao ***@***.***>; Author ***@***.***>; Subject: Re: [google/deepvariant] Fatal Python error: Segmentation fault (Issue #794). Sure thing! You can send me the files at ***@***.******@***.***>. Additionally, Seg faults can sometimes happen from OOMs (running out of memory). Do you have the memory specs of the instance you are running this on? Thanks!. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/794#issuecomment-2021094547>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AE5Y3VRRFLRCYTPDYDZDFY3Y2GX7VAVCNFSM6AAAAABFG7OINSVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDAMRRGA4TINJUG4>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794#issuecomment-2021114740:978,Message,Message,978,,https://github.com/google/deepvariant/issues/794#issuecomment-2021114740,1,['Message'],['Message']
Integrability,"Jillian;; Thanks for offering to help with this. Unfortunately we're still stuck on building with bazel using conda dependencies. I have a build that works around the clif issue, but don't know how to modify the DeepVariant bazel build infrastructure so that we can use an arbitrary prefix. Right not it assumes libraries are present in `/usr` so fails to pick up the anaconda zlib and friends. We've been trying to identify someone who knows bazel to help walk us through how to change the DeepVariant build to support this. Until we can get a native CentOS6 build it unfortunately will have issues on CentOS due to compiling against more recent glibc on Ubuntu in the pre-built binaries. If you have any bazel expertise or want to dig into this, that would be much appreciated.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-390984830:116,depend,dependencies,116,,https://github.com/google/deepvariant/issues/29#issuecomment-390984830,1,['depend'],['dependencies']
Integrability,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577819231:282,wrap,wraps,282,,https://github.com/google/deepvariant/issues/657#issuecomment-1577819231,1,['wrap'],['wraps']
Integrability,"Looking at this error message, I wonder if it's related to Python3 vs Python2.; Currently DeepVariant still only supports Python2. Adding @chapmanb (who maintains DeepVariant on bioconda ) , do you think there might be other issues going on here? Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252#issuecomment-566176982:22,message,message,22,,https://github.com/google/deepvariant/issues/252#issuecomment-566176982,1,['message'],['message']
Integrability,"Looks like the relevant error message is:; ```; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz; ```; On many systems, writing an output file is not possible if the directory is not present already, so can you make sure the directory exists first:. ```; mkdir -p /data/shared/clinical/LongRead/Data//Analysis/; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1584844298:30,message,message,30,,https://github.com/google/deepvariant/issues/659#issuecomment-1584844298,1,['message'],['message']
Integrability,"Might not be the same issue but: I got this error message when doing a custom hello world run. My mistake was that when I removed comments from the example command, a space character was accidentally left behind after a backslash, rendering it useless.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402#issuecomment-901465519:50,message,message,50,,https://github.com/google/deepvariant/issues/402#issuecomment-901465519,1,['message'],['message']
Integrability,"My HPC team helped me with this error, just thought I'd add it here in case future people do this: . ""The error message ""connect: network is unreachable "" is due to the compute nodes being closed to the internet (safety concerns). This means that you cannot download files while on the compute nodes. Our advice is to download any required input files first on the login nodes and then point to them in the job script.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579:112,message,message,112,,https://github.com/google/deepvariant/issues/522#issuecomment-1059260579,1,['message'],['message']
Integrability,No luck either. I figured it seems that the Xeon CPU on my machine does not support AVX (according to /proc/cpuinfo). Some informative error message would be nice in such a situation. Couldn't the docker container be built without assuming avx presence? It would be slower I guess but easier to explore the tool.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/39#issuecomment-358230941:141,message,message,141,,https://github.com/google/deepvariant/issues/39#issuecomment-358230941,1,['message'],['message']
Integrability,"No problem, I just wanted to say I did ask google bard some questions I; thought might be important,. You can have a look here: https://g.co/bard/share/4013d7eb8290. Or start a thread of your own :). And I'll hop out of this thread now as I think I'm cluttering it up, but; drop me a message if you ever want to chat. Also I know some clonal organisms that can change there entire allele; frequency and linkage in less than 5 minutes!. Not trying to distact you, all the best,. Joe. On Mon, 31 Jul 2023, 18:34 Pi-Chuan Chang, ***@***.***> wrote:. > Hi everyone on this thread:; > We (the Google team that works on DeepVariant) have really appreciated our; > GitHub community where people post questions, discussions, and help each; > other out.; > Even when our team answer questions, we don't always have the full; > context. We really appreciate community help and discussion from outside; > the team as well.; > For people who post on our GitHub issues, please be respectful to each; > other, and know that people are trying to help out, even when you might not; > feel like the answers are as helpful.; >; > @Axze-rgb <https://github.com/Axze-rgb> we'll take another look at this; > thread and see what answer we can provide. It is possible that some of the; > topics here might be beyond what our team can support, but we'll try our; > best.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658849274>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2U67RCHYD4ET3E22ALXS7UA7ANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658897383:284,message,message,284,,https://github.com/google/deepvariant/issues/682#issuecomment-1658897383,2,"['Message', 'message']","['Message', 'message']"
Integrability,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:640,Message,Message,640,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773,1,['Message'],['Message']
Integrability,"Not at this time. Are there specific dependencies in Ubuntu 20.04 that are required? Or an older version of DV that has been tested on 18.04?. Would also like some clarification on this statement to help me figure out what is going on, . `Build clif binary from scratch. Might not be ideal because it installs a bunch of dependencies, but this works fine when we used this in a Dockerfile because we don't do build-prereq.sh in the final image.`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476#issuecomment-896286964:37,depend,dependencies,37,,https://github.com/google/deepvariant/issues/476#issuecomment-896286964,2,['depend'],['dependencies']
Integrability,"Oh, btw, the warning message in call_variants.py about image height might be confusing.; It is just a warning, so you should still be able to run. But I agree that this is confusing. We'll think about how to make things more robust. . For now, I would still recommend the one-command approach: https://github.com/google/deepvariant/blob/r1.2/docs/deeptrio-quick-start.md#run-deeptrio-with-one-command; which is our current way of making things more robust.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-929838628:21,message,message,21,,https://github.com/google/deepvariant/issues/488#issuecomment-929838628,1,['message'],['message']
Integrability,"One thing I will propose to do is at least to update the Nucleus message to say something more than just ""Not found"", so that even it fails, the error message will be more understandable.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374#issuecomment-723753324:65,message,message,65,,https://github.com/google/deepvariant/issues/374#issuecomment-723753324,2,['message'],['message']
Integrability,"PYTHONPATH=$PYTHONPATH --test_env=PYTHONPATH=$PYTHONPATH; ```. ## Fix DV Error. ```bash; ################################################################################; # bazel test -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@""//deepvariant:resources_test; # use lscpu to show the actual CPU number; ################################################################################; python -c ""import multiprocessing; print(multiprocessing.cpu_count())"" #160; python -c ""import psutil;print(p/sutil.cpu_count; ())"" #160. vim deepvariant/resources.py; --------------------------------; def _get_cpu_count():; """"""Gets the number of physical cores in this machine.; Returns:; int >= 1 if the call to get the cpu_count succeeded, or 0 if not.; """"""; # return psutil.cpu_count(logical=False) or 0 ==> comment; return 20; --------------------------------. vim deepvariant/resources_test.py; --------------------------------; def test_metrics_is_ok_when_cpu_count_returns_none(self):; # Some psutil functions, such as cpu_freq(), can return None depending on; # the environment; make sure we don't crash when that occurs.; with mock.patch.object(resources.psutil, 'cpu_count', return_value=None):; with resources.ResourceMonitor() as monitor:; #self.assertEqual(monitor.metrics().physical_core_count, 0) ==> comment; self.assertEqual(monitor.metrics().physical_core_count, 20); --------------------------------. ##########################################################################; # //deepvariant/realigner/allele_count_linear:generate_trained_model_test; # ImportError: /root/.local/lib/python2.7/site-packages/sklearn/__check_build/_check_build.so: undefined symbol: PyUnicodeUCS4_DecodeUTF8; ##########################################################################; # reinstall numpy, scipy, Cpython, scikit-learn to fix with AT built Python; python -c ""import numpy"" # prequests of TF 1.12.0; python -c ""import scipy"" # prequests of TF 1.12.0; pip install Cython --force-reinstall --no-deos; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:20800,depend,depending,20800,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['depending']
Integrability,"Peter;; Thanks for testing, it sounds like there is a problem with the recent google-cloud-sdk packages. I'll take a look to see if I can figure out what is going wrong but an immediate thing you could try is to restrict that dependency version to try and avoid the issue:; ```; conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; ```; Hope this helps get it installed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486163437:226,depend,dependency,226,,https://github.com/google/deepvariant/issues/177#issuecomment-486163437,1,['depend'],['dependency']
Integrability,"Phil and Pi-Chuan;; That's exactly right, `dv_make_examples.py` is a simple wrapper for `make_examples.zip` to make it directly callable and is part of the conda packaging process, not a default script from the Google team. If it's not useful, feel free to call directly at the standard installs, it just tries to avoid needing to know the full install path and other details about the parallelization so the command line syntax is simplified. For your example you'd use it like:; ```; dv_make_examples.py; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz \; --regions regions.bed \; --sample test \; --logdir location/to/place/logfiles; --cores 1; ```; Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101#issuecomment-430171385:76,wrap,wrapper,76,,https://github.com/google/deepvariant/issues/101#issuecomment-430171385,2,['wrap'],['wrapper']
Integrability,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385500259:185,depend,dependencies,185,,https://github.com/google/deepvariant/issues/29#issuecomment-385500259,1,['depend'],['dependencies']
Integrability,"Pi-Chuan -- thanks for this. We'd ideally build with CLIF directly in bioconda to avoid you needing to have these custom builds, but will hold off on that until there is an easier to build/install CLIF dependency. Happy to test the new version with reduced glibc requirements when it's ready. Björn -- We do pin to 1.14 now in DeepVariant, with the downside that it's not compatible in a shared environment with other looks that pin to the bioconda 1.12 default. I can work around this for now by having DeepVariant in a separate environment, but would love to synchronize bioconda to 1.14 at some point. Thanks again for all this work and help.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385485505:202,depend,dependency,202,,https://github.com/google/deepvariant/issues/29#issuecomment-385485505,2,"['depend', 'synchroniz']","['dependency', 'synchronize']"
Integrability,"Pi-Chuan -- thanks so much for looking at this.; Paul -- thank you for digging into the bioconda history on the numpy pin. I suspect we could update numpy in bioconda but no one has yet taken that on. Given that and the issues with tensorflow pinning, I'd advise just sticking with 1.14 and we can work on the dependency issues in bioconda. Apologies, I hadn't meant to cause a lot of work and didn't realize about the pinning preventing this. If we can get older glibc compatible binaries that'll cover most of the issues and we can work around the numpy problems by installing in an isolated conda environment for now. Thanks again for looking into the numpy and glibc work.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385125512:310,depend,dependency,310,,https://github.com/google/deepvariant/issues/29#issuecomment-385125512,1,['depend'],['dependency']
Integrability,"Pi-Chuan;; Awesome, thanks so much for the WORKSPACE path tip. That was perfect and exactly solved the clif issue. Nice one. The issue I'm running into now is that the build setup assumes that the libraries and include files are available in standard locations (`/usr`, I'm guessing) and within conda these will be inside the conda environment. So as soon as I compile htslib we get errors about now finding zlib.h, which is present in `$PREFIX/include` instead of `/usr/include`. I've tried hacking this include directory into the htslib copts:; ```; sed -i.bak ""s|\""-Wno-error\"",|\""-Wno-error\"", \""-I${PREFIX}/include\"",|"" third_party/htslib.BUILD; ```; but bazel is too smart and won't let us continue with non-bazel defined references:; ```; (00:28:31) ERROR: /home/conda/.cache/bazel/_bazel_conda/b3bf6b0de2935c6a10ef1e7d7b61873f/external/htslib/BUILD.bazel:209:1: in cc_library rule @htslib//:htslib: The include path '/opt/conda/conda-bld/deepvariant_1525566343740/_b_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_p/include' references a path outside of the execution root.; ```; So at this point I'm stuck by my lack of knowledge of how to incorporate this into the bazel build instructions. I couldn't find any conda bazel builds that already do this as a template and am not familiar enough with it to build up on my own. Would it be possible to make the dependencies you're installing with apt as explicit bazel targets like clif? If so, then I could adjust paths to the conda `$PREFIX` rather than `/usr`. What do you think about that approach? Other bazel tips/tricks would be very welcome. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386869866:1523,depend,dependencies,1523,,https://github.com/google/deepvariant/issues/29#issuecomment-386869866,1,['depend'],['dependencies']
Integrability,"Pi-Chuan;; Thanks for checking around to see if we can get a bazel expert involved. I think that would be the best way forward, as I'm just hacking around and don't have a strong understanding of how best to attack this. It's a general issue where the dependencies are present in a non-system directory and how to inject that into a build. I'm trying to build this inside of bioconda. If you want to get it setup there are instructions here: https://bioconda.github.io/contributing.html and I could share the current recipe I'm working from. Although I don't want to make you wade into a new build system and get familiar with that if we can get more high level bazel advice and sort through improving the DeepVariant build process to handle this case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-387445867:252,depend,dependencies,252,,https://github.com/google/deepvariant/issues/29#issuecomment-387445867,2,"['depend', 'inject']","['dependencies', 'inject']"
Integrability,"Pi-Chuan;; Thanks for following up with the additional work on this. For the pyclif requirement, it sounds like you're hitting the same problem as me: I'm not sure where to put it so that bazel can find it. Do you understand what locations it looks in and if we can tweak them? I'm not able to put it into `/usr/local` like you did, since we're confined to our work directory. Can I stick it in an arbitrary location and let bazel know?. For tensorflow, it seems like that was built on system with a more recent glibc than on CentOS6. It is a pain to have the full dependency try be compatible with older glibc. This is part of what makes conda nice, is that you're guaranteed to have this (well, as long as the dependency exists). It looks from that thread you linked that the conda package for tensorflow is all good on CentOS6 if installing from there for your build is doable. Thanks again for helping tackle this; I look forward to working on actual fun things instead of compiling and porting.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386644191:565,depend,dependency,565,,https://github.com/google/deepvariant/issues/29#issuecomment-386644191,2,['depend'],['dependency']
Integrability,"Pi-Chuan;; Thanks for this update and work. For bazel, we'd gotten that updated and building on conda with CentOS 6 so are good to go with that dependency, it's really just CLIF that we're struggling with for building. For clif, this is great progress, thank you. I resuscitated my bioconda build script and gave it a try with this. It's making better progress but unfortunately needs to reconstitute the system wide python install within the build environment which we can't do in conda. Everything there is in an isolated work directory so won't have the system shared libraries it wants:; ```; $ /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python; /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/usr/local/clif/bin/python: error while loading shared libraries: libpython2.7.so.1.0: cannot open shared object file: No such file or directory; ```; and the python libraries included symlink to the system wide ones you built against:; ```; lrwxrwxrwx 1 conda conda 56 May 2 17:54 _weakrefset.py -> /opt/rh/python27/root/usr/lib64/python2.7/_weakrefset.py; ```; I'm not sure if it's possible to make this more relocatable with any python as part of the build process. Sorry, I know it's a lot more work to make it relocatable like this but will allow install on all the systems we support where users don't have root privileges to rely on system libraries. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386074270:144,depend,dependency,144,,https://github.com/google/deepvariant/issues/29#issuecomment-386074270,1,['depend'],['dependency']
Integrability,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385874525:1048,message,messages,1048,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525,1,['message'],['messages']
Integrability,"Ran it again, and still hitting failures:; ```; done: true; error:; code: 9; message: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; metadata:; '@type': type.googleapis.com/google.genomics.v2alpha1.Metadata; createTime: '2018-11-08T14:27:06.016940Z'; endTime: '2018-11-08T14:30:59.324697Z'; events:; - description: Worker released; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.WorkerReleasedEvent; instance: google-pipelines-worker-4b16fd95b691baddc54b0c5ec50dc6c7; zone: us-west1-b; timestamp: '2018-11-08T14:30:59.324697Z'; - description: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.FailedEvent; cause: 'Execution failed: action 1: unexpected exit status 1 was not ignored'; code: FAILED_PRECONDITION; timestamp: '2018-11-08T14:30:58.518326Z'; - description: Stopped running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStoppedEvent; actionId: 2; exitStatus: 0; stderr: ''; timestamp: '2018-11-08T14:30:58.416239Z'; - description: Started running ""/bin/sh -c gsutil -q cp /google/logs/output gs://canis/CNR-data/deep_variant_files/runner_logs_20181108_082705.log""; details:; '@type': type.googleapis.com/google.genomics.v2alpha1.ContainerStartedEvent; actionId: 2; ipAddress: ''; portMappings: {}; timestamp: '2018-11-08T14:30:55.929647Z'; - description: Unexpected exit status 1 while running ""-c /opt/deepvariant_runner/bin/gcp_deepvariant_runner --project; valis-194104 --zones us-west1-b --docker_image gcr.io/deepvariant-docker/deepvariant:0.7.0 --outfile; gs://canis/CNR-data/TLE_a_001_deep_variant.vcf --staging gs://canis/CNR-data/deep_variant_files --model; gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard --regions; gs://canis/CNR-data/CDS-canonical.bed --bam gs:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437055644:77,message,message,77,,https://github.com/google/deepvariant/issues/116#issuecomment-437055644,1,['message'],['message']
Integrability,"Regarding the pretrained inception model: I was able to find an example that was pretrained on ImageNet classes (it is likely the same or at least very similar to the file that is mentioned in the code): https://github.com/tensorflow/models/tree/master/research/slim [about halfway down that page at Pre-Trained Models]. On the Cloud instance I download and unpack the checkpoint:; wget http://download.tensorflow.org/models/inception_v3_2016_08_28.tar.gz; tar -xzvf inception_v3_2016_08_28.tar.gz. then in the training command you can add; "" --start_from_checkpoint inception_v3.ckpt"". After the tf_logging messages you'll see. model_train.py:160] Initializing model from checkpoint at inception_v3.ckpt; modeling.py:314] Checkpoint was trained against 1001 classes while our dataset is using 3, enabling fine-tuning. to let you know that it loaded the pre-trained network. I'll look into your checkpoint loading issue next.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-364533837:608,message,messages,608,,https://github.com/google/deepvariant/issues/46#issuecomment-364533837,1,['message'],['messages']
Integrability,"Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:3516,message,message,3516,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,2,['message'],['message']
Integrability,"Running with -v, everything looks normal except 2 error messages:. 1st error message:; ==============; ===> LINKING PACKAGE: conda-forge::linecache2-1.0.0-py_1 <===; prefix=/mnt/home/mansourt/miniconda3/envs/deepVar; source=/mnt/home/mansourt/miniconda3/pkgs/linecache2-1.0.0-py_1. pyc file failed to compile successfully; python_exe_full_path: /mnt/home/mansourt/miniconda3/envs/deepVar/bin/python2.7; py_full_path: /mnt/home/mansourt/miniconda3/envs/deepVar/lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.py; pyc_full_path: /mnt/home/mansourt/miniconda3/envs/deepVar/lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.pyc; compile rc: 1; compile stdout: Compiling lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.py ...; File ""lib/python2.7/site-packages/linecache2/tests/inspect_fodder2.py"", line 102; def keyworded(*arg1, arg2=1):; ^; SyntaxError: invalid syntax. compile stderr:. 2nd error message:; ==============; $ bash -x /mnt/home/mansourt/miniconda3/envs/deepVar/bin/.deepvariant-post-link.sh; ==> cwd: /mnt/home/mansourt/miniconda3/envs/deepVar/bin <==; ==> exit code: 1 <==; ==> stdout <==; b'Shell debugging temporarily silenced: export LMOD_SH_DBG_ON=1 for this output (/usr/local/lmod/lmod/init/bash)\nShell debugging restarted\n'; ==> stderr <==; b'+ \'[\' -z \'\' \']\'\n+ case ""$-"" in\n+ __lmod_vx=x\n+ \'[\' -n x \']\'\n+ set +x\n+ unset __lmod_vx\n+ set -eu -o pipefail\n+ MODEL_VERSION=0.7.2\n+ GSUTIL=/mnt/home/mansourt/miniconda3/envs/deepVar/bin/gsutil\n+ for MODEL_TYPE in wgs wes\n+ MODEL_NAME=DeepVariant-inception_v3-0.7.2+data-wgs_standard\n+ GSREF=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard\n+ OUTDIR=/mnt/home/mansourt/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard\n+ mkdir -p /mnt/home/mansourt/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-451711664:56,message,messages,56,,https://github.com/google/deepvariant/issues/137#issuecomment-451711664,3,['message'],"['message', 'messages']"
Integrability,"Saw this the other day. https://github.com/marcelauliano/MitoHiFi/tree/master. On Wed, 26 Jul 2023, 08:05 crazysummerW, ***@***.***> wrote:. > Hello,; > I would like to know if it is possible to use DeepVariant to analyze; > PacBio mitochondrial data. If not, do you have any suitable tools to; > recommend?; >; > Looking forward to your reply.; > Thanks.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/686>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XGN5AXQVMVB6LKOIDXSC6TTANCNFSM6AAAAAA2YDZTZU>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/686#issuecomment-1651102049:664,Message,Message,664,,https://github.com/google/deepvariant/issues/686#issuecomment-1651102049,1,['Message'],['Message']
Integrability,"Seems like it should work. This is indeed an annoying corner case, but the `--help` message is pretty important. :-)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/78#issuecomment-398532779:84,message,message,84,,https://github.com/google/deepvariant/issues/78#issuecomment-398532779,1,['message'],['message']
Integrability,"Since DeepVariant isn't trained on this data, it will depend on how different this new datatype is from our existing training data. For the training data we used, you can see: https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details-training-data.md. DeepVariant is known to be robust even on certain new data types. For example, when DNAnexus published this blog post, DeepVariant wasn't trained on NovaSeq, and yet DeepVariant performed well on NovaSeq data:; https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/. However, we also know that DeepVariant can be significantly more accurate if it has a chance to see the data in its training set. (For example, see: https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html). If you'd like to give it a try and let us know, that will be great, especially if you compare it to other baselines. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/107#issuecomment-430072409:54,depend,depend,54,,https://github.com/google/deepvariant/issues/107#issuecomment-430072409,1,['depend'],['depend']
Integrability,Since sometimes warning messages from Tensorflow may be misleading. Could you please try running call_variants and at the same time monitor the GPU load to make sure the GPU is not used?; You can use `watch -n0.5 nvidia-smi` to check the GPU load in real time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2116222843:24,message,messages,24,,https://github.com/google/deepvariant/issues/820#issuecomment-2116222843,1,['message'],['messages']
Integrability,"Some other things to look in to:. * Is what you have pasted above the full stack trace? If there's any other output, it would be helpful to see that as well.; * Could you try running `make_examples` directly using the below command? This is only using one shard. If this command fails, we may see a more informative error message. ```; sudo docker run \; -v ""/root/quickstart-testdata"":""/input"" \; -v ""/root/quickstart-output"":""/output"" \; google/deepvariant:latest \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" \; --examples ""/output/make_examples.tfrecord@1.gz"" \; --gvcf ""/output/gvcf.tfrecord@1.gz"" \; --regions ""chr20:10,000,000-10,010,000""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325#issuecomment-659076993:322,message,message,322,,https://github.com/google/deepvariant/issues/325#issuecomment-659076993,1,['message'],['message']
Integrability,"Sorry but simple sequence repeats are mutable in prokaryotes too.... On Mon, 31 Jul 2023, 17:45 Axze-rgb, ***@***.***> wrote:. > that's not a feature of this genome we are not in humans.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658755258>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2V75T2P2ATODR6Z6QDXS7OLHANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046:502,Message,Message,502,,https://github.com/google/deepvariant/issues/682#issuecomment-1658757046,1,['Message'],['Message']
Integrability,"Sorry for my late reply! To be honest, I believe I went with the [quick start](https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md) and replaced the test data with my own. Then I started to debug on that ValueError, believing that was a potential bug (because of the error saying I was using `--make_examples_extra_args=""sort_by_haplotypes=true,parse_sam_aux_fields=true""`, while I was actually not; I put that first argument to false, not true).; I don't believe there is something wrong with your user flow! Your github is really nice and the docker and dependencies were very easily installed. I wouldn't want to comment more on that without extensively trying your tool, so maybe I can provide with proper feedback later :) I will definitely let you know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/457#issuecomment-844185505:584,depend,dependencies,584,,https://github.com/google/deepvariant/issues/457#issuecomment-844185505,2,['depend'],['dependencies']
Integrability,"Sorry for my wrong posting of error messages.; I have tried different versions of deepvariant in conda, and messed up the conda environment. ; Actually I posted the deepvarint (1.3.0) error messages. : <(. Here is my attempts on the deepvariant 1.4.0: ; `; $ conda create -n deepvar python=3.7.5; $ conda install -c bioconda deepvariant=1.4.0; $ conda activate deepvar; $ dv_call_variants.py -h . `; And I got the similar error messages:. Baseline DeepVariant arguments; File ""/tmp/Bazel.runfiles_j_t9fnwc/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374; raise ValueError(f'Shape mismatch in {example_info_json} and '; ^; SyntaxError: invalid syntax. Wrapper arguments; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]. DeepVariant call_variants wrapper. optional arguments:; --cores CORES; --outfile OUTFILE; --examples EXAMPLES Example directory from make_examples; --sample SAMPLE Sample name; --model {hybrid,pacbio,wes,wgs}; DeepVariant trained model to use, defaults to wgs; -h, --help; usage: dv_call_variants.py [--cores CORES] --outfile OUTFILE --examples; EXAMPLES --sample SAMPLE; [--model {hybrid,pacbio,wes,wgs}] [-h]; dv_call_variants.py: error: the following arguments are required: --outfile, --examples, --sample",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725:36,message,messages,36,,https://github.com/google/deepvariant/issues/627#issuecomment-1514340725,5,"['Wrap', 'message', 'wrap']","['Wrapper', 'messages', 'wrapper']"
Integrability,"Sorry for the late replay. Unfortunately, it did not work. The error message says:; ERROR : Unknown image format/type: deepvariant.0.8.0.simg; ABORT : Retval = 255",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-499709284:69,message,message,69,,https://github.com/google/deepvariant/issues/178#issuecomment-499709284,1,['message'],['message']
Integrability,"Sorry for the string of messages, but a bit of background: my samples controlH is a direct descendant of the ""output"" one, it is a clonal organism so I am assuming all variants private to the controlH are false calls. By investigating them visually (eyeballing the bam file) it seems they are indeed false positives. What is interesting is that they all seem to be in the direct vicinity of an indel or string of mismatches. Is it a known problem with DeepVariant, miscalls/ inconsistencies between samples, for sites around indels?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/278#issuecomment-591490219:24,message,messages,24,,https://github.com/google/deepvariant/issues/278#issuecomment-591490219,1,['message'],['messages']
Integrability,Still yields a core dump with no error message. Going to test this on docker.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774282552:39,message,message,39,,https://github.com/google/deepvariant/issues/419#issuecomment-774282552,1,['message'],['message']
Integrability,"TTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; >; > Run make_examples:; >; > OUTPUT_DIR=""${PWD}/quickstart-output""; > mkdir -p ""${OUTPUT_DIR}""; >; > python bin/make_examples.zip \; > --mode calling \; > --ref ""${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta"" \; > --reads ""${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz"" \; > --channels ""insert_size""; >; > (To figure out which flags you need to add for each model, you can read; > https://github.com/google/deepvariant/blob/r1.4/scripts/run_deepvariant.py#L236-L253; > . Sorry that we don't have better documentation than that right now); >; > For how to run this with multiple shards, and how to run the rest of the; > commands, please read; > https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-quick-start.md; >; > I just tested the steps above and confirmed that it worked for me on; > v1.4.0, at least for the make_examples step.; > If you encounter more issues with other steps, please feel free to ask; > again. I'd be happy to help.; >; > Note that I don't plan to put this into an official documentation page; > now, because that adds to our maintenance burden to keep it up to date.; > Given that we have the Docker/Singularity solution that works generally; > well for our users, I don't expect many of our users to need to use; > pre-built binaries. @zivlang <https://github.com/zivlang> thank you for; > your question so I have a chance to test it again and document it here.; > Hopefully this is helpful for you. Happy to answer more questions if you; > encounter more problems.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/590#issuecomment-1322695241>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ALBF75YWC2I4SZFXBKRL4KTWJPVBNANCNFSM6AAAAAASFZYTTI>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566:4571,Message,Message,4571,,https://github.com/google/deepvariant/issues/590#issuecomment-1322701566,1,['Message'],['Message']
Integrability,"Thank you again for your reply, especially on the weekend. While I'd like to have a way to run all of the steps, thank you for this feedback. As a sort of similar test, I had a m3.large instance open, so I tested running the command from my home directory. For some reason, I don't get the same error message when I do this, but I also don't get any output and no output files are created. Nevertheless, this did gives me some ideas of other things that I can try. In general, maybe there are a couple things that I need to explain:. **1)** I am using an EC2 instance launched from ECS: https://aws.amazon.com/ecs/. This means Docker is already installed. When I create an instance through ECS, I don't think I have the option to create a micro instance (but that isn't my biggest problem). I thought I created the instance with an extra 100 GB of storage (just in case something extra was needed on the local hard drive, beyond RAM). But I don't think this should be the issue for this last step. There may be some other issue that I am not understanding, but I could run the 1st two steps this way (although with an admittedly longer run-time than I expected), and **postprocess_variants** is what I can't get to work. I also received a reply about converting the regular docker image to an[ ECR](https://aws.amazon.com/ecr/) image (submitted on 4/2), to run in AWS [Batch](https://aws.amazon.com/batch/). If that ends up being helpful, I will let you know. If they at least resolve that issue, the next ticket that I submitted (on 4/4) was for this exact issue (but I haven't heard any feedback from that yet). **2)** I want to be able to have my files (.bam, .fastq, etc.) accessible between instances. While I think Google created a way to do with with S3 buckets, my understanding was that I was supposed to do this with an EFS file system: https://aws.amazon.com/efs/. So, _/mnt/efs-genome_ is the EFS file system that contains my .fastq and .bam files (and DeepVariant output). I have to mount",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480616982:301,message,message,301,,https://github.com/google/deepvariant/issues/167#issuecomment-480616982,2,['message'],['message']
Integrability,"Thank you for the response ; I reordered the conda channels to match yours and installed gsutil but using conda and thus I did not need to edit the path. This is the set of commands I used:. ```; conda create -n deepvariant python=2.7; source activate deepvariant; conda install -c conda-forge google-cloud-sdk; conda install -v -y deepvariant &> deepvariant_insatll.log; ```. I got a successful installation inspite of the first error message just like you; However, running the code is producing another error:. ```; python $HOME/miniconda3/envs/deepVar/share/deepvariant-0.7.2-1/binaries/DeepVariant/0.7.2/DeepVariant-0.7.2+cl-225213413/make_examples.zip \; --mode training --reads ""${BAM}"" --ref ""${REF}"" --examples ""$training.tfrecord.gz"" \; --truth_variants ""${TRUTH_VCF}"" --confident_regions ""${TRUTH_BED}"" \; --exclude_regions ""chr20:14000000-15000000"" --sample_name ""train"" ; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4i44qy/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; import tensorflow as tf; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/__init__.py"", line 30, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/mnt/home/mansourt/miniconda3/envs/deepvariant/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_intern",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453685106:436,message,message,436,,https://github.com/google/deepvariant/issues/137#issuecomment-453685106,1,['message'],['message']
Integrability,"Thank you for your kind help! In fact ,I reran the same commands and got the result files without any error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-964829927:108,message,message,108,,https://github.com/google/deepvariant/issues/488#issuecomment-964829927,1,['message'],['message']
Integrability,"Thank you very much. That will help out the folks new to docker/singularity. After using the -B option, we are good to go now!. From: Pi-Chuan Chang ***@***.***>; Sent: Friday, March 25, 2022 7:19 PM; To: google/deepvariant ***@***.***>; Cc: Jason Phillips ***@***.***>; Mention ***@***.***>; Subject: Re: [google/deepvariant] Image /mnt overriding my machine's /mnt causing errors (Issue #530). Hi @japhill<https://github.com/japhill> , to give you an update, I plan to add this section to our FAQ in the next release:. ________________________________; Issues with /mnt/. User reported that sometimes their setup uses /mnt/, which exists in our Docker image, and it has caused an issue in Singularity. You can use -B in Singularity to avoid this issue. See:; #530 (comment)<https://github.com/google/deepvariant/issues/530#issuecomment-1076923302> for more details. ________________________________. Given that this solution works, and is a standard Singularity flag, I won't plan to remove /mnt from our Docker images in the future. —; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/530#issuecomment-1079509951>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AHAS6GHJOPUE7DQPYAT264TVBZCWLANCNFSM5Q7A6FXQ>.; You are receiving this because you were mentioned.Message ID: ***@***.******@***.***>>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356:1325,Message,Message,1325,,https://github.com/google/deepvariant/issues/530#issuecomment-1079677356,1,['Message'],['Message']
Integrability,"Thanks @Suke-fudan for your update.; And thanks for reporting the confusing warning message.; In the next release, we plan to store the input shape for how each model is trained on, and we'll update our logging message to be more accurate.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488#issuecomment-964833815:84,message,message,84,,https://github.com/google/deepvariant/issues/488#issuecomment-964833815,2,['message'],['message']
Integrability,"Thanks @akolesnikov and @pichuan . Please find the details and the error-log below. Please note that I have upgraded the following dependencies according to Tensorflow - 2.11. . DV_BAZEL_VERSION=""5.3.0""; DV_GCP_OPTIMIZED_TF_WHL_VERSION=""2.11.0""; ABSL_VERSION=20210324.2; PROTOBUF_VERSION=3.19.6. **Error log:** ; (03:11:57) ERROR: /opt/deepvariant/third_party/nucleus/io/BUILD:1051:11: Compiling third_party/nucleus/io/gfile.cc failed: (Exit 1): gcc failed: error executing command; (cd /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/execroot/com_google_deepvariant && \; exec env - \; PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/local/bin/python3 \; PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages \; TF2_BEHAVIOR=1 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -Wunused-but-set-parameter -Wno-. Also, at an earlier point in the build, there is one more issue - . Download from https://mirror.bazel.build/github.com/bazelbuild/rules_cc/archive/081771d4a0e9d7d3aa0eed2ef389fa4700dfb23e.tar.gz failed: class java.io.FileNotFoundException GET returned **404 Not Found**",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705:131,depend,dependencies,131,,https://github.com/google/deepvariant/issues/607#issuecomment-1418449705,1,['depend'],['dependencies']
Integrability,Thanks @yangyxt . Good to know that you're not seeing strange behaviors despite the warning messages. I'll close this issue now.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566#issuecomment-1284747096:92,message,messages,92,,https://github.com/google/deepvariant/issues/566#issuecomment-1284747096,1,['message'],['messages']
Integrability,Thanks Nima. Can you (or an automated message) let me know once this will be fixed.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/118#issuecomment-438773230:38,message,message,38,,https://github.com/google/deepvariant/issues/118#issuecomment-438773230,1,['message'],['message']
Integrability,Thanks Paul.; I added an internal bug to track your suggestion. DeepVariant still has quite a lot error messages that have room for improvement. Thanks for reporting this to us so we can itemize them and improve over time.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353730864:104,message,messages,104,,https://github.com/google/deepvariant/issues/21#issuecomment-353730864,1,['message'],['messages']
Integrability,"Thanks for getting back to me. I have attached the screenshot for error messages:; <img width=""1368"" alt=""error_message_deepvariant"" src=""https://user-images.githubusercontent.com/30123717/145905106-9d351a82-a563-411c-b139-031f0eafa4ad.png"">",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/497#issuecomment-993005113:72,message,messages,72,,https://github.com/google/deepvariant/issues/497#issuecomment-993005113,1,['message'],['messages']
Integrability,"Thanks for reporting back and sorry my guess wasn't very helpful for resolving the problem. I'm a bit confused as to why it doesn't get unzip as a requirement since it's listed in the host dependencies in the conda recipe (https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/meta.yaml#L28). On the next iteration of the recipe we could add it to the `run` requirements to try and avoid this. In the short term, does adding `unzip` to your conda package install for the environment when building the Docker container avoid the issue and get things running? If you have other missing dependencies please let us know and we could have a similar treatment to fix them. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314#issuecomment-638091561:189,depend,dependencies,189,,https://github.com/google/deepvariant/issues/314#issuecomment-638091561,2,['depend'],['dependencies']
Integrability,"Thanks for response @pichuan,; I think i will definitely strive to implement this, but wont be before February 2024.; I'm still having issues getting my analysis to run end to end on its own and also with manual intevention, depending on size of dataset from same sample... I will ask remaining questions in separate tickets.; Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749#issuecomment-1858528241:225,depend,depending,225,,https://github.com/google/deepvariant/issues/749#issuecomment-1858528241,1,['depend'],['depending']
Integrability,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792#issuecomment-2012320998:243,depend,dependencies,243,,https://github.com/google/deepvariant/issues/792#issuecomment-2012320998,1,['depend'],['dependencies']
Integrability,"Thanks for the feedback, I’m using a published docker image 1.4.0 which also latest at the time of this posting.How do I disable openvino?On Dec 13, 2022, at 11:51 PM, Pi-Chuan Chang ***@***.***> wrote:﻿; Hi, it seems like you're using the openvino flag. Please remove that flag and try again.; For context - in recent versions, we haven't been building in openvino because we haven't been able to see extra speedup.; In the next version, I'll try to make this more clear and show the proper message a bit earlier (or, just remove the flag); Please let me know if it works after you remove the openvino flag. —Reply to this email directly, view it on GitHub, or unsubscribe.You are receiving this because you modified the open/close state.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545:492,message,message,492,,https://github.com/google/deepvariant/issues/597#issuecomment-1350769545,3,"['Message', 'message']","['Message', 'message']"
Integrability,"Thanks for the question and sorry about these issues. It seems like there was some kind of download issue with the deepvariant conda package in your environment. The 0.8.0 package has been around ~6 months and not changed recently, so I'm guessing maybe you got a partial download or other issue. If you remove your local download (` /home/ydliu/anaconda3/pkgs/deepvariant-0.8.0-py27h7333d49_0.tar.bz2`) and re-run, does it work cleanly? Is there anything about your network or other messages that might indicate an issue that could help us debug more? Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-547966813:484,message,messages,484,,https://github.com/google/deepvariant/issues/228#issuecomment-547966813,1,['message'],['messages']
Integrability,"Thanks for the reply. I have analyzed some mismatches and here are some thoughts:. Looking at two mismatches, here is the first one:. Chr1:261478C>A :; - GATK (GT:GQ:DP:AD) = 0/1:99:43:31,12; - Deepvariant(GT:AD:DP:GQ:PL:VAF) = 0/0:33,13:46:25:0,24,38:0.282609. GAKT have a minimum BaseQuality of 10 and MQ of 20, deepvariant 10 and 10. . Calculating the probability of AD distribution 31,12 (GATK) and 33,13 (deepvariant) is approximately 0.24% for deepvariant and 0,26% for GATK – so the probability that this distribution occurs assuming diploid and 50% chance of success the GT is most likely 0/0. GATKs GT is 0/1 with a GQ of 99 – it is however flagged a VQSRTrancheSNP99.90to100.00, but this is dependent on the other genomes in the sample( Im not a big fan of VQSR). Manually inspecting the variant using IGV I would annotate this as 0/1, but it is reasonable that it is assigned 0/0 considering the errors introduced by Illumnia sequencing are not random, even though with an error rate of 1:1000 it is highly unlikely that the same error occurs at the same site 13 times. (btw this is PCR-free WGS). The variant chr1:202192T>C:; - GATK (GT:GQ:DP:AD) = 0/1:99:16:10,6; - Deepvariant(GT:AD:DP:GQ:PL:VAF) = 0/0:17,16:23:6:0,5,29:0.26087. The probability is here for deepvariant 1.75% and GATK 22.6% - so the discrepancy of this variant makes sense. Manually inspecting the variant using IGV I would annotate this as 0/1. I guess that by including low quality MQ in this case decreases the GQ for deepvariant and increasing the min_mapping_quality to 20 could be worth trying for comparison. Is there an argument that can be used for this? (im using GCP for this project). I have noticed that deepvariant call variants in soft-clipped regions, but is seems they are assigned 0/0 – this is great – is this an intentional feature in deepvariant algorithm? This could be one reason to the discrepancies as GATK by default also call in soft-clipped regions. I will try to excluded these regions from ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/135#issuecomment-451075737:701,depend,dependent,701,,https://github.com/google/deepvariant/issues/135#issuecomment-451075737,1,['depend'],['dependent']
Integrability,"Thanks for the update @amy-houseman . In your original message, it seems like the main error was:. ```; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for /opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant: no such file or directory; ```. I'm glad that you're able to adjust your command and get this working! Thank you for the update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717#issuecomment-1771016203:55,message,message,55,,https://github.com/google/deepvariant/issues/717#issuecomment-1771016203,1,['message'],['message']
Integrability,"Thanks for trying out the conda deepvariant package and apologies about the issue. unzip is a dependency on the bioconda recipe, and my guess is that it's available in the environmental bin directory ( `/opt/conda/envs/deepvariant/bin/`) but this is not present on your PATH within the docker container so it's not being found. If you add that to the PATH for the container environment, it'll hopefully resolve the problem. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314#issuecomment-637733785:94,depend,dependency,94,,https://github.com/google/deepvariant/issues/314#issuecomment-637733785,1,['depend'],['dependency']
Integrability,"Thanks for your comments @gunjanbaid . I think, since everything else works well on single node systems, having the shuffle script work well on single node systems is also desirable. It is good to be able to shuffle only smaller files like you said. But if we limit ourselves to the original tfrecord outputs, that comes with the limitation that the shuffles are localized and not global as in the current script. However, a way to shuffle globally can be constructed from this idea with an additional step. This additional step will simply partition the input data into random buckets. Then we shuffle each bucket. I believe this is equivalent to a global shuffle with uniform probability for each permutation. This would be something like:; ```; input_data = readers | ""FlattenInputs"" >> beam.Flatten(); partitions = input_data | ""PartitionInputs"" >> beam.Partition(<random_partition_function_name>, <num_partitions>); for i, p in enumerate(partitions):; writing = p | ""WritePartition%d"" % i >> beam.io.WriteTFRecord(...); ```. Then each partition may be shuffled individually using the shuffle script. I have rolled both partitioning and shuffling into the same [script](https://github.com/anands-repo/deepvariant/blob/r1.0/tools/shuffle_tfrecords_beam_for_local.py). I will report back regarding whether this works as expected. This depends on beam.Partition behaving properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365#issuecomment-720871901:1337,depend,depends,1337,,https://github.com/google/deepvariant/pull/365#issuecomment-720871901,2,['depend'],['depends']
Integrability,"Thanks! I haven't tried running with gvcf since we messaged last, but yes I; never got it to work. We'll let you know if we get a chance to get it; working with the newest version. On Fri, Apr 12, 2019 at 12:20 AM Pi-Chuan Chang <notifications@github.com>; wrote:. > Hi @ekofman <https://github.com/ekofman> , with this issue, I think we; > might have left at a place where there is still a mystery.; >; > Now the v0.8.0 is out, do you mind trying this again and see if you're; > still seeing the same issue? Given that I didn't fully understand what the; > problem last time was, at least a sanity check on whether we're still; > seeing the same thing will be helpful.; >; > If I might be forgetting something that I could have followed up from last; > time, please also feel free to remind me. Thanks!; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/164#issuecomment-482431632>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AFMFwhn7aSNq8hIAj0rjxAgqHRnx7D1zks5vgAmCgaJpZM4cBchL>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/164#issuecomment-482616033:51,message,messaged,51,,https://github.com/google/deepvariant/issues/164#issuecomment-482616033,1,['message'],['messaged']
Integrability,"Thanks. I just tried to execute the command, but I get the following error message `ValueError: Cannot find matching files with the pattern ""/tmp/tmphskm66vr/call_variants_output.tfrecord.gz""`. The files in the tmp directory also seem to be gone.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/599#issuecomment-1360396137:75,message,message,75,,https://github.com/google/deepvariant/issues/599#issuecomment-1360396137,1,['message'],['message']
Integrability,"That's everything from the first indication of an error in the output. When I run make_examples directly, it core dumps with no stdout or stderr:; ```bash; singularity exec --bind /scratch:/tmp,/usr/lib/locale docker://google/deepvariant:1.1.0 /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples make_examples.tfrecord@24.gz --alt_aligned_pileup diff_channels --norealign_reads --vsc_min_fraction_indels 0.12 --task 0; ```. Unfortunately, this is collaborator data, so I can't share the BAM, but I checked to make sure the reference contig didn't match anything in [exclude_contigs.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/exclude_contigs.py), and the BAM was generated using our standard alignment arguments for pbmm2. I feel like it must be something obvious, but the lack of error message is making it hard to debug. Both the reference and the BAM are indexed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419#issuecomment-774276776:847,message,message,847,,https://github.com/google/deepvariant/issues/419#issuecomment-774276776,1,['message'],['message']
Integrability,"The actual error is ""The TF examples in /mnt/data/input/gs/wgs-test-shan/test_samples/UDN689484temp/examples/examples_output.tfrecord-00000-of-00064.gz has image/format \'None\' (expected \'raw\') which means you might need to rerun make_examples to genenerate the examples again."". @pichuan @depristo this is odd since the pipeline ran as a single workflow. The model and docker binary paths also seem correct. One issue I can think of is most of the shards being empty (the output has 64 shards, but it's only 1.3KB in total). Do you know if empty shards could cause such an error?. P.S. the 'gsutil not found' error is actually harmless. I think we should provide a 'parser' for these errors based on the logs that provides a meaningful error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355032456:746,message,message,746,,https://github.com/google/deepvariant/issues/27#issuecomment-355032456,1,['message'],['message']
Integrability,"The error message says ""Cannot query without an index"", so I'm guessing your CRAM file might be missing its index too. See: http://www.htslib.org/doc/samtools-index.html; Again see the [documentation](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details.md).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/307#issuecomment-628240247:10,message,message,10,,https://github.com/google/deepvariant/issues/307#issuecomment-628240247,1,['message'],['message']
Integrability,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/310#issuecomment-636143865:10,message,messages,10,,https://github.com/google/deepvariant/issues/310#issuecomment-636143865,1,['message'],['messages']
Integrability,"The exact name for the chromosome depends on your reference file. If the reference FASTA names its chromosomes chr1, chr2 you need to use ""chr"" in the prefix. If the reference FASTA calls them 1, 2, etc you need to use that name.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/72#issuecomment-413278699:34,depend,depends,34,,https://github.com/google/deepvariant/issues/72#issuecomment-413278699,1,['depend'],['depends']
Integrability,"There is a dependency conflict with etils package. Indeed if you try to install deepvariant with the following command:. `conda create -n deepV -c bioconda deepvariant etils -y`. It fails because it is not able to solve all the dependencies. Although I am not an expert of Phyton, It seems to me a problem related to the different phyton versions to use.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279:11,depend,dependency,11,,https://github.com/google/deepvariant/issues/669#issuecomment-1602257279,2,['depend'],"['dependencies', 'dependency']"
Integrability,"Think of a CPU (core) as a basket of functions your program can utilize to take an input data to an output. A GPU has many more baskets, but containing fewer, more specialized functions. A high-end CPU can have 64 cores (baskets), while a nice high-end GPU can have between 2,560-16,384. Thus a GPU can operate on a specialized set of functions much faster in parallel, but with one caveat. The thing is that your program would need to be coded and compiled for a GPU. DeepVariant only can utilize 1 GPU for the middle stage (`call_variants`) of the three stages, as the other two (`make_examples` and `postprocess_variants`) are single-threaded (meaning they are CPU-based). Regarding the compute instance of EC2, that is a high-end one, but you need to experiment to see what works for you and is within your budget. DeepVariant can also utilize a lot of memory depending on what stage it is running, and how much of the genome your are covering.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679300379:864,depend,depending,864,,https://github.com/google/deepvariant/issues/696#issuecomment-1679300379,1,['depend'],['depending']
Integrability,"To: google/deepvariant <deepvariant@noreply.github.com<mailto:deepvariant@noreply.github.com>>; > Cc: Brad Thomas <brad.thomas@neogenomics.com<mailto:brad.thomas@neogenomics.com>>; Author <; > author@noreply.github.com<mailto:author@noreply.github.com>>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:2642,message,message,2642,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['message'],['message']
Integrability,"Update (1) on numpy version:. Turns out getting back to numpy 1.12 is harder than I thought, because TensorFlow 1.4 requires numpy 1.14. When I try to revert the prereq script to numpy 1.12, I keep getting this message:; ""tensorflow 1.4.1 has requirement numpy>=1.12.1, but you'll have numpy 1.12.0 which is incompatible."". which makes me uncomfortable to pin numpy back at 1.12. I did try changing the numpy version to 1.12, and build with build_release_binaries.sh. It seems to build, and call_variants step (which is the main step that uses Tensorflow) seems to run with similar speed. The hap.py numbers are exactly the same.; However, it seems like bad practice to ignore that warning message that shows up in red.; Thoughts?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385118050:211,message,message,211,,https://github.com/google/deepvariant/issues/29#issuecomment-385118050,2,['message'],['message']
Integrability,"We are using singularity 3.5.2 and the image was obtained with this command:. singularity pull docker://gcr.io/deepvariant-docker/deepvariant:0.9.0. We have a wrapper script that we use with deepvariant-0.8.0 to submit to the CentOS 7 based compute cluster without issue and it is used with v0.9.0 with the only change being the version of deepvariant. Also I opened a shell on the 0.9.0 image and ran 'pip freeze | grep intervaltree' and got this version:; ; intervaltree==2.1.0. This is the submit script without the SLURM commands:; ```; export BIN_VERSION=""0.9.0""; export BASE=""${PWD}/deepvariant-run""; export INPUT_DIR=""${BASE}/input""; export REF=""hs37d5.fa.gz""; export BAM=""HG002_NIST_150bp_chr20_downsampled_30x.bam""; export OUTPUT_DIR=""${BASE}/output""; export DATA_DIR=""${INPUT_DIR}/data""; export OUTPUT_VCF=""HG002.output.vcf.gz""; export OUTPUT_GVCF=""HG002.output.g.vcf.gz"". mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". gsutil cp gs://deepvariant/performance-testdata/""${BAM}"" ""${DATA_DIR}""; gsutil cp gs://deepvariant/performance-testdata/""${BAM}"".bai ""${DATA_DIR}"". cd /scratch/rsmith/DeepvariantTests/case-study/; module load deepvariant/0.9.0-gpu-phoenix. run_deepvariant --model_type=WGS --ref=""/input/data/${REF}"" \; --reads=""${DATA}/input/data/${BAM}"" \; 	 --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --regions 20 --num_shards=$(nproc). ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/255#issuecomment-568131537:159,wrap,wrapper,159,,https://github.com/google/deepvariant/issues/255#issuecomment-568131537,1,['wrap'],['wrapper']
Integrability,"We can be (that would imply `min_AQ1 > min_AQ2`), but based on our investigation the current settings were chosen to give the best overall quality for variety of cohort sizes and sequencing depths. Depending on how the cohort VCF is used, one may want to apply additional filters to the cohort, which can either be applied _after_ merging using a standard VCF modification tool (e.g. bcftools), or by changing the merging settings directly in the .yml file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/326#issuecomment-660563647:198,Depend,Depending,198,,https://github.com/google/deepvariant/issues/326#issuecomment-660563647,1,['Depend'],['Depending']
Integrability,"We have a CL going into our internal systems at Google, which we will cherry-pick in r0.4 or wait until the upcoming r0.5 release, depending on timing. Here's the text, though:. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. We currently recommend that the BAM be; duplicate marked, but it's unclear if this is even necessary. Finally, it's not; necessary to recalibrate the base qualities or do any form of indel realignment. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confide",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/35#issuecomment-356807166:131,depend,depending,131,,https://github.com/google/deepvariant/issues/35#issuecomment-356807166,1,['depend'],['depending']
Integrability,"We haven't tried yet to make DeepVariant accessible via pip or in a virtualenv, so unfortunately we don't have any advice for you on this topic. I'm not even sure its possible to get it to work, but if you do decide to work it through please let us know how you do it and we can consider adding official support for working inside of a virtualenv. . Our current recommendations, if you don't want to install DeepVariant dependencies on your machine, is to either (1) run on a cloud instance or (2) run it in a docker container. We build an official docker container for DeepVariant and you can build your own here: https://github.com/google/deepvariant/tree/master/deepvariant/docker. @arostamianfar I don't see any docs on how to access the prebuilt DeepVariant docker image. Could we add a link to it here and add it to the general docs as well?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/25#issuecomment-354157667:420,depend,dependencies,420,,https://github.com/google/deepvariant/issues/25#issuecomment-354157667,1,['depend'],['dependencies']
Integrability,"We're consulting with our teammate @ThomasColthurst to see if he has anything to add here about your error message. In the past, I did try building with both Ubuntu 14 and 18 and didn't encounter such issue, but I have not done so recently. I'll try soon. And bazel - I think 0.21 is the right version we're using.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199#issuecomment-514424604:107,message,message,107,,https://github.com/google/deepvariant/issues/199#issuecomment-514424604,1,['message'],['message']
Integrability,We're seeing this same problem on centos7 on new processors for which there is no working cpufreq support. Why not just wrap the call to `psutil.cpu_freq` in a `try`? I'm happy to provide a PR for that. (cc: @njcarriero @chris-sf),MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/104#issuecomment-430404177:120,wrap,wrap,120,,https://github.com/google/deepvariant/issues/104#issuecomment-430404177,1,['wrap'],['wrap']
Integrability,We've added a more verbose message. It will come out in later releases of Nucleus and DeepVariant. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/73#issuecomment-389700500:27,message,message,27,,https://github.com/google/deepvariant/issues/73#issuecomment-389700500,1,['message'],['message']
Integrability,"We've got a fix for the GCP dependency internally, and will sync it to the OSS github version on our next push. Thanks for the report.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/7#issuecomment-351080961:28,depend,dependency,28,,https://github.com/google/deepvariant/issues/7#issuecomment-351080961,1,['depend'],['dependency']
Integrability,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:253,Message,Message,253,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538,2,['Message'],['Message']
Integrability,"While AWS was better than my local computer (with 8 GB of RAM and 4 cores) for running the 1st step (_make_examples_), I can likewise try running the Docker container in interactive mode on my own computer:. `docker run -it -v /c/Users/Charles/Documents/WGS_Exome_Analysis/My_Veritas_WGS:/mnt/wgs gcr.io/deepvariant-docker/deepvariant`. followed by moving to the appropriate directory and running the following script:. ```; OUTPUT_DIR=Genos_Provided; REF=../hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz""; FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. This is different than running interactive mode for **postprocess_variants** on AWS (which almost immediately ends, without error message or results file), but it has been running for more than one hour. So, I will provide an update if this works, but this sounds different than your experience with the t1.micro AWS instance.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480640009:864,message,message,864,,https://github.com/google/deepvariant/issues/167#issuecomment-480640009,1,['message'],['message']
Integrability,"Yeah I think that would be fine. This case was more unfortunate as the missing OpenVINO leads to the missing `from tensorflow.python.tools import optimize_for_inference_lib` dependency, and so I assumed I had really messed something up as I hadn't changed anything with tensorflow. On a slightly different note, the memory usage for call_variants (open_vino=False) in v1.4 is much better than v1.3 (open_vino=True) at ~ 7gb compared to ~20gb RAM for a lot of medium coverage samples. Also postprocessing seems to be about 70% faster (although the previous runs had IO issues so probably inflated). Great improvements this release!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541#issuecomment-1154742774:174,depend,dependency,174,,https://github.com/google/deepvariant/issues/541#issuecomment-1154742774,1,['depend'],['dependency']
Integrability,"Yes, but take a look at this:. https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md. It just depends on how many coffee breaks you want to have :). ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-464894511:116,depend,depends,116,,https://github.com/google/deepvariant/issues/157#issuecomment-464894511,1,['depend'],['depends']
Integrability,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-586546504:46,wrap,wrapper,46,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504,1,['wrap'],['wrapper']
Integrability,"Yes, sorry about the blunt message. I ran deepvariant with a subset of my data to test. All right with the outputs, but I cannot visualize the visual_report.html. I have the file, but when I open with chrome or firefox nothing is show. I changed the permissions of the files, didn't help. I even tried to generate again usin the just the VCF stats report alone using as input the vcf file generated by DV. This is how I set the commands:. BIN_VERSION=""1.0.0""; BASE=""${PWD}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""10consensus.fasta""; BAM=""268_041_m10.sorted.bam""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""M10.output.vcf.gz""; OUTPUT_GVCF=""M10.output.g.vcf.gz""; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${DATA_DIR}"". sudo docker run --gpus 1 \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=32. [output.visual_report.zip](https://github.com/google/deepvariant/files/5265027/output.visual_report.zip). Thanks.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/290#issuecomment-697073304:27,message,message,27,,https://github.com/google/deepvariant/issues/290#issuecomment-697073304,1,['message'],['message']
Integrability,"You are very welcome - thank you for your help!. FYI, the script recently stopped with the same error message. So, unless that file gives you another troubleshooting idea, I may look into other possible strategies (and/or look into more AWS documentation) and return to this issue when I get to the same stage with running DeepVariant (or at least report the alternative solution that I worked out).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479788147:102,message,message,102,,https://github.com/google/deepvariant/issues/167#issuecomment-479788147,1,['message'],['message']
Integrability,"You said it was random clonal organism, il have a look when the reads are; on the sra database, was just trying to help,. And yes I don't care about SNPs sorry. (Google people) I'll be here to offer random advice to other people if I'm; still allowed,. Joe. On Mon, 31 Jul 2023, 17:48 Axze-rgb, ***@***.***> wrote:. > Ok so you have no idea what we are talking about, this is not a; > prokaryote. I would suggest you avoid hopping into conversations for making; > completely ignorant comments, not even bothering to check about what; > organisms we are dealing with.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658759224>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2TF5UCMO6WOF6KVCJ3XS7OWLANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658762468:882,Message,Message,882,,https://github.com/google/deepvariant/issues/682#issuecomment-1658762468,1,['Message'],['Message']
Integrability,"\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3'); ). ```. #### _*For version 1.10.1*_. ```Python; DESCRIPTOR = _descriptor.FileDescriptor(; name='tensorflow/core/framework/resource_handle.proto',; package='tensorflow',; syntax='proto3',; serialized_options=_b('\n\030org.tensorflow.frameworkB\016ResourceHandleP\001Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\370\001\001'),; serialized_pb=_b('\n/tensorflow/core/framework/resource_handle.proto\x12\ntensorflow\""r\n\x13ResourceHandleProto\x12\x0e\n\x06\x64\x65vice\x18\x01 \x01(\t\x12\x11\n\tcontainer\x18\x02 \x01(\t\x12\x0c\n\x04name\x18\x03 \x01(\t\x12\x11\n\thash_code\x18\x04 \x01(\x04\x12\x17\n\x0fmaybe_type_name\x18\x05 \x01(\tBn\n\x18org.tensorflow.frameworkB\x0eResourceHandleP\x01Z=github.com/tensorflow/tensorflow/tensorflow/go/core/framework\xf8\x01\x01\x62\x06proto3'); ); ```. ProtoBuf 3.6 passes the `serialized_options` argument in the call to `__new__` as shown here:. https://github.com/protocolbuffers/protobuf/blob/3.6.x/python/google/protobuf/descriptor.py#L283. ```Python; def __new__(cls, name, full_name, filename, containing_type, fields,; nested_types, enum_types, extensions, options=None,; serialized_options=None,; is_extendable=True, extension_ranges=None, oneofs=None,; file=None, serialized_start=None, serialized_end=None, # pylint: disable=redefined-builtin; syntax=None):; _message.Message._CheckCalledFromGeneratedFile(); return _message.default_pool.FindMessageTypeByName(full_name); ```. 3. If you look at the `METADATA` file of each TensorFlow package to see the protobuf version requirement, it will look like this:. ```; paul:~/tensorflow$ cat unzip-1.9/tensorflow-1.9.0.dist-info/METADATA | grep protobuf; Requires-Dist: protobuf (>=3.4.0). paul:~/tensorflow$ cat unzip-1.10/tensorflow-1.10.1.dist-info/METADATA | grep protobuf; Requires-Dist: protobuf (>=3.6.0); ```. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/94#issuecomment-423037408:3022,protocol,protocolbuffers,3022,,https://github.com/google/deepvariant/issues/94#issuecomment-423037408,2,"['Message', 'protocol']","['Message', 'protocolbuffers']"
Integrability,"_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /opt/deepvariant/bin/postprocess_variants --ref ""stdchroms.hg38.fa"" --infile ""./call_variants_output.tfrecord.gz"" --outfile ""./SAMPLENAME.deepVariant.vcf.gz"" --cpus ""8"" --gvcf_outfile ""./SAMPLENAME.deepVariant.g.vcf.gz"" --nonvariant_site_tfrecord_path ""./gvcf.tfrecord@8.gz"" --sample_name ""SAMPLENAME""; ```. And here are the two last commands with std out ... ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""./call_variants_output.tfrecord.gz"" --examples ""./make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/pacbio"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0510 12:13:42.483308 47501039724352 call_variants.py:563] Total 1 writing processes started.; I0510 12:13:42.487790 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:42.487916 47501039724352 call_variants.py:588] Shape of input examples: [100, 199, 9]; I0510 12:13:42.488451 47501039724352 call_variants.py:592] Use saved model: True; I0510 12:13:52.162126 47501039724352 dv_utils.py:370] From /opt/models/pacbio/example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0510 12:13:52.163805 47501039724352 dv_utils.py:370] From ./make_examples.tfrecord-00000-of-00008.gz.example_info.json: ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2104434632:1713,depend,dependencies,1713,,https://github.com/google/deepvariant/issues/818#issuecomment-2104434632,1,['depend'],['dependencies']
Integrability,"`unspecified_caller` is an invalid option. I've never actually specified it, but I think the behavior should be that it should crash earlier (hopefully with a meaningful error message). . To train your own sequencing-type specific model, generally if the sequencer's base error rate is not too high, the default `very_sensitive_caller` should just work. But, if it's tricker cases like ONT reads, having a separate candidate generation process which is smarter about proposing candidates, and then using `vcf_candidate_importer` is the right way to go. @maryawood Can you say a bit more about why the default `vcf_candidate_importer` doesn't work for you? Do you get very imbalanced training data?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/433#issuecomment-807187618:176,message,message,176,,https://github.com/google/deepvariant/issues/433#issuecomment-807187618,1,['message'],['message']
Integrability,"abstracting out the data and functionals (i.e. models), to enable a larger analysis platform, rather than still be file-focused which (dynamic) datasets would allow. Maybe tomorrow you want to try [Inception-v4](https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_v4.py) or some other more expressive network topology, which might require changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:1748,message,message,1748,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['message'],['message']
Integrability,"ack (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-28:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449:2941,wrap,wrapped,2941,,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449,1,['wrap'],['wrapped']
Integrability,"ad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. Hi again,; I didn't read carefully so I missed that you said you want to train a model.; If you want to get make_examples to create more candidates, the other flags you need to consider are: vsc_min_count_snps, vsc_min_count_indels, vsc_min_fraction_snps, vsc_min_fraction_indels. With the default values of these flags for VSC (Very Sensitive Caller), you simply won't be able to even get candidates generated for low allele fraction variants. So I would suggest playing around with those flags and see if more candidates come out. Thanks! Let us know how it goes. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-379110341>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqU5J11c7Zr-VYS_8CjFPh-UF6VIYks5tlq76gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-379857500:1583,message,message,1583,,https://github.com/google/deepvariant/issues/62#issuecomment-379857500,8,['message'],['message']
Integrability,"ant the full data anyway (that wouldn't really be a small thing I can try). I understand if you can't even subsample from that.; How about at least posting the commands you used?. From earlier discussions, it sounds like the main thing you're changing about the data representation is the pileup_image_height. You can actually do the same thing on the QuickStart or CaseStudy data too. It will just look like a taller image with the bottom being mostly empty.; (You can use logic like this https://github.com/google/deepvariant/blob/r0.6/docs/visualizing_examples.ipynb to visualize them). And then, I suspect there's a high probability that you can get the same error on the CaseStudy data if you follow the same steps. Once you're able to do that, post every steps (similar to QuickStart and CaseStudy) here. And note the place where you're having an error. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-381167653>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqSFWVfTRnfVvHyl6ecMC-XCahjurks5toMDJgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381209312:1879,message,message,1879,,https://github.com/google/deepvariant/issues/62#issuecomment-381209312,4,['message'],['message']
Integrability,"ant:latest; sudo docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo docker push localhost:5000/deepvariant:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant.simg docker://localhost:5000/deepvariant:latest; ```. To run [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md), instead of using the docker command, you can use this command instead:. ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```; (The extra flags for singularity was added because of the locale issue: https://github.com/BioContainers/containers/issues/206#issuecomment-448698033). ## GPU image; ```; sudo nvidia-docker pull gcr.io/deepvariant-docker/deepvariant_gpu:0.8.0; sudo nvidia-docker tag gcr.io/deepvariant-docker/deepvariant_gpu:0.8.0 localhost:5000/deepvariant_gpu:latest; sudo nvidia-docker run -d -p 5000:5000 --restart=always --name registry registry:2; sudo nvidia-docker push localhost:5000/deepvariant_gpu:latest; SINGULARITY_NOHTTPS=1 singularity build deepvariant_gpu.simg docker://localhost:5000/deepvariant_gpu:latest; ```. Running through Quick Start just to make sure nothing wrong:; ```; singularity -s exec --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_gpu.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. I do see this issue though:; https://github.com/sylabs/singularity/issues/1916; with the; ```; awk: warning: escape sequence `\.' treated as plain `.'; ```; messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/132#issuecomment-482430728:3041,message,messages,3041,,https://github.com/google/deepvariant/issues/132#issuecomment-482430728,2,['message'],['messages']
Integrability,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3134,depend,dependency,3134,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"aviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4160,depend,dependency,4160,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"b/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:55 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:10438,depend,dependency,10438,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2838,depend,dependency,2838,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"below. To get the most accurate information, please visit their websites. (Our team does not maintain these). * [Running DeepVariant on Google Cloud Platform](https://cloud.google.com/genomics/docs/tutorials/deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1140,interface,interface,1140,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,1,['interface'],['interface']
Integrability,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4032,depend,dependency,4032,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5058,depend,dependency,5058,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"cal/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8570,depend,dependency,8570,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"call any negatives at all. But it's just my guess based on what you observe. (2) Before we even dig deeper into the training behavior, can you check this:; What is the distribution of your labeled data? For example, in the regular DeepVariant formulation, we have 3 classes, ""0"" -- HOM_REF, ""1"" -- HET, ""2"" --HOM_ALT. HET and HOM_ALT are the ones that are consider germline variants, while HOM_REF calls will result in `RefCall` in the final DeepVariant VCF files.; To give you an example, in our [0.8 release](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md#deepvariant-training-data), our total number of examples for WGS was 346,505,686. And the distribution was:; ```; class 0, count: 101,679,899; class 1, count: 145,911,730; class 2, count: 98,914,057; ```; There is no fixed recommendation of what the ratio should be. It depends on a lot of factors such as what is your BAM file like, what is the threshold you're using in the first round of very sensitive caller (which picks the candidates that feed into the classifier), and many many other details that are specific to your data. Which is why I said our team cannot help debug the details of your case. But hopefully by examining your own distribution, you can first see if the training (and tuning) data makes sense or not. If the data has very skewed distribution, there are also other techniques that the ML community uses to improve the accuracy. But I won't be able to get into that. It's also not what DeepVariant designed for.; DeepVariant is wrapped around TensorFlow, which is a much more general purpose ML tool. If there are functionalities that we don't provide, please also look into TensorFlow to see if they have something useful for you. I'm closing this issue now because this is not really a DeepVariant issue. But if you believe this actually reveals some bugs in our codebase, I'm happy to discuss further if you can show a reproducible example that demonstrate the error.; Cl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203#issuecomment-518462111:1833,depend,depends,1833,,https://github.com/google/deepvariant/issues/203#issuecomment-518462111,2,['depend'],['depends']
Integrability,"ckages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3847,depend,dependency,3847,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"ckages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4421,depend,dependency,4421,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"cker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m1.293s; user 0m1.033s; sys 0m0.705s; I0910 01:14:39.661637 139749579556608 run_deepvariant.py:364] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 369, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. I wonder is there any way to let me get the hidden parallel error message?. Best,; Jerry",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-689932270:4623,message,message,4623,,https://github.com/google/deepvariant/issues/345#issuecomment-689932270,1,['message'],['message']
Integrability,"com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12121,depend,dependency,12121,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"constraining google-cloud-sdk version worked! thank you. On Wed, Apr 24, 2019 at 3:17 AM Brad Chapman <notifications@github.com>; wrote:. > Peter;; > Thanks for testing, it sounds like there is a problem with the recent; > google-cloud-sdk packages. I'll take a look to see if I can figure out what; > is going wrong but an immediate thing you could try is to restrict that; > dependency version to try and avoid the issue:; >; > conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'; >; > Hope this helps get it installed.; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/177#issuecomment-486163437>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABUBV2DKBMPCNW6H6H2OGKLPSAXVJANCNFSM4HH7EBWQ>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-486821218:377,depend,dependency,377,,https://github.com/google/deepvariant/issues/177#issuecomment-486821218,1,['depend'],['dependency']
Integrability,"copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:10282,depend,dependency,10282,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['depend'],['dependency']
Integrability,"d as well. > ; > I'm trying to understand if additional channels are mutually exclusive choices for the 7th channel when using the `run_deepvariant` command. My first attempt at running with both `insert_size` + `allele_frequency` seemed to work. However, it produced examples with channels `[1, 2, 3, 4, 5, 6, 19]` instead of `[1, 2, 3, 4, 5, 6, 8, 19]`. I would have expected an error, yet `call_variants` produced a vcf output despite not having an `insert_size` channel. Did `allele_frequency` replace the 7th channel correctly? Or did it somehow encode `allele_frequency` data as `insert_size,` if that makes sense?. If you made examples with 8 channels, but the model has 7 channels, the call_variants step should have errors like you've shared in your original post:. ```; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 8.; ```. If you make examples with 7 channels, but your model.ckpt has different 7 channels - then it depends. Starting from v1.4.0, we keep a `model.ckpt.example_info.json` file together with the model. For example:; ```; $ gsutil cat gs://deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-wgs_standard/model.ckpt.example_info.json; {""version"": ""1.4.0"", ""shape"": [100, 221, 7], ""channels"": [1, 2, 3, 4, 5, 6, 19]} ; ```. If the customized_model you're using has this file, and if the examples you're making ends up having 7 channels but different ones, then it should still give you an error. But, if you're using an older model (such as the AF model you're using), and if it didn't have a `model.ckpt.example_info.json` file with it, then I believe the current behavior is that it will try to use the 7 channels directly. Which will cause the issue that it might use the weights for allele_frequency channel for the insert_size channel. Does that make sense?. By the way, in case you haven't seen it, we have a nice blog post that talks about channels: https://",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213:1810,depend,depends,1810,,https://github.com/google/deepvariant/issues/568#issuecomment-1251857213,1,['depend'],['depends']
Integrability,"d in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3920,depend,dependency,3920,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"deepvariant) | Docker-based pipelines optimized for cost and speed. Code can be found [here](https://github.com/googlegenomics/gcp-deepvariant-runner).; * [DeepVariant-on-spark from ATGENOMIX](https://github.com/atgenomix/deepvariant-on-spark) | A germline short variant calling pipeline that runs DeepVariant on Apache Spark at scale with support for multi-GPU clusters (e.g. NVIDIA DGX-1).; * [Parabricks](https://docs.parabricks.com/standalone-tools/deepvariant) | An accelerated DeepVariant pipeline with multi-GPU support that runs our WGS pipeline in just 40 minutes, at a cost of $2-$3 per sample. This provides a 7.5x speedup over a 64-core CPU-only machine at lower cost.; * [DNAnexus DeepVariant App](https://platform.dnanexus.com/app/deepvariant_germline) | Offers parallelized execution with a GUI interface (requires platform account).; * [Nextflow Pipeline](https://github.com/nf-core/deepvariant) | Offers parallel processing of multiple BAMs and Docker support.; * [DNAstack Pipeline](https://app.dnastack.com/auth/realms/DNAstack/protocol/openid-connect/auth?client_id=dnastack-client&redirect_uri=https%3A%2F%2Fapp.dnastack.com%2F%3Fredirect_fragment%3D%252Forg%252F473079%252Fproj%252F473096%252Fapp%252Fworkflow%252F425685%252Frun&state=42231553-9fbc-4d71-a10e-d6ce42415c01&nonce=daf2568d-4fe7-48e2-ab60-858937244a87&response_mode=query&response_type=code&scope=openid) | Cost-optimized DeepVariant pipeline (requires platform account). However, I'm quite curious why 64G didn't work for you. I will need to check back to see how much our postprocess_variants use for the HG002 Case Study. I don't think it should need 64G of RAM. But it would certainly depends on your data and the size of the input. A few things to follow up:; (1) If you can let me know how big is the input file for postprocess_variants (output from call_variants), that might be useful information. (2) I'll follow up with you through a message to see what we can do in order to diagnose this particular run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479685222:1377,protocol,protocol,1377,,https://github.com/google/deepvariant/issues/167#issuecomment-479685222,3,"['depend', 'message', 'protocol']","['depends', 'message', 'protocol']"
Integrability,"delAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380935943>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWTCcHHVi1NrDCRWylTEadlDsGGAks5tn7pQgaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-381164890:3262,message,message,3262,,https://github.com/google/deepvariant/issues/62#issuecomment-381164890,1,['message'],['message']
Integrability,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:9351,depend,dependency,9351,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8874,depend,dependency,8874,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2760,integrat,integrated,2760,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,2,['integrat'],"['integrated', 'integration']"
Integrability,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7896,depend,dependency,7896,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2635,depend,dependency,2635,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3661,depend,dependency,3661,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"ed pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2290,depend,dependency,2290,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"eloper.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2023-12-05 07:43:20.303963: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-05 07:43:24.030774: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-12-05 07:43:24.033082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Which is expected because I didn't pass in more flags for run_deeptrio. The most relevant message is the ones at the bottom:. ```; --model_type is required.; Pass --helpshort or --helpfull to see help on flags.; ```. From here, I should be able to pass in the arguments from Quick Start and have it working. (If you want, I can continue the test). ---. @alanlamsiu ,; Now I got here, please check these two things on your side:. 1. Can you confirm that your .sif is indeed made from `1.6.0`? (Which is our latest version. Please don't use the rc2 one).; 2. Can you try `/opt/deepvariant/bin/deeptrio/run_deeptrio`? You seem to be using a different path. I'm also not sure where that came from -- if we have any GitHub documentation that's inconsistent, please let me know and I can update.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389:4097,message,message,4097,,https://github.com/google/deepvariant/issues/745#issuecomment-1840177389,2,['message'],['message']
Integrability,"eply.github.com>; > Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <; > author@noreply.github.com>; > Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62); >; > CAUTION: This email originated from outside the organization. DO NOT click; > links or open attachments unless you recognize the sender and know the; > content is safe.; >; > I think you'll want:; > tfrecord_path:; > ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064""; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub<; > https://github.com/google/deepvariant/issues/62#issuecomment-380193942>,; > or mute the thread<; > https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>.; >; >; > This message contains confidential information and is intended only for; > the individual named. If you are not the named addressee you should not; > disseminate, distribute or copy this e-mail. Please notify the sender; > immediately by e-mail if you have received this e-mail by mistake and; > delete this e-mail from your system. E-mail transmission cannot be; > guaranteed to be secured or error-free as information could be intercepted,; > corrupted, lost, destroyed, arrive late or incomplete, or contain viruses.; > The sender therefore does not accept liability for any errors or omissions; > in the contents of this message, which arise as a result of e-mail; > transmission. If verification is required please request a hard-copy; > version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort; > Myers, FL 33913, http://www.neogenomics.com (2017); >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/62#issuecomment-380197853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAczBalfAA5qelNx5damo_mTuPg7r4UJks5tnPbigaJpZM4TIm9R>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380935943:2458,message,message,2458,,https://github.com/google/deepvariant/issues/62#issuecomment-380935943,1,['message'],['message']
Integrability,"ermediate_results_dir/make_examples.tfrecord@8.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@8.gz --regions /input/idt_capture_novogene.grch38.bed --task 3"" Error without the more informative error message.; 4. Ran again the command without num shards flag:; BIN_VERSION=""1.2.0"". ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir; ```; 5. It ran and created only three intermediate files: call_variants_output.tfrecord, gvcf.tfrecord-00000-of-00001.gz, make_examples.tfrecord-00000-of-00001.gz without any Error message. I'm copying the last lines here:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; warnings.warn('`layer.apply` is deprecated and '; INFO:tensorflow:Done calling model_fn.; I0911 15:27:04.863512 139825053296448 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I0911 15:27:05.510090 139825053296448 monitored_session.py:247] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/wes/model.ckpt; I0911 15:27:05.510811 139825053296448 saver.py:1298] Restoring parameters from /opt/models/wes/model.ckpt; INFO:tensorflow:Running local_init_op.; I0911 15:27:06.189059 139825053296448 session_manager.py:531] Running local_init_op.; INFO:tensorflow:Done running local_init_op.; I0911 15:27:06.217319 139825053296448 session_manager.py:534] Done running local_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483#issuecomment-917442547:1403,message,message,1403,,https://github.com/google/deepvariant/issues/483#issuecomment-917442547,1,['message'],['message']
Integrability,"es that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5365,depend,dependency,5365,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1979,bridg,bridge,1979,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665,2,['bridg'],['bridge']
Integrability,"examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; ```. It seems like for this particular failure mode (BAM file is truncated), the default stackt race was already clear.; But given that I mentioned different level of verbosity. I tried that next. ## Use `-v` to increase verbosity level. if you run `/opt/deepvariant/bin/make_examples --helpfull` you'll see this flag:. ```; -v,--verbosity: Logging verbosity level. Messages logged at this level or; lower will be included. Set to 1 for debug logging. If the flag was not set; or supplied, the value will be changed from the default of -1 (warning) to 0; (info) after flags are parsed.; (default: '-1'); (an integer); ```; You can try to add `-v 1`. But in this case above, my run already had useful logging and didn't seem to produce more useful logs. ## What if we run with Singularity?. I think you were running with Singularity, so I tried that too. I repeated the steps in https://github.com/google/deepvariant/issues/463#issuecomment-866352316 to install Singularity. And then with the same data, I ran:. ```; # Pull the image.; BIN_VERSION=1.1.0; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" \; --reads ""quickstart-testdata/NA1287",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:14029,Message,Messages,14029,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,1,['Message'],['Messages']
Integrability,"flow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response..",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8929,depend,dependency,8929,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1303, in restore; sess.run(self.saver_def.restore_op_name,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 967, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1190, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1368, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1394, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.NotFoundError: From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:7870,message,message,7870,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,2,['message'],['message']
Integrability,"h:; ```; gcloud beta compute instances create ""${USER}-centos-singularity"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image=centos-7-drawfork-v20181102 \; --image-project=eip-images \; --machine-type ""n1-standard-32"" \; --zone ""us-west1-b""; ```. 2. On the machine, I installed Singularity 2.5.2 with instructions on: https://github.com/sylabs/singularity/blob/2.5.2/INSTALL.md. 3. I copied a Singularity image that I built with [the instructions I posted before](https://github.com/google/deepvariant/issues/132#issuecomment-482430728) on a Ubutun 16.04 machine to this CentOS 7 machine. Then I got the [Quick Start](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md) data and run the command:; ```; singularity -s exec -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant.simg \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${INPUT_DIR}/ucsc.hg19.chr20.unittest.fasta \; --reads=${INPUT_DIR}/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=output.vcf.gz \; --output_gvcf=output.g.vcf.gz ; ```. The run completed without an issue. My CentOS machine has:; ```; $ lsb_release ; LSB Version: :core-4.1-amd64:core-4.1-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-4.1-amd64:desktop-4.1-noarch:languages-4.1-amd64:languages-4.1-noarch:printing-4.1-amd64:printing-4.1-noarch; ```; ```; $ cat /etc/centos-release; CentOS Linux release 7.6.1810 (Core) ; ```. ---. From the original error message:; `ImportError: No module named _multiarray_umath`; It seems like an issue with numpy installation. But given we're using a singularity image, I am having a hard time thinking why this would be the case. (Unless it's not created correctly?). @drtamermansour ; Two questions for you:; (a) When you create the image, can you try to run it on the machine where you create it to make sure it worked there? ; (b) Do you think it'll help if either I or @williamrowell share our *simg file with you to try it out?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/178#issuecomment-487218238:1643,message,message,1643,,https://github.com/google/deepvariant/issues/178#issuecomment-487218238,1,['message'],['message']
Integrability,"hi @pichuan , this log line is generated from my bash function wrapper running singularity version of DeepTrio. I confirm that I am using the official docker image (converted to SIF image with Singularity)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646#issuecomment-1547132456:63,wrap,wrapper,63,,https://github.com/google/deepvariant/issues/646#issuecomment-1547132456,1,['wrap'],['wrapper']
Integrability,https://github.com/google/nucleus/blob/v0.6.0/nucleus/protos/range.proto#L18. ```; // A 0-based half-open genomic coordinate range for search requests.; message Range {; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/656#issuecomment-1573061991:153,message,message,153,,https://github.com/google/deepvariant/issues/656#issuecomment-1573061991,1,['message'],['message']
Integrability,"ibutions in binary form must reproduce the above copyright\n; 11:# notice, this list of conditions and the following disclaimer in the\n; 12:# documentation and/or other materials provided with the distribution.\n; 13:#\n; 14:# 3. Neither the name of the copyright holder nor the names of its\n; 15:# contributors may be used to endorse or promote products derived from this\n; 16:# software without specific prior written permission.\n; 17:#\n; 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n; 19:# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n; 20:# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n; 21:# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n; 22:# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n; 23:# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n; 24:# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n; 25:# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n; 26:# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n; 27:# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n; 28:# POSSIBILITY OF SUCH DAMAGE.\n; 29:\n; 30:from ""third_party/nucleus/util/math.h"":\n; 31: namespace `nucleus`:\n; 32: def `Log10PTrueToPhred` as log10_ptrue_to_phred(\n; 33: log10_ptrue: float, value_if_not_finite: float) -> float\n; 34: def `PhredToPError` as phred_to_perror(phred: int) -> float\n; 35: def `PhredToLog10PError` as phred_to_log10_perror(phred: int) -> float\n; 36: def `PErrorToLog10PError` as perror_to_log10_perror(perror: float) -> float\n; 37: def `PErrorToPhred` as perror_to_phred(perror: float) -> float\n; 38: def `Log10PErrorToPhred` as log10_perror_to_phred(log10_perror: float) -> float\n; 39: def `PErrorToRoundedPhred` as perror_to_rounded_phred(perror: float) -> int\n; 40: def `Log10PErrorToRoundedPhred` as log",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:14210,CONTRACT,CONTRACT,14210,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['CONTRACT'],['CONTRACT']
Integrability,"ing:; Use standard file APIs to check for files with this prefix.; I1213 13:07:13.621948 140638419556096 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I1213 13:07:15.663943 140638419556096 session_manager.py:491] Running local_init_op.; I1213 13:07:15.711944 140638419556096 session_manager.py:493] Done running local_init_op.; I1213 13:07:16.176234 140638419556096 modeling.py:410] Reloading EMA...; I1213 13:07:16.177736 140638419556096 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I1213 13:07:20.278557 140638419556096 call_variants.py:399] Processed 1 examples in 1 batches [1174.939 sec per 100]; I1213 13:07:20.328917 140638419556096 call_variants.py:401] Done evaluating variants. WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.; For more information, please see:; * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md; * https://github.com/tensorflow/addons; If you depend on functionality not listed there, please file an issue. real	0m15.024s; user	0m13.890s; sys	0m3.410s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/input/genome.fa"" --infile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --outfile ""/output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/output.g.vcf.gz"". 2019-12-13 13:07:22.565874: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; 2019-12-13 13:07:22.566377: I deepvariant/postprocess_variants.cc:97] Done reading: /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz. #entries in single_site_calls = 28; 2019-12-13 13:07:22.566443: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 28; 2019-12-13 13:07:22.566459: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-12-13 13:07:22.566492: I deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-565441661:7867,depend,depend,7867,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661,1,['depend'],['depend']
Integrability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:8030,depend,dependency,8030,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2769,depend,dependency,2769,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3795,depend,dependency,3795,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"installed pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2424,depend,dependency,2424,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"is issye has been stalling my thesis for years. Nature high SNP density is a real problem. I have some preliminary promising results with long reads but the bulk of my dara are illumina. Anyway, thanks for your reply and I will keep you posted. Cheers. Alex. Sent from ProtonMail mobile. -------- Original Message --------; On 16 Jun 2023, 06:33, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > It's reasonable to try. There are non-human species for which we know DeepVariant works well,for example [rice](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant),; >; > You are correct that with a good long-read dataset it should be possible to train a short read DeepVariant method. If you are able to make those data and reference available, we could try it.; >; > There are two things to keep an eye on when assessing whether DeepVariant will work well for your samples. First, keep an eye that runtime doesn't get unreasonable. DeepVariant runs per-site, which scales linearly with number of variant positions.; >; > The second, is to look if there are many cases where DeepVariant is calling many variants as 0/0 which have high support for the alternate allele. It's possible that a high variant density will resemble the signature of copy number variation in humans and since humans don't have much variation relative to the reference, it may become hesitant to call such variants. Really the only way to know is to try and then do some QC on the results.; >; > It's my hope will have better support and some more advanced methods for such genomes in the intermediate future.; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1594084840), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54R5OCVCQKEQAK6USQDXLPOX5ANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599:1987,Message,Message,1987,,https://github.com/google/deepvariant/issues/661#issuecomment-1594153599,1,['Message'],['Message']
Integrability,"it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1182,depend,dependency,1182,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,l 5.32.1 2_h7f98852_perl5 conda-forge; pip 21.3.1 pyhd8ed1ab_0 conda-forge; protobuf 3.18.0 py36hc4f0c31_0 conda-forge; psutil 5.8.0 py36h8f6f2f9_1 conda-forge; pyasn1 0.4.8 py_0 conda-forge; pyasn1-modules 0.2.7 py_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge; pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge; pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge; pysocks 1.7.1 py36h5fab9bb_3 conda-forge; python 3.6.15 hb7a2778_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python_abi 3.6 2_cp36m conda-forge; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; requests 2.28.1 pyhd8ed1ab_0 conda-forge; requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge; rsa 4.9 pyhd8ed1ab_0 conda-forge; scipy 1.5.3 py36h9e8f40b_0 conda-forge; setuptools 58.0.4 py36h5fab9bb_2 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge; sqlite 3.42.0 h2c6b66d_0 conda-forge; tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge; tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge; tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge; tensorflow 2.0.0 gpu_py36h6b29c10_0 ; tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 ; tensorflow-estimator 2.0.0 pyh2649769_0 ; tensorflow-gpu 2.0.0 h0d30ee6_0 ; termcolor 1.1.0 pyhd8ed1ab_3 conda-forge; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pyhd8ed1ab_0 conda-forge; typing-extensions 4.1.1 hd8ed1ab_0 conda-forge; typing_extensions 4.1.1 pyha770c72_0 conda-forge; unzip 6.0 h7f98852_3 conda-forge; urllib3 1.26.15 pyhd8ed1ab_0 conda-forge; werkzeug 0.16.1 py_0 conda-forge; wheel 0.37.1 pyhd8ed1ab_0 conda-forge; wrapt 1.13.1 py36h8f6f2f9_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; yarl 1.6.3 py36h8f6f2f9_2 conda-forge; zipp 3.6.0 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hd590300_5 conda-forge; zstd 1.4.9 ha95c52a_0 conda-forge; (dv) dpipe@4de3e1b4384c:/app/dpipe$ ; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:6657,wrap,wrapt,6657,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['wrap'],['wrapt']
Integrability,"le Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3427,depend,dependency,3427,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['depend'],['dependency']
Integrability,"lient/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[{{node save_1/RestoreV2}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1299, in restore; {self.saver_def.filename_tensor_name: save_path}); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 958, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1181, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.NotFoundError: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:7782,message,message,7782,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['message'],['message']
Integrability,"lieve the ""winner"" designations from the precisionFDA challenge can be a bit misleading (even though, to be fair, they do color similar high percentiles, even though they also award a designation to one group per category). If I was the winner of a precisionFDA challenge, I would probably want to mention that somewhere. However, I don't typically see sections like ""Why DeepVariant"" at the top of most program READMEs. So, along with some observations about [run-time and cost](https://github.com/google/deepvariant/issues/171#issuecomment-483903505), I think it may respectfully be worth considering trimming back some of that information (**while continuing to provide excellent support on the issues section of GitHub!**). **7)** My understanding is that there is not a DeepVariant App on precisionFDA. I think they use AWS, and I may be able to create something unofficial using code similar to [my AWS test](https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) (relating to issues #166 and #167). However, perhaps at some point, you could consider offering something that can be more officially (and better) supported by DeepVariant developers? This would be free to the users (since the FDA is covering the costs of using the DNAnexus-based interface), but there are some unique differences (like I had to change the chromosome formatting for my .vcf files, and there was an issue with my [Veritas WGS header](https://www.biostars.org/p/361415/#366669) that I had to fix). I am currently uploading my .fastq files (the .bam alignments are up there and public, but I think the chr format may cause issue with variant calling comparisons). However, all relevant information for these two samples will be publicly available in precisionFDA (from my [charles.warden](https://precision.fda.gov/users/charles.warden) account). You don’t have to re-open the ticket, but I would certainly welcome any feedback / thoughts that you might have. Sincerely,; Charles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-485283894:7982,interface,interface,7982,,https://github.com/google/deepvariant/issues/165#issuecomment-485283894,2,['interface'],['interface']
Integrability,"low/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:10842,interface,interfaces,10842,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['interface'],['interfaces']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================. FAILED: //deepvariant:make_examples_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; (06:29:20) FAIL: //deepvariant:make_examples_test (shard 1 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log); (06:29:20) INFO: From Testing //deepvariant:make_examples_test (shard 1 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 1 of 2):; Traceback (most recent call last):; File ""/root/.cache/baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:99962,message,message,99962,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfile",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14047,message,message,14047,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) [2,482 / 2,523] 16 / 38 tests, 2 failed; Testing //deepvariant:call_variants_test; 0s local ... (41 actions, 1 running); (06:29:09) FAIL: //deepvariant:call_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log); (06:29:09) INFO: From Testing //deepvariant:call_variants_test:; ==================== Test output for //deepvariant:call_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_test.runfiles/com_google_deepvariant/deepvariant/call_variants_test.py"", line 48, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:16505,message,message,16505,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:18742,message,message,18742,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:20897,message,message,20897,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:customized_classes_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:32479,message,message,32479,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25264,message,message,25264,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:28080,message,message,28080,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:tf_utils_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log); (06:29:10) INFO: From Testing //deepvariant:tf_utils_test:; ==================== Test output for //deepvariant:tf_utils_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/tf_utils_test.runfiles/com_google_deepvariant/deepvariant/tf_utils_test.py"", line 40, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:23189,message,message,23189,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30235,message,message,30235,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35359,message,message,35359,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:37514,message,message,37514,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/test",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:41955,message,message,41955,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44364,message,message,44364,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) [2,492 / 2,523] 21 / 38 tests, 7 failed; Testing //deepvariant:model_eval_test [0s (10 actions)] ... (31 actions, 2 running); (06:29:12) FAIL: //deepvariant:model_eval_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:39669,message,message,39669,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:50947,message,message,50947,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:48798,message,message,48798,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) [2,495 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (8 actions)] ... (28 actions, 2 running); (06:29:13) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/sit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:46513,message,message,46513,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:57534,message,message,57534,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55385,message,message,55385,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) [2,498 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_eval_test [0s (5 actions)] ... (25 actions, 2 running); (06:29:14) FAIL: //deepvariant:model_eval_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/sit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:53100,message,message,53100,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-impor",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:59683,message,message,59683,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:64124,message,message,64124,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:61832,message,message,61832,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66279,message,message,66279,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:68434,message,message,68434,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) [2,506 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test (shard 10 of 10); 0s local ... (17 actions, 2 running). FAILED: //deepvariant:model_train_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:70589,message,message,70589,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant/realigner/allele_count_linear:model_evaluation_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log); (06:29:17) INFO: From Testing //deepvariant/realigner/allele_count_linear:model_evaluation_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:model_evaluation_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/model_evaluation_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/model_evaluation_test.py"", line 41, in <module>; from deepvariant import testdata; File ""/ro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:77133,message,message,77133,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:74696,message,message,74696,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/python:allelecounter_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/python:allelecounter_wrap_test:; ==================== Test output for //deepvariant/python:allelecounter_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/allelecounter_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/allelecounter_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:82456,message,message,82456,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>; import ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:80117,message,message,80117,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/python:debruijn_graph_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/python:debruijn_graph_wrap_test:; ==================== Test output for //deepvariant/realigner/python:debruijn_graph_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/python/debruijn_graph_wrap_test.runfiles/com_google_deepvariant/deepvariant/realigner/python/debruijn_graph_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:84950,message,message,84950,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner:window_selector_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:window_selector_test:; ==================== Test output for //deepvariant/realigner:window_selector_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/window_selector_test.runfiles/com_google_deepvariant/deepvariant/realigner/window_selector_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:89801,message,message,89801,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-pac",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:87521,message,message,87521,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant/labeler:haplotype_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log); (06:29:19) INFO: From Testing //deepvariant/labeler:haplotype_labeler_test:; ==================== Test output for //deepvariant/labeler:haplotype_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deep",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:94698,message,message,94698,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_exa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97192,message,message,97192,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:92295,message,message,92295,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:103136,message,message,103136,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant:modeling_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log); (06:29:20) INFO: From Testing //deepvariant:modeling_test:; ==================== Test output for //deepvariant:modeling_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/modeling_test.runfiles/com_google_deepvariant/deepvariant/modeling_test.py"", line 41, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:110076,message,message,110076,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) [2,519 / 2,523] 34 / 38 tests, 20 failed; Testing //deepvariant:model_eval_test (shard 9 of 10); 0s local ... (4 actions, 1 running). FAILED: //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:105992,message,message,105992,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:112151,message,message,112151,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:114659,message,message,114659,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"low; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) INFO: Elapsed time: 14.901s, Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:116806,message,message,116806,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['message'],['message']
Integrability,"ltiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8281,depend,dependency,8281,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['depend'],['dependency']
Integrability,"may 2023, ; running on a dell 730 with 88 cores using google/deepvariant:1.5.0. I do not have a GPU on this server and my cpu are avx2. I get the following messages:. ```; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-04 12:48:39.049123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; ```. how can I get the docker to match my architecture. The quickstart code runs with many such warnings and generates data; is this data OK?. ```; sudo docker run \; -u $(id -u) \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:156,message,messages,156,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['message'],['messages']
Integrability,"me chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. Let me reply to this part first:; ""First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct?"" --> Yes that's correct. ""If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes?""; If you want Chromosome 1 for your training set, and Chromosome 2 for your validation set, you'll run make_examples twice. One run to generate the training set with chr1, the other run to generate validation set as chr2.; Note that in both runs, you'll run make_examples with the `--mode training` flag. This can be be a bit confusing, but `--mode training` in make_examples just means that we will create examples with truth labels.; And you will need truth labels for your training set and validation set, . > ; > Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there.; > ; > Best, Haley; > . I'll separately look at the dependency issue. I'll plan to repeat what is documented in https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md on a clean machine and see if the dependencies still work. Sometimes when we documented it, things worked, but later on some underlying dependencies might have shifted. This is actually why we packaged our variant calling in Docker to make sure the versions are more consistent. But we haven't done so for our shuffling code in the training tutorial. Anyway, let me plan to walk through https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md myself and see if it still works. I'll document my steps here later.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137:3064,depend,dependency,3064,,https://github.com/google/deepvariant/issues/793#issuecomment-2008095137,6,['depend'],"['dependencies', 'dependency']"
Integrability,"me/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-sour",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1540,protocol,protocolbuffers,1540,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"ment python; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python3 /opt/conda/envs/dv/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/make_examples.zip --mode calling --ref /data/dpipe/rundata/refdata/hg/hg19_no_chr6_hap.fasta --reads /data/dpipe/rundata/runs/run1/NA12878/NA12878.bam --regions /data/dpipe/rundata/runs/run1/reference/CP.bed --sample_name NA12878 --examples /data/dpipe/rundata/runs/run1/dvout//NA12878.tfrecord@1.gz --task 0; /opt/conda/envs/dv/bin/python3; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_446zcm18/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 32, in <module>; import dataclasses; ModuleNotFoundError: No module named 'dataclasses'; ```; Its strange that these modules are not found. I see many of the deepvariant package dependencies installed in the environment but not `dataclasses` or `etils` etc.; ```; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python --version; Python 3.6.15; (dv) dpipe@3a2ea3796f25:/app/dpipe$ which python; /opt/conda/envs/dv/bin/python; (dv) dpipe@3a2ea3796f25:/app/dpipe$ python; Python 3.6.15 | packaged by conda-forge | (default, Dec 3 2021, 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_159361046425",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:1039,depend,dependencies,1039,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,1,['depend'],['dependencies']
Integrability,"n: pip 22.2.2; Uninstalling pip-22.2.2:; Successfully uninstalled pip-22.2.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-22.2.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Python 3.8.10; pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [jue 18 ago 2022 14:11:12 CEST] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:3780,depend,dependency,3780,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"ncy conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4940,depend,dependency,4940,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"ning pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:17 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 04:03:18 PM UTC] Stage 'build-prereq.sh complete' starting; ```. The running of `./build_and_test.sh`:; ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/aarch64-linux-gnu; ++ DV_BAZEL_VERSION=5.3.0; ++ export PATH=/root/bin:/root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; ++ PATH=/r",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:10734,depend,dependency,10734,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4172,depend,dependency,4172,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5198,depend,dependency,5198,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"nstall; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:15498,Depend,Dependency,15498,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['Depend'],['Dependency']
Integrability,"nstead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3274,depend,dependency,3274,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"nstead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4300,depend,dependency,4300,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 10:22:17 PM EDT] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:3521,depend,dependency,3521,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:7756,depend,dependency,7756,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:2150,depend,dependency,2150,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"nt; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 03:51:27 PM UTC] Stage 'Install python3 packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:2495,depend,dependency,2495,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"o see what the storage cost is now (although it should be higher now than before, because I have started to do other stuff with my Google Cloud account now). **2)** I wonder if there is an issue with how I have the bucket mounted and/or I am running Docker (although that is probably beyond the scope of this post, and I'll close the issue if I start focusing on stuff not related to DeepVariant). Nevertheless, I have my bucket (cdw-genome) mounted in my home directory (/home/cwarden), because it didn't work if I tried to mount it in /mnt. So, that is why I didn't previously have the ""sudo"" commands, but I have added them and tried again. I also tested adding `gcsfuse --file-mode 777 --dir-mode 777` (but that was for a different reason, because I couldn't run executable files from my bucket, and it looks like that solved that particular problem). However, I did quickly test, and I could run ""touch test.txt"" without adding that extra parameter (or using sudo). However, I am unfortunately still getting an error message:. ```; sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘logs’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1828maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:3327,message,message,3327,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['message'],['message']
Integrability,"of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3905,depend,dependency,3905,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4931,depend,dependency,4931,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"on packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.14' => skip as installed in TF. pip install 'requests>=2.18'; # pip install 'scipy==1.0' => skip as installed in TF; pip install 'oauth2client>=4.0.0'; pip install 'crcmod>=1.7'; pip install six; pip install sklearn; pip install pandas; pip install psutil; pip install --upgrade google-api-python-client. ####################################################################; # depend on opencv-python wheel - build from source; ####################################################################; pip install 'tensor2tensor>=1.9.0'. ####################################################################; # depend on - TensorFlow - 1.12 build from source; ####################################################################; pip install tensorflow-1.12.0-cp27-cp27mu-linux_ppc64le.whl. ####################################################################; # Misc dependencies; ####################################################################. yum install openssl-devel curl-devel zlib-devel bzip2-devel xz-devel; yum install boost-devel; ```. ## DeepVariant. Git Repository [https://github.com/google/deepvariant](https://github.com/google/deepvariant). ```bash; # check out source code; git clone https://github.com/google/deepvariant.git; cd deepvariant; # fetch all tags; git fetch --all --tags --prune; # check out tag; git checkout tags/v0.7.2. # Edit ctx.action with use_default_shell_env=True; vim ./third_party/clif.bzl. # Build and test; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:/qilibj/inst/lib/python2.7/site-packages:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; export BAZEL_PYTHON=/home/qilibj/inst/bin/python; export DV_COPT_FLAGS=""--copt=-maltivec --copt=-Wno-sign-compare --copt=-W",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:17315,depend,dependencies,17315,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['depend'],['dependencies']
Integrability,"ophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:1467,depend,depends,1467,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044,1,['depend'],['depends']
Integrability,"ot currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:50 AM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Mon 05 Jun 2023 01:42:51 AM UTC] Stage 'run-prereq.sh complete' starting; ```. For the other set of commands:; ```; > lsb_release -sc; focal; > wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - ; --2023-06-05 01:38:40-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... 146.75.46.49; Connecting to apt.llvm.org (apt.llvm.org)|146.75.46.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: ‘STDOUT’. - 100%[===================>] 3.07K --.-KB/s in 0s . ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:9069,depend,dependency,9069,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"our name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; > … <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread? — Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages above, or message me to know what we are; > talking about? If this discussion is in public, it's indeed to attract; > interest of others. But just read 2 minutes without suggesting the first; > spontaneous idea you have. Which is not idiot in itself but that's; > something we though of in ... 2014 in my memory serves me well ^^; >; > Allez, useless to have petty fight and it's not a good look. I might have; > overreacted to a genuine sympathetic comment.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658794438>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2WSPRXKHQH7UQOXNVTXS7RHRANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:1799,Message,Message,1799,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137,1,['Message'],['Message']
Integrability,"p2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; --report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; wrote:. > And, just in case the documentation isn't clear:; >; > This part:; >; > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > ...; >; > The variable BIN_VERSION was specified in earlier in the steps:; >; > BIN_VERSION=""1.6.1""; >; > So, in Unix command it's equivalent to:; >; > google/deepvariant:""1.6.1"" \; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749:3400,Message,Message,3400,,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749,1,['Message'],['Message']
Integrability,"pdates InRelease; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Hit:6 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease; Reading package lists... Done ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# ; root@m1ubuntu:/media/HostShared/deepvariant-r1.5# sudo apt-get update; Hit:2 https://download.docker.com/linux/ubuntu focal InRelease ; Hit:3 https://download.sublimetext.com apt/stable/ InRelease ; Hit:4 http://ports.ubuntu.com/ubuntu-ports focal InRelease ; Hit:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease ; Hit:5 http://ports.ubuntu.com/ubuntu-ports focal-updates InRelease; Hit:6 http://ports.ubuntu.com/ubuntu-ports focal-backports InRelease; Hit:7 http://ports.ubuntu.com/ubuntu-ports focal-security InRelease; Reading package lists... Done. > sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; Reading package lists... Done; Building dependency tree ; Reading state information... Done; clang-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11 is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-dev is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:11457,depend,dependency,11457,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['depend'],['dependency']
Integrability,"pileup_image_height? Yes, our depths can be as much as a few thousands. I haven’t looked at the data set yet, but that’s common. Brad Thomas. From: Ryan Poplin [mailto:notifications@github.com]; Sent: Tuesday, February 13, 2018 10:28 AM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [External]Re: [google/deepvariant] Build and test works, binaries do not (#47). Sounds fun! I haven't tried something similar yet. One thing to maybe keep in mind, for 1% allele fraction that means you probably have depth >100x coverage, so you might find that you want to change the default height of the pileup tensors which is set at 100. Here is the flag for that in make_examples: https://github.com/google/deepvariant/blob/r0.5/deepvariant/make_examples.py#L177. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/47#issuecomment-365321032>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqaTkjYsOD0tc8gRnJIGZ6o5VeAfPks5tUbgmgaJpZM4R5yAT>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47#issuecomment-365324431:1166,message,message,1166,,https://github.com/google/deepvariant/issues/47#issuecomment-365324431,2,['message'],['message']
Integrability,"pu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check fo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3987,depend,dependency,3987,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"r/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # AT 11.0 environment; source /etc/profile; module load at11.0; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH. # python2 and pip environment; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages; export PYTHON_BIN_PATH=/home/qilibj/inst/bin/python; export PYTHON_LIB_PATH=/home/qilibj/inst/lib/python2.7/site-packages; ```. ## Cmake 3.13.3. Build and install doc: [https://cmake.org/install/](https://cmake.org/install/). >Note: only can be built with system gcc, AT11.0 will cause build failure. ```bash; # download source code; wget https://github.com/Kitware/CMake/releases/download/v3.13.3/cmake-3.13.3.tar.gz; tar -zxvf cmake-3.13.3.tar.gz; cd cmake-3.13.3. # build scirpt; ./bootstrap; make -j20; make -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:1283,protocol,protocolbuffers,1283,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"ransversion)`, or `if Tv = 0` then it will be reported as a string formatted as `'variant_counts(is_transition) / 0'`. $`2)`$ The genotype is calculated as follows. Reads are collected, and sometimes realigned based on the model selected. Call sites are determined by an allele counter that goes through every position of aligned reads. For every viable call site it will generate a set of matrices based on your sets of aligned reads in that region - for some models it will perform local realignment. These matrices will limit themselves to a maximum of 95 reads (as it will downsample the reads if there are too many), with the first 5 rows representing the reference. This will then go through the model, and it generate three genotype probabilities: homozygous ref, het, and homozygous alt. Based on the maximum genotype probability, that will be used to generate the genotype (as the most likely). $`3)`$ The PL is generated from the 3 probabilities to generate the -10*log10() of the genotypes and zeroing to the most likely one (i.e. normalized with the highest genotype probability having PL=0). Now given the three steps above let's tie them together. Simplifying to the common factors, those would be: read realignment, read quality, and predicted genotype likelihoods. Read alignment and read quality determine the call site and type (i.e. SNP). Then these (via a matrix representation processed through a model) determine predicted genotype likelihoods, which in turn determine the GQ, PL and GT. TiTv counts depends strongly on how the call site was determined by the alignment of reads, and the [thresholds being set](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#why-does-deepvariant-not-call-a-specific-variant-in-my-data). The GQ sometimes might be very low, which affects the GT (i.e. `./.`). This happens with every run, and if you are seeing discrepancies there are other factors that can affect it such as local alignment and read generation. Does that help?. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196:1960,depend,depends,1960,,https://github.com/google/deepvariant/issues/702#issuecomment-1698583196,1,['depend'],['depends']
Integrability,"rmalizer==3.3.2; cloudpickle==2.2.1; crcmod==1.7; Deprecated==1.2.14; dill==0.3.1.1; dnspython==2.6.1; docopt==0.6.2; fastavro==1.9.4; fasteners==0.19; flatbuffers==24.3.7; gast==0.4.0; google-api-core==2.17.1; google-apitools==0.5.31; google-auth==2.28.2; google-auth-httplib2==0.1.1; google-auth-oauthlib==1.0.0; google-cloud-aiplatform==1.44.0; google-cloud-bigquery==3.19.0; google-cloud-bigquery-storage==2.24.0; google-cloud-bigtable==2.23.0; google-cloud-core==2.4.1; google-cloud-datastore==2.19.0; google-cloud-dlp==3.16.0; google-cloud-language==2.13.3; google-cloud-pubsub==2.20.2; google-cloud-pubsublite==1.9.0; google-cloud-recommendations-ai==0.10.10; google-cloud-resource-manager==1.12.3; google-cloud-spanner==3.44.0; google-cloud-storage==2.16.0; google-cloud-videointelligence==2.13.3; google-cloud-vision==3.7.2; google-crc32c==1.5.0; google-pasta==0.2.0; google-resumable-media==2.7.0; googleapis-common-protos==1.63.0; grpc-google-iam-v1==0.13.0; grpc-interceptor==0.15.4; grpcio==1.62.1; grpcio-status==1.62.1; h5py==3.10.0; hdfs==2.7.3; httplib2==0.22.0; idna==3.6; importlib_metadata==7.0.2; keras==2.13.1; libclang==18.1.1; Markdown==3.6; MarkupSafe==2.1.5; numpy==1.24.3; oauth2client==4.1.3; oauthlib==3.2.2; objsize==0.6.1; opt-einsum==3.3.0; orjson==3.9.15; overrides==7.7.0; packaging==24.0; pkg_resources==0.0.0; proto-plus==1.23.0; protobuf==4.23.4; pyarrow==11.0.0; pyasn1==0.5.1; pyasn1-modules==0.3.0; pydot==1.4.2; pymongo==4.6.2; pyparsing==3.1.2; python-dateutil==2.9.0.post0; pytz==2024.1; regex==2023.12.25; requests==2.31.0; requests-oauthlib==1.4.0; rsa==4.9; shapely==2.0.3; six==1.16.0; sqlparse==0.4.4; tensorboard==2.13.0; tensorboard-data-server==0.7.2; tensorflow==2.13.1; tensorflow-estimator==2.13.0; tensorflow-io-gcs-filesystem==0.34.0; termcolor==2.4.0; typing_extensions==4.5.0; urllib3==2.2.1; Werkzeug==3.0.1; wrapt==1.16.0; zipp==3.18.1; zstandard==0.22.0; ```. ---. @helizabeth1103 , can you see which version might be different from yours?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269:7598,wrap,wrapt,7598,,https://github.com/google/deepvariant/issues/793#issuecomment-2008639269,1,['wrap'],['wrapt']
Integrability,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:3765,depend,dependency,3765,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"rrently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:4791,depend,dependency,4791,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"s protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:5225,depend,dependency,5225,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"sorry my solution only fixed the first set of errors, some others persist and I am not sure how to resolve them. maybe these message help the cognoscenti figure out what is wrong, system is ubuntu 16.04, python2.7. [build_and_test_errors_2.txt](https://github.com/google/deepvariant/files/1670605/build_and_test_errors_2.txt)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/43#issuecomment-361020869:125,message,message,125,,https://github.com/google/deepvariant/issues/43#issuecomment-361020869,1,['message'],['message']
Integrability,"strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:4310,depend,dependency,4310,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['depend'],['dependency']
Integrability,"t all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.5 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:43 CEST] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow 2.7.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install CUDA' starting; ========== [jue 18 ago 2022 14:11:45 CEST] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; googleapis-common-protos 1.56.4 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; google-api-core 2.8.2 requires protobuf<5.0.0dev,>=3.15.0, but you have protobuf 3.13.0 which is incompatible.; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'run-prereq.sh complete' starting; ========== [jue 18 ago 2022 14:11:49 CE",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:4800,depend,dependency,4800,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['depend'],['dependency']
Integrability,"t and log file. code is running but neither output generating or error throwing just running.; please see below code and log file. ###### code #############; #!/usr/bin/env nextflow. nextflow.enable.dsl=2; params.outdir = '/home/deepak/integration/resu1'; params.data_dir = '/home/deepak/integration/resu1/4.markDupliM'; params.refhg38 = '/home/deepak/integration/hg381_22XYM'; params.bed = '/home/deepak/integration'. workflow {; // Define channels for input data; Channel; .fromPath(""${params.data_dir}/*_sorted_md.bam""); .map { file -> ; def sample_id = file.baseName.replace('_sorted_md', ''); return [sample_id, file]; }; .set { read_pairs }; /// Step 1. DeepVariant; DeepVariant(read_pairs, params.refhg38, params.bed); }. process DeepVariant {; tag ""deepavar on ${sample_id}""; publishDir ""${params.outdir}/5.finaleepvar"", mode: 'copy'; cpus 4; //BIN_VERSION 1.6.1. input:; tuple val(sample_id), path(read_files); val(params.refhg38); val(params.bed); ; output:; //tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), path(""${sample_id}_rawd.gvcf.gz""), emit: raw_vcfs; tuple val(sample_id), path(""${sample_id}_rawd.vcf.gz""), emit: raw_vcfs. script:; """"""; docker run \; -v ""${params.data_dir}"":/opt/bam -v ""${params.refhg38}"":/opt/refhg38 -v ""${params.bed}"":/opt/bed \; google/deepvariant:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /opt/refhg38/Homo_sapiens_assembly38cleaned.fasta \; --reads /opt/bam/${read_files} \; --regions /opt/bed/hg38_exomeY.bed \; --output_vcf /opt/bam/${sample_id}_rawd.vcf.gz \; --num_shards ${task.cpus}; """"""; }. ######## code ################. terminal:; (base) deepak@ubuntu22:~/integration$ nextflow run final_deepvarian.nf . N E X T F L O W ~ version 24.04.4. Launching `final_deepvarian.nf` [hungry_stonebraker] DSL2 - revision: 4dab17f4f2. executor > local (1); [dd/64034b] DeepVariant (deepavar on SRR26512958) [ 0%] 0 of 2. log file attached; [nextflow.log](https://github.com/user-attachments/files/17008943/nextflow.log)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013:1733,integrat,integration,1733,,https://github.com/google/deepvariant/issues/883#issuecomment-2352056013,1,['integrat'],['integration']
Integrability,"t.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4616,depend,dependency,4616,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['depend'],['dependency']
Integrability,"take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 2.2.0; Uninstalling pyparsing-2.2.0:; Successfully uninstalled pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; ========== [Mon 05 Jun 2023 10:22:34 PM EDT] Stage 'build-prereq.sh complete' starting. > docker run --platform linux/amd64 google/deepvariant:1.5.0; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:10117,depend,dependency,10117,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,['depend'],['dependency']
Integrability,"te ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. Version:. ```; $ uname -a; Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. Install conda:. ```bash; curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. It completed without any error messages. I see:. ```; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/; bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0; call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh; deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip; ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If you can try that and see if it works for you. And do what you'd usually do to run and see if it works, that will be great! P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405:1379,message,messages,1379,,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405,1,['message'],['messages']
Integrability,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:25,depend,depends,25,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051,1,['depend'],['depends']
Integrability,"tly take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' sta",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3472,depend,dependency,3472,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"tobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7007,protocol,protocolbuffers,7007,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLVM tree.; mkdir -p $BUILD_DIR; cd $BUILD_DIR; # Note to remove -DLLVM_TARGETS_TO_BUILD=X86; # ""rm CMakeCache.txt"" to remove cmake cache; cmake -DCMAKE_INSTALL_PREFIX=""$CLIF_VIRTUALENV/clang"" \; -DCMAKE_PREFIX_PATH=""$PROTOC_PREFIX_PATH"" \; -DLLVM_INSTALL_TOOLCHAIN_ONLY=true \; -DCMAKE_BUILD_TYPE=Release \; -DLLVM_BUILD_DOCS=false \; -DLLVM_TARGETS_TO_BUILD=PowerPC \; -DPYTHON_INCLUDE_DIR=""$HOMEPATH/inst/include/python2.7"" \; -DPYTHON_LIBRARY=""$HOMEPATH/inst/lib/libpython2.7.so"" \; -DPYTHON_EXECUTABLE=""$HOMEPATH/inst/bin/python"" \; ""$LLVM_DIR/llvm""; make -j20 clif-matcher; # export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; make -j20 clif_python_utils_proto_util; make -j20 install. ## Get back to the CLIF Python directory and have pip run setup.py.; cd ""$CLIFSRC_DIR""; # Grab the python compiled .proto; cp ""$BUILD_DIR/tools/clif/protos/ast_pb2.py"" clif/protos/; # Grab CLIF generated wrapper implementation for proto_util.; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.cc"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.h"" clif/python/utils/; cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.c",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:14267,wrap,wrapper,14267,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['wrap'],['wrapper']
Integrability,"u have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; tensorboard 2.11.2 requires protobuf<4,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:24 PM UTC] Stage 'Install TensorFlow pip package' starting; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install CUDA' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install TensorRT' starting; ========== [Sun 04 Jun 2023 11:11:26 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is inc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:3332,depend,dependency,3332,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['depend'],['dependency']
Integrability,"ueError: Can not squeeze dim[1], expected a dimension of 1, got 27 for 'InceptionV3/Logits/SpatialSqueeze' (op: 'Squeeze') with input shapes: [64,27,1,3]. I hate to keep bothering people about this. Is there documentation on all of this that I can refer to?. Thanks,; Brad Thomas. From: Pi-Chuan Chang [mailto:notifications@github.com]; Sent: Tuesday, April 10, 2018 1:04 PM; To: google/deepvariant <deepvariant@noreply.github.com>; Cc: Brad Thomas <brad.thomas@neogenomics.com>; Author <author@noreply.github.com>; Subject: [EXTERNAL]Re: [google/deepvariant] Deep sequencing (#62). CAUTION: This email originated from outside the organization. DO NOT click links or open attachments unless you recognize the sender and know the content is safe. I think you'll want:; tfrecord_path: ""/home2/myModelAttempt/output/5PRR-RD_S86.examples.tfrecord-?????-of-00064"". —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://github.com/google/deepvariant/issues/62#issuecomment-380193942>, or mute the thread<https://github.com/notifications/unsubscribe-auth/AifcqWYh56G1_S7aFjDrcbIt_6qII5goks5tnPP8gaJpZM4TIm9R>. This message contains confidential information and is intended only for the individual named. If you are not the named addressee you should not disseminate, distribute or copy this e-mail. Please notify the sender immediately by e-mail if you have received this e-mail by mistake and delete this e-mail from your system. E-mail transmission cannot be guaranteed to be secured or error-free as information could be intercepted, corrupted, lost, destroyed, arrive late or incomplete, or contain viruses. The sender therefore does not accept liability for any errors or omissions in the contents of this message, which arise as a result of e-mail transmission. If verification is required please request a hard-copy version. NeoGenomics Laboratories, Suite 5, 12701 Commonwealth Dr, Fort Myers, FL 33913, http://www.neogenomics.com (2017)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/62#issuecomment-380197853:1229,message,message,1229,,https://github.com/google/deepvariant/issues/62#issuecomment-380197853,2,['message'],['message']
Integrability,"uf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git OpenBLAS-0.3.5; cd OpenBLAS-0.3.5; make TARGET=power8; make TARGET=power8 PREFIX=$HOMEPATH/inst install; ```. ## Boost 1.66.0. ```bash; wget -qc https://dl.bintray.com/boostorg/release/1.66.0/source/boost_1_66_0.tar.gz; tar xzf boost_1_66_0.tar.gz; cd boost_1_66_0; ./bootstrap.sh --with-toolset=gcc --prefix=$HOMEPATH/inst; ./b2 dll-path=""$HOMEPATH/inst/lib"" install; ```. ## Tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:7081,protocol,protocolbuffers,7081,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"uild, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobu",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6082,protocol,protocolbuffers,6082,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['protocol'],['protocolbuffers']
Integrability,"uires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:39 PM UTC] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel:",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:4531,depend,dependency,4531,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['depend'],['dependency']
Integrability,"uires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.; tensorflow-cpu-aws 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.23.2 which is incompatible.; Installing standard CPU-only TensorFlow 2.11.0 wheel; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; werkzeug 2.3.4 requires MarkupSafe>=2.1.1, but you have markupsafe 2.0.1 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:30 PM EDT] Stage 'Install other packages' starting; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; google-api-core 2.11.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; googleapis-common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:5557,depend,dependency,5557,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['depend'],['dependency']
Integrability,"unds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:10377,wrap,wrapping,10377,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['wrap'],['wrapping']
Integrability,"vironment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Installing collected packages: pyparsing; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.20.4 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); ========== [jue 18 ago 2022 14:11:58 CEST] Stage 'build-prereq.sh complete' starting`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:13123,depend,dependency,13123,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,['depend'],['dependency']
Integrability,"your name on the email thread on this public github; > repository I won't reply and I'll loom forward your paper on bioarvix; > hopefully, Honestly all the best, And if you don't care about prokaryotes; > then fair enough, Joe; > … <#m_-2096600892735742938_>; > On Mon, 31 Jul 2023, 17:54 Axze-rgb, *@*.*> wrote: The name of the; > organism has been said in this thread, that you are unable to find it, and; > believe we deal with a prokaryote is pathetic, really. Why would we bother; > with your stupid advice when you didn't even take the time to read the; > thread? — Reply to this email directly, view it on GitHub <#682 (comment); > <https://github.com/google/deepvariant/issues/682#issuecomment-1658768123>>,; > or unsubscribe; > https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ; > <https://github.com/notifications/unsubscribe-auth/BAYQV2XLSQBKI475IIY3253XS7PMTANCNFSM6AAAAAA2QKAKXQ>; > . You are receiving this because you commented.Message ID: @.*>; >; > There are so many trolls it's difficult to know when someone is just; > clumsy. I am willing to give you the benefit of the doubt. Buit really you; > could have read the messages above, or message me to know what we are; > talking about? If this discussion is in public, it's indeed to attract; > interest of others. But just read 2 minutes without suggesting the first; > spontaneous idea you have. Which is not idiot in itself but that's; > something we though of in ... 2014 in my memory serves me well ^^; >; > Allez, useless to have petty fight and it's not a good look. I might have; > overreacted to a genuine sympathetic comment.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658794438>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2WSPRXKHQH7UQOXNVTXS7RHRANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137:1992,message,messages,1992,,https://github.com/google/deepvariant/issues/682#issuecomment-1658806137,3,"['Message', 'message']","['Message', 'message', 'messages']"
Integrability,"}/tempdir"" \; --save_main_session \; --region us-east1; ```. ```; time gcloud compute tpus create ${USER}-demo-tpu \; --network=default \; --version=2.3 \; --zone=us-central1-c; ```. # Below is the main difference from the instruction in r0.9: How to look up the TPU_IP:. Given that it seems like we didn't have the right library to properly look up `tpu_name` (`pip install cloud-tpu-client` is needed, it seems). I will try to fix this in our future Dockerfile and test it. But for now, I'll show up to manually resolve the tpu_name. First, install this:; ```; pip3 install cloud-tpu-client; ``` . And then:. ```; TPU_NAME=""${USER}-demo-tpu""; TPU_IP=$(python3 -c ""import tensorflow as tf; print(tf.distribute.cluster_resolver.TPUClusterResolver(tpu=['${TPU_NAME}'], zone='us-central1-c').get_master())""); ```; Check the IP:; ```; $ echo ${TPU_IP}; grpc://10.33.164.2:8470; ```. ```; ( time sudo docker run \; -v /home/${USER}:/home/${USER} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/model_train \; --use_tpu \; --master=""${TPU_IP}"" \; --dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --train_dir=""${TRAINING_DIR}"" \; --model_name=""inception_v3"" \; --number_of_steps=50000 \; --save_interval_secs=300 \; --batch_size=512 \; --learning_rate=0.008 \; --start_from_checkpoint="""" \; ) 2>&1 | tee ""${LOG_DIR}/train.log""; ```. This now seems to be able to see the TPU. But right now I seem to be having some issue of using ${GCS_PRETRAINED_WGS_MODEL} as `--start_from_checkpoint`, so I might need to continue looking into why. And, at this point, I'm done for now. So I manually deleted the TPU:; ```; gcloud compute tpus delete ${TPU_NAME} --zone us-central1-c; ```. ---. @mattwood-codifiedgenomics Thanks for reporting this. I don't think the `--tpu_name` code path is commonly used. I'll see if I can get the right dependencies installed, and see if I can get proper unit tests to cover this. At the very least, I'll try to get a manual run to succeed.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:7577,depend,dependencies,7577,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['depend'],['dependencies']
Modifiability," M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7777,config,config,7777,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability," Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` fil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1117,config,configure,1117,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['config'],['configure']
Modifiability," TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8356,config,config,8356,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability," TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7879,config,config,7879,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability," TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Success",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7985,config,config,7985,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability, ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/un,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3732,Config,Configure,3732,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,1,['Config'],['Configure']
Modifiability," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8533,config,configs,8533,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,4,['config'],"['config', 'configs']"
Modifiability," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8056,config,configs,8056,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,['config'],"['config', 'configs']"
Modifiability," be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manage",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8162,config,configs,8162,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,['config'],"['config', 'configs']"
Modifiability," echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6274,config,config,6274,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['config'],['config']
Modifiability," in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pypars",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7861,config,configure,7861,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['configure']
Modifiability," newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools is already the newest version (1:11.0.0-2~ubuntu20.04.1).; llvm-11-tools set to manually installed.; The following packages were automatically installed and are no longer required:; docker-ce-rootless-extras slirp4netns; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.; > sudo apt autoremove; Reading package lists... Done; Building dependency tree ; Reading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were cons",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:12678,config,config-,12678,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['config'],['config-']
Modifiability," now the ```rtg-tools format``` miraculously work using the following command. ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And this is the result . ```; Formatting FASTA data; Processing ""/reference/GRCh38_no_alt_analysis_set.fasta"". Detected: 'Human GRCh38 with UCSC naming', installing reference.txt. Input Data; Files : GRCh38_no_alt_analysis_set.fasta; Format : FASTA; Type : DNA; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422. Output Data; SDF-ID : 809c9a82-d8d5-477a-865b-772d28741815; Number of sequences: 195; Total residues : 3099922541; Minimum length : 970; Mean length : 15897038; Maximum length : 248956422; ```. However this ```rtg-tools mendelian``` still result in error of ```Error: An IO problem occurred: ""Not in GZIP format""``` when running the following command below; ```; docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ```. Have tried looking the wrong data and it seems the following GLNexus VCF Merge command is giving corrupted ```HG002_trio_merged.vcf.gz```. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bcftools view - \; | docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759:1700,config,config,1700,,https://github.com/google/deepvariant/issues/632#issuecomment-1512450759,1,['config'],['config']
Modifiability," on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:1251,config,configurations,1251,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,2,['config'],['configurations']
Modifiability," source according to the suggestions. The problem happened while I running the ./build-prereq.sh. The more detail information:. ```; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + git init; Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; +",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:1648,config,config,1648,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,4,"['config', 'variab']","['config', 'variable']"
Modifiability," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1798,variab,variable,1798,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466,1,['variab'],['variable']
Modifiability," these channels contains in their first 5 rows the reference representation as necessary. This way you can ensure reference representation together with alternate representation. If you have multiple alternates, then you have more images as [shown at the following PileupImageCreator() comment](https://github.com/google/deepvariant/blob/r1.5/deepvariant/pileup_image.py#L138-L197), to ensure the genotype is calculated with the best representation of possibilities. Now let me switch topics on how the model is constructed at a high level, as it is important for answering the questions. The model is a convolutional neural network (CNN) based on the [Inception V3 architecture](https://iq.opengenus.org/inception-v3-model-architecture/), and was trained using a truth set of VCFs for confident regions using high-quality BAM files. A CNN basically slices with different sub-matrices (i.e. kernel) shapes within each channel to generate new summary layers of channels to use for further slicing or combining (pooling), with the eventual goal to match with a high probability a specific truth value in the VCF. The way this is done, is by varying the values in these networks of kernels (sub-matrices) until with high-probability the truth values as validated. This collection of trained kernels is what the PacBio model contains. So now to answer the two questions in order:. 1) If you think of the PacBio model retaining previously seen (trained) regions of portions of the channels, it will go through the kernel transformation pooled across the channels to determine the probabilities of genotypes. The highest value will be used to Phred-scale the GQ value. So it's a bit more complex as the sub-slices are taken across a 100 x 199 region for 6 channels, rather than just a 100 x 1 (where the majority ref-representation of alleles reside), but that will also have an impact in the matrix calculations of probabilities of most represented genotype. 2) Now if the probability is low, then the mod",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1604221424:2380,layers,layers,2380,,https://github.com/google/deepvariant/issues/666#issuecomment-1604221424,1,['layers'],['layers']
Modifiability," to run it on a single machine, and didn't focus on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy a",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:1171,config,configure,1171,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,2,['config'],['configure']
Modifiability," type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I launched a training job with model_train.zip:. ```; python ""${BIN_DIR}""/model_train.zip \; --dataset_config_pbtxt output/data.pbtxt \; --start_from_checkpoint """" \; --batch_size 16 \; --alsologtostderr; ```. This is quick overview, but definitely let me know if you have any questions!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1839,config,config,1839,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['config'],['config']
Modifiability," with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox...; > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756; > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS; > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:; > unifier_config:; > drop_filtered: false; > min_allele_copy_number: 1; > min_AQ1: 10; > min_AQ2: 10; > min_GQ: 0; > max_alleles_per_site: 32; > monoallelic_sites_for_lost_alleles: true; > preference: common; > genotyper_config:; > revise_genotypes: true; > min_assumed_allele_frequency: 9.99999975e-05; > snv_prior_calibration: 0.600000024; > indel_prior_calibration: 0.449999988; > required_dp: 0; > allow_partial_data: true; > allele_dp_format: AD; > ref_dp_format: MIN_DP; > output_residuals: false; > more_PL: true; > squeeze: false; > trim_uncalled_alleles: true; > top_two_half_calls: false; > output_format: BCF; > liftover_fields:; > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:958,config,config,958,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['config'],['config']
Modifiability," with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child) calls are the more interesting ones. For this you would need to have more samples to ensure the calls are not false positives, with further IGV inspection and assay validation. If this might be a bit too fun, feel free t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:3954,inherit,inheritance,3954,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,2,['inherit'],['inheritance']
Modifiability,"'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7666,config,config,7666,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10155,extend,extend,10155,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['extend'],['extend']
Modifiability,"(modified from earlier response); 1. I believe this may not work if the file names are not what `make_examples` expects. `make_examples` expects the following naming: . * `<NAME>.bam` for the BAM file; * `<NAME>.bam.bai` or `<NAME>.bai` for the index. 2. There is no way to specify a separate path for the index file. However, you could try to name your symlinks as `data.bam` and `data.bam.bai` / `data.bai`, shown below. I did not get a chance to test this out myself, but let me know if this does not work, and I can look into other possible solutions. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam data.bam; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai data.bam.bai; ```. Regarding the link you shared, the configuration options pertain to the `gcp_deepvariant_runner` which is part of a pipeline that can be used to run DeepVariant on Google Cloud. Based on your command above, it does not seem like you are using this pipeline, but correct me if I'm wrong.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461157985:785,config,configuration,785,,https://github.com/google/deepvariant/issues/149#issuecomment-461157985,1,['config'],['configuration']
Modifiability,"**_DeepVariant Installation problem using Anaconda version 4.8.2_** ; https://anaconda.org/bioconda/deepvariant. **Tried to installed with default python version which is Python 3.7.6**. ```conda create --name deepvariant; conda activate deepvariant; conda install -c bioconda deepvariant; ```; Output:. ```; Collecting package metadata (repodata.json): done. Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: - ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError: ; ```. **Second option with the label**. `conda install -c bioconda/label/cf201901 deepvariant`. Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: / ; Found conflicts! Looking for incompatible packages.; This can take several minutes. Press CTRL-C to abort. failed ; UnsatisfiableError:; ```. **Noticed that required version is 2.7 so I removed the environment and tried to install it with this version**. `conda remove --name deepvariant --all`; `conda create -n deepvariant python=2.7 deepvariant` . Output:; ```; Collecting package metadata (repodata.json): done; Solving environment: - ; Found conflicts! Looking for incompatible packages. failed ; UnsatisfiableError: The following specifications were found to be incompatible with each other:; Output in format: Requested package -> Available versions; Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']; ```. **After reading this issue: https://github.com/google/deepvariant/issues/177, I tried to constrain google-cloud-sdk version**. `conda create -n deepvariant python=2.7 deepvariant 'google-cloud-sdk<243.0.0'`. Output:; ```; Colle",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:429,flexible,flexible,429,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,2,['flexible'],['flexible']
Modifiability,"--------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash; # Prerequisites; cmake --version #3.5+; protoc --version # 3.2.0+ build from source code for both C++ and Python; pip install virtualenv; pip install pyparsing; yum install subversion; yum install ocaml; pip install 'pyparsing>=2.2.0'; pkg-config --libs python # workable. # download source code; cd $HOMEPATH; git clone https://github.com/google/clif.git; cd clif. # set environment; export INSTALL_DIR=""$HOMEPATH/inst""; export CLIFSRC_DIR=""$HOMEPATH/clif""; export LLVM_DIR=""$CLIFSRC_DIR/../clif_backend""; export BUILD_DIR=""$LLVM_DIR/build_matcher"". export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; export PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages""; export PROTOC_PREFIX_PATH=""$(dirname ""$(dirname ""$(which protoc)"")"")"". export CLIF_VIRTUALENV=""$INSTALL_DIR""/clif; export CLIF_PIP=""$CLIF_VIRTUALENV/bin/pip"". virtualenv -p ""$PYTHON_BIN_PATH"" ""$CLIF_VIRTUALENV"". $CLIF_PIP install --upgrade pip; $CLIF_PIP install --upgrade setuptools. # Checkout LLVM and Clang source trees; mkdir -p $LLVM_DIR; cd $LLVM_DIR; svn co https://llvm.org/svn/llvm-project/llvm/trunk@307315 llvm; cd llvm/tools; svn co https://llvm.org/svn/llvm-project/cfe/trunk@307315 clang; ln -s -f -n $CLIFSRC_DIR/clif clif. # Builds must be done outside of the LLV",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:12350,config,config,12350,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['config']
Modifiability,"-c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8037,config,config,8037,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is alr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7468,config,configured,7468,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,". Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python; options.calling_regions.extend(parse_regions_flag(flags_obj.regions)); ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python; regions = processing_regions_from_options(options); ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python; calling_regions = build_calling_regions(ref_contigs, options.calling_regions,; options.exclude_calling_regions); ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```; regions = regions_to_process(; contigs=contigs,; partition_size=options.allele_counter_options.partition_size,; calling_regions=calling_regions,; task_id=options.task_id,; num_shards=options.num_shards). region_list = list(regions); ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python; for region in regions:; candidates, examples, gvc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1719933271:1009,variab,variable,1009,,https://github.com/google/deepvariant/issues/708#issuecomment-1719933271,1,['variab'],['variable']
Modifiability,".. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:**27,1,0**:18:0,18,45,990,990,990:II . chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | AATATAT | 46 | . | AF=0.166667;AQ=15 | GT:DP:AD:GQ:PL:RNC | 0/1:30:5,13:13:44,15,53:.. | 0/0:31:16,0:46:46,990,990:.. | ./.:30:**27,0**:18:0,990,990:II. ![DT_1_04190_chr5_24093912](https://user-images.githubusercontent.com/22089494/115330437-09e80680-a162-11eb-96cd-2f2d27d45896.png); ![DT_1_04190_chr5_24093912_zoom](https://user-images.githubusercontent.com/22089494/115330450-11a7ab00-a162-11eb-8f5c-927bd1445056.png). ### **3) Filtered Denovo-like, QUAL=27, proband GQ=28. Inherited; it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr7 | 54624683 | chr7_54624683_A_AATC | A | AATC | 27 | . | AF=0.166667;AQ=27 | GT:DP:AD:GQ:PL:RNC | 0/1:39:22,16:28:27,0,48:.. | 0/0:40:40,0:50:0,120,1199:.. | 0/0:28:**28,0**:50:0,90,899:.. ![DT_1_04190_chr7_54624683](https://user-images.githubusercontent.com/22089494/115331518-0ce3f680-a164-11eb-91e1-2250266a421c.png). ![DT_1_04190_chr7_54624683_zoom](https://user-images.githubusercontent.com/22089494/115331520-0eadba00-a164-11eb-9c8d-2eeb8e63e1dc.png). What do you think could be a reason for DeepTrio calling to miss some information from read allignments?; Thank you. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-822947862:2870,Inherit,Inherited,2870,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862,1,['Inherit'],['Inherited']
Modifiability,.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:1664,config,configured,1664,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,".list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3708,config,configured,3708,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; > -v; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; > --ref; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; > --report_title MITO60_Stats --sample_name MITO60 --output_vcf; > /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; > --model_type ONT_R104; >; >; > On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; > wrote:; >; >> And, just in case the documentation isn't clear:; >>; >> This part:; >>; >> sudo docker run \; >> -v ""${INPUT_DIR}"":""/input"" \; >> -v ""${OUTPUT_DIR}"":""/output"" \; >> google/deepvariant:""${BIN_VERSION}"" \; >> ...; >>; >> The variable BIN_VERSION was specified in earlier in the steps:; >>; >> BIN_VERSION=""1.6.1""; >>; >> So, in Unix command it's equivalent to:; >>; >> google/deepvariant:""1.6.1"" \; >>; >> —; >> Reply to this email directly, view it on GitHub; >> <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; >> or unsubscribe; >> <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; >> .; >> You are receiving this because you were mentioned.Message ID:; >> ***@***.***>; >>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508:4417,variab,variable,4417,,https://github.com/google/deepvariant/issues/829#issuecomment-2162485508,2,['variab'],['variable']
Modifiability,"/compiler/mlir/tfrt/tests/tf_to_tfrt_data,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config defini",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:7424,Inherit,Inherited,7424,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['Inherit'],['Inherited']
Modifiability,"071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/us",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7873,config,config,7873,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"1. Docker installation is not DeepVariant specific. You may follow steps from the official docker website https://docs.docker.com/engine/install/ubuntu/#install-using-the-repository; 2. From the output it looks like you run it on a machine with 2 cores. In Google tutorial n1-standard-64 instance is used which has 64 cores. ; 3. From the output of the last command it is not clear what the error is. Could you post the command you used for creating an instance? It could be that call_variants command ran out of memory.; 4. Although Google tutorial page contains the pricing for pre-emptible instances it is only given for the reference. It is not recommended to run this tutorial on a pre-emptible instances because in the case the instance is preemted the job cannot restart automatically. More complex configuration (like Kubernetes) is required in order to use pre-emptible instances.; 5. Recently a new version of DeepVariant was released, so instead of using 0.9.0 the new version 1.1.0 and docker path google/deepvariant should be used. Although, using the latest version is preferred the old 0.9.0 should work as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749327121:806,config,configuration,806,,https://github.com/google/deepvariant/issues/399#issuecomment-749327121,2,['config'],['configuration']
Modifiability,"1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:08:50.047413 139654431344384 make_examples.py:648] Writing examples to /tmp/tmpj5fx0phm/make_examples.tfrecord-00000-of-00001.gz; I0629 23:08:50.047621 139654431344384 make_examples.py:648] Writing gvcf records to /tmp/tmpj5fx0phm/gvcf.tfrecord-00000-of-00001.gz; I0629 23:08:50.048107 139654431344384 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:08:50.064639 139654431344384 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:10:07.477154 139654431344384 make_examples.py:648] 102 candidates (110 examples) [77.41s elapsed]; I0629 23:10:11.740366 139654431344384 make_examples.py:648] 202 candidates (223 examples) [4.26s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_jqt5759c/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:4877,extend,extend,4877,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['extend'],['extend']
Modifiability,"100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz; W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; I0327 13:32:13.795566 47138345245376 data_providers.py:369] self.input_read_threads=8; W0327 13:32:13.795886 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:374: parallel_inter leave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_cal ls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.da ta.Options.experimental_determinstic`.; I0327 13:32:13.922482 47138345245376 data_providers.py:376] self.input_map_threads=48; W0327 13:32:13.922731 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:7139,layers,layers,7139,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['layers'],['layers']
Modifiability,"20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with; six.raise_from(AssertionError(_error_message(cause)), cause); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from; raise value; AssertionError",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1808,parameteriz,parameterized,1808,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['parameteriz'],['parameterized']
Modifiability,"23 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pypars",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7384,config,configure,7384,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['configure']
Modifiability,"2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3529,config,configured,3529,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; ========== [Sun 04 Jun 2023 11:11:27 PM UTC] Stage 'run-prereq.sh complete' starting; ```. The last time I tried running `./build-prereq.sh`, I got error on the building of Clif. llvm-11-linker-tools not available.; and now the error is:; ```; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following conf",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:5508,config,config,5508,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['config'],['config']
Modifiability,"3121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; INFO:tensorflow:Calling model_fn.; I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn.; WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; INFO:tensorflow:Done calling model_fn.; I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt; I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt; 2023-12-19 05:41:48.711875: W tensorflow/core/framework/op_kernel.cc:1767] OP_REQUIRES failed at save_restore_v2_ops.cc:184 : Not found: /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py"", line 1365, in _do",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:5546,layers,layers,5546,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,['layers'],['layers']
Modifiability,"34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:10547,config,configure,10547,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configure']
Modifiability,"36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ign",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5848,config,configured,5848,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"45245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be remo ved in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.b atch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of usin g the fused implementation.; I0327 13:32:14.655726 47138345245376 estimator.py:1147] Calling model_fn.; W0327 13:32:14.658678 47138345245376 deprecation.py:323] From /tmp/Bazel.runfiles_ae3s o6ns/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow .python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0327 13:32:14.662806 47138345245376 deprecation.py:323] From /usr/local/lib/python3.6 /dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.kera s.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0327 13:32:21.277438 47138345245376 estimator.py:1149] Done calling model_fn.; I0327 13:32:23.167996 47138345245376 monitored_session.py:240] Graph was finalized.; I0327 13:32:23.169671 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:28.719004 47138345245376 session_manager.py:500] Running local_init_op.; I0327 13:32:28.854336 47138345245376 session_manager.py:502] Done running local_init_o p.; I0327 13:32:29.648829 47138345245376 modeling.py:413] Reloading EMA...; I0327 13:32:29.650222 47138345245376 saver.py:1284] Restoring parameters from /opt/mod els/wgs/model.ckpt; I0327 13:32:39.446630 47138345245376 call_variants.py:402] Processed 1 examples in 1 b atches [2569.441 sec per 100]; I0327 13:32:39.551621 47138345245376",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:8896,layers,layers,8896,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,2,['layers'],['layers']
Modifiability,"57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7767,config,config,7767,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,2,"['Config', 'config']","['Config', 'config']"
Modifiability,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3701,Config,Configuration,3701,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,2,"['Config', 'config']","['Configuration', 'configured']"
Modifiability,"8598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8244,config,config,8244,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,3,"['Config', 'config']","['Config', 'config']"
Modifiability,86286081/work; pytz @ file:///home/conda/feedstock_root/build_artifacts/pytz_1680088766131/work; pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work; requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1656534056640/work; requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1643557462909/work; rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1658328885051/work; scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1604304777848/work; six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work; sortedcontainers @ file:///home/conda/feedstock_root/build_artifacts/sortedcontainers_1621217038088/work; tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1644015949874/work/tensorboard-2.8.0-py3-none-any.whl; tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1629677084688/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl; tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1641458951060/work/tensorboard_plugin_wit-1.8.1-py3-none-any.whl; tensorflow==2.0.0; tensorflow-estimator==2.0.0; termcolor @ file:///home/conda/feedstock_root/build_artifacts/termcolor_1657118200573/work; tf-slim @ git+https://github.com/google-research/tf-slim.git@f67a0b4412c9a15b08fe0ed6daa9c95656751a6d; toolz @ file:///home/conda/feedstock_root/build_artifacts/toolz_1657485559105/work; typing_extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1644850595256/work; urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1678635778344/work; Werkzeug==0.16.1; wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1633440474617/work; yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1625232870338/work; zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:6928,plugin,plugin-wit,6928,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,2,['plugin'],"['plugin-', 'plugin-wit']"
Modifiability,": Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/pyt",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7490,config,configure,7490,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['configure']
Modifiability,":53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7735,config,config,7735,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability,"; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 memory:90000000-90ffffff memory:91800000-91803fff memory:91000000-917fffff memory:c0000-dffff; ```. thanks for your help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:2797,config,configuration,2797,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['config'],['configuration']
Modifiability,"; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7509,config,configs,7509,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['configs']
Modifiability,"> Also it seems like you are saying your bam files name is “BAM”? Please make sure your file’s name matches the parameters you are setting. Generally the BAM file would be “somename.bam”. Sorry, I don't understand, I declare a variable BAM=mysorted.bam"" isn't it how it should work? I will try with providing the absolute path then. . Still puzzled why the dry run works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/685#issuecomment-1646862327:227,variab,variable,227,,https://github.com/google/deepvariant/issues/685#issuecomment-1646862327,1,['variab'],['variable']
Modifiability,"> Hi @X1angyang; > ; > The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; > ; > ```; > import tensorflow as tf; > ; > !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; > checkpoint_path = '/tmp/model.ckpt'; > ; > reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); > shape_map_for_layers = reader.get_variable_to_shape_map(); > print(shape_map_for_layers); > ```; > ; > I just tested that in Colab (https://colab.research.google.com/).; > ; > However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; > It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus.; > ; > I hope that helps!; > Maria. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328#issuecomment-663306398:65,layers,layers,65,,https://github.com/google/deepvariant/issues/328#issuecomment-663306398,2,['layers'],['layers']
Modifiability,"> Hi @husamia; > ; > It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine.; > ; > If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case. the disk has more than 100X times the size. So it's not a disk space. I will rerun after pruning to see if it solves the problem. > Hi @husamia; > Another thing to note is that DeepVariant doesn't currently support Windows. We recommend getting a Linux machine. Let us know if that works for you. Docker Desktop linux containers run natively on Windows 10.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400#issuecomment-749549925:430,config,config,430,,https://github.com/google/deepvariant/issues/400#issuecomment-749549925,1,['config'],['config']
Modifiability,"> Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position).; > ; > Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/).; > ; > May I ask what is your use case that prefers every non-variant position to be written?; > ; > Best, Ted. Thanks @tedyun for your reply. I'm interested not only in non-variant positions but also variant positions to be written individually. Some surrounding variants with the same GQ are written in the same line with `--gvcf_gq_binsize=1`, losing other important information fields for these variants. Best,; David.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/282#issuecomment-966408955:922,config,configuration-and-analysis,922,,https://github.com/google/deepvariant/issues/282#issuecomment-966408955,1,['config'],['configuration-and-analysis']
Modifiability,"> Hi @linlin-coder,; > ; > I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:; > ; > [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore); > ; > [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore); > ; > So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously.; > ; > Hope it helps, Paul. thanks for your suggestion, meanwhile i installed deeptrio-1.5.0 in my computer, but the analysising result of deeptrio-1.5.0 display run failed, error information is ; ```; 2023-08-02 10:09:51.033332: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; To run DeepTrio PACBIO, please use version v1.4.0. See https://github.com/google/deepvariant/blob/r1.4/docs/deeptrio-pacbio-case-study.md; ``` ; cause of this cause of this error information, i use deeptrio v1.4.0 to call variation.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240:175,layers,layers,175,,https://github.com/google/deepvariant/issues/689#issuecomment-1661402240,2,['layers'],['layers']
Modifiability,"> Hi @yangyxt , can you try a command like: https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md#notes-on-singularity ?. Thanks for the response. I tried a command without --env argument in the very beginning and the warning logs were still what is in the screenshot above. Then I started to alter the env variable with --env argument and still got the same warning messages.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584:333,variab,variable,333,,https://github.com/google/deepvariant/issues/566#issuecomment-1253229584,1,['variab'],['variable']
Modifiability,"> https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md; @MariaNattestad Thank you for the quick response.I guess my question was not clear so, I will try to rewrite my query. 1) I wanted to know the base at all coordinates in a list of interval regions. The VCF file does not output all the positions in that interval region. If VCF file is able to give hom-ref, the. why are multiple locations missed? and out of around 1000bp, I am able to get information of around 200 bp?. Is there an option where we can force the deep variant to give information for all base position? and later filter if they are bad quality of not; Just like GATK has BP resolution option when we run the variant caller. 2) ; #CHROM | POS | ID | REF | ALT | QUAL | FILTER | INFO | FORMAT | DRR015476; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5285 | . | T | G | 1.6 | RefCall | . | GT:GQ:DP:AD:VAF:PL | ./.:5:213:107,106:0.497653:0,3,31; -- | -- | -- | -- | -- | -- | -- | -- | -- | --; 6 | 5288 | . | G | A | 22.1 | PASS | . | GT:GQ:DP:AD:VAF:PL | 0/1:20:220:106,107:0.486364:22,0,25. When we see allele depth(AD) for both rows, we observed that both the row has the almost same number of reads supporting it(107,106 and 106,107). Why is no call(./.) given for first and not for second? Can you please elaborate in why are multiple places given ./. despite reads supporting it",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/318#issuecomment-645516372:186,rewrite,rewrite,186,,https://github.com/google/deepvariant/issues/318#issuecomment-645516372,2,['rewrite'],['rewrite']
Modifiability,"@A-Tsai I want to make sure I understand your use case. You want GPU to be used with the specified fraction of memory. In case the GPU is not available, you want to use CPU, limited to one thread on one core. Is this correct?. Update: The code, as it is written, will only use the specified config for lines 330-336, which are running a sanity check. In order to use this config when running the model, it will have to be passed to the estimator.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-471716420:291,config,config,291,,https://github.com/google/deepvariant/pull/159#issuecomment-471716420,2,['config'],['config']
Modifiability,"@A-Tsai In the next release, we plan to add a flag to `call_variants` that will allow users to pass in any desired configuration options for the TensorFlow session config. We won't hard code any options, but you will be able to pass in all of the options included in this pull request. The code will be slightly different from what you have specified as the session configuration will be passed to [the estimator](https://github.com/google/deepvariant/blob/5e6fe205b984c6be116dcacafdfd83ce1df4d2e9/deepvariant/call_variants.py#L344), rather than [this code block](https://github.com/google/deepvariant/blob/5e6fe205b984c6be116dcacafdfd83ce1df4d2e9/deepvariant/call_variants.py#L330). Thanks for the suggested changes!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-472215526:115,config,configuration,115,,https://github.com/google/deepvariant/pull/159#issuecomment-472215526,3,['config'],"['config', 'configuration']"
Modifiability,"@A-Tsai Of course you can especially with `pipe()`, which can take a script/command as input. At the least, you have the following two options to play with:. 1. If you use pipe to launch a script, then you can launch the application through `taskset` to limit the number of cores at launch-time. Here's the options for launching with an example: . _*Options to launch a program*_: `taskset [options] mask command [argument...]`. _*Example to use only 2 specific cores*_: `taskset -c 0,2 python ~/loop.py`. The `pipe(...)` command [as defined in the RDD base-class](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) takes an external command, which in this case would be `taskset -c core-list your-program`:. ![rdd-pipe](https://user-images.githubusercontent.com/6555937/44763439-4c73f500-ab19-11e8-99d7-99adac28c913.png). 2. In Spark, you can also limit the number of cores per task in Spark through the `spark.task.cpus` setting. You probably want to set `spark.cores.max`, and not change the `spark.executor.cores` and `spark.driver.cores`. The Spark config page explains everything in more detail here: . https://spark.apache.org/docs/latest/configuration.html. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-416812290:1087,config,config,1087,,https://github.com/google/deepvariant/issues/90#issuecomment-416812290,2,['config'],"['config', 'configuration']"
Modifiability,"@A-Tsai So now you're getting into determining your lineage graph of computations, which requires a lot of thought on how you set up your data and computational steps. Wide dependencies (multiple child partitions from the parent RDD) should be kept on the same node, while narrow dependencies (one child partition per parent RDD) can go over the network. Spark does hash partitioning to shuffle, and tries by the modulo of the number of partitions that it is somewhat uniformly distributed. Basically you should model your computation tree in order to best organize the data for good distribution and maximal local computation, otherwise you flood you network which slows down your completion-time. If you have 8 compute node with 4 cores each, then the node number will not change. The key to think about is the cores, but you have to determine to resource-allocate properly. The following can be helpful:. http://site.clairvoyantsoft.com/understanding-resource-allocation-configurations-spark-application/. Regarding task information, you get the duration here:. https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.scheduler.TaskInfo@finishTime:Long. You can control the policy with the application as noted here:. https://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application. You can control the assignment by creating pools with a `minShare` of cpu cores guaranteed for that pool. If you have one pool, then that can control it all - especially in FIFO mode - but that would not be efficient use of a Spark cluster. Spark has many moving parts, including how you structure your data and how to chose to process it (batch vs. stream). To properly perform this you have to analyze carefully the design of your computational DAG, and how your work should be processed given the resources. To me 200 partitions does not quite feel right, but if you think that is optimal for your design that's fine. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417185546:974,config,configurations-spark-application,974,,https://github.com/google/deepvariant/issues/90#issuecomment-417185546,1,['config'],['configurations-spark-application']
Modifiability,"@Axze-rgb This has to do with you sequencer detecting the Q-Score (Phred-scaled) read quality, that gets stored in the FASTQ files that and get inherited in converted BAM file when you align the reads. So a SAM/BAM file have read quality scores per base following the sequence, like this:. ```; CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# ; ```. The score is a ASCII encrypted score + the value 33. You can read the Phred value here per ASCII value:. https://support.illumina.com/help/BaseSpace_OLH_009008/Content/Source/Informatics/BS/QualityScoreEncoding_swBS.htm. A full line in a SAM/BAM file would look like this:. ```; readID43GYAX15:7:1:1202:19894/1 256 contig87 540849 1 65M * 0 0 CCTGCACGAACGAAATCCGCATGCGTCTGGTCGTTGTACGGAACGGCGGTTGTGTGACGAACGGC EDDEEDEE=EE?DE??DDDBADEBEFFFDBEFFEBCBC=?BEEEE@=:?::?7?:8-6?7?@??# AS:i:0 XS:i:0 XN:i:0 XM:i:0 XO:i:0 XG:i:0 NM:i:0 MD:Z:65 YT:Z:UU; ```; The columns of this tab-delimited line are usually the following:. QNAME FLAG RNAME POS MAPQ CIGAR RNEXT PNEXT TLEN SEQ QUAL TAGS. 1) Read Name; 2) SAM flag[ --> decode](http://www.google.com/url?q=http%3A%2F%2Fbroadinstitute.github.io%2Fpicard%2Fexplain-flags.html&sa=D&sntz=1&usg=AOvVaw0FjgFbRCGf0ogpCecZqsJX); 3) contig name or * for unmapped; 4) mapped position of base 1 of a read on the reference sequence; 5) MAPQ mapping quality; 6) CIGAR string describing insertions and deletions; 7) Name of mate; 8) Position of mate; 9) Template length; 10) Read Sequence; 11) Read Quality; 12) Additional information in TAG:TYPE:VALUE format. You can read about the SAM/BAM format at the following link:. https://samtools.github.io/hts-specs/SAMv1.pdf. So to answer the last two questions. So having good coverage (read depth) lets you get around sequencing errors -- meaning you might have to repeat the experiment as well for ensuring consistency in the results. Some labs create error models with different known repeated ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491:144,inherit,inherited,144,,https://github.com/google/deepvariant/issues/682#issuecomment-1658769491,2,['inherit'],['inherited']
Modifiability,"@Axze-rgb,. In bash when you declare a variable as:; ```; BAM=HiFi_vaga.sorted.bam; ```; Then you need to use it as `${BAM}`, for example:; ```bash; echo ${BAM}; # vs.; echo BAM; ```; Would show you the difference. This is a good source to know about bash variables: https://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-5.html. In your first run, when it could not locate the BAM file, it gave you the error that it can't locate the BAM file. Then when it was able to locate the bam, it told you that the index is corrupted. These two errors are not related, it was not giving you `can't locate BAM` because your index was corrupted. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450:39,variab,variable,39,,https://github.com/google/deepvariant/issues/685#issuecomment-1646926450,2,['variab'],"['variable', 'variables']"
Modifiability,"@FraSilver You are very close. All you have to do is two things:. $`1)`$ First add the mapping of the output directory to the `docker run` command (below the one for input) as follows:; ```; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; ```. This basically makes the output directory visible from within the running Docker container, so that the results are available after Docker completes its run. $`2)`$ Next define the `FQ` variable, as it seems to be referenced as `$FQ`, but I'm not seeing it defined. If it is already defined previously, then that's fine. If you have multiple lines, you will need to also add the backslash `\` as follows, so that the command doesn't get started before the last line gets pasted:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; ```. Let me know if it helps. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1629492599:444,variab,variable,444,,https://github.com/google/deepvariant/issues/675#issuecomment-1629492599,1,['variab'],['variable']
Modifiability,"@Ge-Lab if possible, please structure your issue using code fences. See [this guide](https://docs.github.com/en/get-started/writing-on-github/working-with-advanced-formatting/creating-and-highlighting-code-blocks) for details. It will make it easier to read and understand if you place logs inside code blocks, for example. [This guide](https://docs.sylabs.io/guides/3.5/user-guide/gpu.html) suggests a few things to try. Interestingly, it apperas you should set `CUDA_VISIBLE_DEVICES=0` within the container itself, but `SINGULARITYENV_CUDA_VISIBLE_DEVICES=0` outside of the container. Were you setting the variable appropriately, within the container or outside of it?. Additionally, since you have two GPUs you will want to set this variable to 0,1. Do you have the `nvidia-container-cli` installed as suggested on the support page?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836:608,variab,variable,608,,https://github.com/google/deepvariant/issues/774#issuecomment-1954774836,4,['variab'],['variable']
Modifiability,"@JosephLalli ,. The current schema of DeepVariant training depends on having GIAB calls that we use as truth available against the reference. The GIAB truth set against T2T is still not available and released so currently we are not using T2T to train our models. Lifting the calls over to the T2T reference would not add too much value as it simply doesn't extend the truth set rather transfers it from one reference to the other. We are connected with the GIAB and T2T team. Once the resources are available, we will add those to our training scheme. Let us know if you have any further questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106:358,extend,extend,358,,https://github.com/google/deepvariant/issues/534#issuecomment-2263703106,2,['extend'],['extend']
Modifiability,"@Luosanmu , . Can you please add:. ```bash; docker run --cpus=$num_shards; ```. This way you set the CPU limits of the container you are running. It's explained more in [here](https://docs.docker.com/config/containers/resource_constraints/#cpu).",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/768#issuecomment-1921762725:200,config,config,200,,https://github.com/google/deepvariant/issues/768#issuecomment-1921762725,1,['config'],['config']
Modifiability,"@MiWitt , . Can you use `--intermediate_results_dir ./intermediate_results_ ${ALIGNMENTNAME}`. I am unsure why you are running postprocessing separately, but, something must be overwriting the files or generating multiple file patterns in the same directory where you are saving everything. One way to better debug is to set `--dry_run=true` for each command and look at the outputs and see if they match with each other. Unfortunately I don't have access to an HPC to replicate this issue. I tried running your script but it has many missing variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818#issuecomment-2111103594:543,variab,variables,543,,https://github.com/google/deepvariant/issues/818#issuecomment-2111103594,1,['variab'],['variables']
Modifiability,"@SHuang-Broad Glad to hear it worked, and thank you for the nice visualization! Docker has multiple layers and DV expands with its own within them, which is something we've noticed with other folks in terms of resource requirements -- which you could sort of tell from the memory/cpu utilization profiles. You might be able to profile it more granularly, but it might easier to start with some simple input files, and maybe a modified version of DV where you add debug information in the code to get an idea of the points of memory/disk expansion. This way you can trace the code-execution with correlated flow of data-processing.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491#issuecomment-961546511:100,layers,layers,100,,https://github.com/google/deepvariant/issues/491#issuecomment-961546511,2,['layers'],['layers']
Modifiability,"@SHuang-Broad, in the upcoming release, we will introduce a section in the DeepVariant documentation that explains ""direct phasing"" for long reads. . To summarize:. * Incorporating phasing reads enhances the accuracy of DeepVariant. With the 1.4 version, we introduced direct phasing, thereby eliminating the need to execute make_examples twice and call an external tool in the process. However, internal phasing does not permit the output of phased variants due to the operation being limited to 25,000 base regions at a time, with no synchronization between these regions. * From version 1.4 onwards, DeepVariant internally handles read phasing, eliminating the need for external phasing to attain the anticipated accuracy. * If desired, one may opt to run third-party phasing after executing DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414:195,enhance,enhances,195,,https://github.com/google/deepvariant/issues/649#issuecomment-1551634414,1,['enhance'],['enhances']
Modifiability,"@amyhouseman Sometimes people decide to extend the regions in the BED files a bit, something like: https://bedtools.readthedocs.io/en/latest/content/tools/slop.html",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/527#issuecomment-1067162845:40,extend,extend,40,,https://github.com/google/deepvariant/issues/527#issuecomment-1067162845,1,['extend'],['extend']
Modifiability,"@asherrar Unfortunately, I'm unable to reproduce what you're seeing. I'm going to write down what I did. Maybe you can spot what differences we have:. Create a CentOS7 machine to test:. ```; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. By default the machine has Python2; ```; [pichuan@pichuan-centos7 ~]$ python --version ; Python 2.7.5; ```. ## Install Python 3.8.10; ```; sudo yum update; sudo yum install gcc openssl-devel bzip2-devel libffi-devel -y; sudo yum install -y wget; sudo yum install -y python3; ```. ```; wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz; tar xvfz Python-3.8.10.tgz; ```. ```; cd Python-3.8.10; ./configure --enable-optimizations; ```. ```; sudo yum install -y make; sudo make altinstall; ```. ```; [pichuan@pichuan-centos7 Python-3.8.10]$ python3.8 --version; Python 3.8.10; ```. ## Install Singularity. ```; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools \; cryptsetup; ```. ```; export VERSION=1.14.12 OS=linux ARCH=amd64; # Downloads the required Go package; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz; # Extracts the archive; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz; # Deletes the ``tar`` file; rm go$VERSION.$OS-$ARCH.tar.gz. export VERSION=3.8.4; wget https://github.com/hpcng/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz; tar -xzf singularity-${VERSION}.tar.gz; pushd singularity-3.8.4; export PATH=/usr/local/go/bin:$PATH. ./mconfig; make -C builddir; sudo make -C builddir install; ```. Check version:; ```; [pichuan@pichuan-centos7 ~]$ singularity --version; singularity version 3.8.4; ```. ## Try running DeepVari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759:885,config,configure,885,,https://github.com/google/deepvariant/issues/610#issuecomment-1429156759,1,['config'],['configure']
Modifiability,"@ashraf123456789 I don't think the commands should vary. Could you check that you have set `${BIN_VERSION}`? I have seen this error message when the variable is not set, causing the image name to be incorrect. You could try running the following:. ```; BIN_VERSION=""0.8.0"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=4; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/181#issuecomment-489226839:149,variab,variable,149,,https://github.com/google/deepvariant/issues/181#issuecomment-489226839,1,['variab'],['variable']
Modifiability,"@danielecook than you for your answer. I tried the solution you suggested but I am having trouble building DeepVariant.; After executing build-prereq.sh I get multiple error and warning messages regarding pip dependencies. `========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [jue 18 ago 2022 14:10:53 CEST] Stage 'Misc setup' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:00 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:03 CEST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:05 CEST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2500k 100 2500k 0 0 21.8M 0 --:--:-- --:--:-- --:--:-- 21.8M; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:301,config,config,301,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,2,['config'],['config']
Modifiability,"@depristo The porting to /usr/local works, thanks a lot. With 0.4.0 build_and_test did complain about the htslib_gcp_oauth.init issue, which is now fixed in 0.4.1. ```; (18:20:33) INFO: Elapsed time: 2.092s, Critical Path: 0.03s; (18:20:33) INFO: Build completed successfully, 2 total actions; ```; Case closed, cheers. . oops, just to append a comment, by exporting an include variable with a non `/usr/local` installation of `clif` doesnot help, `pyclif` still cant find it. This was tested before touching `clif.bzl`. This `clif` thing must have its own include logic.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351350738:378,variab,variable,378,,https://github.com/google/deepvariant/issues/12#issuecomment-351350738,1,['variab'],['variable']
Modifiability,"@drtamermansour You don't have to install glibc, you just need to compile it in a local directory. Basically you just need to run `./configure` and `make` without running `make install`. Then just update `LD_LIBRARY_PATH` to include the local directory of the compiled glibc .so file.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-454185297:133,config,configure,133,,https://github.com/google/deepvariant/issues/137#issuecomment-454185297,1,['config'],['configure']
Modifiability,"@fo40225 Thank you so much! @gunjanbaid will update internally. It will show up in our next release (hopefully soon! We're working on it). We will recognize your contribution in the release notes. @pgrosu We strongly appreciate the contributions of yourself and others to the improvement of DeepVariant. We are happy to incorporate pull requests such as this, but right now have to do so indirectly due to technical configuration. Most internal Google projects use a slightly different tool chain from Github, which is a bit faster for us to develop with, so we keep our source of truth internally and use copybara to export to GitHub. :) Among Google open source repos, some are configured differently (for example, our sibling repo Nucleus can take external PRs). When we do incorporate information from a PR such as this, we will recognize the contribution and author in the release notes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152#issuecomment-475486437:416,config,configuration,416,,https://github.com/google/deepvariant/pull/152#issuecomment-475486437,2,['config'],"['configuration', 'configured']"
Modifiability,"@gunjanbaid Actually, this pull request will take effect when enable the enable_configurable_gpu flag. I try to minimize the code change as possible as I can, so there is no any change in CPU mode. Since CPU resource allocation of Tensorflow can't be limited to one threat, this pull request doesn't change any configuration in CPU mode. It might introduce a little bit overhead due to context switch when running on Spark, but it won't have any impact on turnaround time from my experiments.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/159#issuecomment-471842177:311,config,configuration,311,,https://github.com/google/deepvariant/pull/159#issuecomment-471842177,1,['config'],['configuration']
Modifiability,"@gunjanbaid I think you're right about it being a configuration for a different pipeline. Thanks, I'll try using bam.bai instead of simply .bai and see if that works.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149#issuecomment-461160956:50,config,configuration,50,,https://github.com/google/deepvariant/issues/149#issuecomment-461160956,2,['config'],['configuration']
Modifiability,"@hangy1 ,. 1) You can see from the log:. ```; Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz; ```. seems like sample_name is not set correctly? Unless you replaced them? Can you confirm if you have set the values correctly by printing them before running DeepVariant?. 2) Please use absolute paths rather than relative paths when you are setting paths. so instead of using:; ```bash; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Use:; ```bash; gvcf = ""/path/to/vcf/{sample}.deepVariant.g.vcf.gz""; ```; Hope this helps. You can also run the [quickstart](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md) to see if you can simply copy-paste and run the command fully and then adapt it to the command you are planning to run.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563:794,adapt,adapt,794,,https://github.com/google/deepvariant/issues/839#issuecomment-2195257563,2,['adapt'],['adapt']
Modifiability,"@heznanda Try the following to see if it fixes that requirement:. 1) Install `qemu` and `colima` like this -- colima will temporarily change the Docker runtime context to colima (see below):. ```; brew install qemu; brew install colima; ```. 2) Enable colima to be your runtime for Docker via `colima start` like this with this configuration:. ```; colima start --arch x86_64 --cpu 2 --memory 2 --disk 12 --cpu-type Broadwell-v4; ```. The above settings are in gigabytes, so adjust accordingly to what you have available on your machine. 3) Now try running the docker container for DeepVariant again. . 4) After you are done, then run `colima stop` to change Docker back to its default configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935:328,config,configuration,328,,https://github.com/google/deepvariant/issues/657#issuecomment-1575598935,2,['config'],"['configuration', 'configurations']"
Modifiability,"@husamia Unfortunately usually is not a reliable description that can be applied to Docker, as it performs many layers of abstraction that can cause a spike when you are not even aware of it. 80% is definitely not enough, especially with the layers of abstractions that DeepVariant also adds. On top of that you would have the Windows abstraction either running a virtual machine or ""leaner"" Windows Subsystem for Linux, which has its own abstractions. Think about it this way, how would you even be able to tell if there is a 10 second spike when Docker starts using 97-98-99% of resources if you need to wait for the OS process resource queue to clear in order to check the Docker status, but are limited/prevented to do so by multiple layers of resource management systems?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/412#issuecomment-767885510:112,layers,layers,112,,https://github.com/google/deepvariant/issues/412#issuecomment-767885510,6,['layers'],['layers']
Modifiability,"@mvelinder I think sometimes this could happen when the environment variables were not manually set right. For example, can you do:; ```; echo $BIN_VERSION; ```; (and all the others that you might have in your command) to make sure they're all set as expected?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462#issuecomment-867206060:68,variab,variables,68,,https://github.com/google/deepvariant/issues/462#issuecomment-867206060,1,['variab'],['variables']
Modifiability,@nlopez94 can you remove this parameter: `--config.num_validation_examples=0` and rerun please,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101231433:44,config,config,44,,https://github.com/google/deepvariant/issues/819#issuecomment-2101231433,1,['config'],['config']
Modifiability,"@pgrosu ; Yes!! thank you! For a second, I thought that bazel-5.3.0-linux-arm64 is a folder. but it is an actual bazel bin. ; ```; > bazel; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintain",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:751,config,configurations,751,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['configurations']
Modifiability,"@pgrosu From the choices above, I want to pursue (2) singularity the most. ; 2. Is there an instruction that you can point me to for this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.loca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:668,config,config,668,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['config'],['config']
Modifiability,"@pgrosu I am actually reckoning some sort of include order issue related to clif, which I am not quite familiar with. To my understanding, it is quite normal when defining new types, one would like to extend the defs in limits.h (really bad...). In my case I think it is telling me at the end of my include path list there is a include_next which leads to no where, I am not sure if what I need is a modified derivative for headers. ```; /root/opt/clif/clang/lib/clang/5.0.0/include/limits.h; /root/clif_backend/llvm/tools/clang/lib/Headers/limits.h; /root/clif_backend/build_matcher/lib/clang/5.0.0/include/limits.h; /usr/include/c++/v1/support/ibm/limits.h; /usr/include/c++/4.8/tr1/limits.h; /usr/include/c++/5/tr1/limits.h; /usr/include/limits.h; /usr/include/linux/limits.h; /usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/limits.h; /usr/lib/gcc/x86_64-linux-gnu/5/include-fixed/limits.h; /usr/src/linux-headers-4.4.0-103/arch/arm/include/asm/limits.h; /usr/src/linux-headers-4.4.0-103/include/uapi/linux/limits.h; ```. ```; includes = [; include_htslib,; ""."",; ""/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed/"",; ]; ```. I actually modified the chunk in deepvariant/third_party/htslib.BUILD, to specifically include a particular location **which contains the type python needs and without the include_next loophole**, but still doesnot work, maybe the syntax was wrong, the build_and_test actually issued a warning saying . `WARNING: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/htslib/BUILD.bazel:216:16: in includes attribute of cc_library rule @htslib//:htslib: ignoring invalid absolute path '/usr/lib/gcc/x86_64-linux-gnu/4.8/include-fixed'`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351020472:201,extend,extend,201,,https://github.com/google/deepvariant/issues/12#issuecomment-351020472,1,['extend'],['extend']
Modifiability,"@pgrosu I could not compile the library on my server . I followed the suggestion [here](https://stackoverflow.com/questions/847179/multiple-glibc-libraries-on-a-single-host/851229#851229). I added CFLAGS=""-O2"" to address an optimization request error but still the make command fails to compile; ```; mkdir glibc && cd glibc; wget https://ftp.gnu.org/gnu/glibc/glibc-2.23.tar.gz; tar xvzf glibc-2.23.tar.gz; mkdir glibc-build && cd glibc-build; mkdir ../install; ../glibc-2.23/configure CFLAGS=""-O2"" --prefix $HOME/glibc/install; make -j `nproc`; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-453764783:477,config,configure,477,,https://github.com/google/deepvariant/issues/137#issuecomment-453764783,1,['config'],['configure']
Modifiability,"@pgrosu Thank you for the information. Your suggestion might be a way to port DeepVariant on Spark, but it's not fit on dynamic allocation since the available resource is dynamic changed. ; I did port several popular bioinformatics tools (e.g. BWA, GATK, DELLY2, Samtools, ...) on Spark and they run well. I think the fundamental problem is that the resource configuration (intra_op_parallelism_threads=1, inter_op_parallelism_threads=1) of TensorFlow doesn't work. If there is no way to limit CPU resource on DeepVariant, should I submit this issue to TensorFlow GitHub?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/90#issuecomment-417995073:359,config,configuration,359,,https://github.com/google/deepvariant/issues/90#issuecomment-417995073,1,['config'],['configuration']
Modifiability,"@pgrosu The `tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:826,config,config,826,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"@pgrosu Wow! now I think it fixes it:; ```; > ./bazel-5.3.0-linux-arm64 ; WARNING: Invoking Bazel in batch mode since it is not invoked from within a workspace (below a directory having a WORKSPACE file).; Extracting Bazel installation...; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. But the `bazel` command still returns:; ```; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/.bazel/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461:719,config,configurations,719,,https://github.com/google/deepvariant/issues/657#issuecomment-1577773461,1,['config'],['configurations']
Modifiability,@pichuan @danielecook Thank you guys for such a quick response. It is something I feared. The Cromwell dispatcher is a black box to me. I will try your configurations and some others too to see if it solves the problems.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446#issuecomment-827022298:152,config,configurations,152,,https://github.com/google/deepvariant/issues/446#issuecomment-827022298,1,['config'],['configurations']
Modifiability,"@pichuan hi, i got the zip file and ra run-prereq.sh and this was the output:. solokopi@solokopi-All-Series:~/Desktop/DeepVariant-0.7.0+cl-208818123$ sudo bash run-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 28日 星期二 10:31:05 CST] Stage 'Misc setup' starting; Ign:1 http://dl.google.com/linux/chrome/deb stable InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial InRelease ; Hit:3 http://dl.google.com/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:223,config,config,223,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['config']
Modifiability,"@pichuan when shuffle the datasets using local runner, `direct_num_workers` is set to 0, it will use all the local CPUs.; I got this warning that make me thinking; ```; WARNING:apache_beam.runners.portability.fn_api_runner.fn_runner:If direct_num_workers is not equal to 1, direct_running_mode should be `multi_processing` or `multi_threading` instead of `in_memory` in order for it to have the desired worker parallelism effect. direct_num_workers: 8 ; running_mode: in_memory; ```. Is there a reason we use `in_memory` rather than other modes?. Thank you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/360#issuecomment-2420163031:197,portab,portability,197,,https://github.com/google/deepvariant/issues/360#issuecomment-2420163031,1,['portab'],['portability']
Modifiability,"@pichuan, I got it, thanks! Indeed the experiments are different. I also benchmarked changes without and with logging improvements so can confirm that there were no efficiency difference so we don't need additional experiments. Thanks for your time and warm welcome!. I agree with you that Dockerfile now is in right configuration - build only which is manually enabled. Regarding default value of `use_openvino` I propose a condition `openvino_available and not cuda_available`. Just pushed corresponding commit.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-736009433:317,config,configuration,317,,https://github.com/google/deepvariant/pull/363#issuecomment-736009433,1,['config'],['configuration']
Modifiability,"@pichuan, I'll think and propose a flexible solution for OpenVINO acceleration. Can you please add some details about the latest numbers? Is that regression or improvement? Because last time we saw 266m46.183s --> 198m46.734s for WGS (call_variants) now it's 233m14.191s --> 204m35.065s.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735959351:35,flexible,flexible,35,,https://github.com/google/deepvariant/pull/363#issuecomment-735959351,1,['flexible'],['flexible']
Modifiability,"@pichuan, I'm very sorry for long delay! I tried to build DeepVariant so it can be portable to benchmark on remote target machine. These are initial numbers for [Intel DevCloud](https://devcloud.intel.com/edge/) machines and [quickstart-testdata](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md):. | [Intel® Xeon® Gold 5120](https://devcloud.intel.com/edge/devices/intel-xeon-gold-5120-cpu/) | make_examples | call_variants | postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | real 0m13.111s<br>user 0m8.496s<br>sys 0m4.869s | real 0m19.154s<br>user 0m23.705s<br>sys 0m8.424s | real 0m6.662s<br>user 0m7.946s<br>sys 0m4.841s |; | OpenVINO | real 0m13.083s<br>user 0m8.216s<br>sys 0m4.510s | real 0m9.687s (x1.97)<br>user 0m18.741s (x1.26)<br>sys 0m6.289s (x1.33) | real 0m6.709s<br>user 0m8.165s<br>sys 0m4.676s |. So, probably, my main question is how to interpret real, user and sys time? Maybe it will help us to understand how to improve the pipeline. ---. Here are my steps:. 1. Build locally (see [deepvariant-build-test.md](https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-build-test.md)). After build,; ```bash; ./build_release_binaries.sh; tar -cvzf bazel-deepvariant.tar.gz bazel-deepvariant/*; tar -cvzf bazel-genfiles.tar.gz bazel-genfiles/*; ```; 2. Go to another machine (i.e. Intel DevCloud) and clone repository. Unpack the binaries; ```bash; git clone -b master_openvino https://github.com/dkurt/deepvariant --depth 1; cd deepvariant; tar -xf bazel-deepvariant.tar.gz; tar -xf bazel-genfiles.tar.gz; ```; 3. Apply some patches to resolve local paths:; ```bash; sed -i -E 's|/opt/deepvariant/bin|./bazel-genfiles/deepvariant|' scripts/run_deepvariant.py; sed -i -E 's|/opt/models/wgs/model.ckpt|model.ckpt|' scripts/run_deepvariant.py; ln -s -f $HOME/deepvariant/scripts/ bazel-deepvariant/scripts; ```; 4. Download [GNU parallel](https://launchpad.net/ubuntu/bionic/amd64/parallel/20161222-1) (if you have no root permiss",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-723242914:83,portab,portable,83,,https://github.com/google/deepvariant/pull/363#issuecomment-723242914,1,['portab'],['portable']
Modifiability,"@ptrebert Glad it worked :) DeepVariant is nice but it's written more complex than it has to be, and when you add Docker/Singularity on top of that, that injects many layers of complexity (not easily exposed) creating opportunity for heisenbugs. Docker/Singularity are really meant for smaller applications, since their interaction with the kernel become multiplicative rather than additive for larger applications, which you noticed indirectly via the memory resource requirements.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304#issuecomment-643473196:167,layers,layers,167,,https://github.com/google/deepvariant/issues/304#issuecomment-643473196,1,['layers'],['layers']
Modifiability,"@sanchit-misra , yes, the license extends to the models.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/891#issuecomment-2403235990:34,extend,extends,34,,https://github.com/google/deepvariant/issues/891#issuecomment-2403235990,1,['extend'],['extends']
Modifiability,"@scott7z Well, my feeling is that would be too much work, which could be fostered elsewhere. Unless there is a defined criteria for working off a specific revision, it is usually less of a benefit to be tied to a specific release. It'll also make the cost of supporting it too much of a headache, as the other dependencies will continue to evolve. I always prefer to simplify and feel it's more practical to push compliance of dependencies to their maintainers, while expanding on the fun part of adding community-driven features. For example, there over 1000 commits between the two SHAs, and it would be hard to keep track of so many contributions:. ```Bash; $ git log | grep commit | cat -n | grep '97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a\|ab0fcaceda001825654424bf18e8a8e0f8d39df2'; 1 commit 97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a; 1244 commit ab0fcaceda001825654424bf18e8a8e0f8d39df2; $; ```. Usually more contributions to a dependency might provide us with more opportunities :)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353510712:340,evolve,evolve,340,,https://github.com/google/deepvariant/issues/19#issuecomment-353510712,2,['evolve'],['evolve']
Modifiability,"@sophienguyen01 can you try to run this again without using `--debug=true`? This runs tensorflow in eager mode which will be very inefficient. The other issue is that you don't have a checkpoint file because you didn't train long enough - and no checkpoint outperformed the existing performance on your tune dataset. Try re-running with `--debug=false` and `--config.num_epochs=10` and see where that gets you. If you get an OOM error with batch_size=512, reduce it and try again. If training produces a better model, it will be output in the `experiment_dir`.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033440565:360,config,config,360,,https://github.com/google/deepvariant/issues/802#issuecomment-2033440565,1,['config'],['config']
Modifiability,"A conda package for DeepVariant is now available through bioconda so you should be able to install with:; ```; conda install -c conda-forge -c bioconda deepvariant; ```; It includes wrapper scripts for each of the 3 steps (`dv_make_examples.py`, `dv_call_variants.py`, `dv_postprocess_variants.py`) that handle wrapping the internal locations of the pre-built zip files and models, so you can call these as normal command line options. These don't yet expose all options available in DeepVariant but are hopefully sufficient to run standard germline calling projects. I've also started a separate issue (#29) to discuss improvements we can make to improve portability, but hope the initial package helps for installing and using DeepVariant. I'd be happy for feedback and suggestions on this package as folks have a chance to use it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/9#issuecomment-354748344:656,portab,portability,656,,https://github.com/google/deepvariant/issues/9#issuecomment-354748344,2,['portab'],['portability']
Modifiability,"According to file dv_config.py : ; ```; # If set to 0, use full validation dataset.; config.num_validation_examples = 0; ```. Also, the[ training tutorial](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md) also use `--config.num_validation_examples=0 `",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073360601:85,config,config,85,,https://github.com/google/deepvariant/issues/802#issuecomment-2073360601,2,['config'],['config']
Modifiability,"Actually, what you are asking for I believe partially exists w.r.t. TensorFlow and our models. If you look at modeling.py you can see we aren't tied to InceptionV3 directly. We have trained DV models with mobilenet and ResNet, for example. . And we aren't tied to a specific version of TensorFlow. You can build your own optimized version of TensorFlow from scratch with whatever configuration options you like (AVX, AVX2, MKL) and use that to get all of the performance benefit of your native chipset. I suspect that production-grade deployments of DeepVariant will use custom built, optimized TensorFlow wheels to maximize performance. In fact, it would be possible, with sufficiently advanced orchestration capabilities, to manage a family of TensorFlow wheels and conditionally load the maximally-performant version for the actual machine you are running on, if you are running on a heterogenous fleet (common both in cloud and on-prem). . Our challenge in providing a prebuilt binary is that we needed to come up with a *reasonable* default set of optimization flags, and requiring at least AVX seems reasonable to us given the gap (not on your chart) in performance between with and without AVX itself. As for decoupling from TensorFlow, I can tell you that we will not devote any cycles to that effort ourselves. Perhaps others would be willing to do so, though.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355441693:380,config,configuration,380,,https://github.com/google/deepvariant/issues/21#issuecomment-355441693,1,['config'],['configuration']
Modifiability,"Ahh, I see. Thank you @pichuan, now it makes total sense! I will rerun, adapting the command from https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-exome-case-study.md#run-make_examples",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428677704:72,adapt,adapting,72,,https://github.com/google/deepvariant/issues/99#issuecomment-428677704,1,['adapt'],['adapting']
Modifiability,"Also, to answer your earlier question:. I may be still in the learning phase about some AWS things, but _m5.4xlarge_ (which is what I was using when I submitted the ticket) is listed as having 16 cores and 64 GB of RAM. I am currently testing running on an _r4.8xlarge_ EC2 instance (part of an ECS cluster, which has Docker installed) is listed as having 32 cores and 244 GB of RAM. If that doesn't work, I think I need to take more time to either look into other strategies (like keep trying to get things set up to run in AWS ""Batch"" and/or seeing if there might be a Docker configuration issue that I can change/fix) and/or go ahead and start my Google Cloud comparison. While I want to be fair and clear about having some costs associated with a learning curve, I've already spent more than $250 in ~2 weeks trying to test DeepVariant in the AWS system. So, in order for this to be a viable option over GATK (which I can run on my local computer for WGS data with 4 cores and 8 GB of ram), I need to be able to confirm that I can in fact perform Exome analysis on Google Cloud for $0.20 (and while ~25 min is not essential, I think you would need to update your page if those two metrics didn't apply to typical usage). Still running that **postprocess_variants** command on AWS, but I will keep you posted until the issue is fixed!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-479522653:578,config,configuration,578,,https://github.com/google/deepvariant/issues/167#issuecomment-479522653,2,['config'],['configuration']
Modifiability,"Also, while I am still doing some extra testing, I figured out the cause of my problem (which was actually very simple):. **1)** The error message changed a little bit (so, I was focusing on the memory, when there was actually another error):. ```; terminate called after throwing an instance of 'std::system_error'; what(): Resource temporarily unavailable; ```. **2)** I was providing the relevant parts of the code on GitHub without realizing that everything from step 2 (**call_variants**) was commented out, which _included_ the CALL_VARIANTS_OUTPUT variable (which is the _input_ file for **postprocess_variants**). After uncommenting that line, the file reformatting ran within a minute (on an AWS ECS m5.2xlarge instance). I am very sorry that it took so many messages (and time) for me to eventually figure out this problem, but I hope this helps with other people seeing a similar error message.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-482393946:555,variab,variable,555,,https://github.com/google/deepvariant/issues/167#issuecomment-482393946,2,['variab'],['variable']
Modifiability,"And, just in case the documentation isn't clear:. This part:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ...; ```. The variable BIN_VERSION was specified in earlier in the steps:. ```; BIN_VERSION=""1.6.1""; ```. So, in Unix command it's equivalent to:. ```; google/deepvariant:""1.6.1"" \; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162210763:201,variab,variable,201,,https://github.com/google/deepvariant/issues/829#issuecomment-2162210763,2,['variab'],['variable']
Modifiability,"Apologies in advance for inviting myself into the conversation. I think it's worth mentioning that while there are merits to doing MNV calling, the use-case given (calling amino acid variation from SNPs) isn't the greatest. There's no guarantee that codons occur as triplets in the genome (though they tend to, they can also get split across exons). which suggests that MNV calling doesn't actually solve the problem in general. Phase-aware consequence predictors (bcftools csq) should, on the other hand, work just fine (neglecting phasing errors of course). FWIW, we use the following pipeline: deepvariant or gatk4 -> whatshap -> shapeit4 -> bcftools csq to predict protein polymorphisms (or really, whole proteomes from whole genomes) and that approach should work in this case too. To be clear, having MNV calling would make an excellent addition to DeepVariant, it just may not be a total solution to the problem posed.; -August",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486#issuecomment-984133927:677,polymorphi,polymorphisms,677,,https://github.com/google/deepvariant/issues/486#issuecomment-984133927,2,['polymorphi'],['polymorphisms']
Modifiability,"Are you following the configurations as recommended on the following page, and specifying the number of workers as well?. https://cloud.google.com/genomics/docs/tutorials/deepvariant#pipeline_configurations. If so, then it will build the Pipelines API configs, which basically makes custom machines of this form:. ```; machine_type = 'custom-{0}-{1}'.format(; pipeline_args.make_examples_cores_per_worker,; pipeline_args.make_examples_ram_per_worker_gb * 1024); ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py#L305-L307. Also I believe that `make_examples` is restricted to one core based on this:. ```; if options.n_cores != 1:; errors.log_and_raise(; 'Currently only supports n_cores == 1 but got {}.'.format(; options.n_cores), errors.CommandLineError); ```. Ref: https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py#L1167-L1170. In any case, it will some time to explain the complete control flow, and if you look at the following two files I think you'll discover why based on the design of DeepVariant:. https://github.com/google/deepvariant/blob/r0.7/deepvariant/make_examples.py. https://github.com/google/deepvariant/blob/r0.7/deepvariant/docker/gcp_deepvariant_runner.py. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-460850032:22,config,configurations,22,,https://github.com/google/deepvariant/issues/150#issuecomment-460850032,2,['config'],"['configs', 'configurations']"
Modifiability,"Building a deepvariant Singularity image is indeed really quite simple and portable. I did it and test it on CentOS7 and MacOS X and it run in both case with deepvariant quick-test data. I will post the complete ""how-to"" when I'll have a couple of minutes.; Thank's ink1 for the idea.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6#issuecomment-372953552:75,portab,portable,75,,https://github.com/google/deepvariant/issues/6#issuecomment-372953552,2,['portab'],['portable']
Modifiability,"CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; [bazel release 0.15.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-marc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:9851,config,configurations,9851,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['config'],['configurations']
Modifiability,"Check the result of `gcloud config list`. I suspect you have region set to some default there. If so, you should unset it by `gcloud config set compute/region """"`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96#issuecomment-423567456:28,config,config,28,,https://github.com/google/deepvariant/issues/96#issuecomment-423567456,2,['config'],['config']
Modifiability,"DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; bai=""/data/DATA/ChineseQuartet/ref_based_analysis/aligned_reads/ChineseQuartet/LCL5/ChineseQuartet.LCL5.GRCh38.HiFi.minimap2.bam"",; output:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; run:; chrom, start, end = f""{wildcards.region}"".split(""_""); start = int(start) - 1000; end = int(end) + 1000; shell(""{samtools} view -h -O BAM {input.bam} {chrom}:{start}-{end} > {output.bam}""); shell(""echo '{chrom}\t{start}\t{end}' > {output.bed}""). rule deepvariant:; input:; bam=dir_work + ""bams/ChineseQuartet.{region}.bam"",; bai=dir_work + ""bams/ChineseQuartet.{region}.bam.bai"",; bed=dir_work + ""bams/ChineseQuartet.{region}.bed"",; ref=path_ref; output:; vcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.vcf.gz"",; gvcf_gz=dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.g.vcf.gz""; # gvcf_gz=config[""dir_variants""] + ""dv/dv_details/{sample}/{sample}.{prefix}.dv.raw.g.vcf.gz""; log:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; benchmark:; dir_work + ""vcfs/ChineseQuartet.{region}.deepvariant.log""; threads: 48; run:; dir_tmp = str(output.vcf_gz).rstrip("".vcf.gz"") + ""_tmp""; file_tmp = dir_tmp.split(""/"")[-1]; shell(""mkdir -p "" + dir_tmp); bam_dir = ""/"".join(str(input.bam).split(""/"")[:-1]); bam_file = str(input.bam).split(""/"")[-1]; bed_file = str(input.bed).split(""/"")[-1]; ref_dir = ""/"".join(str(input.ref).split(""/"")[:-1]); ref_file = str(input.ref).split(""/"")[-1]; output_dir = ""/"".join(str(output.vcf_gz).split(""/"")[:-1]); output_file = str(output.vcf_gz).split(""/"")[-1].rstrip("".vcf.gz""). shell('docker run '; '-v ""{bam_dir}"":""/input"" '; '-v ""{ref_dir}"":""/ref"" '; '-v ""{output_dir}"":""/output"" '; 'google/deepvariant:1.1.0 /opt/deepvariant/bin/run_deepvariant '; '--model_type=PACBIO '; '--ref=/ref/{ref_file} '; '--reads=/input/{bam_file} '; '--regions /input/{bed_file} '; '--output_vcf=/outp",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792:2187,config,config,2187,,https://github.com/google/deepvariant/issues/660#issuecomment-1589724792,1,['config'],['config']
Modifiability,"DT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7671,config,config,7671,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"Dear @XXH123a,. Congrats! You seem to be running DeepVariant now, which is most important. Don't worry about trying out anything else. The ids `aff53ed783a7` and `45f6c7767ff0` are just image ids that are another way to launch DeepTrio and DeepVariant images, as opposed to using the canonical names `google/deepvariant:deeptrio-1.5.0` or `google/deepvariant:1.5.0`. . I'm not seeing the backslashes after each line, but according to the running commands, it seems to have received the necessary parameters. You might have image layers taking up space, but if things are working for you just stick with that for now, in order to complete the analysis, which is more important. Well done!; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/700#issuecomment-1687480011:529,layers,layers,529,,https://github.com/google/deepvariant/issues/700#issuecomment-1687480011,1,['layers'],['layers']
Modifiability,"Dear @oschwengers,. I will soon have to call variants from E.coli bacteria genomes and ONT SUP reads and wonder if I can use the newly introduced haploid option to tell Deepvariant that my bacterial reference genome is haploid, like shown in [this page for X and Y](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-xy-calling-case-study.md). `--haploid_contigs ""<Ecoli_chromosome>"" `. Also, will the _ONT_R104_ model be affected by this extra argument?. The paper referred to above by @mbhall88 dates from 2020 and may not be accurate anymore for the haploid aspect of variant calling in bacteria if DeepVariant has evolved in that domain. Thanks for your feedback",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486:632,evolve,evolved,632,,https://github.com/google/deepvariant/issues/183#issuecomment-1824632486,2,['evolve'],['evolved']
Modifiability,"DeepVariant is built with bazel. Internally we have an add-on in CLion to import bazel project. If this plug-in is available you can try to do that. Bazel project is in the file named ""BUILD"" which exists in all sub-directories. If importing bazel project is not available then you may need to create your project from scratch in your IDE and then add all source files.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792#issuecomment-2010856611:104,plug-in,plug-in,104,,https://github.com/google/deepvariant/issues/792#issuecomment-2010856611,1,['plug-in'],['plug-in']
Modifiability,"Did you edit settings.sh? Look where it says:. export DV_COPT_FLAGS=""--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3"". you have to remove the options your CPU doesn't support. Something like this, for a minimal case. export DV_COPT_FLAGS="""". or this for something tuned to your particular CPU. export DV_COPT_FLAGS=""--copt=-march=native"". The DV_COPT_FLAGS variable is used by our scripts to pass flags to the build system.; The --copt= construction is a flag for Bazel, the build system that passes flags to the compiler,; The ""-march=native"" part is a flag for GCC or Clang that says to try to auto-detect the architecture.; So read the GCC manual to see what other options are available.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/16#issuecomment-352893787:365,variab,variable,365,,https://github.com/google/deepvariant/issues/16#issuecomment-352893787,1,['variab'],['variable']
Modifiability,"Does build_and_test.sh run properly without this modification? There's really nothing special about our pre-built CLIF binary. Here's our exact build commands:. ```; # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ```. which is similar to your script but not identical. In fact you may be getting burned by `sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh` which changes the install dir from ~/opt to /usr/local/. Is there a reason not to use this pre-built binary? If you are able to do git clone you should be able to reach GCS to get the binary. If you don't want to do that inside the script, you can always do it outside the script once, install it manually, and then build_and_test.sh won't try to refetch. Or you can follow the exact instructions above and it should create the actual clif binaries we distribute. We are looking forward to an official binary version of CLIF from that team...",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351260272:290,config,config,290,,https://github.com/google/deepvariant/issues/12#issuecomment-351260272,2,['config'],"['config', 'configure']"
Modifiability,"Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8618,config,config,8618,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8141,config,config,8141,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8247,config,config,8247,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability,"Fantastic information, thank you. On Thu, May 26, 2022 at 9:01 AM Andrew Carroll ***@***.***>; wrote:. > Hi @avilella <https://github.com/avilella>; >; > DeepVariant has been used on MGI datasets, both using the standard; > Illumina model, as well as retrained models. There is some complexity that; > the MGI/BGI technologies have evolved over time, so some demonstrations may; > not reflect the newest methods.; >; > The general finding is that the Illumina models tend to work well for MGI; > data, though we find examples of retraining for certain datasets improve; > further.; >; > Our advanced training tutorial; > <https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md>; > walks through retraining an Illumina model for data from BGISEQ 500 and this; > comparison; > <https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/>; > was conducted several years ago using the out-of-the-box Illumina model.; >; > If you know of any genome in a bottle sequencing datasets that are; > available from more recent MGI platforms, I'd be interested in pointers to; > those locations. I would be quite curious to see how the technology has; > evolved over the last several years.; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/538#issuecomment-1138272184>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AABGSN3EDTSIAXWBYLGQ3PDVL4VV7ANCNFSM5W4SRYCA>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160:332,evolve,evolved,332,,https://github.com/google/deepvariant/issues/538#issuecomment-1138380160,2,['evolve'],['evolved']
Modifiability,"First of all, from the error you're seeing, I think you forgot to set up the variables in that shell. Basically, if you do `echo $LOG_DIR` in that shell, you'll find it's empty. And, instead of directly using a Google Cloud shell in the browser, you can consider ssh into your machine from a terminal, like in this section:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#request-a-machine; You can do something like `gcloud compute ssh ""${USER}-deepvariant-casestudy"" --zone ""us-west1-b""`. Using screen should certainly work. You'll just need to paste in the variable settings in this section again, because otherwise they'll all be empty strings.; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-case-study.md#preliminaries",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/88#issuecomment-414378920:77,variab,variables,77,,https://github.com/google/deepvariant/issues/88#issuecomment-414378920,2,['variab'],"['variable', 'variables']"
Modifiability,"First thing I'm trying to do is to see if I can follow similar steps in:; https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md. Context: We switched our training tutorial to GPU since then, because GPU is a more common use case. The one in v0.9.0 is the latest that I can refer to, so I'll start from there. The following steps is a combination of; https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-tpu-training-case-study.md; and; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-training-case-study.md. ---. I got a CPU machine:; ```; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1804-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" \; --zone ""us-west2-b"" \; --min-cpu-platform ""Intel Skylake""; ```. Set variables; ```; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; BIN_VERSION=""1.1.0"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${BIN_VERSION}/DeepVariant-inception_v3-${BIN_VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469#issuecomment-871936544:913,variab,variables,913,,https://github.com/google/deepvariant/issues/469#issuecomment-871936544,1,['variab'],['variables']
Modifiability,"For the use case you mentioned, one should just use published docker images (see images pointed in https://cloud.google.com/genomics/docs/tutorials/deepvariant). These config files are meant to be used by cloud build. See ""Build using a build config file"" in https://cloud.google.com/cloud-build/docs/quickstart-docker for how they are used. The tool you suggested will be useful once DeepVariant accepts external contributions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-414456827:168,config,config,168,,https://github.com/google/deepvariant/issues/87#issuecomment-414456827,2,['config'],['config']
Modifiability,"Gladly Stephane, and that's understandable though Docker containers usually reside under `/var/lib/docker`:. ```; $ docker info | grep Root; Docker Root Dir: /var/lib/docker; $; ```. Since Docker is container-based it doesn't see the operating system outside of its own isolated environment. That's why the `-v` parameter maps (bind-mounts) a folder from the outside, to be visible inside. Basically if you don't map in `/tmp`, the container is not aware of it at runtime, and continues to use space under `/var/lib/docker`, by creating new temporary layers of changes to its running image. In any case, give it a try and someone will be here to help you out if you run into any issues. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-1625516054:551,layers,layers,551,,https://github.com/google/deepvariant/issues/296#issuecomment-1625516054,1,['layers'],['layers']
Modifiability,"Got it. Thanks for the context! If you end up tweaking the config, let me know whether it works for you or not.; I'll close this issue now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/271#issuecomment-586523576:59,config,config,59,,https://github.com/google/deepvariant/issues/271#issuecomment-586523576,1,['config'],['config']
Modifiability,"HI @pichuan, . I trained on a new dataset and run into similar issue. This time there are files created in checkpoint but I still get the same error. Only the first epoch has low tune/categorical_accuracy and the next remaining epoch the accuracy higher than 0.9. I attached the log file here ; [train_041924.log](https://github.com/google/deepvariant/files/15082130/train_041924.log). Here is the parameter I used to train: ; ```-config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=32 \; ```. Would you take a look and let me know what's going wrong? Thank you",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073289661:431,config,config,431,,https://github.com/google/deepvariant/issues/802#issuecomment-2073289661,4,['config'],['config']
Modifiability,"Hello @pichuan ,. Thank you for your response! Right now I get the right bed file from my data provider. It seems far better: ~320k calls in a single file, while ~420k after joint calling with glnexus and DeepVariantWES config. I remember the first time I got this data Deepvariant has much less calls than GATK, but then calling was made by our data provider and I don't possess any details on that. I can't remind the exact number, but I think it was far below 300k calls. Most of my comparison was based on that *legacy* data. Sorry for that unfair and unchecked comparison. In case you are interested I could try to generate a fresh GATK calling with the same bed file according to the GATK best practices. Otherwise I could let you know if I give a try to GATK in the future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/501#issuecomment-1004229605:220,config,config,220,,https://github.com/google/deepvariant/issues/501#issuecomment-1004229605,1,['config'],['config']
Modifiability,"Hello Andrew,. I am having the same problem... I am trying to call SNPs in a species with a very high density of SNPs, and short read callers do not perform well, they miss a lot of variants in cohort samples, resulting in ridiculously high mutation rates, or the opposite they reject everything as mapping errors. I am now turning to long reads... I have the hope to obtain a solid golden reference set. That might be useful for training a DeepVariant model with short reads. I will definitely let you know if I succeed (but that's a big IF). Thanks; Alex. Sent with [Proton Mail](https://proton.me/) secure email. ------- Original Message -------; On Wednesday, June 14th, 2023 at 18:59, Andrew Carroll ***@***.***> wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772:1176,extend,extended,1176,,https://github.com/google/deepvariant/issues/661#issuecomment-1592619772,1,['extend'],['extended']
Modifiability,"Hello Andrew,. Thank you for the information! It is interesting!. chr7:54624686 A-ATC and chr7:54624683 A-AATC are different variants. I see both in my output. But still my output for chr7:54624686 A-ATC is not the same. It was not called in my proband. The one in my father looks similar but QUAL and GQ are quite different. I think it comes from the DeepTrio calling step. How can our outputs be so different? ; I use deepvariant_deeptrio-1.1.0.sif. For any case, I show all three 54624683/54624686 variants here:; My proband:; `chr7 54624683 . A AATC 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:39:22,16:0.410256:27,0,48`; `chr7 54624686 . A C 27.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:28:38:16,22:0.578947:27,0,53`; `chr7 54624686 . A ATC - no call`. My father:; `chr7 54624683 . A AATC - no call`; `chr7 54624686 . A C - no call`; `chr7 54624686 . A ATC 26.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:26:33:18,15:0.454545:26,0,44`. My GLnexus VCF:; `chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:28:28,0:50:0,90,899:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A C 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/1:38:16,22:28:27,0,53:.. 0/0:40:40,0:50:0,120,1199:.. 0/0:33:18,0:26:26,990,990:..`; `chr7 54624686 chr7_54624686_A_C;chr7_54624686_A_ATC A ATC 27 . AF=0.166667;AQ=26 GT:DP:AD:GQ:PL:RNC 0/0:38:16,0:28:27,990,990:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:33:18,15:26:26,0,44:..`. I use the following command for glnexus. I had to switch to --config DeepVariant_unfiltered as --config DeepVariantWGS filtered out most of my DeNovo calls as they tend to have QUAL of <20. `module load glnexus/1.2.7`; `glnexus_cli ; --config DeepVariant_unfiltered ; --threads $(nproc) ; $FATHER.g.vcf.gz ; $MOTHER.g.vcf.gz ; $CHILD.g.vcf.gz ; 	| bcftools view - | bgzip -c > ${FAMILY}.deeptrio.vcf.gz`. Best regards,; Maria.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-839088398:1535,config,config,1535,,https://github.com/google/deepvariant/issues/440#issuecomment-839088398,3,['config'],['config']
Modifiability,"Hey there,. I was able to get a training started up on GCP today. Here is what I did, hopefully it is helpful:. First I created an instance using the command-line from the doc you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:844,variab,variables,844,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['variab'],['variables']
Modifiability,"Hey! I managed to do this using the batch. This was my script that worked (it takes a file which has a list of the ID files and picks them out to insert into the file and then runs each one as a batch! - you can adapt this to be more efficient obviously!) - I just left my last used file names in for ease with copy and paste. ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p htc; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB; #SBATCH --qos=maxjobs100. module purge; module load parallel; module load singularity. EXOME_IDs_FILE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/RAW_input/NG0921_sampleIDs; HG38_REFERENCE=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/bwa/index/GCA_000001405.15_GRCh38_no_alt_analysis_set_refandindex/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; PICARDMARKDUPLICATES_SORTEDBAM=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/picard/markduplicates/markedduplicates/{}PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analysis_set.bam; BED_REGIONS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/bed/Agilent-SureSelect-XT-Reagent-Kit-Human-AllExon-V6-hg38.bed; OUTPUT_VCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/vcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.vcf.gz; OUTPUT_GVCF=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/gvcf/{}PE_output_GCA_000001405.15_GRCh38_no_alt_analysis_set.g.vcf.gz; INTERMEDIATE_RESULTS=/scratch/c.c21087028/Polyposis_Exome_Analysis_NG0921/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_I",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717#issuecomment-1770571154:212,adapt,adapt,212,,https://github.com/google/deepvariant/issues/717#issuecomment-1770571154,1,['adapt'],['adapt']
Modifiability,"Hi @AndrewCarroll , . Thanks for the detailed reply. This is still kind of in early days. Our team is mostly from the National Museum of Natural History so we all work on a variety of taxa from snakes to plants. At this point we were trying to figure out the best non-model group(s) to start with because if we could start with a taxon with genome trios sequenced (or I suppose WES) then we could try the technique from the mosquito paper. But, in most cases with non-model groups we don't have any trios, and often we just have one representative genome that at times is not that well resolved. In terms of choosing test groups we could try one with genome characteristics more like humans, and then test of something more different. ; I certainly like the second suggestion of sampling the GIAB data, and if the genome in question is not too different then it could be a good approach. One thing we are thinking about is how far out evolutionarily you can extend the model we train. In a lot of pop gen and phylogenetic studies sampling is interspecific, and so we are also thinking about trying to evaluate how well a model would work across a clade or genus. The fact that DV worked well on mice is encouraging in this respect. I will keep you posted if we come up with any breakthroughs!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459#issuecomment-858159795:958,extend,extend,958,,https://github.com/google/deepvariant/issues/459#issuecomment-858159795,1,['extend'],['extend']
Modifiability,"Hi @AndrewCarroll ,. I followed the instructions to merge gvcf file into a final vcf via GLnexus with the default parameters like this:. `singularity exec glnexus.sif glnexus_cli --config DeepVariantWGS $gvcf_path/*.gvcf.gz > ${output_bcf}`. But it only output 62409 SNPs in the final vcf file (pepper.merged.glnexus.vcf.gz 6.8M), there are 5 input gvcf files (each of one is about 11GB, the sample is from the whole genome of pig). . the below is the log from GLnexus. > INFO: Convert SIF file to sandbox...; > WARNING: underlay of /etc/localtime required more than 50 (77) bind mounts; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] glnexus_cli release v1.4.1-0-g68e25e5 Aug 13 2021; > [71420] [2024-04-03 09:10:42.182] [GLnexus] [info] detected jemalloc 5.2.1-0-gea6b3e973b477b8061e0076bb257dbd7f3faa756; > [71420] [2024-04-03 09:10:42.183] [GLnexus] [info] Loading config preset DeepVariantWGS; > [71420] [2024-04-03 09:10:42.190] [GLnexus] [info] config:; > unifier_config:; > drop_filtered: false; > min_allele_copy_number: 1; > min_AQ1: 10; > min_AQ2: 10; > min_GQ: 0; > max_alleles_per_site: 32; > monoallelic_sites_for_lost_alleles: true; > preference: common; > genotyper_config:; > revise_genotypes: true; > min_assumed_allele_frequency: 9.99999975e-05; > snv_prior_calibration: 0.600000024; > indel_prior_calibration: 0.449999988; > required_dp: 0; > allow_partial_data: true; > allele_dp_format: AD; > ref_dp_format: MIN_DP; > output_residuals: false; > more_PL: true; > squeeze: false; > trim_uncalled_alleles: true; > top_two_half_calls: false; > output_format: BCF; > liftover_fields:; > - {orig_names: [MIN_DP, DP], name: DP, description: ""##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\""Approximate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Al",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:181,config,config,181,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,3,"['config', 'sandbox']","['config', 'sandbox']"
Modifiability,"Hi @AndrewCarroll . Thanks for pointing out the preprint, I hadn't come across that yet so it helped a great deal understanding the differences between the parent-child models and the training. Glad to hear that the `DeepVariant_unfiltered` config is now the recommended preset. Many thanks,; Macabe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475#issuecomment-898812475:241,config,config,241,,https://github.com/google/deepvariant/issues/475#issuecomment-898812475,1,['config'],['config']
Modifiability,"Hi @Axze-rgb ,. instead of setting INPUT_DIR=""${PWD}"" please provide the absolute path. You can also do echo $INPUT_DIR to make sure the variable is set correctly. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/685#issuecomment-1646860852:137,variab,variable,137,,https://github.com/google/deepvariant/issues/685#issuecomment-1646860852,1,['variab'],['variable']
Modifiability,"Hi @Axze-rgb . We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one. The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1591660025:407,extend,extended,407,,https://github.com/google/deepvariant/issues/661#issuecomment-1591660025,1,['extend'],['extended']
Modifiability,"Hi @Axze-rgb,. So we know that this organism replicates through clonal inheritance to transfer of mutations to daughter cells that would be driven by genetic drift or selection forces, and usually not by lineages. Though you say you noticed genetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inherit",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:71,inherit,inheritance,71,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876,2,"['evolve', 'inherit']","['evolve', 'inheritance']"
Modifiability,"Hi @Axze-rgb,. You did pretty awesome, in terms of how far you got! Kishwar's suggestions are perfect, and always never hesitate to just post here if you feel you are spending too much time. It also took me time to understand the DeepVariant ecosystem, but once you see how it all works together it becomes a joy to use. Basically just freely ask, and someone here can get you there quicker :). You did great, and only a few minor things:. $`1)`$ So you are correct that you will need the index file, as DeepVariant uses [Nucleus](https://github.com/google/nucleus), which in turn uses [HTSlib](https://github.com/samtools/htslib/blob/master/hts.c#L4508-L4559), and will complain with an error like this (when trying to locate it):. ```; [E::idx_find_and_load] Could not retrieve index file for '/input/mysorted.bam'; ```. Maybe this helps, so regarding the types of files DeepVariant needs is in the following document:. https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md. $`2)`$ The reason the first time it failed is because the BAM variable wasn't referenced with ${BAM}, and was written as `--reads=/input/BAM`, which is totally understandable to overlook. I've done that myself too many times to count :). $`3)`$ Regarding the dry run mode, that [just prints the commands](https://github.com/google/deepvariant/blob/r1.5/scripts/run_deepvariant.py#L492-L497) without actually running them. . Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/685#issuecomment-1646901356:1061,variab,variable,1061,,https://github.com/google/deepvariant/issues/685#issuecomment-1646901356,1,['variab'],['variable']
Modifiability,"Hi @BowenKwan you can try modifying the `PILEUP_DEFAULT_WIDTH` contant in [this file](https://github.com/google/deepvariant/blob/r0.10/deepvariant/dv_constants.py#L41). I didn't try this myself, so some additional changes may be needed, but this is a good place to start. For local training, copying the data files locally and updating paths makes sense. Some other changes you will need are below. Does the machine you plan to use have a GPU?. * Run the `model_train` and `model_eval` binaries directly, rather than running via Docker. Examples on how to use binaries directly are in [this WES case study script](https://github.com/google/deepvariant/blob/r0.10/scripts/run_wes_case_study_binaries.sh). DeepVariant comes with scripts to build binaries on Ubuntu, with Ubuntu 16 recommended. Binaries can only be built for a UNIX-based OS. Depending on what system you are using, you will need to modify these scripts. If possible, I would suggest using Docker as that will have the simplest setup. * Currently, we use [DataflowRunner](https://beam.apache.org/documentation/runners/dataflow/) to [shuffle the generated TFRecords](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each). You will probably want to use some other runner here, such as [DirectRunner](https://beam.apache.org/documentation/runners/direct/), since DataflowRunner is for use with Google Cloud.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291#issuecomment-607407000:1268,config,configuration-file-for-each,1268,,https://github.com/google/deepvariant/issues/291#issuecomment-607407000,2,['config'],['configuration-file-for-each']
Modifiability,"Hi @Fred-07,. As @akolesnikov mentions, the reason that the reads are re-aligned in this way has to do with some fundamental aspects of how alignment penalties work. The alignment method, Smith Waterman, has alignment score penalties for gap open and gap extend. These penalties make the alignments you see as scoring higher than ones that open and extend a gap early in the repeat. Those alignment penalties are determined from looking at mismatch, insertion, and deletion rates across large amounts of homologous sequences. They work very well for high sequence complexity regions which have standard types of DNA replication error. In this highly repetitive context, the ways that sequence (specifically repeats) can be extended or deleted is a bit different. It might, in theory, be possible to derive better gap extend scores for regions like this, but this is both very hard and a more fundamental problem. I don't think this case is one we can easily solve without very extensive work. . I hope that the @akolesnikov suggestion of turning off realigner in regions of interest will be satisfactory. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/763#issuecomment-2148541698:255,extend,extend,255,,https://github.com/google/deepvariant/issues/763#issuecomment-2148541698,4,['extend'],"['extend', 'extended']"
Modifiability,"Hi @GaianX39,. Did you use WhatsHap to improve accuracy? The recommended way is to run `DeepVariant -> WhatsHap -> DeepTrio`, as [noted here for adding an additional signal of information for variant qualification](https://github.com/google/deepvariant/issues/689#issuecomment-1660748817). DeepTrio is complex, as it combines the child and parent information together, since models were trained with the assumption that the child resides in the middle between the two parents (as in the pileup image shown below):. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). When that happens, candidate alleles that were generated by `make_examples` could have lower quality probabilities (going through the model), as by design it is selected across all samples (which might not yield high probability for a genotype). Truth set just means what expected variants from a gold standard you might have for your study, or in case of DeepTrio's that were [used to train the model](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#training-set), as listed here:. https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-details-training-data.md. Another way you can compare against is to use `DeepVariant -> WhatsHap -> DeepVariant -> GLnexus` [as shown here](https://google.github.io/deepvariant/posts/2021-02-08-the-haplotype-channel/). The truth sets are there based on expected results for a controlled experiment. If you know the variants do not follow normal Mendelian inheritance patterns, such as de novo ones, then you would need more replicate samples to validate against - which might also require validation via other assays. Let me know where I should add more clarification, as there are many ways to expand on this. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704#issuecomment-1705867623:1556,inherit,inheritance,1556,,https://github.com/google/deepvariant/issues/704#issuecomment-1705867623,1,['inherit'],['inheritance']
Modifiability,"Hi @JakeHagen . Thank you for the report, and for including the quality readout from the HTML file. One thing I want to mention is that this distribution is something that we have seen in some samples - see Figure 1 of [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144). In this figure, some of the analyzed cohorts do have bimodal GQ distributions for DeepVariant calls, while others (e.g. GIAB) do not. Supplementary Figure 3 of that paper indicates that a reasonable component of the bimodal distribution relates to sequence depth, at lower sample sequence depths, GIAB becomes more bimodal. I believe that we internally stratified calls and (though my memory is hazy) found that another factor in the bimodal distribution is whether a site is HET or HOM. Specifically, HET sites with lower depth have lower GQs, and I believe the explanation for this is that as coverage drops, it can become difficult to tell a HET site from either a REF or HOM, while HOM sites have more effective signal for them as non-REF. I don't think that the model is likely to be less confident in 100bp reads because they are not as much of the training data, but I expect the fact that 100bp reads are harder to uniquely map and will results in more variability in the coverage of high-MAPQ reads would indirectly contribute.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/586#issuecomment-1320629275:1321,variab,variability,1321,,https://github.com/google/deepvariant/issues/586#issuecomment-1320629275,1,['variab'],['variability']
Modifiability,"Hi @MariaNattestad ,. Thank you for the reply,; 1) NextFlow pipeline is written by me. 3) I ran with Chr20, below is the log, 4) in NextFlow Input out we can give any name for the files as input/out.; ```; Jun-08 12:02:05.261 [main] INFO nextflow.cli.CmdRun - Launching `dv.nf` [spontaneous_wright] DSL2 - revision: fbe7d83e44; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins default=[]; Jun-08 12:02:05.261 [main] DEBUG nextflow.plugin.PluginsFacade - Plugins resolved requirement=[]; Jun-08 12:02:05.268 [main] DEBUG nextflow.secret.LocalSecretsProvider - Secrets store: /home/kiran.patil/.nextflow/secrets/store.json; Jun-08 12:02:05.271 [main] DEBUG nextflow.secret.SecretsLoader - Discovered secrets providers: [nextflow.secret.LocalSecretsProvider@58472096] - activable => nextflow.secret.LocalSecretsProvider@58472096; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Session UUID: 1e83b778-2b0d-4f02-9875-bf3b18b4a30a; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Run name: spontaneous_wright; Jun-08 12:02:05.320 [main] DEBUG nextflow.Session - Executor pool size: 96; Jun-08 12:02:05.330 [main] DEBUG nextflow.util.ThreadPoolBuilder - Creating thread pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.execut",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:370,plugin,plugin,370,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,8,"['Plugin', 'plugin']","['Plugins', 'PluginsFacade', 'plugin']"
Modifiability,"Hi @Npaffen . This is an interesting question. One advantage is that is these sites give a complete accounting of all positions that the neural network acts on. This makes it easier for us to debug issues (either internally or user-presented) as we remove the variable of whether a row in the VCF was not present because a candidate was not generated, or because the neural network decided it was reference. From a developer perspective (whether internal or external), this is much cleaner. . There is another advantage in allowing users to determine the ideal threshold for sensitivity (e.g. if they highly value sensitivity, they may want to consider positions which are ./. calls of low GQ). These are more advanced use cases and it does seem that most users opt for simpler approaches. . I also tend to think that including these variants won't confuse users, and that they would be used to variant callers using things like the FILTER field to indicate non-variant calls. But if you have a strong opinion that this will be broadly confusing, I will take that into account.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347:260,variab,variable,260,,https://github.com/google/deepvariant/issues/666#issuecomment-1611797347,2,['variab'],['variable']
Modifiability,"Hi @PengJia6 . To your question about whether DeepTrio could operate on a quartet. The framework for DeepTrio could be extended to quartets. This is something that we had talked about when building DeepTrio. However, we'd need to get extensively characterized training data for this, and it would take additional work to train this model. In the end, we concluded that there aren't going to be enough people with quartets that it justified extending DeepTrio for this use case at that time (consider all the other things we are working on for DeepVariant and DeepConsensus). It is something that we might revisit someday, but not for the near future.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660#issuecomment-1594251244:119,extend,extended,119,,https://github.com/google/deepvariant/issues/660#issuecomment-1594251244,2,['extend'],"['extended', 'extending']"
Modifiability,"Hi @RaphaelSanchesUSP . It will likely be very difficult to do so regardless of the approach. However, I'd like to better understand what sort of output you would look for? Would it be to extend DeepVariant's output classes beyond REF, HET, HOM into more than diploid output, or is it instead to train on polyploid species and repurpose HET to be (some number of non-ref and non-hom alleles without further specification)?. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/562#issuecomment-1234776784:188,extend,extend,188,,https://github.com/google/deepvariant/issues/562#issuecomment-1234776784,1,['extend'],['extend']
Modifiability,"Hi @Taghrid-M,. This is good! One small thing, I think the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:680,layers,layers,680,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177,2,['layers'],['layers']
Modifiability,"Hi @WeiweiBian . In our discussion over email, I recommended that you use a somatic caller instead of trying to adapt DeepVariant to your needs, and I still think this is the best way to go. To answer your questions:; 1. No, this is a limit within Inception V3 that DeepVariant uses.; 2. We don't have any other training tutorials for other systems.; 3. We have done some exploratory work on a somatic variant caller, but it is not available yet, and it is meant for tumor-normal pairs with much lower coverage and higher allele frequency. It will not be possible to use this for your case. I think you would be much better off using a somatic variant caller for your research.; If you want to take transforming DeepVariant into a somatic caller on as a research project, you are welcome to do so, since it's open source. But we unfortunately don't have the bandwidth to guide you very much, since we have other exciting improvements to DeepVariant that we are already working on. Best of luck with your work!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308#issuecomment-628304654:112,adapt,adapt,112,,https://github.com/google/deepvariant/issues/308#issuecomment-628304654,2,['adapt'],['adapt']
Modifiability,"Hi @Wenfei-Xian . I finished the experiments. There is certainly a noticeable effect from MAPQ limits, more than I expected. For my experiment, I rewrote the BAM file, setting the MAPQ to 60 for any read with MAPQ of 36 or higher (I observed 44 as the highest MAPQ value an more variability to MAPQ values than seen with BWA. |Experiment|SNP Recall|SNP Precision|SNP F1|INDEL Recall|Indel Precision|Indel F1|; |------------|----------|--------------|--------|------------|---------------|--------|; |Default BAM|0.9673|0.9967|0.9817|0.9717|0.9956|0.9835|; |MAPQ 36+ -> 60|0.9758|0.9964|0.9859|0.9829|0.9960|0.9894|. This implies you will get better performance with DeepVariant if you set those higher MAPQ values to 60. Note that in general, DeepVariant hasn't been trained with Bowtie2 data and you'd likely get better performance overall by a re-training for it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/809#issuecomment-2067922240:279,variab,variability,279,,https://github.com/google/deepvariant/issues/809#issuecomment-2067922240,1,['variab'],['variability']
Modifiability,"Hi @X1angyang . The model is InceptionV3. You can see the layers of one of the DeepVariant models like this:; ```; import tensorflow as tf. !gsutil cp gs://deepvariant/models/DeepVariant/0.10.0/DeepVariant-inception_v3-0.10.0+data-wgs_standard/model* /tmp/; checkpoint_path = '/tmp/model.ckpt'. reader = tf.compat.v1.train.NewCheckpointReader(checkpoint_path); shape_map_for_layers = reader.get_variable_to_shape_map(); print(shape_map_for_layers); ```; I just tested that in Colab (https://colab.research.google.com/). However, reimplementing all of DeepVariant from bam to output VCF would be a huge project. If you are interested in something smaller to get started, I'd like to bring this blog post to your attention: https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/.; It has an associated Colab notebook that walks through some smaller but still challenging examples of how to use genomic data in machine learning using TensorFlow and Nucleus. I hope that helps!; Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328#issuecomment-663252998:58,layers,layers,58,,https://github.com/google/deepvariant/issues/328#issuecomment-663252998,2,['layers'],['layers']
Modifiability,"Hi @aderzelle ,; I think you can use the TensorFlow config string to constraint the CPU use of the call_variants step. For example, this discussion talked about how to run TF on one core:; https://medium.com/@liyin2015/tensorflow-cpus-and-gpus-configuration-9c223436d4ef. If you're running call_variants directly, you can add a flag like `--config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""`; For example:. ```; sudo docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --config_string ""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1""; ```. With this extra arg, I do see that:; ```; I1015 19:57:25.812590 140110048159488 estimator.py:212] Using config: {'_model_dir': '/tmp/tmpt8af___k', '_tf_random_seed': None, '_save_summary_steps': 10; 0, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {; key: ""cpu"" ; value: 1; }; intra_op_parallelism_threads: 1; inter_op_parallelism_threads: 1; ```; But I'm not 100% sure that TensorFlow actually fully follow the instructions here or not. If you're using the one-step script (`run_deepvariant`), you can add:; `--call_variants_extra_args=""config_string=\""device_count {key: 'cpu' value: 1} intra_op_parallelism_threads:1 inter_op_parallelism_threads:1\""""`",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/361#issuecomment-709562825:52,config,config,52,,https://github.com/google/deepvariant/issues/361#issuecomment-709562825,3,['config'],"['config', 'configuration-']"
Modifiability,"Hi @aderzelle . Thank you, this is a good question. We have observed this phenomenon as well. The answer is somewhat complicated. . DeepVariant seems to have learned something about the concept of segmental duplication, where positions that appear to be variants are actually due to mismapping of similar regions which may (or may not) be captured in the reference genome. The way this manifests in a genome pileup is as one phased haplotype that is mostly reference and (one or more) phased haplotype that is variant-dense. The signal for this is further enhanced when the VAF is closer to 0.33 or 0.25 (more directly suggesting copy number 3 or 4), but it can also occur close to 0.5 (which can still indicate a copy number of 4). These regions can be variants in thee diploid genome that are incorrectly called as REF, or they could be markers of a copy number variant. In human genomes, this can suggest a user look into that region for either known copy number variants or coverage differences. One question to ask - are these regions at generally higher coverage than you would expect? . In certain variant-dense species, we have observed this phenomenon to complicate calling [in this blog we investigate this for mosquito genomes](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). In this blog, we show the ability to re-train for a variant-dense species using a pedigree. If you have a pedigree available, we could also explore this with you. We are working on ways that will allow DeepVariant to more explicitly indicate when it thinks this is the case, and to provide more information (e.g. average coverage in the sample) to DeepVariant that will allow it to better separate variants from makers of segmental duplication.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/266#issuecomment-580713806:556,enhance,enhanced,556,,https://github.com/google/deepvariant/issues/266#issuecomment-580713806,2,['enhance'],['enhanced']
Modifiability,Hi @aderzelle ; The flag option I gave was just an example.; Maybe use_per_session_threads in the config proto can be relevant. I haven't used this option before so I can't be completely sure. ; Are you trying to limit to just one thread?,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/271#issuecomment-586441972:98,config,config,98,,https://github.com/google/deepvariant/issues/271#issuecomment-586441972,1,['config'],['config']
Modifiability,"Hi @aderzelle ; This is not unexpected:; the relationship between n_shards and how many processes is not 1:1 in the `call_variants` step. In `call_variants` step, the parallelism is decided by TensorFlow, which can be controlled by the `config_string` flag of call_variants. If you're using the run_deepvariant script, you should be able to add a flag like this:; `--call_variants_extra_args config_string=""gpu_options: {per_process_gpu_memory_fraction: 0.5}""` to specify your config_string. I think this is the options you have: https://www.tensorflow.org/api_docs/python/tf/compat/v1/ConfigProto; (I can't find a better documentation, though. @gunjanbaid do you know?). @aderzelle - Let me know if this works. I usually don't specify this because the default setting works well for my own use case. It'll be great if you share what your use case is, and let us know whether you're successful by setting this flag.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/271#issuecomment-586402168:586,Config,ConfigProto,586,,https://github.com/google/deepvariant/issues/271#issuecomment-586402168,1,['Config'],['ConfigProto']
Modifiability,"Hi @aderzelle, thank you for your question. I believe this is coming from the cohort merging step (by GLnexus) assuming that you used `--config DeepVariantWGS` or `--config DeepVariant` for merging, not the DeepVariant itself. The merging step by GLnexus includes filtering alleles and revising genotypes and we have conducted an extensive study [1] on finding the best merging parameters for DeepVariant outputs, which was later pushed to the open-source GLnexus. The current version of the merging parameters uses `min_AQ1 = min_AQ2 = 10` (AQ means ""allele quality"") as you can see here https://github.com/dnanexus-rnd/GLnexus/blob/4d057dcf24b68b33de7a9759ae65ca2b144a3d47/src/cli_utils.cc#L874 The definitions of these parameters can be found at https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration . I believe this is the reason why you are seeing the sharp drop at `GQ = 10` as the alleles corresponding to them were already filtered. If you'd like to merge DeepVariant gVCFs without any filtering or genotype revision, you can download the `.yml` file here https://gist.github.com/tedyun/1d4f57ca67fb18647b7b251f9e0b35c2 and use `--config DeepVariant_nomod.yml` instead when running GLnexus. I hope this helps and please let us know if you have any more questions/comments. [1] https://doi.org/10.1101/2020.02.10.942086. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/326#issuecomment-659742784:137,config,config,137,,https://github.com/google/deepvariant/issues/326#issuecomment-659742784,4,"['Config', 'config']","['Configuration', 'config']"
Modifiability,"Hi @anands-repo . There are a large number of differences in the error profile of WGS and WES, including capture kit efficiency, additional errors from PCR in preparation, differences in the amount of on-target reads, greater coverage variability in exomes, and probably many other factors that are not completely understood. This paper: https://www.pnas.org/content/112/17/5473 is probably a good place to start on some of the factors that differ between the assays. For deduplication, we use Picard MarkDuplicates as run by GATK. We observe only very negligible differences in variant call quality with and without MarkDuplicates, which only become observable at lower coverages (15x-22x). This is one reason we indicate MarkDuplicates as an optional step in our BestPractices.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/329#issuecomment-663696939:235,variab,variability,235,,https://github.com/google/deepvariant/issues/329#issuecomment-663696939,1,['variab'],['variability']
Modifiability,"Hi @anands-repo . This is something that we observe in training as well, and has also been reported by other users - [see this GitHub issue](https://github.com/google/deepvariant/issues/185) for deeper discussion. In short, this occurs because not all variables are loaded when warmstarting a model. Retraining does quickly re-learn, but this is the reason for the initial drop. You should still be able to train models to high accuracy, despite this phenomenon.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/383#issuecomment-727745570:252,variab,variables,252,,https://github.com/google/deepvariant/issues/383#issuecomment-727745570,2,['variab'],['variables']
Modifiability,"Hi @anands-repo . With respect to your second point - The starting training data begins from various coverages, ranging roughly from 27x-60x. From the starting coverage of the BAM files, downsampling is applied in 0.1 increments until the coverage would reach around 20x. As a result, higher coverage ranges are slightly less represented than the 30x-20x range, but not by a substantial amount. There are two main purposes of this - the first is to allow DeepVariant to perform well across many different coverages. The second is to ensure that there are many hard examples to learn from. Our strategy in downsampling is not fixed - for example, as we add more training data in the future, we may decide to have fewer downsample increments to generally keep the same range of examples. The coverage range has also evolved over time, for example in v0.7, the range was roughly 60x-30x. Extending the range to 20x caused a very small decline in accuracy at 50x (and also therefore the case study), but resulted in more substantial gains in the 20x-30x range. I would stress that there isn't one correct downsampling strategy, as long as it represents a diversity of coverages and helps create hard examples.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230#issuecomment-546050008:814,evolve,evolved,814,,https://github.com/google/deepvariant/issues/230#issuecomment-546050008,3,"['Extend', 'evolve']","['Extending', 'evolved']"
Modifiability,"Hi @andrewrech ; I'll be closing this issue.; To add on the previous answer about `TF_CUDA_VERSION`: currently run-prereq.sh has multiple paths to install tensorflow. If you end up building tensorflow from scratch it self, the env variable `TF_CUDA_VERSION` might be picked up by that. Internally we don't really use that code path anymore so I'm not sure if it actually still works. I'll make a note to simplify and clean up run-prereq.sh in the future. Please feel free to open another bug if you have more questions. If you have more suggestions regarding this particular issue, feel free to follow up here as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-467081816:231,variab,variable,231,,https://github.com/google/deepvariant/issues/145#issuecomment-467081816,2,['variab'],['variable']
Modifiability,"Hi @avilella . DeepVariant has been used on MGI datasets, both using the standard Illumina model, as well as retrained models. There is some complexity that the MGI/BGI technologies have evolved over time, so some demonstrations may not reflect the newest methods. The general finding is that the Illumina models tend to work well for MGI data, though we find examples of retraining for certain datasets improve further. Our [advanced training tutorial ](https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-training-case-study.md) walks through retraining an Illumina model for data from BGISEQ 500 and [this comparison](https://blog.dnanexus.com/2018-07-02-comparison-of-bgiseq-500-to-illumina-novaseq-data/) was conducted several years ago using the out-of-the-box Illumina model. If you know of any genome in a bottle sequencing datasets that are available from more recent MGI platforms, I'd be interested in pointers to those locations. I would be quite curious to see how the technology has evolved over the last several years.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/538#issuecomment-1138272184:187,evolve,evolved,187,,https://github.com/google/deepvariant/issues/538#issuecomment-1138272184,2,['evolve'],['evolved']
Modifiability,"Hi @claudiologiudice ,; You can find the answers in this older thread: https://github.com/google/deepvariant/issues/115; Our answer would still be the same. I think some of our users have adapted DeepVariant to run on RNAseq data. If some of them would like to share here, that will be great. But we don't have an official solution right now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/283#issuecomment-599859404:188,adapt,adapted,188,,https://github.com/google/deepvariant/issues/283#issuecomment-599859404,1,['adapt'],['adapted']
Modifiability,"Hi @crazysummerW,. That might be tricky as its deeply encoded in the pileup image construction - and how it's processed - which would would require some code-rewrite to extract it properly. If you have reads that don't span more than 95 for that specific variant position - or you downsample to 95 - you can reconstruct that from the BAM or DeepVariant realigned BAM files. . There might be other avenues I haven't thought about yet, though I'll keep thinking. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/707#issuecomment-1715948232:158,rewrite,rewrite,158,,https://github.com/google/deepvariant/issues/707#issuecomment-1715948232,1,['rewrite'],['rewrite']
Modifiability,"Hi @cwarden45, just to make sure I understand correctly, does your time estimate of 24 hours on Google Cloud include upload time to the machine?. 1a) ; The 5 hour runtime is based on performance on a specific set-up: a 64-core CPU-only setup (which is not the most cost-optimized configuration for running DeepVariant). We state this runtime as a baseline, and mention that many accelerated pipelines exist that allow WGS to run as fast as 40 minutes. I want to clarify the runtimes a bit more. Previously, our estimates for speed was based on the GCP cloud-orchestration framework developed external to our team. The speed-optimized one here is 70 min. However, as there are now multiple orchestration options (including Parabricks and ATGENOMIX) we have decided to separate the single-machine runtime from what can be achieved with orchestration. I understand you are running on your own local hardware, note that the speed will be a function of how new your CPU is, the AVX acceleration used by DeepVariant runs faster on newer machines. However, your estimation of 24 hours on a 4-core machine is 96 total core-hours. This is not particularly slow for DeepVariant, in fact, it is fast. The DeepVariant WGS case study finishes in 242 core-hours, though it is probably possible to finish somewhat more efficiently on a smaller machine. However, if you are able to run GATK Best Practices in 96 CPU hours, this would be surprising based on external estimates, which place the total compute at 300-400 CPU-hours (e.g - https://www.ibm.com/downloads/cas/LY1OY9XJ). When you say you are using an identical command on Google Cloud, are you also using identical hardware?. 1b); With respect to cost, a large factor in the cost-efficiency of DeepVariant will be whether you are using pre-emptible instances on GCP, or spot instances on AWS. Can you clarify which type of instance you are using in these tests?. The 30x WGS case study, which runs for a wall-clock time of 4.5 hours on a 64-CPU standard inst",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483448362:280,config,configuration,280,,https://github.com/google/deepvariant/issues/171#issuecomment-483448362,1,['config'],['configuration']
Modifiability,"Hi @danielecook , I was trying various things that would require least amount of effort. I ended up just skipping and using the `google/deepvariant` docker image as-is.; I'm no expert in docker, just trying to get things running. ; Then I also have the issue that singularity can't use/convert the deepvariant docker image:; ```; $ singularity build --sandbox deepvariant_1_1_0 docker://gcr.io/deepvariant-docker/deepvariant:1.1.0; WARNING: Building sandbox as non-root may result in wrong file permissions; Docker image path: gcr.io/deepvariant-docker/deepvariant:1.1.0; ERROR MANIFEST_UNKNOWN: Manifest with tag '1.1.0' has media type 'application/vnd.docker.distribution.manifest.v2+json', but client accepts 'application/json'.; Cleaning up...; ```; This may be my inexperience in these things, but I'm simply having trouble getting them running.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445#issuecomment-822613451:352,sandbox,sandbox,352,,https://github.com/google/deepvariant/issues/445#issuecomment-822613451,4,['sandbox'],['sandbox']
Modifiability,"Hi @danielecook ,; I tried without ```--debug=false``` and set ```--config.num_epochs=10``` but I still get the same error that ```--config.num_epochs=10```. I attached my log file here . [train_040324.log](https://github.com/google/deepvariant/files/14873081/train_040324.log). THis is the command I used:; ```; BIN_VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${BIN_VERSION}"". time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=s3-mount/deepvariant_training/script/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \; --config.num_epochs=10 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=512; ```. Did I miss anything?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2037432899:68,config,config,68,,https://github.com/google/deepvariant/issues/802#issuecomment-2037432899,10,['config'],['config']
Modifiability,"Hi @digitalemerge. There isn't technically anything stopping the current DeepVariant models from calling structural variants, and in fact we have seen this happen, especially with PacBio reads. The main limitation is that the current way DeepVariant identifies variants is by looking within the read alignment signatures. SVs won't usually be captured within each short read, which is why most SV callers use split read or discordant pair signatures, something DeepVariant doesn't do because it was designed for calling small variants. In long reads, DeepVariant actually does capture some larger insertions and deletions as a natural extension of calling small indels, but it isn't perfect because SVs generally don't show up as neatly in the reads as small variants do. That is why dedicated SV callers like [pbsv](https://github.com/PacificBiosciences/pbsv) have methods built-in to evaluate evidence from reads that don't match perfectly. That being said, we are exploring some strategies and experimenting with how we might extend DeepVariant to call structural variants. Of course, you and anyone else out there who is interested in experimenting with DeepVariant should also feel free to do so!. Maria",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/298#issuecomment-617918041:1029,extend,extend,1029,,https://github.com/google/deepvariant/issues/298#issuecomment-617918041,1,['extend'],['extend']
Modifiability,"Hi @ed5152 ,; Thanks for your question.; Our codebase pretty much hardcoded the number of classes as 3. Even the customized_classes option is currently just a way to fill the labels with different fields in the truth VCF, but still assumes 3 classes.; In the future we might support more flexible number of classes, but this is not currently on our roadmap.; Our codebase also doesn't currently support just retraining the final layer. Our codebase uses an old library https://github.com/google-research/tf-slim . It is possible that you could modify our codebase to make it happen, but this is outside of our support for DeepVariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/495#issuecomment-981903611:288,flexible,flexible,288,,https://github.com/google/deepvariant/issues/495#issuecomment-981903611,1,['flexible'],['flexible']
Modifiability,"Hi @ekofman , ; thanks for reporting this!. It seems like you're using the example from the WGS case study. From my experience, suppose you on a 4-core machine, if you specify a $numShards bigger than 4 (say, 64), `parallel` will actually still just run 4 at a time. Therefore, it might run 0-3 first. And when they finished, `parallel` will proceed with the range of 4-7, etc. Because of that, if you run 64 shards on 4-core machine, you won't see all 64 jobs started at the same time. Can you first confirm whether that's the case for you? ; In your case, if somehow all 64 `make_examples` started on the same machine that 4 cores, it is possible that the machine starts becoming resource constrained and swapping a lot, therefore nothing will finish. But the first thing will be to check whether `parallel` handles this for you (which is my experience in the past).; @ekofman , please let me know what you observe. And, it'll also be useful to know how many cores are on your machine, and how much RAM. If you can check your system resources while you're running to see what resource might be exhausted, that will also be useful to know. If your BAM file is sharable / public, I'm also happy to run a few test runs myself. I expect changing $numShards should work, so I'd really like to know what went wrong here and fix it. Thanks!. The GCP runner code that @pgrosu mentioned is actually a separate example. The GCP runner config is evolving over time as well, so it's by no means the best (or only) config that we recommend. But it's a useful example to see how to orchestrate these jobs on multiple machines on Google Cloud. For example, the amount of RAM it requests for its workers can be a good example of how much you might need.; I'm adding @nmousavi, our collaborator on the Cloud team, as a FYI since Paul's comment mentioned the GCP runner.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150#issuecomment-460861609:1427,config,config,1427,,https://github.com/google/deepvariant/issues/150#issuecomment-460861609,2,['config'],['config']
Modifiability,"Hi @genieusbio,. Does the BED file include the 783006 region of chromosome 1, and did you also specify the `MAKE_EXAMPLE_ARGS` variable? Version 1.1 is a bit old, but I understand why it is necessary from the code for `vcf_candidate_importer.py`, as the `get_candidate_positions()` function body is not necessary. . Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1718654990:127,variab,variable,127,,https://github.com/google/deepvariant/issues/708#issuecomment-1718654990,1,['variab'],['variable']
Modifiability,"Hi @gunjanbaid I do not see the same issue now. I switched to openjdk-11. However, I still need to pass the --host_javabase option, which is not present by default in the build scripts. I do see another issue. The trace is as follows; ```; (02:26:24) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:413:1: ClifProtoLibraryGeneration third_party/nucleus/protos/bedgraph_pyclif.h failed (Exit 1): proto failed: error executing command; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.cc -h bazel-out/ppc-opt/bin/third_party/nucleus/protos/bedgraph_pyclif.h '--strip_dir=bazel-out/ppc-opt/bin' '--source_dir='\''.'\''' third_party/nucleus/protos/bedgraph.proto). Execution platform: @bazel_tools//platforms:host_platform ; Traceback (most recent call last): ; File ""bazel-out/host/bin/external/clif/proto"", line 5, in <module>; from clif.python.proto import start ; File ""/root/opt/clif/lib64/python3.6/site-packages/clif/python/proto.py"", line 29, in <module>; from clif.python.utils import proto_util ; ImportError: libprotobuf.so.24: cannot open shared object file: No such file or directory. ```. This is for the following command from build_and_test.sh:; ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. Why is bazel running ```exec env -``` here? This is invalidating library paths (LD_LIBRARY_PATH variable) where it would find ```libprotobuf.so```. As expected, when I run the failing command from the trace without ```exec env -```, it is fine.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355#issuecomment-697093712:1565,variab,variable,1565,,https://github.com/google/deepvariant/issues/355#issuecomment-697093712,1,['variab'],['variable']
Modifiability,"Hi @hosseinvk,. Glad to hear it worked! Regarding the analysis, yes you will need to run it again given the new assignment. So there is both a parent and child model that was created, through which the tensor image (generated from your reads) is fed to provide inference about your child or parent variation. The reason you will need to run it again is because these models were trained with the assumption that the child resides in the middle between the two parents, as in the pileup image shown below. With such a trained configuration, your data would also need to be formatted with the same tensor configuration -- as provided through your assignments -- as it would be most informative when calling using the child model with the parents providing the supporting evidence -- and vice versa when processing the tensor through the parent model:. ![image](https://github.com/google/deepvariant/assets/6555937/080684de-68b9-4f8b-8c45-1625484d96af). This is provided in the [DeepTrio paper](https://www.biorxiv.org/content/10.1101/2021.04.05.438434v1.full.pdf) with additional details. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111:525,config,configuration,525,,https://github.com/google/deepvariant/issues/687#issuecomment-1654874111,2,['config'],['configuration']
Modifiability,"Hi @husamia . It looks like the error message is indicating that the disk is full, which can occur even when there is memory. Can you check how much hard disk space is present on the machine. If you are running low on space unexpectedly, you may want to check that you aren't accumulating space from Docker runs that weren't cleaned up by the system. It could be worth looking at [this page](https://docs.docker.com/config/pruning/) if that is the case.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/400#issuecomment-749251835:416,config,config,416,,https://github.com/google/deepvariant/issues/400#issuecomment-749251835,1,['config'],['config']
Modifiability,"Hi @imdanique ,; Thanks for the update.; Can try to get to a point where when you run the docker command, your ls can see the file? Because if the ls command can't list the file, that means deepvariant binary would have no chance to find the file.; Does that make sense?; To diagnose the problem, maybe simplifying the script or script to remove the use of variables could help too, in case they were not set correctly. https://docs.docker.com/storage/volumes/ might also be helpful to read.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624:357,variab,variables,357,,https://github.com/google/deepvariant/issues/577#issuecomment-1285622624,2,['variab'],['variables']
Modifiability,"Hi @jaqueytw . To combine DeepVariant gVCFs together, we currently recommend using [GLnexus](https://github.com/dnanexus-rnd/GLnexus) with the --config deepvariant flag. This should combine DeepVariant gVCFs in a valid manner without filtering or re-genotyping. We have used GLnexus in this way in some of our early cohort investigations (for example in [this blog](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/) on a mosquito pedigree). We are currently in the process of more fully evaluating the optimal ways to use GLnexus to merge DeepVariant calls. As this functionality matures, we will update our recommendations. Finally, to clarify our recommendations regarding GATK-based methods to combine gVCFs. Although these may run, in these sense that the gVCF format used by DeepVariant can be successfully used, we do not currently recommend their use to combine DeepVariant gVCFs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/45#issuecomment-482355184:145,config,config,145,,https://github.com/google/deepvariant/issues/45#issuecomment-482355184,1,['config'],['config']
Modifiability,"Hi @jaspez yeah my apologies - when I read your comment more carefully, it seemed like you used `1`, so I removed my previous answer. `--gvcf_gq_binsize=1` will only merge records when they have identical GQ values, so there would be no loss of information at least in terms of GQ, but as you already saw this will not be exactly the same as what you specified (single line per position). Unfortunately we do not currently have an option to keep every single non-variant position, as this would make the gVCF files extremely large. In general I'd recommend not doing this anyway, since this will eventually slow down any downstream processing of the gVCFs such as merging gVCFs using GLnexus, etc. I think the easiest way to achieve this would be by post-processing the gVCFs generated with `--gvcf_gq_binsize=1`. For example, the `break_blocks` option in [gvcftools](https://sites.google.com/site/gvcftools/home/configuration-and-analysis) seems to do this, based on an answer in this [forum](https://www.biostars.org/p/136461/). May I ask what is your use case that prefers every non-variant position to be written?. Best,; Ted",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/282#issuecomment-954067904:913,config,configuration-and-analysis,913,,https://github.com/google/deepvariant/issues/282#issuecomment-954067904,1,['config'],['configuration-and-analysis']
Modifiability,"Hi @jumpyknight ,. We have also noticed that when warmstarting from a checkpoint, there seems to be dip in accuracy at the beginning. You can see an example like this in this plot in our training case study from r0.7:; https://raw.githubusercontent.com/google/deepvariant/r0.7/docs/images/TensorBoardAccuracy.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:813,variab,variables,813,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,4,['variab'],['variables']
Modifiability,"Hi @karoliinas ,. The DeepTrio model we trained and provided wasn't trained in the condition you described. So it won't work if you try to apply our model that way. ; DeepVariant is a general framework that could be extended to multiple samples. (DeepTrio is basically an extension as a 3-sample model, where we trained two models - one to predict child, one to predict parents). ; In order to create your own model with your customized semantics, you'll need to carefully create the examples and labels correctly, and train a model your own. We don't currently plan to extend the use cases for our officially released models.; If you're interested in the advanced usage (creating your own images+labels and train a model), you can look at a few pointers: https://github.com/google/deepvariant/blob/r1.4/deepvariant/multisample_make_examples.py , https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-training-case-study.md . But we won't be able to provide step-by-step instructions for each use cases. In terms of de novo - I will ask @AndrewCarroll to give you a better answer.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703:216,extend,extended,216,,https://github.com/google/deepvariant/issues/609#issuecomment-1421504703,2,['extend'],"['extend', 'extended']"
Modifiability,"Hi @kishwarshafin,. The training process starts as expected with GPU activity visible, but it abruptly stops without any error message while processing the first epoch and determining the best checkpoint metric (code snippet). This step completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** ; ```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:635,config,config,635,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,8,['config'],['config']
Modifiability,"Hi @kokyriakidis ; MKL is an important part of the speed improvement for the `call_variants` step in the v0.7 release.; We don't currently have tried on AMD, so I can't say what the performance will be like. I'd like to learn more if you try it and have some numbers to share. . In general it's pretty difficult for the DeepVariant team to give advice on hardware or fine-tuning for a specific configuration. I usually think of that as an open-ended research problem itself, because many factors could come into play. I think @pgrosu 's advice above is the best. Start documenting numbers with your current configuration, continue with various hypotheses (""can I use less RAM?"", ""am I building with the optimized flags for my setting?"", etc) and trying them out. Continue to document the numbers and see if your hypotheses got proved or disproved. If you have any observations that are different from the statement we made in our GitHub documentations, we'd really appreciate you let us know. Your original question (in the title) is interesting - I only recently heard about Tensor Cores. We've mostly rely on TensorFlow (which DeepVariant used in the `call_variants` step) to interface with various hardware (CPU/GPU/TPU) below. But as you noticed that there are still things users/developers need to know at the level of DeepVariant, such as making sure we use MKL-enabled TensorFlow version, etc. Regarding Tensor Cores, I don't know enough to answer your question yet. But I'll ask around and reply back when I hear more. For now, I'll close this issue because I think @pgrosu and I have provided enough meta-information as we can. Feel free to open another issue if you have more questions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-465225098:394,config,configuration,394,,https://github.com/google/deepvariant/issues/157#issuecomment-465225098,4,['config'],['configuration']
Modifiability,"Hi @kriestof ; Sorry for the delay of response, we are off work until tomorrow so we haven't responded. If you can help with a few clarifications that will be great:; 1. I don't expect the variants in the VCF from DeepVariant to be 10x less. Are you seeing 10x less variants even just from the VCF files?; 2. You later mentioned ""when I change glnexus config from DeepVariantWES to DeepVariant (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity)"" -- how many calls did you get from 75 samples when you used ""DeepVariantWES""?. Thanks",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/501#issuecomment-1004215320:352,config,config,352,,https://github.com/google/deepvariant/issues/501#issuecomment-1004215320,1,['config'],['config']
Modifiability,"Hi @kunmonster ,; if I understand correctly, we haven't been able to reproduce the issue on our side, therefore it has been difficult for us to help. I know you've already run `tensorflow.test.is_gpu_available()`. can you also try this and see what you see?. ```; sudo docker run --gpus all google/deepvariant:1.6.1-gpu python3 -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""; ```. When I run this on my machine, I see:. ```; [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]; ```. Given that your is_gpu_available is True, I think it'll also list something. Just want to double check here. If there's any other information that you can provide, in order for us to reproduce on our side, please let us know.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820#issuecomment-2164273245:366,config,config,366,,https://github.com/google/deepvariant/issues/820#issuecomment-2164273245,1,['config'],['config']
Modifiability,"Hi @linlin-coder,. I noticed that you were using 1.4.0 of DeepTrio, and there are Docker containers for DeepTrio 1.5.0:. [deeptrio-1.5.0](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0/images/sha256-82759ab1e1289b4ebcf5af8760a1446013ceb8e538aa1ffbf6bea5402012960c?context=explore). [deeptrio-1.5.0-gpu](https://hub.docker.com/layers/google/deepvariant/deeptrio-1.5.0-gpu/images/sha256-6876344f0cbc235909326fd7757129ed80cd9cfc6ef04251bd9d330c4301ad84?context=explore). So DeepTrio has its own models, as does DeepVariant, both of which are version-specific. Regarding DeepVariant specifically I know that the variant call probabilities are well-calibrated within each model individually, so that GLnexus can operate on them -- something that Andrew confirmed previously. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689#issuecomment-1660193654:161,layers,layers,161,,https://github.com/google/deepvariant/issues/689#issuecomment-1660193654,2,['layers'],['layers']
Modifiability,"Hi @maricatovictor . 1. Accessing the pre-logit layer (and other layers) is demonstrated in the code here: https://github.com/google/deepvariant/blob/r1.0/deepvariant/modeling.py#L1161. This is probably the best place to start experimenting if you would like to take information from within the layers for other purposes. 2. There is a new (and somewhat experimental) method to force-call on positions in a VCF. I am attaching a PDF with those instructions. Note that this feature is new, and we may not have enough bandwidth to provide full support for issues that arise in development. This might be what you mean when asking about VCF input. If you are asking whether it is possible to read in other data from FORMAT or INFO field values of a VCF, this is not yet possible, and definite plans for it are not currently on the roadmap. [(2020-09-28) Tutorial_ Force calling with DeepVariant.pdf](https://github.com/google/deepvariant/files/5440613/2020-09-28.Tutorial_.Force.calling.with.DeepVariant.pdf)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371#issuecomment-716733704:65,layers,layers,65,,https://github.com/google/deepvariant/issues/371#issuecomment-716733704,2,['layers'],['layers']
Modifiability,"Hi @meghanasp21 . I suspect that what is occurring is that previous, no longer used Docker containers from DeepVariant runs are taking up space on your filesystem. Can you run . **docker system prune** (https://docs.docker.com/config/pruning/). This should clean up those images. I will also make a note for us to remove intermediate files from within the Docker image after runs (this would only come out in future releases) which should decrease the impact of previous Docker images on storage space. Thank you,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-609147540:227,config,config,227,,https://github.com/google/deepvariant/issues/296#issuecomment-609147540,1,['config'],['config']
Modifiability,"Hi @moldach . Thank you for the header data. The chromosome names in the BAM (I, I, III, IV, V, X, MtDNA) differ from those in your original reference file (chrI_pilon, chrII_pilon), etc... This is what is causing the first error, DeepVariant requires chromosome names to match in order to run. The second error is a bit harder to diagnose, but I would guess this is because the sequence of your reference genome does not extend as far as some of the mapping positions in the BAM file. My suspicion is that the reference length is mismatched, but it might be possible that the reference sequence ends in real genomic DNA that reads are mapping to (and then extending beyond the reference length) and this causes DeepVariant to fail (I haven't seen this before, but it could be a plausible behavior). I think the safest thing is to remap the reads to what you know for sure is the same reference you will call on, and let me know if this issue remains. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292#issuecomment-608968890:422,extend,extend,422,,https://github.com/google/deepvariant/issues/292#issuecomment-608968890,2,['extend'],"['extend', 'extending']"
Modifiability,"Hi @olechnwin ,. The thresholds are usually chosen empirically. Based on what tasks we're trying to achieve, we choose it to find the best tradeoff between sensitivity and the amount of noises we bring in. This is more a research problem, especially that you're trying to adapt DeepVariant code to a different problem. So we won't be able to easily share a recipe here for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/578#issuecomment-1317450563:272,adapt,adapt,272,,https://github.com/google/deepvariant/issues/578#issuecomment-1317450563,1,['adapt'],['adapt']
Modifiability,"Hi @pgrosu ; This is a fascinating perspective on how to use variant callers here... actually, we don't ""know"" they reproduce through clonal inheritance. No one has ever seen a male or male organ in 2 centuries. And in the labs, when we see the pattern we have here, we tend to believe indeed they reproduce through clonal inheritance. . But. there are (solid) data and papers that show that when you go outside of the lab do some population genetics, the results are perfectly compatible with sexual reproduction. That doesn't mean it's happening here, of course. But we actually can't assume they are clonal. Or, we can, but we must keep in mind it's absolutely not guaranteed. . Do you think it makes sense to compare with Clair3?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650261666:141,inherit,inheritance,141,,https://github.com/google/deepvariant/issues/682#issuecomment-1650261666,2,['inherit'],['inheritance']
Modifiability,"Hi @pichuan, are you sure that doc uses GPU?. >For this case study, we used a 64-core non-preemptible instance with 128GiB and no GPU. and I didn't see GPU configuration in the link to the command you posted?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428637497:156,config,configuration,156,,https://github.com/google/deepvariant/issues/99#issuecomment-428637497,1,['config'],['configuration']
Modifiability,"Hi @pichuan,. like I mentioned before, I was also working with another tool that can make use of the GPU (SpliceAI) and I did some configurations and package installations to make this tool runable. Somehow this also fixed my problems with DeepVariant now. The error is gone and call_variants stage went through in 37 min using our GPU. Maybe there was something wrong with the CUDA drivers... I don't know. I'm very sorry for the inconveniences!. Best regards,. Sebastian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321#issuecomment-675317201:131,config,configurations,131,,https://github.com/google/deepvariant/issues/321#issuecomment-675317201,1,['config'],['configurations']
Modifiability,"Hi @pichuan,; Thanks for the response. ; As our data is not publicly available, soI tried to use the benchmark data in the following link to get the better comparison.; https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-exome-case-study.md; In the link below, you mentioned about ""sec per 100"" on the log file is .67 sec for your hardware configuration. That should be proportionally adjustable on my machine's configuration (I use an 8 core machine and 64GB memory). ; https://github.com/google/deepvariant/issues/74. But unfortunately It is not the case on my machine. I got way higher time for different runs. from 20 second to some times 1 minutes per 100.; Surprisingly singularity does not use the full memory, 64 GB made available to it.; I am confused that if I set num_shards to the number of cores for example in my case 8, it makes the process even slower than when I set it to one.; May I kindly ask how much memory normally singularity uses?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/463#issuecomment-864505178:353,config,configuration,353,,https://github.com/google/deepvariant/issues/463#issuecomment-864505178,2,['config'],['configuration']
Modifiability,"Hi @poddarharsh15 , it seems like you're certain that the DeepTrio run finished correctly.; In that case, I agree with @akolesnikov 's original assessment that this can be an issue for the downstream glnexus step, which we can't directly support. One suggestion to try:; If you need to check your run a bit more closely, maybe breaking it down to just running this part first:. ```bash; udocker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz; ```. before piping to the next step. Maybe that could help you identify what the errors are coming out from that step?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/815#issuecomment-2098938294:497,config,config,497,,https://github.com/google/deepvariant/issues/815#issuecomment-2098938294,1,['config'],['config']
Modifiability,"Hi @prasundutta87 . Both DeepVariant and DeepTrio can produce gVCFs, so you can joint call in a similar manner. In both cases you should use the DeepVariant_unfiltered preset for GLnexus, because there is family structure present which the other filtering presets wouldn't know about. Because you can joint genotype multiple trio gVCFs from either DeepVariant or DeepTrio in the same way, I would use DeepTrio to produce the gVCFs, take all of the gVCFs and run them together through glnexus, and then you can still use allele frequency information. To be clear, the unfiltered preset looks like this:. ```; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/child_trio_1.g.vcf.gz\; /output/parent1_trio_1.g.vcf.gz \; /output/parent2_trio_1.g.vcf.gz \; /output/child_trio_2g.vcf.gz\; /output/parent1_trio_2.g.vcf.gz \; /output/parent2_trio_2.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/trio_cohort_merged.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/816#issuecomment-2101970308:722,config,config,722,,https://github.com/google/deepvariant/issues/816#issuecomment-2101970308,2,['config'],['config']
Modifiability,"Hi @ptrebert . I'd like to extend on Alexey's answer. DeepVariant takes only the base quality scores from the QUAL field of the BAM. At this time, it does not use additional data tags for PacBio sequencing. So if you have pbmm2 reads mapped from FASTQ, this should be identical in behavior to the BAM entry (assuming mapping is fully deterministic). . I have not see BAM files for HiFi reads contain some of the tags I saw in previous CLR files. I just want to confirm that you are starting from the output of CCS (i.e. that these reads are HiFi). This version of DeepVariant doesn't call variants in uncorrected CLR reads. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/275#issuecomment-587751831:27,extend,extend,27,,https://github.com/google/deepvariant/issues/275#issuecomment-587751831,1,['extend'],['extend']
Modifiability,"Hi @raphaelbetschart,. It's beginning to feel more and more there are some issues with this site. With a mappability score so low for that region, and a 4000x depth recovering only 203 reads, suggests multiple scenarios one of which could be as Andrew mentioned of segmental duplication. This would require more analysis of the region and the reads, one of which could be as Andrew suggested varied isoforms. Keep in mind DeepVariant will also [cap the reads at 1500](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#how-are-ad-and-dp-values-calculated) if the depth is too high. Another thing about low mappability is that for the WGS model there will be local realignment triggered, and that can result also in the lower number of reads. To get a BAM of your realigned reads for that region, just add the following to your DeepVariant script:. ```; --make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads; ```. as described in the [FAQ section](https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md#what-is-the-realigner-and-how-does-it-work) or in the [following comment](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). Make sure to create the `realigned_reads` directory first, by adding the following line to your script (assuming you have a defined `OUTPUT_DIR` variable):. `mkdir -p ""${OUTPUT_DIR}/realigned_reads""` . After you relaunch the script, the BAM file(s) representing that region would be the one you would use in IGV. What percentage of your call sites exhibit this behavior? Were you able to look closer at the reads to compare among each other as Andrew suggested to see if they are either high expression or varied isoforms?. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/701#issuecomment-1697167719:1356,variab,variable,1356,,https://github.com/google/deepvariant/issues/701#issuecomment-1697167719,1,['variab'],['variable']
Modifiability,"Hi @rickymagner ,; If you want to learn more about channels, you can take look at this blog post:; https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/. Two things to mention:; 1. This would be an advanced use of the DeepVariant codebase, both from the research and engineering aspects. We can try to answer your questions, but can't guarantee that we'll have bandwidth or knowledge to support your use case all the way.; 2. Since r1.6, we've made quite a lot of refactoring regarding the channels internal implementation. These new code will come out (with documentation) in a future r1.7, but right now it's not quite ready yet. That said, if you have a specific use case now, it's worth looking at the blog post and the r1.6 to try to implement your own channel! If you have any findings or questions, feel free to share or ask here. We can try our best to answer. I've always enjoyed seeing our users building on top of DeepVariant!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/790#issuecomment-1996353013:488,refactor,refactoring,488,,https://github.com/google/deepvariant/issues/790#issuecomment-1996353013,2,['refactor'],['refactoring']
Modifiability,"Hi @shadrinams . My apologies for the delay in reply. It took me awhile to get to this issue. I have some thoughts about how to improve consistency in the trio calling that may help some of these cases. But it may take a bit of time to explore those. For the chr7_54624683 case, I suspect there is some interaction with the variant call and the normalization done by GLnexus. When I look at the VCF for the Proband and Father, I see:. Proband; ```; chr7 54624686 . A ATC 61.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:61:39:17,21:0.538462:61,0,7; ```; Father; ```; chr7 54624686 . A ATC 54.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:55:33:18,15:0.454545:54,0,68; ```. In my GLnexus VCF, I also see the following line, which has a call in both the child and the father (the variant position is a bit different, but this does seem to reflect the reads I see in the sample). ; ```; chr7 54624686 chr7_54624686_A_ATC A ATC 61 . AF=0.333333;AQ=61 GT:DP:AD:GQ:PL:RNC 0/1:33:18,15:55:54,0,68:.. 0/0:32:32,0:50:0,180,1799:.. 0/1:39:17,21:61:61,0,76:..; ```. Would you mind pasting the command that you used to merge the gVCFs from this sample? When you jointly genotyped, were there also other samples at the time of joint genotyping, or was it only this sample?. For reference, this is the command that I used on the gVCFs of the files to generate this gVCF:. ```; sudo apt-get -y install bcftools; sudo apt-get -y install tabix; sudo docker pull quay.io/mlin/glnexus:v1.2.7. sudo docker run \; -v ""${PWD}/outputs"":""/outputs"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWGS \; /outputs/Child_chr7_54624683.g.vcf.gz \; /outputs/Father_chr7_54624683.g.vcf.gz \; /outputs/Mother_chr7_54624683.g.vcf.gz \; | bcftools view - | bgzip -c > outputs/GitHub440.cohort.vcf.gz; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-836280023:1562,config,config,1562,,https://github.com/google/deepvariant/issues/440#issuecomment-836280023,1,['config'],['config']
Modifiability,"Hi @sophienguyen01 , ; Is there a reason why you're setting `--config.num_validation_examples=0`? You'll need to have a reasonable amount of num_validation_examples for the model to be able to evaluate and pick a reasonable checkpoint.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2073324455:63,config,config,63,,https://github.com/google/deepvariant/issues/802#issuecomment-2073324455,1,['config'],['config']
Modifiability,"Hi @sophienguyen01 ; You can try https://github.com/google/deepvariant/blob/r1.6.1/deepvariant/labeler/labeled_examples_to_vcf.py; This is more experimental and not officially documented yet. But you can find in our Docker:; https://github.com/google/deepvariant/blob/r1.6.1/Dockerfile#L152. For now, please read the code and use the flags there. As mentioned, this is experimental and we have not officially supported it yet, so I can't say it'll work for the specific use case that you want. But you can at least look at the code and see whether you can adapt it. . Another more typical approach is to run through the rest of the steps (call_variants, postprocess_variants) and get the list of variants that way.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/808#issuecomment-2062163191:556,adapt,adapt,556,,https://github.com/google/deepvariant/issues/808#issuecomment-2062163191,1,['adapt'],['adapt']
Modifiability,"Hi @splaisan ,; like earlier comments mentioned, if you don't specify intermediate_results_dir, it won't be saved separately. A few more tricks for cleaning up after Docker:; 1. https://docs.docker.com/config/pruning/ has information about how to clean up for Docker.; 2. If you run Docker with `--rm` , I believe it'll also clean up after your run completes.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/296#issuecomment-1625614434:202,config,config,202,,https://github.com/google/deepvariant/issues/296#issuecomment-1625614434,1,['config'],['config']
Modifiability,"Hi @tgelafr-btx , thanks for waiting. It took me a while to get back to this. Before I share my work log, one observation from your error earlier:; It seems like you're using Python 3.10. Note that DeepVariant 1.5.0 is bulit with Python 3.8. So, can you try with Python 3.8?. ---. Here is what I tried. On a GCE instance, I ran:. ```bash; wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. Then, I ran:. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. Which seems to work (without error). I don't actually know how to use conda (or deepvariant in conda). But I did see the files here: . ```; (dv-env) pichuan@pichuan-cpu:~$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0/; call_variants.zip licenses.zip model_train.zip runtime_by_region_vis.zip; call_variants_keras.zip make_examples.zip multisample_make_examples.zip settings.sh; deeptrio make_examples_somatic.zip postprocess_variants.zip show_examples.zip; freeze_graph.zip model_eval.zip run-prereq.sh vcf_stats_report.zip; ```. These are probably the files that were packaged with the last release: https://github.com/google/deepvariant/releases/tag/v1.5.0. @tgelafr-btx Question for you: Have you consider using Docker or Singularity, which are better supported by our team? Like the example in https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md or https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-pacbio-model-case-study.md . Anyway, hopefully my test with conda install was somewhat informative. If you figure out how to install+use it, please update here. I don't think I would be able to provide further support on conda here t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736#issuecomment-1829204521:569,config,config,569,,https://github.com/google/deepvariant/issues/736#issuecomment-1829204521,3,['config'],['config']
Modifiability,"Hi Amy,. That makes perfect sense now, and the realigned BAM file confirms the counts within expected values produced by the VCF. Basically it will realign unless you add the `--norealign_reads` (or `--realign_reads=false`), as Pi-Chuan [mentioned earlier](https://github.com/google/deepvariant/issues/691#issuecomment-1662968609). So when you used the regular BAM file it realigned the reads -- because the above parameter is has a default value of `True` -- and when you used the realigned BAM file, it didn't need to align much or at all. The parameters used are defined as follows:. * `realign_reads` -> If `True` (the default value) then it will locally realign reads before calling variants. * `emit_realigned_reads` -> This will produce realigned reads if the `realigner_diagnostics` variable is also enabled. * `realigner_diagnostics` -> If this variable is not empty (i.e. set with a path), then the above and the DeBruijn graph will be saved, otherwise if it is empty the realigned BAM or Graphviz (dot) files will not be saved. There is always more that can be done if you really want to be sure, but this is fairly satisfactory. By the way, VCF files can also be loaded in IGV as well so that the comparison can be done directly. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454:791,variab,variable,791,,https://github.com/google/deepvariant/issues/691#issuecomment-1670332454,2,['variab'],['variable']
Modifiability,"Hi Charles,. Great! So if you run the following steps, what do you see (I know some are obvious, but it's just a sanity check):. $`1)`$ The following will test that your file is at the correct location:. ```; ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs . ls -l /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`2)`$ The following will test that your `INPUT_DIR` environment variable works properly:. ``` ; INPUT_DIR=""${PWD}/inputs"" . echo ${PWD}. ls -l ${INPUT_DIR}. ls -l ${INPUT_DIR} | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. $`3)`$ The following will test that your `INPUT_DIR` environment variable works properly inside Docker:. ```; INPUT_DIR=""${PWD}/inputs"" ; BIN_VERSION=""1.5.0"". echo ${PWD}. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input. sudo docker run -v ""${INPUT_DIR}"":""/input"" google/deepvariant:""${BIN_VERSION}"" ls -l /input | grep NC_045426.1_A_filt_fixed_markdup_csort.bam; ```. You should see the file `NC_045426.1_A_filt_fixed_markdup_csort.bam` for all 3 steps. Please list for which step you don't see that file, and what you actually see for that step (in terms of the complete output of the step). Of course the other way to explicitly name the input and output variables, by changing them to the following (and not relying on `${PWD}` - the present working directory - which could change):. ```; INPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"". OUTPUT_DIR=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/outputs"". ```; Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751:496,variab,variable,496,,https://github.com/google/deepvariant/issues/184#issuecomment-1699969751,3,['variab'],"['variable', 'variables']"
Modifiability,"Hi Daniel,; Thanks for your response!. If I run that exact command, I get the following error:. > FATAL: failed to retrieved path for /gpfs/scratch/decarlson/deepvariant_test/nproc: lstat /gpfs/scratch/decarlson/deepvariant_test/nproc: no such file or directory; > ERROR : Child exit with status 255. However, if I specify the image:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc; ```; The value returned is:. > 1. That said, I think this is some weird interaction with the Slurm HPC scheduler I'm using. I'm doing this in an interactive job, where I've set `--ntasks-per-node=28`. But I'm not seeing 28 available CPUs. But if I open up a new terminal and ssh directly to the node my interactive job is running on, and then do. `singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}""-gpu nproc`. The value returned is now:. > 28. Moreover, when I run the same command that I listed in my first post in this second terminal, make_examples is parallelized across all 28 shards as expected. I think this is probably a configuration issue with our cluster, not something related to DeepVariant specifically. If you agree, feel free to close the ticket. Thanks for your help!; Dave",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896:1139,config,configuration,1139,,https://github.com/google/deepvariant/issues/546#issuecomment-1180869896,1,['config'],['configuration']
Modifiability,"Hi Fra,. So let me go through a few items:. $`1)`$ It fails to find variant candidates in the `make_examples` stage. Assuming the reads were generated from Illumina whole-exome sequencing (WES), this BAM file is fairly large. To troubleshoot, it is best to specify a smaller region you know candidates should be, as you will need significant memory for this to work for the the whole genome. The flag specifying a region is of this format, and adjust accordingly for your best candidate region of your study: . ```; --regions ""chr20:10,000,000-10,010,000""; ```. You can just use Docker as you are running using that already (via `docker run`), to keep the number of variables small. Before you run Docker again, please look at the next two items as well. $`2)`$ Assuming you used the same reference sequence file (`Homo_sapiens_assembly38.fasta`) to perform the alignment, please run the following command (also you don't need to mark duplicates with DeepVariant):. ``samtools view -H $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. $`3)`$ To perform a preliminary QC on your BAM file can you run the following command:. ``samtools flagstat $FQ.align.sort.marked.bam``. You can provide this via a gist or an attachment. If you have `fastQC` you can perform the following to generate the report, which you can attach a printed PDF file of the HTML output -- or you can attach the html with the accompanying zip file, if that is easier:. ` fastqc $FQ.align.sort.marked.bam`. $`4)`$ When you run the `docker run` command for `run_deepvariant` could you please provide the full output, to see where it gets stuck with errors, or how it completes. Also how much memory and disk space is freely available on the VM? I'm assuming the BAM file is several gigabytes. DeepVariant can read BAM files and will generate the VCF and GVCF, so no additional tools are required for it to work out of the box. We are just using a few tools to troubleshoot, but usually they are not necess",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175:666,variab,variables,666,,https://github.com/google/deepvariant/issues/675#issuecomment-1629892175,1,['variab'],['variables']
Modifiability,"Hi Fra,. The Docker container has it's own internal version of Python so it becomes independent of any system it would be run on. . I have 4 questions:. 1) Did the sanity check Maria suggested work successfully for you? This is important to complete first as it gives context that your VM is compatible with a successful DeepVariant run. 2) Can you please provide the whole command that you just ran including any variables that were defined. Without both of these it becomes impossible to eliminate possible causes of the error, as they provide context as to how the command was set up. . 3) Please provide the `ls -l ${INPUT_DIR}` output to make sure it lists the BAM file using that variable definition. 4) Can you provide the complete output of the run, including error. This gives context as how far it ran before it stopped, which includes anything that worked. Thanks,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046:414,variab,variables,414,,https://github.com/google/deepvariant/issues/675#issuecomment-1632300046,2,['variab'],"['variable', 'variables']"
Modifiability,"Hi I'm the GLnexus maintainer. From that side we're currently lacking a motivating, medium/large scale study with DeepVariant gVCF files which would provide the context to chase down the gnarly corner cases that arise in gVCF merging / joint genotyping, completing the current ""experimental"" integrated configuration. There are a few candidates in the pipeline but more would be really welcome. Ping me if I can help!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/142#issuecomment-465059695:303,config,configuration,303,,https://github.com/google/deepvariant/issues/142#issuecomment-465059695,1,['config'],['configuration']
Modifiability,"Hi Maria,. Thanks for the response. My Intent is to generate frozen graph from the available checkpoints. The code snippet is my own. Do you think it needs any fixing / addition? . Is there anyone who can provide steps/methods/guidance? I tried from inside as well as outside the Docker. I even tried Google Colab with different Hardware configurations -- i.e. combination of CPU, GPU, TPU. And also with different TF versions. I am never able to import the meta graph and restore the checkpoint. I always get ""No Op Kernel was registered"" with different Op names. Additional question: for training:. - Can I train on the ""quickstart-testdata"" provided in deepvariant? ; - And how do I produce a frozen graph during a training run?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339#issuecomment-681204545:338,config,configurations,338,,https://github.com/google/deepvariant/issues/339#issuecomment-681204545,2,['config'],['configurations']
Modifiability,"Hi Mark,. I understand, but it would be nice to dig deeper in understanding under what circumstances is the [~20% improvement](https://github.com/google/deepvariant/blob/dba790c6f1618831a58b7230535ec14c323461d7/settings.sh#L63-L70) being observed, as there might be bigger optimization opportunities. If you add the Intel® Math Kernel Library for Deep Neural Networks (Intel® MKL-DNN) optimizations, that will beat AVX by 50% and AVX2 by 27% (using an Inception v3 model) as shown [here](https://github.com/tensorflow/tensorflow/blob/f6fd7db9d254b42da74c7397cb18f7a6db2188e2/tensorflow/docs_src/performance/performance_guide.md#comparing-compiler-optimizations) - I reshaped the table for batch size of 32 as it's not properly formatted on that TensorFlow page:. | Optimization | Data Format | Images/Sec (step time) | Intra threads | Inter Threads | ; | ------------ | ----------- | ------------- | ------------- | ------------- | ; | MKL | NCHW | 10.3 (3,104ms) | 4 | 1 | ; | AVX2 | NHWC | 7.5 (4,255ms) | 4 | 0 | ; | AVX | NHWC | 5.1 (6,275ms) | 4 | 0 | ; | SSE3 | NHWC | 2.8 (11,428ms)| 4 | 0 |. Yes, Tensorflow is targetting AVX pre-builds for two versions from now (1.6), but that I why I was suggesting modularity for DeepVariant as a workflow/pipeline where users can plug-in other frameworks and/or models besides just Tensorflow (i.e. MXNet, Caffe, etc) and Inception/Mobilenet/ResNet50/etc. The more flexible and ease-of-entry the analysis pipeline is, the faster its community will grow while minimizing the support requirements. . Best,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-355440557:1276,plug-in,plug-in,1276,,https://github.com/google/deepvariant/issues/21#issuecomment-355440557,2,"['flexible', 'plug-in']","['flexible', 'plug-in']"
Modifiability,"Hi Masaru,; It looks like you're using DirectRunner, which is fine for smaller datasets, but when we have larger datasets we instead use DataflowRunner (see where we shuffle training set [here](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each)). In the Case Study, the DirectRunner is used for the small validation set and DataflowRunner is used for the large training set. The fact that it works when you split up the data into smaller pieces suggests this may be the issue. Please try running with DataflowRunner and let me know if that works for you.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133#issuecomment-450955687:335,config,configuration-file-for-each,335,,https://github.com/google/deepvariant/issues/133#issuecomment-450955687,1,['config'],['configuration-file-for-each']
Modifiability,"Hi Nicolas,. Aside from Pi-Chuan's recommendation -- which I think is very good to try -- see if also doing these two steps yield any results (which might make the region issue go away naturally):. 1) To minimize the number of working variables, first let's try Docker by itself. This would be without NextFlow, just by using the command-line (in a terminal Bash session) replicating exactly the [DeepVariant QuickStart](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md). If it does work, then the next step should be easier to configure. 2) If step 1 works, then you can customize a NextFlow pipeline based on the configuration of step one, using [containerOptions](https://www.nextflow.io/docs/latest/process.html#containeroptions), which I'm sure you already know or a custom process. I am assuming your NextFlow implementation allows you to create custom pipelines, which do not add any default volume mounts or environment variables. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287:235,variab,variables,235,,https://github.com/google/deepvariant/issues/640#issuecomment-1550852287,4,"['config', 'variab']","['configuration', 'configure', 'variables']"
Modifiability,"Hi Nils,. Glad to hear it was helpful, and I enjoyed it as well! Yeah the project is pretty amazing $`-`$ it's incredible how the layers perform specific segmentations and amplifications: . ![image](https://github.com/google/deepvariant/assets/6555937/331f1ab9-155b-4aa5-98ca-4b08e40fe8cd). Thank you and hope your research is also going well!; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1636726993:130,layers,layers,130,,https://github.com/google/deepvariant/issues/666#issuecomment-1636726993,1,['layers'],['layers']
Modifiability,"Hi Nima,. That is a good point, and glad to see the tutorials page updated today. Though it would have been nice to have it highlight the `--use_tpu` flag option and `0.7.0` version, rather than the `0.6.x`. The tags in the docker images for the runner are still as a release candidate `0.7.0rc1`, with `0.6.1` as the `latest`. In any case, you guys are doing great work, and there is quite a lot of expertise on this side of the world as well to expand on DeepVariant's capabilities, and foster an even greater community. You might want to update the [contributing document](https://github.com/google/deepvariant/blob/master/CONTRIBUTING.md), since it mentions 1H 2018 for external contributions and CI testing on GitHub - and it's almost September. Version 0.7 seemed to have just dropped out of the sky without any formal review process, and we could have helped you iron out some of the kinks (i.e. such as minor spelling on the [TPU case study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md#shuffle-each-set-of-examples-and-generate-a-data-configuration-file-for-each) [s/Optoinal/Optional]). Your [Docker documentation](https://github.com/google/deepvariant/blob/master/docs/deepvariant-docker.md#run-the-pipeline) still lists your image to use as `0.4.1`, which has an older channel representation of the input data - though using `latest` is a comment that might make a new user not realize the amount of difference between the versions. Some things seemed a bit rushed, and doing it together would have provided more eyes with expanded tutorials for attracting new users to lower any barriers to entry for new users - who can be quite picky when trying out new software. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-414549806:1091,config,configuration-file-for-each,1091,,https://github.com/google/deepvariant/issues/87#issuecomment-414549806,1,['config'],['configuration-file-for-each']
Modifiability,"Hi Oskar,. Since your WDL workflow is using Docker, the simplest approach is to include a Docker-specific argument for `--cpuset-cpus`, or change the Session configuration which I've detailed at, the following location:. https://github.com/google/deepvariant/issues/42#issuecomment-360510853. For information regarding the `--cpuset-cpus` here's a reference:. https://docs.docker.com/config/containers/resource_constraints/#configure-the-default-cfs-scheduler. There are many ways to change DeepVariant, but I think this will will get you the quickest results for the issue you're facing. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366745899:158,config,configuration,158,,https://github.com/google/deepvariant/issues/49#issuecomment-366745899,6,['config'],"['config', 'configuration', 'configure-the-default-cfs-scheduler']"
Modifiability,"Hi Paul,. Thank you again for your replies and sorry mine are so slow (I think we are in very different time zones). So I’m administering this system but am not really a system administrator (I’m a biologist). Since I have root access, if there is something that can be done (or undone) to re-establish the ability of non-root users to run deepvariant, I can do that, I just don’t know enough to fix this specific issue myself. 1) The following gives me ; docker run --mount type=bind,source=""/tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling/inputs"",target=""/input"" google/deepvariant:""1.5.0"" ls -l /input. docker: Error response from daemon: invalid mount config for type ""bind"": bind source path does not exist: /tiger/home/ajp1/analysis/demography/tasmanian_devil/variant_calling.; See 'docker run --help'. 2) I’ve run the following, but confess I don’t understand what is going on past the second command and then deleting what was done at the end. If creating volumes can be skirted by changing some admin permission that is doable on my end. docker volume create --name dv-vol; dv-vol. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" touch /input/a-new-file. docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file. docker container create --name dv-cp-cont -v dv-vol:/input-path-cont hello-world; 4d2015a9c2dbe3fc6d27854c440ce8505222ed6a8f2a3c945d36315081a832b6. docker cp dv-cp-cont:/input-path-cont . ls input-path-cont. a-new-file; touch input-path-cont/file-2. -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker cp input-path-cont/file-2 dv-cp-cont:/input-path-cont . docker run --mount source=dv-vol,target=""/input"" google/deepvariant:""1.5.0"" ls -l /input; total 0; -rw-r--r-- 1 root root 0 Sep 1 00:21 a-new-file; -rw-rw-r-- 1 1005 1010 0 Sep 1 00:26 file-2. docker rmi --force hello-world:latest. U",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973:677,config,config,677,,https://github.com/google/deepvariant/issues/184#issuecomment-1701949973,2,['config'],['config']
Modifiability,"Hi Pi-Chuan,. Yes, that is correct - you should see it in the generated output files (VCF and GVCF). The control-flow logic for finding these candidates is as follows for [r1.1 of make_examples.py](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py):. $`1)`$ On [line 613](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L613), `options.calling_regions` is extended with the values from the user-defined `--regions` flag:. ```Python; options.calling_regions.extend(parse_regions_flag(flags_obj.regions)); ```. $`2)`$ On [line 2082](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2082), in function `make_examples_runner()` a request is made to provide these regions for processing:. ```Python; regions = processing_regions_from_options(options); ```. $`3)`$ On [lines 1960-1961](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1960-L1961), in function `processing_regions_from_options()`, the variable `calling_regions` is initialized based on the contents of `options.calling_regions` and `options.exclude_calling_regions`:. ```Python; calling_regions = build_calling_regions(ref_contigs, options.calling_regions,; options.exclude_calling_regions); ```. Which is then used to construct the `regions` and `region_list` on [lines 1968-1975](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L1968-L1975):. ```; regions = regions_to_process(; contigs=contigs,; partition_size=options.allele_counter_options.partition_size,; calling_regions=calling_regions,; task_id=options.task_id,; num_shards=options.num_shards). region_list = list(regions); ```. $`4)`$ Continuing with function `make_examples_runner()`, on [lines 2105-2106](https://github.com/google/deepvariant/blob/r1.1/deepvariant/make_examples.py#L2105-L2106) it finds candidates based on these regions, creating also the corresponding example files:. ```Python; for region in regions:; candidates, ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/708#issuecomment-1719933271:412,extend,extended,412,,https://github.com/google/deepvariant/issues/708#issuecomment-1719933271,2,['extend'],"['extend', 'extended']"
Modifiability,"Hi Sami,. That's great news! Yes, example tfrecords is what you want -- this creates the variant candidates with the accompanying images. Whatever gets you to the right result is a good solution, and it's a team effort to quickly get you there. Understanding what exactly is being -- or not being created -- and why is just as important, so you have full insight of the steps of the experiment. Just from your setup I can infer you are probably running data from a PacBio sequencer. You can go down some nice rabbit holes with DeepVariant, if you really want to understand the process. The models for different types of sequencers where optimized with specific parameters in mind -- some more flexible than others. The `make_examples` step generates the right image configurations for those, or you can rely on `run_deepvariant` to do that for you -- ideally all your parameters were properly interpreted in Python. Though the models can at times be resilient under some data conditions, you want to provide the right data to the correct model to be sure your results are properly interpreted as they were naturally optimized under those conditions :). Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771:693,flexible,flexible,693,,https://github.com/google/deepvariant/issues/677#issuecomment-1636029771,2,"['config', 'flexible']","['configurations', 'flexible']"
Modifiability,"Hi Saurabh,. Currently the `run_deepvariant_keras` is experimental as we plan to develop this further in the future. The model for `run_deepvariant` is for tf-slim and would not simply extend to keras. For now, you should use `run_deepvariant` as that's the one we officially support.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245:185,extend,extend,185,,https://github.com/google/deepvariant/issues/636#issuecomment-1520586245,2,['extend'],['extend']
Modifiability,"Hi Sophie,. Regarding requirements for resources, it all depends on how large your data is that you need to process. Also only the middle step `call_variants` can utilize 1 GPU if you prefer, but you can run the CPU version of DeepVariant. Below are the two Docker versions:. [google/deepvariant:1.5.0 (CPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0/images/sha256-6a27bc877db9191c8fad0fe5f98dce6192fd4eb6392551e1a3add8d01e08af2e?context=explore). [google/deepvariant:1.5.0-gpu (GPU Version)](https://hub.docker.com/layers/google/deepvariant/1.5.0-gpu/images/sha256-312af65c01d27e4fc8bb34a4c933ca708bd24d2e7d8ac3076c9c7c078afa20e9?context=explore) . Regarding machines, people have had luck with [r4.8xlarge EC2 (32 cores and 244 GB of RAM)](https://github.com/google/deepvariant/issues/167#issuecomment-479738587), especially since the different steps of DeepVariant [utilize different amounts of resources](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-details.md). So FASTQ files are used to align to a genome reference sequence such as [GRCh38](https://www.ncbi.nlm.nih.gov/datasets/genome/GCF_000001405.26/) to generate BAM files. The recommended on is `GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz`, which you can download from the following location:. https://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/. Regarding aligning your FASTQ file, that depends on your type of sequencer. If you are using PacBio HiFi it is recommended you use [pbmm2](https://github.com/PacificBiosciences/pbmm2), otherwise you can use [bwa](https://github.com/lh3/bwa). Regading Docker containers for both, here are a couple of links:. [pbmm2](https://quay.io/repository/biocontainers/pbmm2?tab=tags). [bwa](https://hub.docker.com/r/biocontainers/bwa/tags). Otherwise you can always use BioConda:. [pbmm2](https://anaconda.org/bioconda/pbmm2). [bwa](https://anaconda.org/bioconda/bwa). Hope it helps,; P",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044:340,layers,layers,340,,https://github.com/google/deepvariant/issues/696#issuecomment-1679264044,2,['layers'],['layers']
Modifiability,"Hi Sophie,. So as you know, besides the genetic information passed on from the parents, each of us is born with an additionally small number of novel genetic changes called _de novo_ mutations (i.e. from environmental effects, etc). These traits are thus not passed from the parents, thus violating Mendelian inheritance. So when you use `rtg-tools mendelian` with the `--output` flag, it will save an updated VCF file annotated with calls violating Mendelian inheritance, thus highlighting the _de novo_ mutations. The information (header) fields in these updated annotated VCF files will have the following:. ```; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency"">; ##INFO=<ID=MCV,Number=.,Type=String,Description=""Variant violates mendelian inheritance constraints"">; ##INFO=<ID=MCU,Number=.,Type=String,Description=""Mendelian consistency status can not be determined"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=DN,Number=1,Type=String,Description=""De novo allele"">; ##FORMAT=<ID=MCP,Number=.,Type=String,Description=""Describes the expected genotype ploidy in cases where the given genotype does not match the expected ploidy"">. ```. Each de novo call that violated Mendelian inhertance will be annotated like this:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT father mother son1 son2 daughter1 daughter2-initial daughter2; Chr1 4917 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr1 15214 . G C . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; Chr2 4883 . T G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr2 11369 . G A . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr3 11754 . A G . . MCV=daughter2:0|0+0|0->0|1 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 0|1:Y; Chr4 37470 . C T . . MCV=daughter2:0|0+0|0->1|0 GT:DN 0|0 0|0 0|0 0|0 0|0 0|0 1|0:Y; ```. Below are a few tools that can also perform trio analysis (generating their own VCF), or can perform VCF ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:309,inherit,inheritance,309,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,6,['inherit'],['inheritance']
Modifiability,"Hi Sophie,. So it is best to train across all the samples. The reason is that the model will need to be representative of all those samples, and you don't want to overfit for one sample over another. Today there is more data than computing power and memory, so training happens by batching the data, training on a batch, and then tweaking the model in the follow-up batches. Here's a visual example of what could happen:. ![image](https://github.com/google/deepvariant/assets/6555937/3ff13990-c2cc-4863-a904-bb0219791b06). In the first batch, the coefficients will be biased in a monotonically increasing function. Then when the second batch is used to tweak the model's parameters, it will shift towards a monotonically decreasing function shifting it towards an opposite direction. Then when the third batch comes in it will shift the parameters in the other direction. Sure, the independent variable (x-axis) here is a simple one-dimensional type, but this can become complex with multiple channels (`read base`, `base quality`, `mapping quality`, etc). And of course if the training data is shuffled, then the validation data and tuning data should also be shuffled in order to have the same data representation - with the exception of the test data, which will be used for benchmarking the model as a real scenario (though there is additional read shuffling when downsampling with too many reads for a pileup image). As @akolesnikov mentioned, the `input_pattern_list` parameter of the shuffle script takes a list of files, so you can do it all at once. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219:894,variab,variable,894,,https://github.com/google/deepvariant/issues/698#issuecomment-1711046219,2,['variab'],['variable']
Modifiability,"Hi Tobi,. Since you're using Docker you can use the `--cpuset-cpus` CLI argument, or if you want to change it in Tensorflow for DeepVariant, that could be performed via the `Session` configuration - though that would require a small change in the code in `call_variants.py`:. ```; config = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1, \; allow_soft_placement=True, device_count = {'CPU': 1}); session = tf.Session(config=config); ```. The change in `call_variants.py` would be in the following line of code, and can be changed based on your preference:. https://github.com/google/deepvariant/blob/r0.4/deepvariant/call_variants.py#L343. Hope it helps,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/42#issuecomment-360510853:183,config,configuration,183,,https://github.com/google/deepvariant/issues/42#issuecomment-360510853,5,"['Config', 'config']","['ConfigProto', 'config', 'configuration']"
Modifiability,"Hi Zhuyi,. DeepVariant is complaining here because your BAM has a record that says it has a mapped mate but mtid, which is the htslib variable holding the offset into the chromosome array, isn't set so its actual mapped location isn't present. This would normally indicate that's something corrupted with your BAM. Where did you get? How was it aligned? I'd recommend running ValidateSamFile to see if it complains. It could be we've being overly strict in Nucleus for parsing our BAMs. It'd be great if you can determine if your BAM is considered valid or not, and let us know if we need to be more permissive in our parsing or if the BAM needs to be fixed up. All the best,. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-426810736:134,variab,variable,134,,https://github.com/google/deepvariant/issues/99#issuecomment-426810736,1,['variab'],['variable']
Modifiability,"Hi all,. sorry about the late reply I was testing the UKBiobank WES Protocol provided by Andrew but unfortunately it does not seem fix our problem.; The general issue is that to identify runs of homozygosity(ROH) with plink you can also just provide a vcf file but this vcf file needs a base resolution e.g. an entry for each position, whether it is variable or not:. #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT PacBio_CCS; SUPER_1 1 . C . . . DP=49 GT:AD:DP:RGQ 0/0:49:49:99; SUPER_1 2 . C . . . DP=50 GT:AD:DP:RGQ 0/0:50:50:99; SUPER_1 3 . T . . . DP=54 GT:AD:DP:RGQ 0/0:54:54:99; SUPER_1 4 . A . . . DP=61 GT:AD:DP:RGQ 0/0:61:61:99; ... And we were just wondering if there is a possibility to generate such a vcf file using DeepVariant. Thanks again for all the replies and help. best regards,. Max",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277:350,variab,variable,350,,https://github.com/google/deepvariant/issues/571#issuecomment-1283869277,1,['variab'],['variable']
Modifiability,"Hi everyone, . I am trying to install the DeepVariant bioconda on RedHat Entreprise Server 7.2; I am really not familiar with conda but this looks like the most straightforward way to run deepvariant on a machine for which I do not (and will never get) sudo privileges. The above discussion helped to pass a lot of kinks but I am still struggling. . I am having the two following problems : ; - conda now installs libcrypto.so.1.1 instead of libcrypto.so.1.0.0 . I solved it by adding a hard link from libcrypto.so.1.1 to a libcrypto.so.1.0.0 which is dirty and may break things down the line. - I am really struggling with compiling GLIBC-2.23 . I configured it with ""-O2 -g -Wall"" which are the gcc flags recommended in the RHES doc.; The make command ran well (no error of what I can see) but when I do make check it crashes with ; test-math-isinff.cc: Command not found; If I add the GLIBC path to my LD_LIBRARY_PATH as suggested by @pgrosu , it then corrupts the environment (i.e. every command goes to segmentation fault core dump) so I assume the GLIBC is not built properly . Do you have any idea about how to solve this ?; Any help would be greatly appreciated. Thanks !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-480128599:649,config,configured,649,,https://github.com/google/deepvariant/issues/137#issuecomment-480128599,1,['config'],['configured']
Modifiability,"Hi, @pgrosu. Thanks for your reply. ; I find a lot of lib files following your command:; ```; cd /usr; find | grep cuda | grep lib; ```; Here is the output file.; [find_cude_lib.txt](https://github.com/google/deepvariant/files/10988201/find_cude_lib.txt). Do you have any suggestion for me to set the variable before starting the deepvariant-gpu singularity program?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471488330:301,variab,variable,301,,https://github.com/google/deepvariant/issues/619#issuecomment-1471488330,1,['variab'],['variable']
Modifiability,"Hi, @pichuan, thanks for your quick replay.; I ran the command `/opt/deepvariant/bin/run_deepvariant --version` using GPU version in singularity image . It return the `CUDA_ERROR_UNKNOWN` error as above I mentioned. Now I am trying to use the 1.5.0 version. I use the same test command and it return the same error as followings:; ```; Singularity> /opt/deepvariant/bin/run_deepvariant --version; 2023-03-16 09:39:40.831726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-03-16 09:39:40.953050: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2023-03-16 09:39:42.599918: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; 2023-03-16 09:39:42.599962: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: yy01.local; 2023-03-16 09:39:42.599972: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: yy01.local; 2023-03-16 09:39:42.600020: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; 2023-03-16 09:39:42.600049: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.5.0; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851:1018,variab,variable,1018,,https://github.com/google/deepvariant/issues/619#issuecomment-1471145851,1,['variab'],['variable']
Modifiability,"Hi,. Evaluation is done using [Estimator class](https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#evaluate). You may create a simple script calling Estimator.evaluate in a loop passing different checkpoints. > checkpoint_path: Path of a specific checkpoint to evaluate. If None, the latest checkpoint in model_dir is used. If there are no checkpoints in model_dir, evaluation is run with newly initialized Variables instead of ones restored from checkpoint. Hope it helps.; Alexey",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/198#issuecomment-512031369:422,Variab,Variables,422,,https://github.com/google/deepvariant/issues/198#issuecomment-512031369,1,['Variab'],['Variables']
Modifiability,"Hi,; Thanks for trying the command. v0.10.0 is out today, and the Quick Start can be found here:; https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md. First of all, clean up the example command a bit: Don't include the part ` **Replace this string with exactly one of the following [WGS,WES,PACBIO]**` in your command. And, the environment variables like ""${BIN_VERSION}"" and other variables will need to be set. Please refer to the documentation above to set it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-604760300:369,variab,variables,369,,https://github.com/google/deepvariant/issues/287#issuecomment-604760300,2,['variab'],['variables']
Modifiability,"Hi,; can you tell me where you got your inception_v3.ckpt* model files?; And, can you paste the content of your test_train.config.txt file?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-350781969:123,config,config,123,,https://github.com/google/deepvariant/issues/10#issuecomment-350781969,1,['config'],['config']
Modifiability,"I agree this issue is probably system specific. ; This creates a problem when using the container in nextflow since nextflow automatically configures few folder bindings when it prepares the run, namely the working directory, the directories of files staged into the process as inputs and the temp dir indicated by $TMPDIR.; Since it prepares all the scripts in advance, the $TMPDIR points to the standard /tmp location if I start nextflow from a login node, while in my system this is set to a node specific scratch space (/local scratch) when the job is submitted to a computing node by SLURM. Thus, I end up having the tmp dir not correctly mounted in the container. I'm not sure how common such a configuration is, so maybe it's a problem affecting just me and few others. What I've done is to add a line like this before the actual `run_deepvariant` command in my `script` section in the Nextflow process:; `export TMPDIR=""$PWD/tmp_dir""`. This overwrites the original variable and set the TMPDIR to a subfolder in the working directory. It works fine in this context since deepvariant is the only operation running in the process and thus changing TMPDIR does not interfere with anything else.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987:139,config,configures,139,,https://github.com/google/deepvariant/issues/524#issuecomment-1067597987,3,"['config', 'variab']","['configuration', 'configures', 'variable']"
Modifiability,"I changed the ""num_examples"" form 1 to 2 only,not add any tfrecord.gz file, then it start run, but it is really slow, it have run two hours stilly. So is this situation normal? The tfrecord.gz file is about 2.4M size. ![image](https://user-images.githubusercontent.com/15261087/33869274-d250d54c-df42-11e7-9d37-a6cb401e4cdc.png). Here my config.txt:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 2; ```; This is my command; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt /leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt --start_from_checkpoint inception_v3.ckpt; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-350952138:338,config,config,338,,https://github.com/google/deepvariant/issues/10#issuecomment-350952138,2,['config'],['config']
Modifiability,"I consulted with the team to get an idea of the effort required, and we came to the conclusion that the DeepVariant code is full of diploid assumptions, so this would likely take a lot of effort, and unfortunately it isn't something that we have enough bandwidth to advise anyone else through either. You're of course welcome to fork DeepVariant and play around with it, but it will likely require many changes to the code to make it work. It is definitely possible though. You could also consider whether other ML-based variant callers are easier to adapt to polyploid, or if any polyploid callers exist already, which could be fun to add ML on top of as well.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/519#issuecomment-1054575852:551,adapt,adapt,551,,https://github.com/google/deepvariant/issues/519#issuecomment-1054575852,1,['adapt'],['adapt']
Modifiability,"I had set the environment variables just didn't include them in the above message. That error was certainly caused by the `**Replace this string with exactly one of the following [WGS,WES,PACBIO]**` string. However, I'm still getting an _unrelated_ error:; ## Set the environment; ```; [moldach@cdr767 bin]$ BIN_VERSION=""0.10.0""; [moldach@cdr767 bin]$ INPUT_DIR=""${PWD}/quickstart-testdata""; [moldach@cdr767 bin]$ OUTPUT_DIR=""${PWD}/quickstart-output""; [moldach@cdr767 bin]$ salloc --time=0:30:0 --mem=8000; [moldach@cdr767 bin]$ singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; > --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; > --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; > --num_shards=1; WARN[0000] ""/run/user/3019658"" directory set by $XDG_RUNTIME_DIR does not exist. Eithe r create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such fil e or directory: Trying to pull image in the event that it is a public image.; I0327 13:31:58.252949 47794500922048 run_deepvariant.py:241] Re-using the directory fo r intermediate results in /tmp/tmp63xxmwmi. ***** Intermediate results will be written to /tmp/tmp63xxmwmi in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mo de calling --ref ""/scratch/moldach/bin/quickstart-testdata/ucsc.hg19.chr20.unittest.fa sta"" --reads ""/scratch/moldach/bin/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmp63xxmwmi/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/tmp63xxmwmi/gv cf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Paral",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:26,variab,variables,26,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['variab'],['variables']
Modifiability,"I have copy-n-paste the wrong script. ; I have already written -v ""${OUTPUT_DIR}"":""/output"", declared my $FQ variable and used backslash.; It doesn't work.; In my ""/input"" directory I have .fasta file, .fasta.fai, bam and .bai file. I suppose I don't require nothing else. (maybe, also VCF tools if the aim is the vcf output ""prediction"". . Any suggestion?; singularity run doesn't work as well.; I get the ""deepvariant_1.5.0.sif "" image file after running singularity pull... Thanks in advance,; Fra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1629787618:109,variab,variable,109,,https://github.com/google/deepvariant/issues/675#issuecomment-1629787618,1,['variab'],['variable']
Modifiability,"I have generated those files and added those using the '--ref_fai' and '--ref_gzi' flags respectively and let you know the results. Here is my new configuration:; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=valis-194104; OUTPUT_BUCKET=gs://canis/CNR-data; STAGING_FOLDER_NAME=deep_variant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz \; --ref_fai gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.fai \; --ref_gzi gs://canis/CNR-data/GRCh38_Verily_v1.genome.fa.gz.gzi \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116#issuecomment-437006790:147,config,configuration,147,,https://github.com/google/deepvariant/issues/116#issuecomment-437006790,1,['config'],['configuration']
Modifiability,"I personally had run the samtools command against the hg19.fa file instead of the .fasta file ( samtools faidx hg19.fa ) but I don't know if that matters. I'd still double check your environment variables. MY environment variables looked like the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617s.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617s.vcf.gz""; OUTPUT_GVCF=""2009617s.g.vcf.gz"". and the following for docker execution:. sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc). Hopefully that helps you to just replace my variables with yours and see if it works. I assume there is something going on with your environment variables.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/310#issuecomment-637685215:195,variab,variables,195,,https://github.com/google/deepvariant/issues/310#issuecomment-637685215,4,['variab'],['variables']
Modifiability,"I was able to get around this issue with my version of singularity (3.4.2) by cleaning the environment, limiting what's passed to singularity from the environment, and setting the tmp dir explicitly in the working directory on the NFS. here's my code chunk:; ```; WORKING_DIR=/mnt/scratch/Precision/Hub/PROCESS/DH4749/; export SINGULARITY_CACHEDIR=$WORKING_DIR; export SINGULARITY_TMPDIR=$WORKING_DIR/tmp/; mkdir -p $WORKING_DIR/tmp/. singularity exec \; 	-e \; 	-c \; 	-H $WORKING_DIR \; 	-B $WORKING_DIR/tmp:/tmp \; 	-B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/genomedir/$FASTA_FILE"" \; --reads=""/bamdir/$PROBAND_BAM"" \; --output_vcf=""/output/$PROBAND_VCF"" \; --output_gvcf=""/output/$PROBAND_GVCF"" \; --intermediate_results_dir=""/output/intermediate"" \; --num_shards=$NSLOTS ; ```. With the newer versions of singularity I think they do less inclusion of environmental variables, which includes the PYTHONPATH among other things in home directory and /usr/local/src...which is why you couldn't reproduce the error on a fresh cloud deployment. . Can keep closed just figured it out on my end...may be useful to someone with same issue on shared HPC with older singularity versions.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727:1082,variab,variables,1082,,https://github.com/google/deepvariant/issues/514#issuecomment-1156819727,1,['variab'],['variables']
Modifiability,"I would begin by performing an empirical serial study first with your current configuration, starting with the bare minimum amount of memory, and increasing it with some consistency. Then based on that, project out what would be satisfactory - if the current setup is not sufficient.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/157#issuecomment-464903381:78,config,configuration,78,,https://github.com/google/deepvariant/issues/157#issuecomment-464903381,1,['config'],['configuration']
Modifiability,I'm not sure I understand what you are trying to do. If you'd like to build your own docker image that includes DeepVariant you may follow commands in DeepVariant docker config - deepvariant/Dockerfile,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/448#issuecomment-828660901:170,config,config,170,,https://github.com/google/deepvariant/issues/448#issuecomment-828660901,1,['config'],['config']
Modifiability,"I'm seeing an OOM in the logs:; ```; OP_REQUIRES failed at conv_ops.cc:698 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16384,32,37,110]; ```; It also shows your training params:; ```; Training Examples: 8264746; Batch Size: 16384; Epochs: 1; Steps per epoch: 504; Steps per tune: 1500000; Num train steps: 504; ```. It seems that the `--config.batch_size=512` is not being picked up. It could be related to setting `num_epochs=0`, try changing that to the original 10. If that doesn't work, you could edit the batch_size in `dv_config.py` directly. . Let me know if that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802#issuecomment-2033253696:353,config,config,353,,https://github.com/google/deepvariant/issues/802#issuecomment-2033253696,1,['config'],['config']
Modifiability,"I've just discovered that when I change glnexus config from `DeepVariantWES` to `DeepVariant` (which is the case for 1KG) I got around 115k calls from 75 samples (same ethnicity) which seems more probable in relation to 1kg 1.7 mln from 3500 samples. Anyway, would be nice to hear if it sounds reasonable for exome.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/501#issuecomment-1003194651:48,config,config,48,,https://github.com/google/deepvariant/issues/501#issuecomment-1003194651,1,['config'],['config']
Modifiability,"Is it possible to merge the tfrecords files though?. On 19 Feb 2018 5:40 pm, ""Paul Grosu"" <notifications@github.com> wrote:. > Hi Oskar,; >; > Since your WDL workflow is using Docker, the simplest approach is to; > include a Docker-specific argument for --cpuset-cpus, or change the; > Session configuration which I've detailed at, the following location:; >; > #42 (comment); > <https://github.com/google/deepvariant/issues/42#issuecomment-360510853>; >; > For information regarding the --cpuset-cpus here's a reference:; >; > https://docs.docker.com/config/containers/resource_; > constraints/#configure-the-default-cfs-scheduler; >; > There are many ways to change DeepVariant, but I think this will will get; > you the quickest results for the issue you're facing.; >; > Hope it helps,; > Paul; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/49#issuecomment-366745899>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ARIS2lTrmFjJMsaw6LyJkF9atLo9sDIkks5tWaPmgaJpZM4SKal_>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49#issuecomment-366748047:294,config,configuration,294,,https://github.com/google/deepvariant/issues/49#issuecomment-366748047,6,['config'],"['config', 'configuration', 'configure-the-default-cfs-scheduler']"
Modifiability,"It depends on what you want. In the simplest case, people take the genome (fasta) + callset (vcf) as a representation of this individual's genome sequence. This is a bit simplistic, though, as it doesn't differentiate between regions where we are confidently the sample is the same as the reference vs. those where we are uncertain. That information is captured in the ""genome VCF"" or ""gVCF"" which DeepVariant can generate (see `--gvcf` in `make_examples`) but currently isn't so usable as the records come out in TFRecord of Variant proto format. We are working on adding support for creating a normally-formatted gVCF by extending postprocess_variants to merge those gVCF records and the callset together, which we hope to release soon. But in the meantime the best representation you can get from DeepVariant (without coding up merging logic for the gVCF yourself, which you are more than welcome to do) is VCF + genome. . I can't comment on the suitability of FastaAlternateReferenceMaker for your specific needs (despite being the original author of that tool) as I don't believe it was widely used or whether it is maintained now. I would post to biostars or other equivalent forum to ask for recommendations on what people typically do to combine a genome FASTA + VCF to make a diploid (or haploid) reference genome sequence. There are many options (e.g., FASTG, particularly important if you have diploid organisms) but I don't know what's widely used in the community. Hope that helps!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13#issuecomment-351172185:623,extend,extending,623,,https://github.com/google/deepvariant/issues/13#issuecomment-351172185,2,['extend'],['extending']
Modifiability,"It took about 8 hours, but I could run the **postprocess_variants** step on my local computer (using the commands [specified above](https://github.com/google/deepvariant/issues/167#issuecomment-480640009)). If Official Amazon Support doesn't have a solution for running this program on AWS, I might cross-post this on StackExchange (to see if I can figure out if there is some sort of configuration issue on AWS, and/or if I am not using ECS efficiently/correctly). However, I realize you have a lot of support to provide, so I will close this ticket and provide the successful output from my local computer:. ```; 2019-04-07 21:06:30.591035: I deepvariant/postprocess_variants.cc:88] Read from: Genos_Provided/call_variants_output.tfrecord.gz; 2019-04-07 21:06:33.504711: I deepvariant/postprocess_variants.cc:97] Done reading: Genos_Provided/call_variants_output.tfrecord.gz. #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505181: I deepvariant/postprocess_variants.cc:101] Total #entries in single_site_calls = 91732; 2019-04-07 21:06:33.505270: I deepvariant/postprocess_variants.cc:103] Start SortSingleSiteCalls; 2019-04-07 21:06:34.914308: I deepvariant/postprocess_variants.cc:105] Done SortSingleSiteCalls; I0407 21:06:36.217032 139687245461248 postprocess_variants.py:596] Writing output to VCF file: Genos_Provided/output.vcf.gz; I0407 21:06:36.221911 139687245461248 genomics_writer.py:163] Writing Genos_Provided/output.vcf.gz with NativeVcfWriter; I0407 21:06:36.231071 139687245461248 postprocess_variants.py:601] 1 variants written.; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167#issuecomment-480702286:385,config,configuration,385,,https://github.com/google/deepvariant/issues/167#issuecomment-480702286,1,['config'],['configuration']
Modifiability,"It works. Both with the tutorial data and my data.; It doesn't read bam file with a variable previously declared. I've changed $FQ with SAMPLE01 and it works. Thanks. ; Kind regard,; Fra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675#issuecomment-1632602170:84,variab,variable,84,,https://github.com/google/deepvariant/issues/675#issuecomment-1632602170,1,['variab'],['variable']
Modifiability,"Let me think a bit before we take the next step, as we are in a good place now. Also, since we're building DeepVariant from scratch, then Docker will not be necessary after all is done as we'll be using DeepVariant directly, and it will be much smaller and faster. Docker basically wraps it's own version of DeepVariant, and it has many layers of translation, which would not be as nimble as this approach.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577819231:337,layers,layers,337,,https://github.com/google/deepvariant/issues/657#issuecomment-1577819231,1,['layers'],['layers']
Modifiability,"M is just a recurrent neural network that is memory efficient to remember past events without gradient instabilities, by going through the input forward and in reverse while getting output information from the previous step. All this means is that it has a bunch of weights and biases that the input (and previous output) progresses through in a time-series-like pattern -- traversing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1250,variab,variable,1250,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['variab'],['variable']
Modifiability,"No problem, your variant of interest isn't a genomic region that may be; hyper variable ie a simple sequence repeat (they can occur in coding; regions) or something else that may lead to the variability your seeing?. Joe. On Mon, 31 Jul 2023, 17:30 Axze-rgb, ***@***.***> wrote:. > nothing is amplified no, it's all PCR free; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/682#issuecomment-1658733075>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BAYQV2X446P2BMPITLE5763XS7MRXANCNFSM6AAAAAA2QKAKXQ>; > .; > You are receiving this because you commented.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773:79,variab,variable,79,,https://github.com/google/deepvariant/issues/682#issuecomment-1658739773,4,['variab'],"['variability', 'variable']"
Modifiability,"ON-20.04}""; ABSL_PIN=""${ABSL_PIN-29bf8085f3bf17b84d30e34b3d7ff8248fda404e}""; PROTOBUF_VERSION=3.13.0; -CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.8}""; +CLIF_PYTHON_VERSION=""${CLIF_PYTHON_VERSION-3.9}""; # CLIF_PIN can be set to a specific commit hash on; # https://github.com/google/clif/commits/main.; # If not set, the default is to checkout the latest commit.; @@ -65,6 +65,21 @@ apt-get install ""${APT_ARGS[@]}"" --no-install-recommends \; wget \; unzip; ; +apt-get install ""${APT_ARGS[@]}"" python3-apt; +cd /usr/lib/python3/dist-packages; +if [ -e apt_pkg.so ]; then; + rm apt_pkg.so; +fi; +ln -s apt_pkg.cpython-38-aarch64-linux-gnu.so apt_pkg.so; +cd -; +; +export PATH=/root/.local/bin/:$PATH; +apt-get install ""${APT_ARGS[@]}"" libcairo2-dev; +pip install pygobject; +apt-get install ""${APT_ARGS[@]}"" libgirepository1.0-dev; +pip install --upgrade pygobject; +sed -i 's/isAlive/is_alive/g' /usr/lib/python3/dist-packages/softwareproperties/SoftwareProperties.py ; +; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; @@ -79,7 +94,6 @@ apt-get install ""${APT_ARGS[@]}"" \; libllvm11 \; llvm-11 \; llvm-11-dev \; - llvm-11-linker-tools \; python3-dev \; zlib1g-dev; ; @@ -147,4 +161,5 @@ if [[ ! -z ${CLIF_PIN} ]]; then; git checkout ""${CLIF_PIN}""; fi; ; +sed -i 's/11.1.0/11.0.0/g' clif/cmake/modules/CLIFUtils.cmake ; ./INSTALL.sh; ```; After these changes, I am stuck again at building clif because of the following error:; ```; [100%] Linking CXX executable clif-matcher; /usr/bin/ld: libclifMatcher.a(matcher.cc.o): in function `absl::lts_20230802::log_internal::LogMessage& absl::lts_20230802::log_internal::LogMessage::operator<< <27>(char const (&) [27])':; matcher.cc:(.text._ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc[_ZN4absl12lts_2023080212log_internal10LogMessagelsILi27EEERS2_RAT__Kc]+0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217:4507,Config,Configure,4507,,https://github.com/google/deepvariant/issues/879#issuecomment-2334801217,1,['Config'],['Configure']
Modifiability,"Pi-Chuan -- sorry, that's right, we would want to build on CentOS6 to be compatible with bioconda. They have a restricted build environment for portability so we'd need to have all the dependencies installable by bioconda (rather than system packages). I had looked at this earlier and realized all the pre-requisites so got afraid of tackling it. It's definitely a help to have that information but I think would still take a bit of work to port over.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385500259:144,portab,portability,144,,https://github.com/google/deepvariant/issues/29#issuecomment-385500259,1,['portab'],['portability']
Modifiability,"Pi-Chuan and Mike;; Thanks for all this background and help. I'm trying to fit this into the conda recipe bazel build for DeepVariant but am not sure how to take advantage of using the local anaconda python in that context. The error I'm seeing is that bazel can't find pyclif_proto:; ```; (17:56:01) INFO: Found 1 target...; (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; Target //deepvariant:binaries failed to build; (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; ```; which I thought was triggered by the difficulty running pyclif without having the local python installed. It could also be due to not installing is in `/usr/local/bin` since I have to remain sandboxed in the work directory, but I did adjust the PATH to include the download location. Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either understanding how to handle a root install of the pre-build pyclif or tweaking to use the local python would be helpful. Alternatively, if you can already build DeepVariant on a CentOS6 system yourself I could use the pre-build binaries the way we're doing now, just with the build against an older glibc. Thanks again for the help with this.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386250002:1105,sandbox,sandboxed,1105,,https://github.com/google/deepvariant/issues/29#issuecomment-386250002,1,['sandbox'],['sandboxed']
Modifiability,"Pichuan, to increase the ease use and expand adoption within the Bioinformatics community it might not hurt to have a collection of customized build-and-test environments at Google that match a variety of environment configurations that users have in place, or that common packages recommend out here. Sometimes folks will be curious to try out some new Bioinformatics software package, and the faster they get it to a running state on their own machines, the happier the experience enabling the community for that package to grow faster. Basically most people just want to use stuff - and want a turn-key solution - though some of us like tinkering with puzzles :) If their experience is good on something local - or even a cluster - then they'll see the obvious need to try it out on a Cloud environment. I sort of did it from the other side. Many times when I tested most of the GoogleGenomics tools, I would try them out in some real-world scenarios, I usually ran them against a variety of configurations. That helped with having better error messages, control flow decisions, documentation or additional features. Basically you have developed a great software - which is evolving - and now comes the service component of supporting it, which is just as important. Just a friendly recommendation,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385874525:217,config,configurations,217,,https://github.com/google/deepvariant/issues/29#issuecomment-385874525,2,['config'],['configurations']
Modifiability,"Please share the full runner log, as well as config (YAML) file used.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70#issuecomment-387572014:45,config,config,45,,https://github.com/google/deepvariant/issues/70#issuecomment-387572014,1,['config'],['config']
Modifiability,"Reinitialized existing Git repository in /root/clif/.git/; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820:3346,config,config,3346,,https://github.com/google/deepvariant/issues/737#issuecomment-1818093820,2,['config'],['config']
Modifiability,"Resulting image that is fed into CNN contains multiple layers. Two of those are 'Read supports allele', and 'Read supports ref'. By removing an ambiguous read we may slightly improve the accuracy (in theory). Yes, your last example is difficult to make it right and we don't have a near future plans to address those.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/213#issuecomment-527648978:55,layers,layers,55,,https://github.com/google/deepvariant/issues/213#issuecomment-527648978,1,['layers'],['layers']
Modifiability,"Right, but which version of the Cuda Toolkit do you have? You can easily install the 9.0 from the link below, but I would do it as a local (non-sudo) user so you sandbox the changes to your environment in order to easily remove them later, if necessary:. https://developer.nvidia.com/cuda-90-download-archive. ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102#issuecomment-429154858:162,sandbox,sandbox,162,,https://github.com/google/deepvariant/issues/102#issuecomment-429154858,1,['sandbox'],['sandbox']
Modifiability,"Samtools use this approach to pass the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/38#issuecomment-372143466:582,variab,variable,582,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466,1,['variab'],['variable']
Modifiability,"So from what I see you've completed the following steps:. 1) Built and installed CLIF; 2) Installed Bazel; 3) Installed the Tensorflow Python Module; 4) Downloaded and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:168,config,configured,168,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['config'],['configured']
Modifiability,"Solve the problem --> the pipeline of using deeptrio bcftools cause error. Installing bcftools and using the following script works... I hope this help to improve the pipelining. ```; docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | bcftools view -Oz -o ${PWD}/output/HG002_trio_merged.vcf.gz ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424:293,config,config,293,,https://github.com/google/deepvariant/issues/632#issuecomment-1512478424,1,['config'],['config']
Modifiability,Solving the first error (libcublas.so.12) by creating a sandbox with singularity and adding the location of libcublas.so.12 to the env. I guess creating a soft link with ln -s would also work.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722#issuecomment-1780922055:56,sandbox,sandbox,56,,https://github.com/google/deepvariant/issues/722#issuecomment-1780922055,1,['sandbox'],['sandbox']
Modifiability,"Sorry @pichuan, I forgot to follow up. @nmousavi 's suggestion to unset gcloud config set compute/region """" helped.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96#issuecomment-427101350:79,config,config,79,,https://github.com/google/deepvariant/issues/96#issuecomment-427101350,1,['config'],['config']
Modifiability,Sorry about the install issues. The `post-link.sh` script that is failing downloads the trained models that DeepVariant uses from Google Buckets:. https://github.com/bioconda/bioconda-recipes/blob/master/recipes/deepvariant/post-link.sh. Failures here are typically due to problems with gsutil picking up other configuration options on your system. This thread has a bunch of debugging and some suggestions to try to work around the gsutil problem:. https://github.com/bcbio/bcbio-nextgen/issues/2613. If you can try the tips there and let us know more details of the failures if you get stuck happy to help with specific suggestions. Hope this gets deepvariant installed for you.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-504940567:311,config,configuration,311,,https://github.com/google/deepvariant/issues/177#issuecomment-504940567,1,['config'],['configuration']
Modifiability,"Sorry, i used model_train.zip really, just pasted the incorrect command to issue. ; This is my correct command:; ```; python /leostore/software/deepvariant/bazel-bin/deepvariant/model_train.zip --dataset_config_pbtxt ""/leostore/analysis/development/liteng/deepvariant_test/test_train.config.txt"" --start_from_checkpoint inception_v3.ckpt; ```; The inception_v3.ckpt is downloaded from https://github.com/tensorflow/models/tree/master/research/slim#Data ; This is my config file:; ```; name: ""test-training-dataset""; tfrecord_path: ""/leostore/analysis/development/liteng/deepvariant_test/train_set/test_train.tfrecord.gz""; num_examples: 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/10#issuecomment-350909874:284,config,config,284,,https://github.com/google/deepvariant/issues/10#issuecomment-350909874,2,['config'],['config']
Modifiability,"Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3349,config,configured,3349,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"Thank you for the reply. I am running it on a linux server instead of using GCE. . The ""sec per 100"" is ""2.68 sec per 100"". My machine has 16Gb RAM and DeepVariant added 1-2 Gb RAM usage and total RAM usage peaks at 5Gb with 8 cores fully utilized. The data file, make_examples and call_variants result are all written to local storage instead of network storage. . I was trying to get a timing profile and check if the program runs as expected. One thing I am curious is that the time taken to run make_examples matches the report time closely, however, the call_variants is much slower. I wonder if there is any problem in setting or configuration as the reported time of call_variants is much faster.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/74#issuecomment-391321427:636,config,configuration,636,,https://github.com/google/deepvariant/issues/74#issuecomment-391321427,1,['config'],['configuration']
Modifiability,"Thank you for your reply!; 1、My machine doesn't have `/usr/bin/python3`.I don't have permission to change `/usr/bin/`.I tried to modify the source code, but I couldn't locate the code.; 2、I tried to install Singularity. I use the parameter `--without -- seccomp --without -- conmon` because of the following error; `./mconfig --without-suid --without-seccomp --without-conmon --prefix=/path/singularity && make -C ./builddir &&make -C ./builddir install`; ```; seccomp headers are required to build Singularity with seccomp support.; To disable seccomp support run mconfig using '--without-seccomp'. Cannot build conmon for OCI support without libseccomp headers.; Use --without-conmon to disable build and use conmon on PATH if present.; ```; Then I try to run it; ```; singularity run -B /path/locale/:/path/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /path/dpv_singu \; > --model_type=PACBIO \; > --ref=/path/ref_fasta/QJref.fa \; > --reads=/path/bam_files/F1N_sorted.merged.addg.uniq.rmdup.bam \; > --output_vcf=/path/output.vcf.gz \; > --output_gvcf=/path/output.g.vcf.gz \; > --intermediate_results_dir /path/intermediate_results_dir; ```; The error information is as follows; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; FATAL: while handling /home/my_user_name/.singularity/cache/oci-tmp/sha256.83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18: while extracting image: root filesystem extraction failed: extract command failed: ERROR : Failed to create user namespace: user namespace disabled; : exit status 1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260:1267,sandbox,sandbox,1267,,https://github.com/google/deepvariant/issues/598#issuecomment-1359523260,1,['sandbox'],['sandbox']
Modifiability,"Thank you very much @pgrosu for such detailed answer! You are absolutely right. So, @amy-houseman, in summary, if a candidate variant passes all of the VSC's (very sensitive caller) thresholds and then the neural network prediction is confident on the genotype, `post_processing` will assign a PASS to the variant. One more thing to note, we train DeepVariant at several downsampled coverages so the model can capture the coverage variability of regions and different sequencing runs. This also makes DeepVariant robust to different coverages. Hopefully that answers your question. . @pgrosu, again thank you for such detailed and excellent answer. This Q/A is an excellent candidate for our FAQ (https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md). We maintain this as a hub for all common answers. Let us know if it would be OK if we link to your response here in our FAQ.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952:431,variab,variability,431,,https://github.com/google/deepvariant/issues/684#issuecomment-1645793952,1,['variab'],['variability']
Modifiability,Thank you very much for your quick replay.; I changed the version of the singularity to > 3 the problem was solved. however when I try to run the image using the following command ; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /home/my_username/deepvariant_1.3.0.sif --model_type=WES --ref=input/Homo_sapiens_assembly38.fasta --reads=/oldHome/my_username/exome_data/EX2015.sorted.bam \ --output_vcf=output/output.vcf.gz --intermediate_results_dir=outout --num_shards=10; it returns the following error and stop; INFO: Convert SIF file to sandbox...; ERROR : Failed to create user namespace: user namespace not supported by your system; Thanks,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620:546,sandbox,sandbox,546,,https://github.com/google/deepvariant/issues/513#issuecomment-1027691620,1,['sandbox'],['sandbox']
Modifiability,"Thanks @Stikus , I noticed this and was just looking at it!. I currently have some changes that seem to work for now:. In build_clif.sh , if you remove this block:; ```; # Configure LLVM 11 apt repository; wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; ```. and then , right in front of the line of `./INSTALL.sh` , add this line:; ```; sed -i -e 's/LLVM 11.1.0/LLVM 11/g' clif/cmake/modules/CLIFUtils.cmake; ```; Then it seems to build. Given that there are some apt installation issues, I wonder if this is temporary. But try my hack first for now. I'm considering updating our r1.2 branch to have this hack, but internally still getting the code review for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489#issuecomment-940166600:172,Config,Configure,172,,https://github.com/google/deepvariant/issues/489#issuecomment-940166600,1,['Config'],['Configure']
Modifiability,Thanks for sharing your thoughts! I was able to get some syntax highlighting to work in VSCode (though not autocomplete beyond local variables). I might try out CLion and set it up to be configured with Bazel so that it might be more aware of dependencies for autocompleting. Thanks!,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792#issuecomment-2012320998:133,variab,variables,133,,https://github.com/google/deepvariant/issues/792#issuecomment-2012320998,2,"['config', 'variab']","['configured', 'variables']"
Modifiability,"Thanks for the reply. I think it solves my problem.; I also agree that loading all variables is not the best, but I'd like to try the suggested code and check if the model would be the same first.; But still I think some vars like the EMA ones should be loaded in the warm-up stage, and I'll try to figure out what vars are needed.; I'll reply here if I make any further progress.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-495054046:83,variab,variables,83,,https://github.com/google/deepvariant/issues/185#issuecomment-495054046,1,['variab'],['variables']
Modifiability,"Thanks for the response Dr. Nattestad. I was only given the hg19.fa files along with the cram files. ; Initially I discovered I was using an old version of Samtools so once I updated I tried using use the hg19.fa. I have updated and rebuilt my fai files using the command samtools faidx hg19.fa . I've set my environment variables as the following:; BIN_VERSION=""0.10.0""; BASE=""${HOME}/deepvariant-run""; INPUT_DIR=""${BASE}/input""; REF=""hg19.fa""; BAM=""2009617.cram""; OUTPUT_DIR=""${BASE}/output""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_VCF=""2009617.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; OUTPUT_GVCF=""2009617.bam.g.vcf.gz""; Here is the command to execute deepvariant:; sudo docker run \; -v ""${DATA_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/input/${REF}"" \; --reads=""/input/${BAM}"" \; --output_vcf=/output/${OUTPUT_VCF} \; --output_gvcf=/output/${OUTPUT_GVCF} \; --num_shards=$(nproc); and here is the new errors I'm seeing:; ValueError: Failed precondition: Cannot query without an index; parallel: This job failed:. Which eventually leads to the following errors where the process fails and ends: Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 15 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples; --mode calling --ref ""/input/hg19.fa"" --reads ""/input/2009617.cram"" --exampl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/307#issuecomment-628230170:321,variab,variables,321,,https://github.com/google/deepvariant/issues/307#issuecomment-628230170,1,['variab'],['variables']
Modifiability,"Thanks for your reply. Hopefully reads seem to map with a MAPQ of 60 so Deepvariant should see them. We believe the issue is high SNP density making some standing variant hard to call. It's also very possible there is no signal, i.e. the bdelloids have evolved a very low mutation rate and we are chasing ghosts. Since they reproduce asexually (at least in the lab) by automixis, it is a possibility. We developed a simple script, for each SNP that seems to be a de novo one, we look in the ancestral pileup, and we always find the SNP there but not called. I think the high density messes up callers internal maths, and some SNP get a low chance of being called. Anyway, the ONT sequencing is ongoing. We will see with the pileup there. There are also solutions with comparing assembly graphs.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716#issuecomment-1761318730:253,evolve,evolved,253,,https://github.com/google/deepvariant/issues/716#issuecomment-1761318730,2,['evolve'],['evolved']
Modifiability,"That's good, but let's go a bit slower just to be sure each individual component is working properly. I'm not sure Bazel is working properly, so let's try the following steps:. 1) Complete these last steps of `clif`:. ```; sudo mkdir -p /usr/clang/bin/; sudo ln -sf /usr/local/bin/clif-matcher /usr/clang/bin/clif-matcher; sudo mkdir -p /usr/local/clif/bin; sudo ln -sf /usr/local/bin/pyclif* /usr/local/clif/bin/; DIST_PACKAGES_DIR=$(python3 -c ""import site; print(site.getsitepackages()[0])""); sudo ln -sf ""${DIST_PACKAGES_DIR}""/clif/python /usr/local/clif/; ```. 2) Let's troubleshoot `bazel`, as `bazel` is also a bit tricky to install. First do the following:. ``sudo mv /root/.bazel /root/.bazel-orig``; ``sudo mv /root/bin/bazel /root/bin/bazel-orig``. Could you try the following steps and let me know what you see -- it would be nice to run as sudo and not as root directly:. ```; rm .bazelrc; curl -L -O https://github.com/bazelbuild/bazel/releases/download/5.3.0/bazel-5.3.0-installer-linux-x86_64.sh; chmod +x bazel-*.sh; ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; ```. When you run it and launch it, it should look something like this:. ```; $ ./bazel-5.3.0-installer-linux-x86_64.sh --user > /dev/null; Extracting Bazel installation...; Starting local Bazel server and connecting to it...; $ bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; ...; ```. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377:1809,config,configurations,1809,,https://github.com/google/deepvariant/issues/657#issuecomment-1577183377,1,['config'],['configurations']
Modifiability,The error messages sounds like it's not seeing your fasta index file. Since you are utilizing environment variables I would suggest verifying that those are accurate and that both your fasta and it's corresponding index file are present in that same directory.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/310#issuecomment-636143865:106,variab,variables,106,,https://github.com/google/deepvariant/issues/310#issuecomment-636143865,1,['variab'],['variables']
Modifiability,"The root cause here is that we are cloning a specific version of Tensorflow, while it (and other things) evolved to deal with the Bazel change. Normally, though, we don't want to just clone from TF HEAD because that can break us in other ways. So a fix is to change settings.sh to use a more recent commit, like; export DV_TENSORFLOW_GIT_SHA=""97a4c226e8a9e7c5c36fc38e2b9f8459c77abd5a""; (That compiles, but is otherwise untested.)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19#issuecomment-353481304:105,evolve,evolved,105,,https://github.com/google/deepvariant/issues/19#issuecomment-353481304,1,['evolve'],['evolved']
Modifiability,"The thing is that InceptionV3 works on one combined tensor (using all channels), while slicing them together across different convolutions. Spatial information and cross-channel information can be important if you want to train on those types of inputs, where layers of [depthwise separable convolutions (Xception architecture)](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf) across inputs and [squeeze-excitation networks (SEnet) ](https://arxiv.org/pdf/1709.01507.pdf) become useful in boosting features that travel together. The thing is that the network would need to be tweaked by applying separate input convolutions for only channels that contain the spatial information and not others, as currently DeepVariant models are trained with a collection of channels of which some are only binary possibly only having some effect with more data as the other channels normalize -- though they might become more purposeful as filters. Using additional attention models or custom filters at specific locations across the network can highlight features of interest like variants that possibly travel together, that trigger a separate model flow for the resultant output of interest.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520#issuecomment-1561873437:260,layers,layers,260,,https://github.com/google/deepvariant/issues/520#issuecomment-1561873437,1,['layers'],['layers']
Modifiability,"The vcf_stats_report script can only use the data that is in the VCF file. Since this is built for DeepVariant VCF files, other callers may not include all the same metrics in their output VCFs. We made sure the script is flexible enough to finish creating a report despite this missing info, but the charts cannot be made if the data isn't there.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/267#issuecomment-581533399:222,flexible,flexible,222,,https://github.com/google/deepvariant/issues/267#issuecomment-581533399,1,['flexible'],['flexible']
Modifiability,"Unfortunately, I have no access to similar 64 cores configuration but I tried once again Xeon 6258R which has 28 cores on 8 chromosomes:. | [Intel® Xeon® Gold 6258R](https://devcloud.intel.com/edge/devices/intel-xeon-gold-6258r-cpu/) | make_examples | call_variants	| postprocess_variants |; |---|---|---|---|; | TensorFlow MKL-DNN | 58m54.584s | 103m44.907s | 19m27.091s |; | OpenVINO | 59m2.299s | 68m25.176s (x1.51) | 19m36.495s |. I think more number of cores will show more speedup. ```bash; python3 ./bazel-deepvariant/scripts/run_deepvariant.py \; --model_type=WGS \; --ref=./input/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz \; --reads=./input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.bam \; --output_vcf=${OUTPUT_DIR}/HG002.output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/HG002.output.g.vcf.gz \; --num_shards=16 \; --regions ""chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8"" \; --call_variants_extra_args=""use_openvino=True""; ```. @pichuan, GCP team denied an access to 64 cores machine, unfortunately.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-725869282:52,config,configuration,52,,https://github.com/google/deepvariant/pull/363#issuecomment-725869282,1,['config'],['configuration']
Modifiability,"Use could build docker image with this command. It needs to be run from 'deepvariant' directory.; I used arbitrary values for PROJECT_ID and VERSION_NUMBER, you may replace them. . PROJECT_ID=my-deepvariant-docker; VERSION_NUMBER=0.7.2 ; gcloud builds submit --project ""${PROJECT_ID}"" --config cloudbuild.yaml --substitutions TAG_NAME=""${VERSION_NUMBER}"" --timeout 2h .",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99#issuecomment-428366972:287,config,config,287,,https://github.com/google/deepvariant/issues/99#issuecomment-428366972,1,['config'],['config']
Modifiability,"We are 80% of the way there on CRAM support. Under the hood DeepVariant uses htslib to read it's reads datasets, which supports cram. But we need to extend the IO systems to pass in the reference genome to read a CRAM file, and that's not possible right now. We are hopeful that the community would extend DeepVariant to handle CRAM files, so please feel free to contribute a pull request with the functionality if you decide to add it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/38#issuecomment-357025416:149,extend,extend,149,,https://github.com/google/deepvariant/issues/38#issuecomment-357025416,2,['extend'],['extend']
Modifiability,"We've started testing DeepVariant on a machine with 128 cores from AMD. Setting --num_shards=$(nproc) results in this error:. > OpenBLAS blas_thread_init: RLIMIT_NPROC 4096 current, 8254915 max; > OpenBLAS blas_thread_init: pthread_create failed for thread 63 of 64: Resource temporarily unavailable. It seems to create way more threads than the machine can handle. Setting num_shards=40 will work on the AMD machine, but the number of shards it creates is variable and much more than 40. I've seen 81, 116, and 101 intermediate shards created. From what I remember, in all our previous usage on machines with 40 cores or less the number of shards always matched the number of cores when using num_shards=$(nproc). The AMD machine with num_shards=40 also runs much slower compared to our Skylake machines with 40 cores and num_shards=$(nproc). The AMD machine takes over 7 hours per WGS file compared to less then 5 hours with the Skylake machine. It looks like when we try to run on the AMD machine it creates 2x as many tasks than available processors which might explain the slowdown.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/274#issuecomment-597715853:457,variab,variable,457,,https://github.com/google/deepvariant/issues/274#issuecomment-597715853,1,['variab'],['variable']
Modifiability,"What's your opinion of DeepVariant performance on non model organisms? In its current state? After all people use GATK all the time and we can't say it's particularly tailored for anything else than human. Sent from ProtonMail mobile. -------- Original Message --------; On 14 Jun 2023, 18:59, Andrew Carroll wrote:. > Hi ***@***.***(https://github.com/Axze-rgb); >; > We do have interest in potentially releasing some non-human models. The mosquito model was trained with a much older version of DeepVariant, and there have no been so many improvements to the main branch, it would probably make sense to train a new one as opposed to release the old one.; >; > The main limitation is actually high quality training data. For mosquitos, the advantage we have is the extended pedigree. Are there are any good sources for the sort of labelled training data we might need for this?; >; > Thanks,; > Andrew; >; > —; > Reply to this email directly, [view it on GitHub](https://github.com/google/deepvariant/issues/661#issuecomment-1591660025), or [unsubscribe](https://github.com/notifications/unsubscribe-auth/ATOL54WNMFQN53D3HST5553XLHUYLANCNFSM6AAAAAAZGA4ITE).; > You are receiving this because you were mentioned.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538:767,extend,extended,767,,https://github.com/google/deepvariant/issues/661#issuecomment-1593485538,1,['extend'],['extended']
Modifiability,"Yeah, but @pichuan folks will then ask the obvious question of what would be the right number of shards to get the correct variant calls. Probably some validation tests might be required with comparative benchmarks that show consistent predictability. I'm sure you guys have done some of this already, which would be nice if it could be shared for optimally configuration of the settings. Ideally it's something that users can then replicate themselves, in order to run verification tests at different granular stages of the DeepVariant workflow - and would be compared to validated metrics for proper evaluation - just to convince the users all is working properly.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112#issuecomment-433250645:358,config,configuration,358,,https://github.com/google/deepvariant/issues/112#issuecomment-433250645,1,['config'],['configuration']
Modifiability,"Yeah, that's a Debian instance. You will need an Ubuntu 16 instance. There are instructions for configuring an Ubuntu 16 instance in; https://github.com/google/deepvariant/blob/master/docs/deepvariant-case-study.md. The relevant command is. gcloud beta compute instances create ""${USER}-deepvariant-casestudy"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" --image-project ""ubuntu-os-cloud"" \; --machine-type ""custom-64-131072"" \; --boot-disk-size ""300"" --boot-disk-type ""pd-ssd"" \; --boot-disk-device-name ""deepvariant-casestudy"" \; --zone ""us-west1-b"". On Fri, Dec 22, 2017 at 10:01 PM, Zihua Liu <notifications@github.com>; wrote:. > Hi, Thomas; > The output of uname -a is; >; > Linux cs-6000-devshell-vm-d5fc574d-7d85-4a60-b2ee-5248f8d10e41; > 3.16.0-4-amd64 #1 <https://github.com/google/deepvariant/issues/1> SMP; > Debian 3.16.51-2 (2017-12-03) x86_64 GNU/Linux; >; > I run it on Google Cloud Platform; >; > —; > You are receiving this because you commented.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/22#issuecomment-353703108>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AWascUbKdMeaUed4ob1t06DxAKeuXaIbks5tDGzxgaJpZM4RLiml>; > .; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/22#issuecomment-353703390:96,config,configuring,96,,https://github.com/google/deepvariant/issues/22#issuecomment-353703390,1,['config'],['configuring']
Modifiability,"Yes, The problem caused by $HOME variable in bash.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/141#issuecomment-455963819:33,variab,variable,33,,https://github.com/google/deepvariant/issues/141#issuecomment-455963819,1,['variab'],['variable']
Modifiability,"Yes, if you use `run_deepvariant` (which is a wrapper for the 3 separate steps), the `*_extra_args` flags are just a more flexible way to allow you specify flags for each step. ; In `run_deepvariant`, we hard-coded some of the commonly used flags but not all of them. For example, you can't directly specify `--ws_use_window_selector_model` to `run_deepvariant`.; We thought people might eventually have use cases for all other less known flags. (Which is why the `*_extra_args` flags exist, but haven't been advertised or documented other than just the flag description. `make_examples_extra_args` description can be found here:; https://github.com/google/deepvariant/blob/97cd861800ccb43d750f392b518e99d514adddd8/scripts/run_deepvariant.py#L92-L96. Regarding the downside of turning off `ws_use_window_selector_model` in make_examples -- yes, it'll be slower. It's a small model where we used to decide whether we need to realign a window or not. When we set it to default, we found that it significantly decreased runtime, with negligible trade-offs. . You can find the description of this feature when it's first released in v0.7:; https://github.com/google/deepvariant/releases/tag/v0.7.0. ""Changed window selector to use a linear decision model for choosing realignment candidates. This can be controlled by a flag. `-ws_use_window_selector_model` which is now on by default.""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272#issuecomment-586546504:122,flexible,flexible,122,,https://github.com/google/deepvariant/issues/272#issuecomment-586546504,1,['flexible'],['flexible']
Modifiability,"Yes, the error log is the same. I also added `unset PYTHONPATH` before the singularity command. How can I prevent it from ; using the local libraries? . ```; INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal import api_implementation; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/internal/api_implementation.py"", line 104, in <module>; from google.protobuf.pyext import _message; TypeError: bases must be types; INFO: Cleaning up image...; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106:197,sandbox,sandbox,197,,https://github.com/google/deepvariant/issues/580#issuecomment-1304645106,1,['sandbox'],['sandbox']
Modifiability,"Yes, you can't do that with bazel - it doesn't allow you to do absolute path operations like that in general due to their approach to sandboxing / hermetic builds. I suspect moving CLIF to the expected location may fix it.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/12#issuecomment-351271748:134,sandbox,sandboxing,134,,https://github.com/google/deepvariant/issues/12#issuecomment-351271748,1,['sandbox'],['sandboxing']
Modifiability,"You need to specify all the variable in the same script. For example:. ```; #!/usr/bin/zsh; BIN_VERSION=""0.8.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/195#issuecomment-509390976:28,variab,variable,28,,https://github.com/google/deepvariant/issues/195#issuecomment-509390976,1,['variab'],['variable']
Modifiability,_hs37d5/hs37d5.fa*. -rw-rw-r-- 1 zhoujianglin zhoujianglin 3042M Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 5985M Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 9725M Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 1M May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 749M Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac; singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Hum,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:3484,sandbox,sandbox,3484,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['sandbox'],['sandbox']
Modifiability,"``; gcr.io/deepvariant-docker/deepvariant:0.5.1; ```. I get this error instead:; ```; WARNING: Logging before flag parsing goes to stderr.; W0307 21:19:10.482634 140039107020544 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; W0307 21:19:10.488955 140039107020544 call_variants.py:299] Unable to read any records from shardedExamples/examples.tfrecord@64.gz. Output will contain zero records.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 391, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 382, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/call_variants.py"", line 317, in call_variants; predictions = model.create(images, 3, is_training=False)['Predictions']; File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/genomics/deepvariant/modeling.py"", line 360, in create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 483, in inception_v3; depth_multiplier=depth_multiplier); File ""/tmp/Bazel.runfiles_i7Wypy/runfiles/org_tensorflow_slim/nets/inception_v3.py"", line 104, in inception_v3_base; net = slim.conv2d(inputs, depth(32), [3, 3], stride=2, scope=end_point); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py"", line 181, in func_with_args; return func(*args, **current_args); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/layers/python/layers/layers.py"", line 1011, in convolution; input_rank); ValueError: ('Convolution not supported for input with rank', 1); ```; Have you seen this error before?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371288942:1912,layers,layers,1912,,https://github.com/google/deepvariant/issues/52#issuecomment-371288942,3,['layers'],['layers']
Modifiability,"`tools/build_clif.sh` was successful!; Thank you!. After that I tried running this:; ```; > sudo docker run google/deepvariant:""${BIN_VERSION}""; WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested; exec /opt/deepvariant/bin/run_deepvariant: exec format error; > sudo docker run --platform linux/amd64 google/deepvariant:""${BIN_VERSION}""; exec /opt/deepvariant/bin/run_deepvariant: exec format error; ```. Sorry, but what should I run before this?; Should I run the `./build-prereq.sh` and `./build_and_test.sh`?; Should I uncomment the `./run-prereq.sh` sections as well?. Actually, I am currently running the `./build-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 03:51:21 PM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 03:51:23 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:24 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 8653k 0 --:--:-- --:--:-- --:--:-- 8623k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:1013,config,config,1013,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/ir,tensorflow/compiler/mlir/tfrt/tests/analysis,tensorflow/compiler/mlir/tfrt/tests/jit,ten",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:5250,Inherit,Inherited,5250,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['Inherit'],['Inherited']
Modifiability,"a21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7835,config,config,7835,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"age 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 03:51:40 PM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 03:51:41 PM UTC] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 03:51:42 PM UTC] Stage 'Install bazel' starting; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: cannot execute binary file: Exec format error; /root/bin/bazel: line 220: /root/.bazel/bin/bazel-real: Success; ~/bazel /media/HostShared/deepvariant-r1.5; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 46.5M 100 46.5M 0 0 23.2M 0 0:00:02 0:00:02 --:--:-- 27.1M; /media/HostShared/deepvariant-r1.5; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Download and configure TensorFlow sources' starting; ========== [Mon 05 Jun 2023 03:51:44 PM UTC] Stage 'Cloning TensorFlow from github as ../tensorflow doesn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Ex",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:6032,config,configure,6032,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['configure']
Modifiability,"ains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:9053,config,config,9053,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['config']
Modifiability,"alculateGenotypePosteriors refinement](https://gatk.broadinstitute.org/hc/en-us/articles/360037226592-CalculateGenotypePosteriors), and [an additional informational link](https://hpc.nih.gov/training/gatk_tutorial/workflow-overview.html); * [DeNovoGear](https://github.com/ultimatesource/denovogear). The key point to take away from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), o",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:3394,config,config,3394,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,6,['config'],['config']
Modifiability,"and configured Tensorflow from GitHub (we might need to checkout the proper version). The final step is to build the DeepVariant binaries. Just a note, you will require several GB of space for this to work. The DeepVariant zip files we eventually get will be quite small. (The zip files are basically Python scripts and bytecode with a compiled shared library.). Regarding what's missing or incompatible during the build, we will let the compiler tell us that, which will make it a bit easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1160,config,configure,1160,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['config'],['configure']
Modifiability,"and son samples)."". The DeepTrio manuscript has benchmarks for this strategy in the following section (near the end of results):. > The Genome in a Bottle truth sets do not contain chromosomeX or chromosomeY variants in a; > male individual. As a result, DeepTrio has never been trained with hemizygous sites. Because we; > train DeepTrio to perform duo calling, it is likely that DeepTrio would call variants on; > chromosomeX similar to how it would call a duo sample. To assess this, we ran DeepVariant and; > DeepTrio on chromosomeX of the son (HG002) and measured the number of heterozygous; > variant calls in the non-PAR regions of chromosomeX.; > ; > For DeepVariant, 4.45% (455/101866) of calls in non-PAR regions of chromosomeX are; > heterozygous. In DeepTrio, 24.5% (21633/88314) are heterozygous. This substantial difference; > suggests that applying DeepTrio directly to chromosomeX in male samples is problematic.; > Since chromosomeX in males is inherited from the mother, we performed calling on; > chromosomeX with only the mother provided as the parent. This reduced heterozygous calls to; > 3.37% (3518/104427), which is better than in the DeepVariant case. For male samples, this; > recommends that variant calling should be run with both parents on the autosomal and PAR; > regions using a BED file to restrict location, and additional variant calling should be performed; > using only the mother’s file provided as parent for the non-PAR regions of chromosomeX, and; > only the father’s provided for the non-PAR regions of chromosomeY.; > ; > This experiment indicates that allowing the model to infer a hemizygous chromosome through; > coverage and explicitly training for hemizygous variants is an opportunity for improvement,; > both for DeepVariant and DeepTrio. Over the long term:. We anticipate that the T2T consortium will be shortly be making available complete assemblies of ChrX and ChrY for the HG002 sample. This should allow us to make training labels for ChrX an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025:1855,inherit,inherited,1855,,https://github.com/google/deepvariant/issues/518#issuecomment-1045294025,2,['inherit'],['inherited']
Modifiability,"around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7770,config,config,7770,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"at might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![image](https://github.com/google/deepvariant/assets/6555937/13e95fe0-71b1-40aa-bccc-4d8c5463de6f). We want to see for which genotype the set of patterns (the feature map above) maximizes for, which will indicate the genotype present with a specific maximal probability. First we test for $`homozygous`$ $`reference`$:. ![image](https://github.com/google/deepvariant/assets/6555937/be5e3074-4c2f-4600-9ea3-9cb6bfda58f8). Next we test for $`heterozy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:3781,layers,layers,3781,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['layers'],['layers']
Modifiability,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8665,Config,Configuration,8665,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,1,['Config'],['Configuration']
Modifiability,"ation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.22.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WAR",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:8188,Config,Configuration,8188,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['Config'],['Configuration']
Modifiability,"atures; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") ; -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") ; -- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.8.so (found version ""3.8.10"") ; -- Configuring done; -- Generating done; -- Build files have been written to: /root/clif/build; ```; which succeeded (and then proceed to the next step). @pioneer-pi , given that I can't reproduce this error, I'll need more information from you to understand why it failed. This is the step where your setup failed , but mine worked:. ```bash; root@pichuan-cpu:/home/pichuan/deepvariant# cd /root/clif/build; root@pichuan-cpu:~/clif/build# cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; ```. It'll be good to understand how/why it failed on your side. If you can provide more information there, I'm happy to see how I can help. And @pioneer-pi , another question is : Can you consider using our Docker? If you're trying to install on a machine, you must already have root permission. (Our ""build from source"" option only works with root permission)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:3246,Config,Configuring,3246,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['Config'],['Configuring']
Modifiability,"ave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0911 02:28:45.531516 139937686464256 estimator.py:1147] Calling model_fn.; W0911 02:28:45.537137 139937686464256 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0911 02:28:50.978360 139937686464256 estimator.py:1149] Done calling model_fn.; I0911 02:28:52.395916 139937686464256 monitored_session.py:240] Graph was finalized.; I0911 02:28:52.397383 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:53.423366 139937686464256 session_manager.py:500] Running local_init_op.; I0911 02:28:53.495537 139937686464256 session_manager.py:502] Done running local_init_op.; I0911 02:28:54.010662 139937686464256 modeling.py:415] Reloading EMA...; I0911 02:28:54.011811 139937686464256 saver.py:1284] Restoring parameters from /opt/models/wgs/model.ckpt; I0911 02:28:56.213706 139937686464256 call_variants.py:444] Processed 1 examples in 1 batches [1145.647 sec per 100]; I0911 02:28:56.265293 13993768646",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:9228,layers,layers,9228,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,2,['layers'],['layers']
Modifiability,"ay from this is not that there are options, but how these options internally work to infer the genotype and its probability given the data. Some work better with longer reads, and some with shorter reads. You want to play with them to get a feel of what is happening given different data. If you are curious, you can read the papers and mathematics behind each approach, and you'll be surprised by their similarity in approaches of inferring the call and its probability (quality). I have included a list of papers with links in the reference section below. Now if the above is too easy, and you want to make _de novo_ variant calling more exciting, you can use the `glnexus` with the config `--config DeepVariant_unfiltered`, which is basically the following [Yaml config file](https://github.com/google/deepvariant/blob/r1.5/deepvariant/cohort_best_practice/DeepVariant_unfiltered_v1.yml) indicating to GLnexus to operate [under specific parameters conditions](https://github.com/dnanexus-rnd/GLnexus/wiki/Configuration). So when you perform GLnexus joint variant calling, you will get the three sample columns (father/mother/child) in your joint VCF. To determine a _de novo_ call, you just look for genotypes that would not follow Mendelian inheritance, such as `0/0 0/0 0/1`, such as:. ```; chr7 54624683 chr7_54624683_A_AATC A AATC 27 . AF=0.166667;AQ=27 GT:DP:AD:GQ:PL:RNC 0/0:39:22,16:28:27,0,48:.. 0/0:40:40,0:50:0,120,1199:.. 0/1:28:28,0:50:0,90,899:..; ```; Though keep in mind DeepTrio/GLnexus might produce [false positives](https://www.technologynetworks.com/genomics/news/false-positives-a-problem-for-snp-chips-345637) - based on low read quality (low MAPQ), or other factors such as over-representation of multi-site aligned reads - where such a call might be labeled `0/1 0/0 0/0`, with IGV supporting more the call of `0/1 0/1 0/0`. Otherwise if the read quality is good, and alignments are unique with proper coverage then it might actually be _de novo_, though the proband (child)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969:3717,Config,Configuration,3717,,https://github.com/google/deepvariant/issues/699#issuecomment-1716447969,1,['Config'],['Configuration']
Modifiability,"bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: py",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8312,config,config,8312,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"ble advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8148,config,config,8148,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8451,config,config,8451,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['config']
Modifiability,cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai': No such file or directory; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac': No such file or directory; INFO: Cleaning up image... singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data/toolsDB/deepvariant.sif which ls; ls -al /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls; INFO: Cleaning up image...; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. The real case command `singularity run -B /usr/lib/locale/:/usr/lib/locale/ /lustre/Data,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269:4527,sandbox,sandbox,4527,,https://github.com/google/deepvariant/issues/653#issuecomment-1575922269,1,['sandbox'],['sandbox']
Modifiability,"chuan-cpu:~/deepvariant$ git log | head; commit ab068c4588a02e2167051bd9e74c0c9579462b51; Author: pichuan <pichuan@google.com>; Date: Mon Feb 27 23:03:48 2023 -0800. Update README.md; ; PiperOrigin-RevId: 512838102. ```. From there, I followed the instructions on https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; So I ran:. ```bash; sudo su; ./build-prereq.sh; ```. My run succeeded. I looked at my log to see the section close to where your error occurred. And I see:. ```; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; -- Found GTest: /usr/local/lib/cmake/GTest/GTestConfig.cmake (found version ""1.10.0"") ; -- Found PythonInterp: /usr/local/bin/python3 (found version ""3.8.10"") ; -- Found PythonLibs: /usr/lib/x86_6",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785:2449,config,config,2449,,https://github.com/google/deepvariant/issues/739#issuecomment-1823278785,1,['config'],['config']
Modifiability,"common-protos 1.59.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 3.13.0 which is incompatible.; pyclif 0.4 requires pyparsing==2.2.0, but you have pyparsing 3.0.9 which is incompatible.; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'run-prereq.sh complete' starting; ========== [Mon 05 Jun 2023 10:22:31 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=mon",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:6738,config,configure,6738,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['configure']
Modifiability,"cs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I1219 05:41:38.123484 139694699493120 call_variants.py:435] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; WARNING:tensorflow:From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_deterministi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:2143,config,config,2143,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['config'],['config']
Modifiability,"d to replicate this issue, and here is what I found. I created an instance from [this CentOS7 VM](https://console.cloud.google.com/marketplace/details/centos-cloud/centos-7). I chose the default location for all installations. When asked if I wanted to update my PATH during installations, I chose to do so. I was able to install DeepVariant through Bioconda using the below steps. . I ran into a particular error with `gsutil`. After running `source ~/.bashrc`, I saw an error when I ran `gsutil`. `gsutil` is used by the DeepVariant installation, so that failed as well. To address this, I referenced [this post](https://stackoverflow.com/questions/38783140/importerror-no-module-named-google-compute-engine) and ran `export BOTO_CONFIG=/dev/null` before installing DeepVariant again. Running these commands in order allows me to successfully install on the VM. ```; # install gsutil; curl https://sdk.cloud.google.com | bash; exec -l $SHELL; # verify that gsutil is working; gsutil. # install wget and bzip2, which are both needed to download miniconda; sudo yum install bzip2 wget; wget https://repo.anaconda.com/miniconda/Miniconda2-latest-Linux-x86_64.sh; bash Miniconda2-latest-Linux-x86_64.sh ; source ~/.bashrc. # gsutil is failing now; gsutil; export BOTO_CONFIG=/dev/null; # gsutil should be working again; gsutil. # create new conda env, add channels, install deepvaraint; conda create -n dv python=2.7; conda activate dv; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda install -n dv deepvariant -v; ```. In the output from running `conda install -n dv deepvariant -v`, I see the first error you posted even with a successful installation. I was not able to replicate the second error. Some sanity checks for you:. * Are you able to successfully run `gsutil`?; * Did you add all conda channels in the correct order?; * Could you post the entire output from running `conda install -v deepvariant`?. CC @melkerdawy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-452921108:1457,config,config,1457,,https://github.com/google/deepvariant/issues/137#issuecomment-452921108,3,['config'],['config']
Modifiability,"d, and has a very practical use-case for many users that would like to automate their analysis. I'm sort of hinting at something else. So just imagine you are pitching this to some Bioinformatician that has been doing their analysis on an internal cluster for years. S/he has heard from others of the benefits of DeepVariant through Google Cloud resources, and now has a little bit of free time to try it out to convince their boss to use it. The idea is that the root directory should be as simple as possible, so users don't get overwhelmed or confused - with all the other directories in the tree flowing naturally from it. A minimal number of obvious scripts (with associated documentation) should naturally transition them from source code, through the build phase, and as quickly to the analysis process. Basically less is more, and you do not want to add more to the main directory as you begin to approach version 1.0. So ideally what would be nice is to have one build script - let's call it `build` - that reads a `config/build` directory - and does the following:. 1. Analyzes hidden build-generated files under `.build/...`, and detects if the `prereq` and `build-and-test` phases have been successfully processed. Otherwise, it would report back to the user the build status, and query the user with a recommendation on the next required step to run. For example:. ```; $ build status. Checking DeepVariant build prerequisites... OK; Checking DeepVariant test environment compatibility [GPU]... OK; Checking DeepVariant build binaries... MISSING. Proceed with building binaries [Y/n]?. ```. 2. Once the builds have been successfully completed, then it would tell the user the available options to run (i.e. `make_examples`, `call_variants`, `postprocess_variants`, `pkrusche/hap.py`, etc.) as well as any analysis that has been completed if a `results` or `analysis-results` folder is present. 3. For Cloud Build, that would also be part of the `build` script, when they provide the `help",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/87#issuecomment-413745335:1075,config,config,1075,,https://github.com/google/deepvariant/issues/87#issuecomment-413745335,2,['config'],['config']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an inv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8020,config,configured,8020,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4446,config,configured,4446,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Targ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6400,config,configured,6400,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"d-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.l",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2581,config,configured,2581,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"dSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session; config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore; err, ""a Variable name or other graph key that is missing""); tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, arg",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:15829,Variab,Variable,15829,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['Variab'],['Variable']
Modifiability,"de -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc '-f-I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags_gflags -Ibazel-out/k8-opt/bin/external/com_github_gflags_gflags -Iexternal/com_google_protobuf -Ibazel-out/k8-opt/bin/external/com_google_protobuf -Iexternal/zlib -Ibazel-out/k8-opt/bin/external/zlib -Iexternal/local_config_python -Ibazel-out/k8-opt/bin/external/local_config_python -Iexternal/com_google_protobuf/src -Ibazel-out/k8-opt/bin/external/com_google_protobuf/src -Iexternal/local_config_python/python_include -Ibazel-out/k8-opt/bin/external/local_config_python/python_include -Iexternal/local_config_cc -std=c++17' third_party/nucleus/util/python/math.clif); # Configuration: dc5f3e671e47dda93c7e5877bba31e0dc0cd0cda8bb0d2473db6563f34168f46; # Execution platform: @local_execution_config_platform//:platform. Line .123456789.123456789.123456789.123456789; 1:# Copyright 2018 Google LLC.\n; 2:#\n; 3:# Redistribution and use in source and binary forms, with or without\n; 4:# modification, are permitted provided that the following conditions\n; 5:# are met:\n; 6:#\n; 7:# 1. Redistributions of source code must retain the above copyright notice,\n; 8:# this list of conditions and the following disclaimer.\n; 9:#\n; 10:# 2. Redistributions in binary form must reproduce the above copyright\n; 11:# notice, this list of conditions and the following disclaimer in the\n; 12:# documentation and/or other materials provided with the distribution.\n; 13:#\n; 14:# 3. Neither the name of the copyright holder nor the names of its\n; 15:# contributors may be used to endorse or promote products derived from this\n; 16:# software without specific prior written permission.\n; 17:#\n; 18:# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLD",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:12543,Config,Configuration,12543,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['Config'],['Configuration']
Modifiability,"dir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1"") ; -- Checking for module 'protobuf'; -- Found protobuf, version 3.13.0; -- Checking for module 'libglog'; -- Found libglog, version 0.4.0; -- Looking for pthread.h; -- Looking for pthread.h - found; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"". real	2m44.183s; user	0m18.337s; sys	0m18.865s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:6138,config,configuration,6138,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,3,"['Config', 'config']","['Configuring', 'configuration']"
Modifiability,"e list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6215,config,configured,6215,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"e more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more fluid hierarchical analysis to take place down the line. Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:3097,refactor,refactoring,3097,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['refactor'],['refactoring']
Modifiability,"e of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7437,config,configuring,7437,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['configuring']
Modifiability,"e when there is; # an official solution from CLIF.; # GitHub issues such as https://github.com/google/deepvariant/issues/29 has; # some relevant pointers. set -eux -o pipefail. # Figure out which linux installation we are on to fetch an appropriate version; # of CLIF binary. Note that we only support now Ubuntu (14 and 16), and Debian.; if [[ $(python -mplatform) == *""Ubuntu-16""* ]]; then; export DV_PLATFORM=""ubuntu-16""; # For ubuntu 16 we install cmake; sudo -H apt-get -y install cmake; elif [[ $(python -mplatform) == *""Ubuntu-14""* ]]; then; export DV_PLATFORM=""ubuntu-14""; # For ubuntu 14 we install cmake3; sudo -H apt-get -y install cmake3; elif [[ $(python -mplatform | grep '[Dd]ebian-\(rodete\|9.*\)') ]]; then; export DV_PLATFORM=""debian""; # For recent debian, we install cmake.; sudo -H apt-get -y install cmake; else; export DV_PLATFORM=""unknown""; exit ""unsupported platform""; fi. CLIF_DIR=/usr/local/clif; CLIF_PACKAGE=""oss_clif.${DV_PLATFORM}.latest.tgz"". # Install prereqs.; sudo -H apt-get -y install ninja-build subversion; sudo -H apt-get -y install virtualenv python-pip pkg-config; sudo -H pip install 'pyparsing>=2.2.0'; sudo -H pip install 'protobuf>=3.4'. echo === building protobufs. sudo -H apt-get install -y autoconf automake libtool curl make g++ unzip; wget https://github.com/google/protobuf/releases/download/v3.4.1/protobuf-cpp-3.4.1.tar.gz; tar xvzf protobuf-cpp-3.4.1.tar.gz; (cd protobuf-3.4.1 &&; ./autogen.sh &&; ./configure &&; make -j 32 &&; make -j 32 check &&; sudo make -j 32 install &&; sudo ldconfig). echo === building CLIF. git clone https://github.com/google/clif.git; sed -i 's/\$HOME\/opt/\/usr\/local/g' clif/INSTALL.sh; sed -i 's/-j 2//g' clif/INSTALL.sh; (cd clif && sudo ./INSTALL.sh). echo === creating package tgz. sudo find ${CLIF_DIR} -type d -exec chmod a+rx {} \;; sudo find ${CLIF_DIR} -type f -exec chmod a+r {} \;; tar czf ""${CLIF_PACKAGE}"" /usr/local/lib/libproto* ""${CLIF_DIR}"". echo === SUCCESS: package is ""${CLIF_PACKAGE}""; ; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-385130636:2814,config,config,2814,,https://github.com/google/deepvariant/issues/29#issuecomment-385130636,4,['config'],"['config', 'configure']"
Modifiability,e) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80],MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2212,config,configured,2212,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"eading state information... Done; The following packages will be REMOVED:; docker-ce-rootless-extras slirp4netns; 0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.; After this operation, 19.2 MB disk space will be freed.; Do you want to continue? [Y/n] Y; (Reading database ... 177786 files and directories currently installed.); Removing docker-ce-rootless-extras (5:24.0.2-1~ubuntu.20.04~focal) ...; Removing slirp4netns (0.4.3-1) ...; Processing triggers for man-db (2.9.1-1) ...; ```. Extra commands output:; ```; > llvm-config-11 --version; 11.0.0; > cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); > cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.0.0); ```. Finally the output from `sudo tools/build_clif.sh` installation of CLIF (still the same error I guess):; ```; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD; -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed; -- Looking for pthread_create in pthreads; -- Looking for pthread_create in pthreads - not found; -- Looking for pthread_create in pthread; -- Looking for pthread_create in pthread - found; -- Found Threads: TRUE ; CMake Error at clif/cmake/modules/CLIFUtils.cmake:37 (find_package):; Could not find a configuration file for package ""LLVM"" that is compatible; with requested version ""11.1.0"". The following configuration files were considered but not accepted:. /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake, version: 11.0.0; /usr/lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0; /lib/llvm-11/cmake/LLVMConfig.cmake, version: 11.0.0. Call Stack (most recent call first):; clif/CMakeLists.txt:22 (include). -- Configuring incomplete, errors occurred!; See also ""/root/clif/build/CMakeFiles/CMakeOutput.log"".; See also ""/root/clif/build/CMakeFiles/CMakeError.log"".; ```. Do you have any suggestions?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:13538,config,configuration,13538,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,5,"['Config', 'config']","['Configuring', 'configuration']"
Modifiability,ease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.goo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:1843,config,configured,1843,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"ect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-imag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:3894,config,configured,3894,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"ection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages w",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7651,config,configured,7651,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"enetic clusters forming stemming from the ancestor population. So if the mutations are more localized and are still diploid, that means it would not behave in the way true germline diploid genomes would transfer their genotypes. This means you would have more alterations -- that might be real, or stem from sub-populations of clonal events affecting your sample purity -- and thus you cannot rely on ploidy to support your allele frequency (as illustrated by your plot above). This is of course assuming artifacts of sequencer errors, and others are removed. So instead you would need to do something like ""panel of clonals"" to compare against, to compare between clonal events. Then among these you would denoise for the common variants, so you can construct a phylogenetic analysis how the mutations evolve. All these alterations you would need to validate through your reads supported by multiple replicates. Though you are fighting your clonal events here, and would need to validate that the coverage of the labeled events (samples) in your cohort study are individually relatively consistent in the specific variant distribution (i.e. you want no/minimal heterogeneity of samples contaminating each individual labeled sample). This way your samples are comparable. Based on the above, hope it makes sense why the `./.` and `0/0` make sense to analyze. Given that you cannot rely on the ploidy for your allele frequency, this can also be illustrated by the diversity in allele frequency, instead of unique clusters you would normally observe in a true germline variant. Given that, there probably is heterogeneity in the sample purity of variants, which would lead to a spread of the VAF. Now you need to isolate the variants and determine their inheritance, validated by large coverage to ensure they are true variants. This will provide you the drivers of the mutations and transmissions -- some of which might be more stable than others given different selection forces. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876:1994,inherit,inheritance,1994,,https://github.com/google/deepvariant/issues/682#issuecomment-1650246876,1,['inherit'],['inheritance']
Modifiability,"erimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; W1219 05:41:38.273121 139694699493120 deprecation.py:323] From /tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; INFO:tensorflow:Calling model_fn.; I1219 05:41:38.770528 139694699493120 estimator.py:1162] Calling model_fn.; WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; W1219 05:41:38.779899 139694699493120 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; INFO:tensorflow:Done calling model_fn.; I1219 05:41:46.711198 139694699493120 estimator.py:1164] Done calling model_fn.; INFO:tensorflow:Graph was finalized.; I1219 05:41:48.157609 139694699493120 monitored_session.py:246] Graph was finalized.; INFO:tensorflow:Restoring parameters from /opt/models/pacbio/model.ckpt; I1219 05:41:48.159860 139694699493120 saver.py:1293] Restoring parameters from /opt/models/pacbio/model.ckpt; 2023-12-19 05:41:48.71187",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:5220,layers,layers,5220,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,['layers'],['layers']
Modifiability,es used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt auto,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7835,config,configured,7835,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"es used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/bin",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4261,config,configured,4261,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"esn't exist' starting; Cloning into 'tensorflow'...; remote: Enumerating objects: 1585302, done.; remote: Counting objects: 100% (346968/346968), done.; remote: Compressing objects: 100% (5367/5367), done.; remote: Total 1585302 (delta 342939), reused 342327 (delta 341589), pack-reused 1238334; Receiving objects: 100% (1585302/1585302), 920.91 MiB | 18.57 MiB/s, done.; Resolving deltas: 100% (1307043/1307043), done.; Updating files: 100% (29800/29800), done.; Updating files: 100% (12761/12761), done.; Note: switching to 'v2.11.0'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Buil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7179,config,config,7179,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,4,"['config', 'variab']","['config', 'variable']"
Modifiability,"et me try to unpack them. -- There are some engineering constraints in DeepVariant that assume we train only 3 classes. One example is https://github.com/google/deepvariant/search?utf8=%E2%9C%93&q=DEFAULT_NUM_CLASSES&type= but there may be others. If you want to change the number of classes you'll need to track these down and generalize them. If you do that please send us a CL and we'll make sure to incorporate the changes into the master codebase here. Once you do that it should be possible to train as with as many labels as you like. -- You'll need to write your own labeler algorithm to provide a label for each training variant. These classes are in deepvariant/labeler. There are two (a PositionalLabeler and a HaplotypeLabeler) so you can use these as templates to hook up your own labeling algorithm. At the core these just need to return an integer label (0 ... N) for each example. Once you get that going it should be simple to label your own examples. Technically the labels can be anything. For example, if you emitted the reference base value as 0 for A, 1 for T, etc I'm pretty sure the machinery can learn to call the reference base from the image with perfect accuracy. -- The machinery of training really doesn't know anything about NGS data. It just takes the examples and tries to learn to predict the label. Once you get your labels in and the machinery doesn't expect just 3 it should be possible to train anything you want. You can use standard evaluation machinery to decide if your DeepVariant training run is successfully learning something. -- You'll likely need to write your own version of postprocess_variants (or extend it in some way) to bridge from the predicted labels and the information you want to export in the VCF (or other file format). In germline DeepVariant we learn to predict 0, 1, and 2 which mean hom-ref, het, and hom-alt, respectively, which is encoded in postprocess_variants. You'll need your own equivalent to this code. Hope this helps!. Mark",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/67#issuecomment-383764665:1953,extend,extend,1953,,https://github.com/google/deepvariant/issues/67#issuecomment-383764665,2,['extend'],['extend']
Modifiability,"eturn self._sess_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session; config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore; err, ""a Variable name or other graph key that is missing""); tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvar",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:15999,Variab,Variable,15999,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['Variab'],['Variable']
Modifiability,"external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; (06:29:06) WARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.; (06:29:06) INFO: Current date is 2019-02-14; (06:29:06) Loading: ; (06:29:06) Loading: 0 packages loaded; (06:29:07) INFO: Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:10938,config,configs,10938,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['config'],['configs']
Modifiability,"f_idx ]` return `true`.; here is the script:; ```shell; #!/bin/bash. dvsif=""/lustre/Data/toolsDB//deepvariant.sif""; ref_idx=""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa"". wkdir=""/lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP""; bamdir=""${wkdir}/mappinged_bams""; logdir=""${wkdir}/logs""; vcfoutdir=""${wkdir}/DeepVariant_outputs"". source activate ~/.conda/envs/zjlEnv. echo -e ""ref_idx is $ref_idx\n""; if [ -f ""$ref_idx"" ];; then; echo -e ""ref_idx $ref_idx exists!\n""; which ls; echo -e ""ls -al --block=M ${ref_idx}*\n""; ls -al --block=M ""${ref_idx}*""; echo -e ""ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*\n""; ls -al --block=M ""/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*""; echo -e ""/bin/ls -al --block=M ${ref_idx}*\n""; /bin/ls -al --block=M ""${ref_idx}*"". else; echo -e ""Warning!! ref_idx [$ref_idx] not exist!\n""; fi. ls -al ""${ref_idx}*""; singularity run $dvsif ls $ref_idx. ```. Here is the running output:; ```shell; $ bash scripts/02_run_deepvariant.sh ; ref_idx is /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa. ref_idx /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa exists!. /usr/bin/ls; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; /bin/ls -al --block=M /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*. /bin/ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; ls: cannot access /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*: No such file or directory; INFO: Converting SIF file to temporary sandbox...; /usr/bin/ls: cannot access '/lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa': No such file or directory; INFO: Cleaning up image... ```. Could you please help me?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761:3313,sandbox,sandbox,3313,,https://github.com/google/deepvariant/issues/653#issuecomment-1568200761,1,['sandbox'],['sandbox']
Modifiability,"g; ========== [jue 18 ago 2022 14:11:49 CEST] Stage 'Update package list' starting; W: Fallo al obtener http://dl.bintray.com/basespace/BaseMount-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: Fallo al obtener http://dl.bintray.com/basespace/BaseSpaceFS-DEB/dists/saucy/InRelease Falló la conexión [IP: 18.194.81.109 80]; W: No se han podido descargar algunos archivos de índice, se han omitido, o se han utilizado unos antiguos en su lugar.; ========== [jue 18 ago 2022 14:11:52 CEST] Stage 'build-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:6781,config,configure,6781,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['configure']
Modifiability,"g; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7629,config,config,7629,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"genomics_reader.py:223] Reading /input/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:22:14.780351 140118199654144 make_examples.py:648] Writing examples to /output/make_examples.tfrecord.gz; I0629 23:22:14.780567 140118199654144 make_examples.py:648] Writing gvcf records to /output/gvcf.tfrecord.gz; I0629 23:22:14.781277 140118199654144 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:22:14.797983 140118199654144 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:23:33.302437 140118199654144 make_examples.py:648] 102 candidates (110 examples) [78.50s elapsed]; I0629 23:23:37.605793 140118199654144 make_examples.py:648] 202 candidates (223 examples) [4.30s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_im0i33s_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:11564,extend,extend,11564,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['extend'],['extend']
Modifiability,"gle/deepvariant/issues/185 . The advice in my comment https://github.com/google/deepvariant/issues/185#issuecomment-494919509 has a code change that you could try out, only if you want to experiment with different warmstarting logic on your own. . What we currently use is what you can find in our r0.10 codebase:; https://github.com/google/deepvariant/blob/4b937f03a1336d1dc6fd4c0eef727e1f83d2152a/deepvariant/modeling.py#L560. When warmstarting and training with a small(er) amount of data, it's always possible that the curve might look a bit weird at the beginning. Here is a rule of thumb I use: a stable training setup should be mostly reproducible. Meaning, if you run the same training multiple times, the curves should eventually converge to about the same place, and shouldn't behave drastically different. They won't look the exactly same because of randomness in training process. But if half of the runs don't converge, or behave very differently from the other half, then something needs to be improved. You might also have a question on whether you need to warmstart. That is an empirical question. Here is an example from my experience:; For our PacBio training, at this point I actually feel like we have enough data to not have to warmstart from the WGS model. But we're still warmstarting (at least for now) because I find that it converges faster and the resulting accuracy is about the same. We make these decisions based on empirical evidence and our intuition on ML and the data. These decisions can also evolve over time. ( Btw, one thing that we might not have documented - if you want to try with *not* warmstarting from anything, and want to just randomly init, you can set `--start_from_checkpoint=""""`. ). I think your setup above looks reasonable to me. If you want to share more later (like what your training looks like, how long it takes to converge, whether your new model works better on your data, etc), feel free to add more in this issue. I will close it for now.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312#issuecomment-638521636:1834,evolve,evolve,1834,,https://github.com/google/deepvariant/issues/312#issuecomment-638521636,2,['evolve'],['evolve']
Modifiability,"hi @gambalab ,. Our current DeepVariant training code is only optimized to train on GPUs externally and will not work on TPUs. As you can see the TPU initialization is not achieved as the API is not present in the current code:; ```bash; tensorflow.python.framework.errors_impl.InvalidArgumentError: No OpKernel was registered to support Op 'ConfigureDistributedTPU' used by {{node ConfigureDistributedTPU}} with these attrs: [compilation_failure_closes_chips=false, tpu_cancellation_closes_chips=2, tpu_embedding_config="""", embedding_config="""", enable_whole_mesh_compilations=false, is_global_init=false]; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/841#issuecomment-2197206085:342,Config,ConfigureDistributedTPU,342,,https://github.com/google/deepvariant/issues/841#issuecomment-2197206085,2,['Config'],['ConfigureDistributedTPU']
Modifiability,"hink the average read length is 48,060 based on [this publication](; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9205427/pdf/main.pdf). The thing is that Guppy 3.6.0 is a bit old, and will have a higher error rate when processing the FAST5 signals from the R9 nanopore through the bidirectional RNN to generate the FASTQ file, [as shown in the following post](https://github.com/kishwarshafin/pepper/issues/90). . So that the proper SNP Pepper model gets selected internally, you can use the `--ont_r9_guppy4_hac` argument with `run_pepper_margin_deepvariant call_variants`, though not sure [version r0.8](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore) has the Guppy 4 model. Otherwise you can use [version r0.4 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.4/images/sha256-0a609f60aa45620b1949c0ffa06299554248d77e23f4186aaa1c3306a95f20a2?context=explore). . Ideally maybe you can get the FAST5 files from the [following Amazon S3 page](https://registry.opendata.aws/ont-open-data/) and reprocess them with Guppy 5 $`-`$ as that's the latest version that the Pepper model seems to be trained against $`-`$ so that you can then utilize the `--ont_r9_guppy5_sup` parameter with the [r0.8 container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.8/images/sha256-70908591ad67e8567a6e4551119b2cfc33d957ad39701c8af51b36b516214645?context=explore), or [version r0.5 of the Docker container](https://hub.docker.com/layers/kishwars/pepper_deepvariant/r0.5/images/sha256-13906107b84849590400074e84ce12aba051447a8118c84f31f2b20540fbd807?context=explore). Regarding troubleshooting maybe you can run it with `--dry` so you can get the individual commands, so you can run each one individually to determine where the bottleneck is stemming from. I'll wait for @kishwarshafin to confirm what would be the most effective approach. Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177:1423,layers,layers,1423,,https://github.com/google/deepvariant/issues/681#issuecomment-1641408177,2,['layers'],['layers']
Modifiability,"iables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my teammate @emschorsch who has been looking into warmstarting. )",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1948,variab,variables,1948,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,6,"['refactor', 'variab']","['refactoring', 'variables']"
Modifiability,"iant-wgs-exome; OUTPUT_BUCKET=gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/DeepVariant_Output; STAGING_FOLDER_NAME=wgs_staging; OUTPUT_FILE_NAME=WGS_Provided.vcf. ## Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.2/DeepVariant-inception_v3-0.7.2+data-wgs_standard. IMAGE_VERSION=0.7.2; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --sample_name VeritasProvided \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://cdw-genome/Charles_Human/Veritas_WGS/Combined_Veritas_Alignment/veritas_wgs.filter.bam \; --ref gs://cdw-genome/Ref/hg19.gatk.fasta \; --gcsfuse""; ; # Run the pipeline.; # run after 'gcloud config set compute/region """"'; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --regions us-west2 \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; 	--docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I admittedly didn't know what were the default settings, and I saw larger numbers in the later commands. While that gives me other options to test to decrease run-time, it sounds like I should point out that these are two cloud comparisons (*not* being run on my local machine - that was only GATK). It also looks like I used an earlier version of one of the tutorials (not exactly what is provided in the Exome case study). So, I apologize for the confusion. **1b)** I think my answer is in *1a)* above. After I finish running testing with computations on Google Cloud, I'll keep an eye on the credits to see what the storage cost is now (although it should b",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171#issuecomment-483490946:1292,config,config,1292,,https://github.com/google/deepvariant/issues/171#issuecomment-483490946,1,['config'],['config']
Modifiability,"iant/assets/6555937/858f235a-79d1-4fdb-bfad-a9ee79ade41a). If I generate a t-SNE plot of this, I see something like the following using colors to denote the different channels (feature maps):. ![image](https://github.com/google/deepvariant/assets/6555937/20c87e1a-e87f-4b7d-b14d-a7f4d6bfbfef). Now comes the fun part! Let's have different matrices (like the ones that generated the new channels above) that will identify interesting features that might expose one type of a genotype or another with confidence. Imagine that instead of dividing by 10, I find the values (matrices) that best helps separate the data in a BAM file I know should have specific variants at specific loci. I want to maximize that precision to be able to recall. Now you notice that the original data and transformations (feature maps) are linked preserving this $`propagation`$ $`of`$ $`information`$, as this flow of information is enabled through these sets of transformations. An interesting thing then begins to emerge as you move up the layers of transformation. For example, early on in the neural network's set of transformations you will see patterns like this:. ![image](https://github.com/google/deepvariant/assets/6555937/bc3cff8b-efa8-4029-abbe-75ad06973d24). You might notice an explosion of features, with no specific patterns. These early steps are to generate a large variety of features to be able to have selection power for the later layers to use as input, for helping with the separation into distinct patterns for mapping to the different classes of genotypes confidently. For example, you can see distinct patterns forming as it reaches the later stages: . ![image](https://github.com/google/deepvariant/assets/6555937/9f69f9dc-8dec-4370-aa69-e0295265e7f0). ![image](https://github.com/google/deepvariant/assets/6555937/83edefd6-8d77-4a7a-8fb3-921ec7c3cff1). Once the pattern has been achieved like the following, then one can proceed with testing each genotype's representation of the variant:. ![im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088:3370,layers,layers,3370,,https://github.com/google/deepvariant/issues/666#issuecomment-1612282088,2,['layers'],['layers']
Modifiability,"icense of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 = \1 ]]; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11 deepvariant/...; (05:40:22) INFO: Options provided by the client:; Inherited 'common' options: --isatty=1 --terminal_columns=166; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'common' options: --experimental_repo_remote_exec; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --define tsl_protobuf_header_only=true --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --experimental_cc_shared_library --experimental_link_static_libraries_once=false --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tenso",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:5112,Inherit,Inherited,5112,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['Inherit'],['Inherited']
Modifiability,"igure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point type with the IEEE 754 binary128 format, and this glibc; includes corresponding *f128 interfaces for it. */; #if defined _ARCH_PWR8 && defined __LITTLE_ENDIAN__ && (_CALL_ELF == 2) \; && defined __FLOAT128__; # define __HAVE_FLOAT128 1; #else; # define __HAVE_FLOAT128 0; #endif. /* add the following block of fix tensorflow build error */; #if CUDART_VERSION; #undef __HAVE_FLOAT128; #define __HAVE_FLOAT128 0; #endif. /* Defined to 1 if __HAVE_FLOAT128 is 1 and the type is ABI-distinct; from the default float, double and long double types in this glibc. */; #if __HAVE_FLOAT128; -------------------------------------. # build; bazel clean. export PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python""; bazel build --config=opt --config=cuda //tensorflow/tools/pip_package:build_pip_package --cxxopt=""-D_GLIBCXX_USE_CXX11_ABI=0"" --action_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --test_env=PYTHON_BIN_PATH=$PYTHON_BIN_PATH --distinct_host_configuration=false. # generate package; bazel-bin/tensorflow/tools/pip_package/build_pip_package $HOMEPATH/tensorflow_package. # install; pip install $HOMEPATH/tensorflow_package/tensorflow-1.12.0-*.whl. # verification; python -c ""import tensorflow as tf; print(tf.__version__)""; ```. ## CLIF. > Note: CLIF can be built with AT 11.0. Git Repository: [https://github.com/google/clif](https://github.com/google/clif). ```bash; # Prerequisites; cmake --version #3.5+; protoc --version # 3.2.0+ build from source code for both C++ and Python; pip install virtualenv; pip install pyparsing; yum install subversion; yum install ocaml; pip install 'pyparsing>=2.2.0'; pkg-config --libs python # workable. # download source code; cd $HOMEPATH; git clone https://github.com/google/clif.git; cd clif. # set environment; export INSTALL_DIR=""$HOMEPATH/inst""; export CLIFSRC_DIR=""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:11466,config,config,11466,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,4,['config'],['config']
Modifiability,"ike;; > Thanks for all this background and help. I'm trying to fit this into the; > conda recipe bazel build for DeepVariant but am not sure how to take; > advantage of using the local anaconda python in that context. The error I'm; > seeing is that bazel can't find pyclif_proto:; >; > (17:56:01) INFO: Found 1 target...; > (17:56:01) [0 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; > (17:56:01) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1: //third_party/nucleus/protos:variants_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; > Target //deepvariant:binaries failed to build; > (17:56:01) ERROR: /opt/conda/conda-bld/deepvariant_1525283132666/work/deepvariant-0.6.1/third_party/nucleus/protos/BUILD:165:1 1 input file(s) do not exist; >; > which I thought was triggered by the difficulty running pyclif without; > having the local python installed. It could also be due to not installing; > is in /usr/local/bin since I have to remain sandboxed in the work; > directory, but I did adjust the PATH to include the download location.; >; > Sorry I'm stuck here due to me limited knowledge of bazel tweaking. Either; > understanding how to handle a root install of the pre-build pyclif or; > tweaking to use the local python would be helpful. Alternatively, if you; > can already build DeepVariant on a CentOS6 system yourself I could use the; > pre-build binaries the way we're doing now, just with the build against an; > older glibc. Thanks again for the help with this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/29#issuecomment-386250002>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ABQZ2kaD2Oo0vlzfw55tL9A65ZhknIu-ks5tutlEgaJpZM4RQhCy>; > .; >. -- ; Thanks,; --Mike",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/29#issuecomment-386327937:1726,sandbox,sandboxed,1726,,https://github.com/google/deepvariant/issues/29#issuecomment-386327937,1,['sandbox'],['sandboxed']
Modifiability,"in/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make clean; make -j20; # optional; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Protobuf 3.6.1 Python from source code. Python: [https://github.com/protocolbuffers/protobuf/blob/master/python/README.md](https://github.com/protocolbuffers/protobuf/blob/master/python/README.md). > Note: Protobuf 3.6.1 should be built from source code or CLIF cannot find protobuf. ```bash; # share build for Python; python --version # python 2.7 or newer; protoc --version; # build; cd protobuf-3.6.1/python/; python setup.py build; python setup.py test; # install from source as deepvariant needed; python setup.py install; # install from wheel; python setup.py bdist_wheel; pip install protobuf-3.6.1-py2.py3-none-any.whl --force-reinstall; # verify; python -c ""import google.protobuf""; ```. ## OpenBLAS 0.3.5. ```bash; git clone -b v0.3.5 https://github.com/xianyi/OpenBLAS.git Open",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:6609,config,configure,6609,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configure']
Modifiability,"in:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:4165,config,configuration,4165,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configuration']
Modifiability,"inux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. ## Install Singularity. I don't have Singularity on the machine yet, so:. https://docs.sylabs.io/guides/4.1/user-guide/quick_start.html#quick-installation-steps. ```bash; sudo yum update -y && \; sudo yum groupinstall -y 'Development Tools' && \; sudo yum install -y \; openssl-devel \; libuuid-devel \; libseccomp-devel \; wget \; squashfs-tools; ```. ```; sudo yum groupinstall -y 'Development Tools'; # Install RPM packages for dependencies; sudo yum install -y \; autoconf \; automake \; cryptsetup \; fuse3-devel \; git \; glib2-devel \; libseccomp-devel \; libtool \; runc \; squashfs-tools \; wget \; zlib-devel; ```. ```bash; sudo dnf install dnf-plugins-core; sudo dnf copr enable dctrud/squashfs-tools-ng; sudo dnf install squashfs-tools-ng; ```. ```bash; export VERSION=1.21.0 OS=linux ARCH=amd64 && \; wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \; sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \; rm go$VERSION.$OS-$ARCH.tar.gz; ```. ```bash; echo 'export PATH=/usr/local/go/bin:$PATH' >> ~/.bashrc && \; source ~/.bashrc; ```. ```bash; export VERSION=4.1.0 && \; wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-ce-${VERSION}.tar.gz && \; tar -xzf singularity-ce-${VERSION}.tar.gz && \; cd singularity-ce-${VERSION}; ```. ```bash; ./mconfig && \; make -C builddir && \; sudo make -C builddir install; ```. At this point, I have singularity installed. ```bash; $ singularity --version; singularity-ce version 4.1.0; ```. ## Get data and run DeepVariant. Now, let me try to follow similar steps:. ```bash; singularity build DeepVariant_1.6.1.sif docker://google/deepvariant:1.6.1; ```. From here, I used https:/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716:1987,plugin,plugins-core,1987,,https://github.com/google/deepvariant/issues/812#issuecomment-2076206716,2,['plugin'],['plugins-core']
Modifiability,"ion with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/v",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8106,config,config,8106,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"ion... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5662,config,configured,5662,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"ion.py"", line 432, in __iter__; for blr in self.blr_iter:; File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/plurality_checkable_iterator.py"", line 60, in _PopulateHead; e = self.base_iterator.next(); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/wildcard_iterator.py"", line 476, in IterAll; expand_top_level_buckets=expand_top_level_buckets):; File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/wildcard_iterator.py"", line 215, in __iter__; provider=self.wildcard_url.scheme, fields=listing_fields):; File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/gcs_json_api.py"", line 595, in ListObjects; global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List; config, request, global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod; http, http_request, **opts); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest; check_response_func=check_response_func); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 391, in _MakeRequestNoRetry; redirections=redirections, connection_type=connection_type); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1570, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-clou",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549130970:5746,config,config,5746,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970,1,['config'],['config']
Modifiability,ir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # build from scratch; PROTOC=$HOMEPATH/inst/bin/protoc ./compile.sh; rsync -avP output/bazel $HOMEPATH/inst/bin/; # verification; bazel info; ```. ## Advance Toolchain 11.0. Doc: [https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/](https://developer.ibm.com/linuxonpower/advance-toolchain/advtool-installation/). ```bash; # gpg public key; wget ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; rpm --import gpg-pubkey-6976a827-5164221b. # Configure the Advance Toolchain repositories; [root@cit1074 deepvariant]# cat /etc/yum.repos.d/advance-toolchain.repo; #Begin of configuration file; [advance-toolchain]; name=Advance Toolchain Unicamp FTP; baseurl=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7; failovermethod=priority; enabled=1; gpgcheck=1; gpgkey=ftp://ftp.unicamp.br/pub/linuxpatch/toolchain/at/redhat/RHEL7/gpg-pubkey-6976a827-5164221b; # End of configuration file. # Install the Advance Toolchain; yum install advance-toolchain-at11.0-runtime; yum install advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source ,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:3861,config,configuration,3861,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configuration']
Modifiability,"ist:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6923,config,configured,6923,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"ither DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the varian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:2693,variab,variable,2693,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['variab'],['variable']
Modifiability,"ithub.com/google/deepvariant/blob/r1.1/docs/deepvariant-quick-start.md. For the main DeepVariant command, I ran:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; --call_variants_extra_args=""use_openvino=true"" \; 2>&1 | tee /tmp/deepvariant.log; ```. With this run above, all steps (including call_variants) completed without errors. After that run, I repeated the call_variants step:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" \; --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" \; --checkpoint ""/opt/models/wgs/model.ckpt"" \; --use_openvino; ```; which worked fine too. You mentioned you used DeepVariant1.1.0 version via Docker, but you also mentioned your command was:; `python /opt/DeepVariant-1.1.0/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/DeepVariant-1.1.0/models/DeepVariant-inception_v3-1.1.0+data-wes_standard/model.ckpt --use_openvino --num_readers 32`. Can you be more specific about how you run this command?. And, another pointer for you:; In our Dockerfile, we set these environment variables:; https://github.com/google/deepvariant/blob/r1.1/Dockerfile#L156-L157. You should probably first check whether those directories exist in your environment. And if they do exist, you'll need to set those environment variables too. Hope this helps.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/432#issuecomment-806341687:2316,variab,variables,2316,,https://github.com/google/deepvariant/issues/432#issuecomment-806341687,2,['variab'],['variables']
Modifiability,"k we are in perfect agreement. Supporting what the community wants is my preferred approach as well. I was only suggesting it after having performed a reduction upon the functional set of DeepVariant's design. Just in case the community requires more variety of labeled inputs, having a parameter processor might help in mapping between DeepVariant's core operation and different parameter sets. This would create flexibility for both the goals of the design team and community. For example, the design team might want flexibility in playing around with different transformations of read signals and parameter search spaces - which might change over time or with different data sets - without worrying that the parameter list might require updating. Let's take the following simplified model optimization:. ![image](https://github.com/google/deepvariant/assets/6555937/f7c33960-59fc-45c9-9cdd-0f015ebd1aae). The team would let this model optimize itself to determine the parameter set under which it performs ideally. Then metadata about the training would be saved along with the optimized model. Then users can query the model's metadata via some program, which would provide information about the training data and parameters limits for different model types. Such metadata would be automatically populated through the model training step (where I'm including the validation and tuning steps). Based on that, a default set of parameters can be auto-generated for general users, and advanced users can then map their custom parameters however they want. It basically makes the training turn-key and usage automatic. Such training could even be remotely initiated through a browser, and different users can make some of their models available to others if they want, thus helping out the community and growing it at the same time. This would also make adapting new models much faster, including where a large set of specialized ensemble models might be required to check against. Best regards,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838:1871,adapt,adapting,1871,,https://github.com/google/deepvariant/issues/518#issuecomment-1696738838,2,['adapt'],['adapting']
Modifiability,kages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2029,config,configured,2029,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"ke -j20 install; export PATH=/usr/local/bin:$PATH; ```. ## Protobuf 3.6.1 C++ static. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from /usr/bin/gcc, static for bazel. ```bash; # download source code; wget https://github.com/protocolbuffers/protobuf/releases/download/v3.6.1/protobuf-all-3.6.1.tar.gz; tar -zxvf protobuf-all-3.6.1.tar.gz; cd protobuf-3.6.1/. # clean static protobuf build; make uninstall; make distclean. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin. # share build for C++; # install under /usr instead of /usr/local; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CXXFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --disable-shared --enable-static; make clean; make -j20; make -j20 check # check if protobuf build is good; # install; make install; sudo ldconfig # refresh shared library cache - /opt/at11.0/sbin/ldconfig; #cd .. # verify; echo ""$(which protoc)""; echo ""$(protoc --version)""; ```. ## Bazel 0.15.0. Build instructions: [https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel](https://docs.bazel.build/versions/master/install-compile-source.html#bootstrap-bazel). Edit crosstool: [http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html](http://biophysics.med.jhmi.edu/~yliu120/tensorflow.html). > Note: Bazel 0.15.0 should be built from /usr/bin/gcc. ```bash; # download source code; wget https://github.com/bazelbuild/bazel/releases/download/0.15.0/bazel-0.15.0-dist.zip; mkdir bazel-0.15.0; unzip -n bazel-0.15.0-dist.zip -d ./bazel-0.15.0; cd bazel-0.15.0. # environment; export CPU=power8; export HOMEPATH=/home/qilibj; export JAVA_HOME=/usr/lib/jvm/java-1.8.0; export PATH=$HOMEPATH/inst/bin:$JAVA_HOME/bin:/usr/local/cuda-10.0/bin:/usr/local/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:2061,config,configure,2061,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configure']
Modifiability,"kg_lock.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7399,config,config,7399,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability,l 5.32.1 2_h7f98852_perl5 conda-forge; pip 21.3.1 pyhd8ed1ab_0 conda-forge; protobuf 3.18.0 py36hc4f0c31_0 conda-forge; psutil 5.8.0 py36h8f6f2f9_1 conda-forge; pyasn1 0.4.8 py_0 conda-forge; pyasn1-modules 0.2.7 py_0 conda-forge; pycparser 2.21 pyhd8ed1ab_0 conda-forge; pyjwt 2.7.0 pyhd8ed1ab_0 conda-forge; pyopenssl 22.0.0 pyhd8ed1ab_1 conda-forge; pyparsing 3.0.9 pyhd8ed1ab_0 conda-forge; pyrsistent 0.17.3 py36h8f6f2f9_2 conda-forge; pysocks 1.7.1 py36h5fab9bb_3 conda-forge; python 3.6.15 hb7a2778_0_cpython conda-forge; python-dateutil 2.8.2 pyhd8ed1ab_0 conda-forge; python_abi 3.6 2_cp36m conda-forge; pytz 2023.3 pyhd8ed1ab_0 conda-forge; pyu2f 0.1.5 pyhd8ed1ab_0 conda-forge; readline 8.2 h8228510_1 conda-forge; requests 2.28.1 pyhd8ed1ab_0 conda-forge; requests-oauthlib 1.3.1 pyhd8ed1ab_0 conda-forge; rsa 4.9 pyhd8ed1ab_0 conda-forge; scipy 1.5.3 py36h9e8f40b_0 conda-forge; setuptools 58.0.4 py36h5fab9bb_2 conda-forge; six 1.16.0 pyh6c4a22f_0 conda-forge; sortedcontainers 2.4.0 pyhd8ed1ab_0 conda-forge; sqlite 3.42.0 h2c6b66d_0 conda-forge; tensorboard 2.8.0 pyhd8ed1ab_1 conda-forge; tensorboard-data-server 0.6.0 py36hc39840e_0 conda-forge; tensorboard-plugin-wit 1.8.1 pyhd8ed1ab_0 conda-forge; tensorflow 2.0.0 gpu_py36h6b29c10_0 ; tensorflow-base 2.0.0 gpu_py36h0ec5d1f_0 ; tensorflow-estimator 2.0.0 pyh2649769_0 ; tensorflow-gpu 2.0.0 h0d30ee6_0 ; termcolor 1.1.0 pyhd8ed1ab_3 conda-forge; tk 8.6.12 h27826a3_0 conda-forge; toolz 0.12.0 pyhd8ed1ab_0 conda-forge; typing-extensions 4.1.1 hd8ed1ab_0 conda-forge; typing_extensions 4.1.1 pyha770c72_0 conda-forge; unzip 6.0 h7f98852_3 conda-forge; urllib3 1.26.15 pyhd8ed1ab_0 conda-forge; werkzeug 0.16.1 py_0 conda-forge; wheel 0.37.1 pyhd8ed1ab_0 conda-forge; wrapt 1.13.1 py36h8f6f2f9_0 conda-forge; xz 5.2.6 h166bdaf_0 conda-forge; yarl 1.6.3 py36h8f6f2f9_2 conda-forge; zipp 3.6.0 pyhd8ed1ab_0 conda-forge; zlib 1.2.13 hd590300_5 conda-forge; zstd 1.4.9 ha95c52a_0 conda-forge; (dv) dpipe@4de3e1b4384c:/app/dpipe$ ; ```,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053:6095,plugin,plugin-wit,6095,,https://github.com/google/deepvariant/issues/664#issuecomment-1593808053,1,['plugin'],['plugin-wit']
Modifiability,"lename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://storage.googleapis.com/bazel-apt/dists/stable/InRelease Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:2396,config,configured,2396,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; pkg-config is already the newest version (0.29.1-0ubuntu1).; unzip is already the newest version (6.0-20ubuntu1).; zip is already the newest version (3.0-11).; curl is already the newest version (7.47.0-1ubuntu2.8).; zlib1g-dev is already the newest version (1:1.2.8.dfsg-2ubuntu4.1).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; python-wheel is already the newest version (0.29.0-1).; python-dev is already the newest versi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:8338,config,config,8338,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['config']
Modifiability,"lling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Igno",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7615,config,configs,7615,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['configs']
Modifiability,"loud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; ========== [2018年 08月 28日 星期二 10:55:10 CST] Stage 'Install development packages' starting; Reading package lists... Done; Build",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7282,config,configured,7282,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,m/linux/chrome/deb stable Release ; Hit:4 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:6 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Hit:8 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Err:9 http://storage.googleapis.com/bazel-apt stable InRelease ; Cannot initiate the connection to storage.googleapis.com:80 (2404:6800:4012:1::2010). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::2010 80]; Err:10 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-clo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:1484,config,configured,1484,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"mate read depth (reads with MQ=255 or with bad mates are filtered)\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [AD], name: AD, description: ""##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\""Allelic depths for the ref and alt alleles in the order listed\"">"", type: int, number: alleles, default_type: zero, count: 0, combi_method: min, ignore_non_variants: false}; > - {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}; > - {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}; > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] config CRC32C = 2932316105; > [71420] [2024-04-03 09:10:42.191] [GLnexus] [info] init database, exemplar_vcf=/public/home/zenglingsen/01.data/01.ONT_data/01.ONT_20X_fastq_SNP_calling/03.pepper/01.gvcf/gvcf_file/AW.new.excluded.mnps.gvcf.gz; > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] Initialized GLnexus database in GLnexus.DB; > [71420] [2024-04-03 09:10:42.611] [GLnexus] [info] bucket size: 30000; > [71420] [2024-04-03 09:10:42.612] [GLnexus] [info] contigs: NC_010443.5 NC_010444.4 NC_010445.4 NC_010446.5 NC_010447.5 NC_010448.4 NC_010449.5 NC_010450.4 NC_010451.4 NC_010452.4 NC_010453.5 NC_010454.4 NC_010455.5 NC_010456.5 NC_010457.5 NC_010458.4 NC_010459.5 NC_010460.4 NC_010461.5 NC_010462.3 NW_018084777.1 NW_018084778.1 NW_018084779.1 NW_018084780.1 NW_018084781.1 NW_018084782.1 NW_018084783.1 NW_018084784.1 NW_018084785.1 NW_018084786.1 NW_018084787.1 NW_018084788.1 NW_018084789.1 NW_018084790.1 NW_018084791.1 NW_018084792.1 NW_018084793.1 NW_018084794.1 NW_018084795.1 NW_018084796.1 NW_018",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:2704,config,config,2704,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['config'],['config']
Modifiability,"mits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7986,config,configs,7986,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['configs']
Modifiability,"move and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.lis",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:6031,config,configured,6031,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"n by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see wh",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1856,variab,variability,1856,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,4,"['adapt', 'variab']","['adapt', 'variability']"
Modifiability,"n.; Bazel 3.7.2 already installed on the machine, not reinstalling; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [jue 18 ago 2022 14:11:53 CEST] Stage 'Download and configure TensorFlow sources' starting; M	tensorflow/core/kernels/mlir_generated/build_defs.bzl; HEAD está ahora en c256c071bb2 Merge pull request #52891 from tensorflow/mm-update-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -ypa",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7543,config,configuring,7543,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['configuring']
Modifiability,"n_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions for updating:; If using Keras pass *_constraint arguments to layers.; W0911 02:28:44.788271 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; W0911 02:28:44.973487 139937686464256 deprecation.py:323] From /tmp/Bazel.runfiles_z274deps/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:7982,layers,layers,7982,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['layers'],['layers']
Modifiability,"nfig definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:9600,config,config,9600,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,2,['config'],['config']
Modifiability,"no-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dynamic_kernels in file /tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; (05:40:23) INFO: Current date is 2023-12-18; (05:40:23) DEBUG: /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/external/org_tensorflow/third_party/repo.bzl:132:14: ; Warning: skipping import of repository 'com_google_protobuf' because it already exists.; (05:40:23) INFO: Analyzed 189 targets (0 packages loaded, 0 targets configured).; (05:40:23) INFO: Found 141 targets and 48 test targets...; (05:40:24) ERROR: /deepvariant/third_party/nucleus/util/python/BUILD:13:11: CLIF wrapping third_party/nucleus/util/python/math.clif failed: (Exit 3): pyclif failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/18a0a1d061f0109a3b92fbdafbaf1bbd/execroot/com_google_deepvariant && \; exec env - \; bazel-out/k8-opt-exec-50AE0418/bin/external/clif/pyclif --modname third_party.nucleus.util.python.math -c bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.cc -g bazel-out/k8-opt/bin/third_party/nucleus/util/python/math.h -i bazel-out/k8-opt/bin/third_party/nucleus/util/python/math_init.cc --prepend clif/python/types.h -I. -Ibazel-out/k8-opt/bin -Iexternal/com_google_absl -Ibazel-out/k8-opt/bin/external/com_google_absl -Iexternal/clif -Ibazel-out/k8-opt/bin/external/clif -Iexternal/com_google_glog -Ibazel-out/k8-opt/bin/external/com_google_glog -Iexternal/com_github_gflags",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:10223,config,configured,10223,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['configured']
Modifiability,"not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for ; best performance.; I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters; W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T; I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra; in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non; e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':; None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus; ter': 0, '_master': ''}; I1217 09:08:42.446381 139680301201152 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; W1217 09:08:42.450340 139680301201152 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.; python.framework.ops) is deprecated and will be removed in ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749313156:2622,config,config,2622,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156,1,['config'],['config']
Modifiability,"nroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install numpy==1.14.6; # verify; python -c ""import numpy"". # dependecy of scipy 1.2.0; OPT=""-O3 -mcpu=$CPU -mtune=$CPU -maltivec -mvsx -ffast-math -fpeel-loops -funroll-loops -ftree-vectorize -fno-builtin-malloc -fno-builtin-calloc -fno-builtin-realloc -fno-builtin-free"" BLAS=$HOMEPATH/inst/lib/libopenblas.so LAPACK=$HOMEPATH/inst/lib/libopenblas.so ATLAS=$HOMEPATH/inst/lib/libopenblas.so pip install scipy==1.2.0; # verify; python -c ""import scipy"". # pip package dependencies; # pip install pip six wheel mock; pip install wheel autograd h5py==2.9.0 enum34; pip install keras_applications==1.0.6 keras_preprocessing==1.0.5. # download source code; git clone -b r1.12 https://github.com/tensorflow/tensorflow.git tensorflow-1.12; cd tensorflow-1.12. # configure in tensorflow/.tf_configure.bazelrc; PYTHON_BIN_PATH=""$HOMEPATH/inst/bin/python"" \; PYTHON_LIB_PATH=""$HOMEPATH/inst/lib/python2.7/site-packages"" \; TF_NEED_IGNITE=""0"" \; TF_ENABLE_XLA=""0"" \; TF_NEED_OPENCL_SYCL=""0"" \; TF_NEED_ROCM=""0"" \; TF_NEED_MPI=""0"" \; TF_NEED_TENSORRT=""0"" \; TF_NEED_CUDA=""1"" \; TF_CUDA_VERSION=""10.0"" \; CUDA_TOOLKIT_PATH=""/usr/local/cuda"" \; TF_CUDNN_VERSION=""7"" \; CUDNN_INSTALL_PATH=""/usr/local/cuda-10.0"" \; TF_NCCL_VERSION=""2"" \; NCCL_INSTALL_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib"" \; NCCL_HDR_PATH=""/usr/local/cuda-10.0/targets/ppc64le-linux/lib/../include"" \; TF_CUDA_COMPUTE_CAPABILITIES=""3.7"" \; TF_CUDA_CLANG=""0"" \; GCC_HOST_COMPILER_PATH=""/opt/at11.0/bin/gcc"" \; CC_OPT_FLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" \; TF_SET_ANDROID_WORKSPACE=0 \; ./configure. # fix build error; vim /opt/at11.0/include/bits/floatn.h; -------------------------------------; #include <features.h>. /* Defined to 1 if the current compiler invocation provides a; floating-point typ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:9748,config,configure,9748,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configure']
Modifiability,"nstall advance-toolchain-at11.0-devel; yum install advance-toolchain-at11.0-perf; yum install advance-toolchain-at11.0-mcore-libs. # Load enviroment; export PATH=/opt/at11.0/bin:/opt/at11.0/sbin:$PATH; # Do not need to export; # export LD_LIBRARY_PATH=/opt/at11.0/lib64:/lib64:$LD_LIBRARY_PATH ; # Load environment; sudo yum install environment-modules; source /etc/profile; module load at11.0; module unload at11.0; ```. ## Python 2 and Pip 19.0.1. Build instructions: [https://docs.python.org/2/using/unix.html](https://docs.python.org/2/using/unix.html). > Note: Python 2 should be built from AT 11.0. ```bash; # download source code ; wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tgz; tar -zxvf Python-2.7.15.tgz; cd Python-2.7.15. # environment; export HOMEPATH=/home/qilibj; export CPU=power8. # check gcc before build, should be AT11.0; which gcc. # build; CFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" CPPFLAGS=""-mcpu=$CPU -mtune=$CPU -O3"" ./configure --prefix=$HOMEPATH/inst --enable-shared --disable-static; make -j20; make install. # set environment; export PATH=$HOMEPATH/inst/bin:/usr/local/cuda-10.0/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin; export LD_LIBRARY_PATH=/home/qilibj/inst/lib:$LD_LIBRARY_PATH; export PYTHONPATH=/home/qilibj/inst/lib/python2.7/site-packages. # verify 2.7.15; echo ""$(python --version)"". # Pip 19.0.2; wget -qc https://bootstrap.pypa.io/get-pip.py --no-check-certificate; $HOMEPATH/inst/bin/python ./get-pip.py --prefix $HOMEPATH/inst; #pip install --upgrade --force-reinstall pip; echo ""$(pip --version)""; pip install setuptools nose asv cython future protobuf==3.6.1 six mock; pip install --upgrade setuptools; ```. ## Protobuf 3.6.1 C++ shared. C++: [https://github.com/protocolbuffers/protobuf/blob/master/src/README.md](https://github.com/protocolbuffers/protobuf/blob/master/src/README.md). > Note: Protobuf 3.6.1 should be built from AT 11.0, C++ shared for TF 1.12. ```bash; # download source code; wget https://github.com/p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:5224,config,configure,5224,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['config'],['configure']
Modifiability,"oftware.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:2039,config,config,2039,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"omponents-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple ti",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5303,config,configured,5303,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"on to packages.cloud.google.com:80 (2404:6800:4008:803::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:803::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'goo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:4077,config,configured,4077,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"on3.8; ++ export PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api --java_runtime_version=remotejdk_11'; + bazel; [bazel release 5.3.0]; Usage: bazel <command> <options> ... Available commands:; analyze-profile Analyzes build profile data.; aquery Analyzes the given targets and queries the action graph.; build Builds the specified targets.; canonicalize-flags Canonicalizes a list of bazel options.; clean Removes output files and optionally stops the server.; coverage Generates code coverage report for specified test targets.; cquery Loads, analyzes, and queries the specified targets w/ configurations.; dump Dumps the internal state of the bazel server process.; fetch Fetches external repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command.; + [[ 1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:3879,config,configurations,3879,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['configurations']
Modifiability,oral data:. ```; Wed May 17 18:54:58 2023 call_variants_profile.txt. 12867185 function calls (11416794 primitive calls) in 61.905 seconds. Ordered by: internal time; List reduced from 3098 to 300 due to restriction <300>. ncalls tottime percall cumtime percall filename:lineno(function); 9 31.607 3.512 31.608 3.512 {built-in method tensorflow.python.client._pywrap_tf_session.TF_SessionRun_wrapper}; 2 14.137 7.069 14.137 7.069 {built-in method tensorflow.python.client._pywrap_tf_session.TF_CloseSession}; 434721 0.900 0.000 0.913 0.000 {built-in method builtins.hasattr}; 241294 0.868 0.000 3.111 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5866(get_controller); 241296 0.462 0.000 0.462 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:932(_mode); 413316 0.452 0.000 0.541 0.000 /usr/lib/python3.8/contextlib.py:82(__init__); 9 0.409 0.045 0.409 0.045 {built-in method tensorflow.python.client._pywrap_tf_session.ExtendSession}; 413315/164299 0.370 0.000 1.371 0.000 /usr/lib/python3.8/contextlib.py:117(__exit__); 825922/196 0.347 0.000 61.452 0.314 {built-in method builtins.next}; 113550 0.334 0.000 0.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:5887(_get_outer_context_and_inner_device_stack); 111268 0.304 0.000 4.646 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6030(executing_eagerly_outside_functions); 1379 0.300 0.000 0.300 0.000 {method 'copy' of 'dict' objects}; 257584 0.293 0.000 0.293 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py:950(executing_eagerly); 413316/164111 0.253 0.000 2.725 0.000 /usr/lib/python3.8/contextlib.py:108(__enter__); 659/110 0.247 0.000 0.248 0.002 /usr/local/lib/python3.8/dist-packages/tensorflow/python/keras/utils/version_utils.py:97(swap_class); 413316 0.239 0.000 0.781 0.000 /usr/lib/python3.8/contextlib.py:238(helper); 3367 0.228 0.000 0.228 0.000 {built-in method posix.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:2517,Extend,ExtendSession,2517,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,1,['Extend'],['ExtendSession']
Modifiability,"ourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/plurality_checkable_iterator.py"", line 60, in _PopulateHead\n e = self.base_iterator.next()\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/wildcard_iterator.py"", line 476, in IterAll\n expand_top_level_buckets=expand_top_level_buckets):\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/wildcard_iterator.py"", line 215, in __iter__\n provider=self.wildcard_url.scheme, fields=listing_fields):\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/gcs_json_api.py"", line 595, in ListObjects\n global_params=global_params)\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List\n config, request, global_params=global_params)\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod\n http, http_request, **opts)\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 351, in MakeRequest\n max_retry_wait, total_wait_sec))\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/util.py"", line 1719, in WarnAfterManyRetriesHandler\n http_wrapper.HandleExceptionsAndRebuildHttpConnections(retry_args)\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest\n check_response_func=check_response_func)\n File ""/mnt/home/mansourt/miniconda3/envs/deepVar/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/137#issuecomment-451711664:6139,config,config,6139,,https://github.com/google/deepvariant/issues/137#issuecomment-451711664,1,['config'],['config']
Modifiability,"p2_freebayes_Final/Direct_minimap/Bam/run3:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3; -v; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result:/media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --reads; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/MITO60_sorted.bam; --ref; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/Batch1_3runs/1_minimap2_freebayes_Final/Direct_minimap/Bam/run3/hg38_chrM.fa; --report_title MITO60_Stats --sample_name MITO60 --output_vcf; /media/manish/Data/Jyoti_Mridha_AIC/Nanopore_Sequencing/Jyoti_mito_Analysis_GATK/1_FINAL_NANOPORE/deepvariants_Trial/deepvariant_result; --model_type ONT_R104. On Wed, Jun 12, 2024 at 12:03 PM Pi-Chuan Chang ***@***.***>; wrote:. > And, just in case the documentation isn't clear:; >; > This part:; >; > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}"":""/output"" \; > google/deepvariant:""${BIN_VERSION}"" \; > ...; >; > The variable BIN_VERSION was specified in earlier in the steps:; >; > BIN_VERSION=""1.6.1""; >; > So, in Unix command it's equivalent to:; >; > google/deepvariant:""1.6.1"" \; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/google/deepvariant/issues/829#issuecomment-2162210763>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/BDQL2ZE35INJI4WP7BHRNK3ZG7TVPAVCNFSM6AAAAABJFTFWYKVHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMZDCNRSGIYTANZWGM>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749:2857,variab,variable,2857,,https://github.com/google/deepvariant/issues/829#issuecomment-2162307749,2,['variab'],['variable']
Modifiability,"please see the error has:. ```bash; --ref is required.; Pass --helpshort or --helpfull to see help on flags.; ```. Meaning your variable ~{ref_fasta} is either NULL or set to a value that is not accessible to the program. Can you try absolute path to make sure the files are being passed correctly? Also, please run the quick-start locally to make sure the program is running correctly. The quick-start can be found here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/783#issuecomment-2010181773:128,variab,variable,128,,https://github.com/google/deepvariant/issues/783#issuecomment-2010181773,1,['variab'],['variable']
Modifiability,"py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1630, in make_examples_runner; region_processor.initialize(); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 870, in initialize; self._initialize(); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 853, in _initialize; self.realigner = realigner.Realigner(; File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 559, in __init__; self.diagnostic_logger = DiagnosticLogger(self.config.diagnostics); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 327, in __init__; self._csv_file = open(self._root_join(self.metrics_filename), 'w'); File ""/tmp/Bazel.runfiles_mpmtqvy4/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 346, in _root_join; tf.io.gfile.makedirs(subdir); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/lib/io/file_io.py"", line 514, in recursive_create_dir_v2; _pywrap_file_io.RecursivelyCreateDir(compat.path_to_bytes(path)); tensorflow.python.framework.errors_impl.PermissionDeniedError: /output; Read-only file system; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch/c.c21087028/coverage_graph_and_clincnv_files/ClinCNV/hg38/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads /scratch/c.c21087028/checking_variant_deepvariant/E036-H-013_TAAGGCGA-ACTGCATA_L007_PE_markedduplicates_GCA_000001405.15_GRCh38_no_alt_analy",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113:6821,config,config,6821,,https://github.com/google/deepvariant/issues/691#issuecomment-1667692113,1,['config'],['config']
Modifiability,"python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 662, in create_session; return self._get_session_manager().prepare_session(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 314, in prepare_session; sess, is_loaded_from_checkpoint = self._restore_checkpoint(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 233, in _restore_checkpoint; _restore_checkpoint_and_maybe_run_saved_model_initializers(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/session_manager.py"", line 71, in _restore_checkpoint_and_maybe_run_saved_model_initializers; saver.restore(sess, path); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 1319, in restore; raise _wrap_restore_error_with_msg(; tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. From /job:tpu_worker/replica:0/task:0:; Unsuccessful TensorSliceReader constructor: Failed to find any matching files for gs://deepvariant/models/DeepVariant/1.3.0/DeepVariant-inception_v3-1.3.0+data-wgs_standard/model.ckpt; [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_2gnuyvf0/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_2g",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874:17508,Variab,Variable,17508,,https://github.com/google/deepvariant/issues/537#issuecomment-1140056874,1,['Variab'],['Variable']
Modifiability,"re changes to your input tensor. I suspect things will become more of an ensemble model approach in the future, where you you want to push your data through multiple pipelines for comparison. At that point you're not even writing code anymore, but rather directing transformation among (ephemeral) datasets through an intermediate analysis language ([DSL](https://en.wikipedia.org/wiki/Domain-specific_language)) to try out different ideas upon a collection of data -- where some might contain/become golden standards. For example, now you have collections of transformed datasets streamlined as protobuf message, which can be filter-inspected as JSON as necessary for validation purposes, which you can do now such as for `CallVariantsOutput` among others:. ```; variant {; reference_bases: ""A""; alternate_bases: ""C""; calls {; info {; key: ""AD""; ...; call_set_name: ""Sample_Diag-excap51-HG002-EEogPU""; }; end: 1115835; reference_name: ""1""; start: 1115834; ...; ```. So what I'm humbly proposing is something more subtle and flexible. Imagine you open a genome browser, and search for a specific disease. It then can have you drill down to the available samples or the variation graph. For the variation graph it will allow you to see a model of transitions among samples relating the temporal behavior of subtypes of cancer. All these steps are multiple pipelines that are auto-triggered to run accordingly, or would be pre-cached results. As new samples come in, it improves the disease models, or other functional analysis. This opens doors to Personlized Medicine -- clinicians, research, or patients -- enabling integrated analysis that is centralized for different types of datasets. You already have the protobuf data-structures for initiating that, thus allowing your stored collection of pipelines to drive that integration and comparison to take place, of which DeepVariant would be one of many. Thus flexibility in the form of modularity when refactoring the codebase will allow for more f",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21#issuecomment-353849339:2168,flexible,flexible,2168,,https://github.com/google/deepvariant/issues/21#issuecomment-353849339,1,['flexible'],['flexible']
Modifiability,"re/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2297525000 Hz; 2020-09-11 02:28:44.724518: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5886480 executing computations on platform Host. Devices:; 2020-09-11 02:28:44.724555: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-09-11 02:28:44.727738: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W0911 02:28:44.755701 139937686464256 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp8bwus8dp; I0911 02:28:44.756375 139937686464256 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp8bwus8dp', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4549cfe518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0911 02:28:44.756798 139937686464256 call_variants.py:426] Writing calls to /output/intermediate_results_dir/call_variants_output.tfrecord.gz; W0911 02:28:44.767428 139937686464256 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.; Instructions ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345#issuecomment-690842263:6657,config,config,6657,,https://github.com/google/deepvariant/issues/345#issuecomment-690842263,1,['config'],['config']
Modifiability,"repositories that are prerequisites to the targets.; help Prints help for commands, or the index.; info Displays runtime info about the bazel server.; license Prints the license of this software.; mobile-install Installs targets to mobile devices.; print_action Prints the command line args for compiling a file.; query Executes a dependency graph query.; run Runs the specified target.; shutdown Stops the bazel server.; sync Syncs all repositories specified in the workspace file; test Builds and runs the specified test targets.; version Prints version information for bazel. Getting more help:; bazel help <command>; Prints help and options for <command>.; bazel help startup_options; Options for the JVM hosting bazel.; bazel help target-syntax; Explains the syntax for specifying targets.; bazel help info-keys; Displays a list of keys used by the info command. ```. What should we do next?. ```; > ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 10:22:12 PM EDT] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 10:22:13 PM EDT] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 10:22:14 PM EDT] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:15 PM EDT] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5392k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:1852,config,config,1852,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"rflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/graph_executor,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.baz",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:7709,Inherit,Inherited,7709,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,2,"['Inherit', 'config']","['Inherited', 'config']"
Modifiability,"riod_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 281, in prepare_session; config=config); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/session_manager.py"", line 195, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1268, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: /input/mosquito_model/model.ckpt. real	0m7.387s; user	0m9.233s; sys	0m4.817s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/268#issuecomment-586584341:1925,config,config,1925,,https://github.com/google/deepvariant/issues/268#issuecomment-586584341,2,['config'],['config']
Modifiability,"riod_secs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 749, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1231, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 902, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/monitored_session.py"", line 669, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 295, in prepare_session; config=config); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/session_manager.py"", line 209, in _restore_checkpoint; saver.restore(sess, checkpoint_filename_with_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py"", line 1315, in restore; err, ""a Variable name or other graph key that is missing""); tensorflow.python.framework.errors_impl.NotFoundError: Restoring from checkpoint failed. This is most likely due to a Variable name or other graph key that is missing from the checkpoint. Please ensure that you have not altered the graph expected based on the checkpoint. Original error:. /opt/models/pacbio/model.ckpt.data-00000-of-00001; No such file or directory; 	 [[node save_1/RestoreV2 (defined at usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py:629) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_8s8eyzhg/runfiles/com_google_deepvariant/deepvariant/call_variants.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:15520,config,config,15520,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,['config'],['config']
Modifiability,"rsing the input data both forward and in reverse -- remembering internally long-term dependencies stored as states. This means it is fairly memory efficient and fairly fast. In terms of sequence it captures the variants given a progression known by past progressions. As an analogy, given sequence of historical stock prices, can you predict the future stock price. Instead of stock price it produces variants. The ResNet is helpful to train faster as it is uses skip (shortcut) connection by utilizing the residual difference of input and output to learn from (residual learning). They use spatial pyramid pooling (SPP) for variable coverage of full-alignments. This still a convolutional neural network of weights and biases. It still operates like DeepVariant on channels constructed from your read information (`reference_base`, `alternative_base`, `mapping_quality`, `base_quality`, `strand_info`, `variant_type`, `insert_base`, `phasing_info`). What does this mean for you? We know your organism adapts significantly to the external environment. The deep learning models -- either DeepVariant or Clair3 -- have semi-fixed input assumptions. Sure they have been trained with some flexibility in the input data to capture some variability, but not to the extent that they will adapt to significant allele, or possible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data cond",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:1627,adapt,adapts,1627,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['adapt'],['adapts']
Modifiability,"s ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 MB.; It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/en-us/openvino-toolkit/choose-download?cid=&source=upgrade&content=2020_3_LTS or on the GitHub*; WARNING:tensorflow:From /tmp/Bazel.runfiles_wz_ompqi/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3681,layers,layers,3681,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,4,"['config', 'layers']","['config', 'configuration', 'layers']"
Modifiability,"s for all rare disease trio samples. 2. In order to resolve your issue, do we need to solve this hemizygous representation problem specifically, or will producing more 1/1 calls in the manner described in my prior comment be sufficient?. If a variant is present in 100% of the reads, representing as 1/1 would be ideal as this is what's expected by downstream tools. The expectation for a male is to have majority variants on X as 1/1. It would be dangerous if a female had majority 1/1. 3. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples). Sounds like this involves a lot of mangling of samples and VCF cutting/manipulation...and no I'm not thrilled about having this pushed on the user. You're recommendation would be:; 1. Run DeepTrio on trio.; 2. BCFtools to cut-out chrX + Y; 3. Rerun DeepTrio on duo, once with mother and once with father, keeping the Y variants from paternal analysis, and X from maternal analysis. (?What happens with female child, no change?); 4. Now you've got VCFs with improper columns (mom-proband, dad-proband, mom-dad-proband)so you can't just concat them together...; 5. Mangle the 2-3 VCFs together. Essentially you're asking the user to run DeepTrio >2 times for it to work on a trio with a male proband, including manual VCF mangling on the user side. It would be much appreciated if this process can be internalized and parameterized within the run deepvariant command. Alternatively, if this is your best practice then providing code chunks to this effect would be appreciated. . At the end of the day, me, as a user, would prefer to run 1-2 commands to get a trio merged VCF. If you provided that code chunk to run the existing tool in the configuration you describe, then I'd be happy to insert it and test on my cases. Thanks,; Phil",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100:2258,parameteriz,parameterized,2258,,https://github.com/google/deepvariant/issues/518#issuecomment-1048108100,2,"['config', 'parameteriz']","['configuration', 'parameterized']"
Modifiability,"s.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe88a5b6190>, '_model_dir': '/tmp/tmpr4M5u5', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I1213 13:07:08.528713 140638419556096 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; W1213 13:07:08.533111 140638419556096 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-565441661:3989,config,config,3989,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661,1,['config'],['config']
Modifiability,"s.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; I1213 19:19:36.526952 140624564107008 call_variants.py:381] Writing calls to /tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz; W1213 19:19:36.531224 140624564107008 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-565657419:1358,config,config,1358,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419,1,['config'],['config']
Modifiability,"sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Failed to fetch http://packages.cloud.google.com/apt/dists/cloud-sdk-xenial/InRelease Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4012:1::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4012:1::200e 80]; W: Some index files failed to download. They have been ignored, or old ones used instead.; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:7103,config,configured,7103,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17 --host_cxxopt=-std=c++17 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; (05:40:22) INFO: Found applicable config definition build:dyna",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8826,config,config,8826,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['config']
Modifiability,"sible structural variation. I mean think of a convolutional neural network like this simplified example:. $`y\_output = weights_\gamma \cdot ( (weights_\alpha \cdot input + bias_\alpha) + (weights_\beta \cdot input + bias_\beta) ) + bias_\gamma`$. This is a simple regression (linear model). If you adjust the weights and bias terms for produce specific output given the input, and then you modify the inputs significantly and hope to get the same results, it will naturally fail. The same with DeepVariant and Clair3, they are bounded and determined by their weights and biases. These deep learning models were meant to work at scale under relatively normal data conditions (data conditions they were trained and validated for). The way to adjust for variable datasets is to create an ensemble model. What that means is that you have a model for each subtype of your organism. Then your run your data across all models to see which ones respond with high confidence. In your case, you don't have that and have to adjust for the data you have; as training a new model would require a confident variant truth set. So what is the known starting point is the biology of your organism. To control for variability, the first step is probably to ensure you have good isolates of your organism that you can get sequence data from -- you've seen that in your case it was multiallelic so it probably is not a pure isolate. You probably don't need to arrest them if you validate for known markers and sequenced at the same time to ensure they do not modify. These replicates would form your panels. Then there is probably a whole slew of analysis, and experiments you would need to understand these variants. Part of that can be to see how you can confidence-label the variants generated by DeepVariant and Clair3 given their specific parameters adjusted accordingly. If they become reliably predictable under a diverse dataset, then you can probably use them for your downstream analysis. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832:3138,variab,variability,3138,,https://github.com/google/deepvariant/issues/682#issuecomment-1650719832,2,['variab'],['variability']
Modifiability,"singularity.sh. #### For your Ubuntu instance . You are getting very close! To simplify the install in the `run-prereq.sh` file you can comment out (with the `#` symbol) the following sections:. 1) For the ""Install TensorFlow pip package"" keep only the ones with **CPU-only**, and comment out the others. 2) For ""Install CUDA"", comment out everthing. 3) For ""Install TensorRT"", comment out everthing. And then run it again. The rest of the errors in the `run-prereq.sh` are easy to fix, which we can do later individually by removing each one, and installing the minimum required version. Before we fix `clif`, could you tell me what you get for the following:. ```; lsb_release -sc. wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - . add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main"". sudo apt-get update. sudo apt-get install -y llvm-11 llvm-11-dev clang-11 llvm-11-tools; ```. You might have a mismatch of a previous version of `clif` or its installed configuration files. You can check that via the following commands:. ```; llvm-config-11 --version; ```. The configs might be an older version, which you can check via the following:. ```; cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; ```. Below is what I have:. ```; $ cat /usr/lib/llvm-11/lib/cmake/llvm/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /usr/lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; set(LLVM_PACKAGE_VERSION 11.1.0); $ cat /lib/llvm-11/cmake/LLVMConfig.cmake | grep PACKAGE_VERSION; cat: /lib/llvm-11/cmake/LLVMConfig.cmake: No such file or directory; $; ```. If you have a mismatch between the version and config, you can make a clean removal of `llvm` via the following:. ```; sudo apt-get remove llvm-11*; ```. The way clif is installed is via the following:. ```; #!/bin/bash; source settings.sh; sudo tools/build_clif.sh; ```. Let me know what you see. Thanks,; ~p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236:1288,config,configuration,1288,,https://github.com/google/deepvariant/issues/657#issuecomment-1575894236,8,['config'],"['config', 'config-', 'configs', 'configuration']"
Modifiability,"sorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the sourc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:8444,config,config,8444,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['config']
Modifiability,"sorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Installing collected packages: pyparsing; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the sourc",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7967,config,config,7967,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"sorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8073,config,config,8073,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability,"squished and expanded views) with corresponding output in DeepTrio VCF to show you discrepancies between DeepTrio VCFs and BAMs. I think it could be a part of the GQ issue in DeNovos.; The order of FORMAT fields in multisample VCF is proband, mother, father.; The order of samples in IGV is father, mother, proband (from top to bottom). ### **1) True Denovo, QUAL=3, proband GQ=5. It looks good except very low proband GQ.**. chr5 | 92696737 | chr5_92696737_C_T | C | T | 3 | . | AF=0.166667;AQ=3 | GT:DP:AD:GQ:PL:RNC | 0/1:32:17,15:**5**:3,0,32:.. | 0/0:27:27,0:50:0,108,1079:.. | 0/0:21:21,0:50:0,105,1049:.. ![DT_1_04190_chr5_92696737](https://user-images.githubusercontent.com/22089494/115329914-0902a500-a161-11eb-9ab6-a3dc47a92aaf.png); ![DT_1_04190_chr5_92696737_zoom](https://user-images.githubusercontent.com/22089494/115330134-7d3d4880-a161-11eb-9202-10a392b98c07.png). ### **2) Filtered Denovo-like, QUAL=46, proband GQ=13. Mulitallelic, inherited; when VCF is normalized, it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | A,AATATAT | 46 | . | AF=0.333333,0.166667;AQ=46,15 | GT:DP:AD:GQ:PL:RNC | 1/2:30:5,12,13:13:44,15,55,15,0,53:.. | 0/1:31:16,15,0:46:46,0,70,990,990,990:.. | ./.:30:**27,1,0**:18:0,18,45,990,990,990:II . chr5 | 24093912 | chr5_24093912_AAT_A;chr5_24093912_A_AATAT | AAT | AATATAT | 46 | . | AF=0.166667;AQ=15 | GT:DP:AD:GQ:PL:RNC | 0/1:30:5,13:13:44,15,53:.. | 0/0:31:16,0:46:46,990,990:.. | ./.:30:**27,0**:18:0,990,990:II. ![DT_1_04190_chr5_24093912](https://user-images.githubusercontent.com/22089494/115330437-09e80680-a162-11eb-96cd-2f2d27d45896.png); ![DT_1_04190_chr5_24093912_zoom](https://user-images.githubusercontent.com/22089494/115330450-11a7ab00-a162-11eb-8f5c-927bd1445056.png). ### **3) Filtered Denovo-like, QUAL=27, proband GQ=28. Inherited; it passes DeNovo filter as father's genotype misses ALT allele information ** ###. chr7 | 54624683 | chr7_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440#issuecomment-822947862:1936,inherit,inherited,1936,,https://github.com/google/deepvariant/issues/440#issuecomment-822947862,2,['inherit'],['inherited']
Modifiability,"st-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-parameter --copt=-Wno-stringop-overflow --copt=-Wno-array-bounds --copt=-Wunused-result --copt=-Werror=unused-result --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++17",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8597,config,config,8597,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['config']
Modifiability,"stall development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install bazel' starting; WARNING: Value of --bazelrc is ignored, since --ignore_all_rc_files is on.; Bazel 5.3.0 already installed on the machine, not reinstalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7293,config,config,7293,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"stalling; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Install CLIF binary' starting; CLIF already installed.; ========== [Mon 05 Jun 2023 10:22:32 PM EDT] Stage 'Download and configure TensorFlow sources' starting; HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; You have bazel 5.3.0 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 10:22:33 PM EDT] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manag",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053:7560,config,config,7560,,https://github.com/google/deepvariant/issues/657#issuecomment-1577789053,1,['config'],['config']
Modifiability,"t easier to troubleshoot. Just to be sure we minimize surprises, we will do a couple of things first. To ensure the TensorFlow code we will compile against is of the same version as the TensorFlow python package (2.11.0), run the following in your `git clone` TensorFlow folder (the assumption is that your TensorFlow folder is outside your DeepVariant one):. ```; # Assuming you are in the DeepVariant directory. cd ../tensorflow; git checkout origin/r2.11 -f. # Here configure like before with the defaults; ./configure. cd ../deepvariant; ```. Once in the DeepVariant directory, perform each of these separately so we can isolate any issues:. ```; #!/bin/bash; source settings.sh. wget https://raw.githubusercontent.com/google/deepvariant/r1.5/.bazelrc. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries; ```. You may notice that we removed `.bazelrc` previously, that was so that we can ensure while we troubleshoot the `bazel` installation nothing gets triggered by the `.bazelrc` config. The last two things you have to run to complete the DeepVariant build are the following, though the above should give most of what you need:. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip //deepvariant/labeler:labeled_examples_to_vcf; ```. ```; #!/bin/bash; source settings.sh. bazel build -c opt ${DV_COPT_FLAGS} --build_python_zip :binaries-deeptrio; ```. Your zip files will be under the `bazel-bin/deepvariant/`. The `zip` files will require some patching, which I can show you once you have a successful built of the above. The output of a successful build should look something like this:. ```; ...; bazel-bin/deepvariant/postprocess_variants; bazel-bin/deepvariant/postprocess_variants.zip; bazel-bin/deepvariant/runtime_by_region_vis; bazel-bin/deepvariant/runtime_by_region_vis.zip; bazel-bin/deepvariant/show_examples; bazel-bin/deepvariant/show_examples.zip; bazel-bin/deepvariant/vcf_stats_report; bazel-bin/deepvariant/vcf_stats_re",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104:1649,config,config,1649,,https://github.com/google/deepvariant/issues/657#issuecomment-1581784104,1,['config'],['config']
Modifiability,"t recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_baze",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1467,Config,Configuration,1467,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['Config'],['Configuration']
Modifiability,"t):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; bui",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:2680,Config,Configuration,2680,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['Config'],['Configuration']
Modifiability,"t/tpu,tensorflow/core/tfrt/utils; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; Inherited 'build' options: --action_env PYTHON_BIN_PATH=/opt/conda/envs/py38/bin/python3.8 --action_env PYTHON_LIB_PATH=/usr/local/lib/python3.8/dist-packages --python_path=/opt/conda/envs/py38/bin/python3.8; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; Inherited 'build' options: --jobs 128 --config=monolithic --show_timestamps --verbose_failures --define=use_fast_cpp_protos=true --copt=-Wno-maybe-uninitialized --copt=-Wno-unused-function --cxxopt=-std=c++17 --python_top=//:deepvariant_python_runtime --incompatible_use_python_toolchains=false; (05:40:22) INFO: Reading rc options for 'test' from /tensorflow/.tf_configure.bazelrc:; 'test' options: --flaky_test_attempts=3 --test_size_filters=small,medium; (05:40:22) INFO: Reading rc options for 'test' from /deepvariant/.bazelrc:; 'test' options: --test_output=errors; (05:40:22) INFO: Found applicable config definition build:short_logs in file /tensorflow/.bazelrc: --output_filter=DONT_MATCH_ANYTHING; (05:40:22) INFO: Found applicable config definition build:v2 in file /tensorflow/.bazelrc: --define=tf_api_version=2 --action_env=TF2_BEHAVIOR=1; (05:40:22) INFO: Found applicable config definition test:v2 in file /tensorflow/.tf_configure.bazelrc: --test_tag_filters=-benchmark-test,-no_oss,-gpu,-oss_serial,-v1only --build_tag_filters=-benchmark-test,-no_oss,-gpu,-v1only; (05:40:22) INFO: Found applicable config definition build:monolithic in file /tensorflow/.bazelrc: --define framework_shared_object=false --define tsl_protobuf_header_only=false --experimental_link_static_libraries_once=false; (05:40:22) INFO: Found applicable config definition build:linux in file /tensorflow/.bazelrc: --host_copt=-w --copt=-Wno-all --copt=-Wno-extra --copt=-Wno-deprecated --copt=-Wno-deprecated-declarations --copt=-Wno-ignored-attributes --copt=-Wno-unknown-warning --copt=-Wno-array-para",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245:8315,config,config,8315,,https://github.com/google/deepvariant/issues/753#issuecomment-1857120245,1,['config'],['config']
Modifiability,"tc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; Reading package lists... Done; Building dependency tree ; Reading state information... Done; sudo is already the newest version (1.8.16-0ubuntu1.5).; The following packages were automatically installed and are no longer required:; libllvm5.0 linux-headers-4.13.0-36 linux-headers-4.13.0-36-generic; linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; ========== [2018年 08月 28日 星期二 10:43:08 CST] Stage 'Update package list' starting; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en_US) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations (main/i18n/Translation-en) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11 (main/dep11/Components-amd64.yml) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target DEP-11-icons (main/dep11/icons-64x64.tar) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89#issuecomment-416438760:5483,config,configured,5483,,https://github.com/google/deepvariant/issues/89#issuecomment-416438760,2,['config'],['configured']
Modifiability,"te-relnotes; You have bazel 3.7.2 installed.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing ins",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:7941,config,config,7941,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['config'],['config']
Modifiability,"tensorflow/compiler/xla/service/service.cc:168] XLA serv ice 0x5289690 executing computations on platform Host. Devices:; 2020-03-27 13:32:13.625958: I tensorflow/compiler/xla/service/service.cc:175] Stream Executor device (0): Host, Default Version; 2020-03-27 13:32:13.629139: I tensorflow/core/common_runtime/process_util.cc:115] Crea ting new thread pool with default inter op setting: 2. Tune using inter_op_parallelism _threads for best performance.; I0327 13:32:13.749661 47138345245376 modeling.py:563] Initializing model with random p arameters; W0327 13:32:13.750545 47138345245376 estimator.py:1821] Using temporary folder as mode l directory: /tmp/tmpj5q00h0m; I0327 13:32:13.751226 47138345245376 estimator.py:212] Using config: {'_model_dir': '/ tmp/tmpj5q00h0m', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoi nts_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoin t_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow .python.training.server_lib.ClusterSpec object at 0x2adfb39cd2b0>, '_task_type': 'work er', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': ' ', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I0327 13:32:13.751701 47138345245376 call_variants.py:384] Writing calls to /tmp/tmp63 xxmwmi/call_variants_output.tfrecord.gz; W0327 13:32:13.760179 47138345245376 deprecation.py:506] From /usr/local/lib/python3.6 /dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseR esourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with const raint is deprecated and will be removed in a future version.; Instructions for updati",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/287#issuecomment-605306987:5822,config,config,5822,,https://github.com/google/deepvariant/issues/287#issuecomment-605306987,1,['config'],['config']
Modifiability,"testdata/NA12878_S1.chr20.10_10p1mb.truncated.bam with NativeSamReader; I0629 23:43:41.804690 139796154570496 make_examples.py:648] Writing examples to quickstart-output/sing.make_examples.tfrecord.gz; I0629 23:43:41.804903 139796154570496 make_examples.py:648] Writing gvcf records to quickstart-output/sing.gvcf.tfrecord.gz; I0629 23:43:41.805349 139796154570496 make_examples.py:648] Overhead for preparing inputs: 0 seconds; I0629 23:43:41.821153 139796154570496 make_examples.py:648] 0 candidates (0 examples) [0.02s elapsed]; I0629 23:44:41.827408 139796154570496 make_examples.py:648] 102 candidates (110 examples) [60.01s elapsed]; I0629 23:44:45.517579 139796154570496 make_examples.py:648] 202 candidates (223 examples) [3.69s elapsed]; [E::bgzf_read] Read block operation failed with error 4 after 0 of 4 bytes; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1611, in region_reads; reads.extend(sam_reader.query(region)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 82, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7g_iun5k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in m",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465#issuecomment-870990381:18145,extend,extend,18145,,https://github.com/google/deepvariant/issues/465#issuecomment-870990381,2,['extend'],['extend']
Modifiability,"th CentOS at the time. So let me try that. # Get a CentOS machine to test. I used:. ```bash; gcloud compute instances create ""${USER}-centos7"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""centos-7"" \; --image-project ""centos-cloud"" \; --machine-type ""e2-standard-16"" \; --zone ""us-west1-b"". gcloud compute ssh pichuan-centos7 --zone ""us-west1-b""; ```. Version:. ```; $ uname -a; Linux pichuan-centos7 3.10.0-1160.114.2.el7.x86_64 #1 SMP Wed Mar 20 15:54:52 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux; ```. Install conda:. ```bash; curl https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > Miniconda3-latest-Linux-x86_64.sh; bash Miniconda3-latest-Linux-x86_64.sh -b -u -p $HOME/miniconda; eval ""$(${HOME}/miniconda/bin/conda shell.bash hook)""; ```. To repeat what I did in, I tried: https://github.com/google/deepvariant/issues/736#issuecomment-1829204521. ```bash; conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge; conda create -y -n dv-env deepvariant; conda activate dv-env; ```. It completed without any error messages. I see:. ```; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/; bash-completion deepvariant-1.5.0-0 doc et examples google-cloud-sdk-359.0.0-0 icu info keyutils licenses locale man tabset terminfo zsh; (dv-env) [pichuan@pichuan-centos7 ~]$ ls /home/pichuan/miniconda/envs/dv-env/share/deepvariant-1.5.0-0/binaries/DeepVariant/1.5.0/DeepVariant-1.5.0; call_variants_keras.zip freeze_graph.zip make_examples.zip multisample_make_examples.zip runtime_by_region_vis.zip vcf_stats_report.zip; call_variants.zip licenses.zip model_eval.zip postprocess_variants.zip settings.sh; deeptrio make_examples_somatic.zip model_train.zip run-prereq.sh show_examples.zip; ```. As mentioned in my previous investigation, I don't actually know how to use Conda though. But it seems like at least with the setting above, it can install. If yo",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405:1160,config,config,1160,,https://github.com/google/deepvariant/issues/806#issuecomment-2067274405,3,['config'],['config']
Modifiability,"thanks for your answer,. depends what acceptable is, is analysing 30depth chicken bam for 5h a good time? (I have 50 such genomes to process). I started building from source but quickly it stopped complaining about my python version.; I have ubuntu 20 on that server, luckily since it is apparently the only supported version. When I build from source, can I put myself in a virtenv or conda env with no risk of overwriting some system file or will it also put things in the OS path (/usr/...)?; I fear for my OS and do not dare to proceed in that direction. Finally, conda install does not install, which leaves me with only the docker which is not a perfect match for my cpus. This is all not really optimal! weird for one of the current leaders in variant calling, I was expecting a more carefully built set of docker(s) to match different cpu configurations.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051:847,config,configurations,847,,https://github.com/google/deepvariant/issues/452#issuecomment-1497101051,1,['config'],['configurations']
Modifiability,"that model. That is why you want to shuffle across all your samples. The `make_examples` script only takes one BAM file via the `--reads` parameter, and you don't need to merge multiple BAM files into one, as the shuffling happens afterwards on the generated TFRecords. So the process is roughly as follows: ; 1) Run `make_examples` on training set BAMs, and run `make_examples` on validation set BAMs -- both of which will generate TFRecords.; 2) Shuffle TFRecords for training set, then shuffle the ones for validation set separately.; 3) Run `model_train` on shuffled training set shuffled data.; 4) Run `model_eval` on shuffled validation set data to evaluate generated checkpoints, which will generate `best_checkpoint.txt` and `best_checkpoint.metrics` files.; 5) Pick best model listed in the `best_checkpoint.txt` file.; 6) Test the best model with `run_deepvariant` by providing it to the `--customized_model` parameter, and for the `--reads` parameter setting it with the test data BAM file. ; 7) Benchmark by comparing your VCF with your Truth set VCF via [`hap.py`](https://github.com/Illumina/hap.py), and check the metrics against a baseline appropriate to your study.; 8) Then if you want, you could train/retrain a (new or previous) model by tuning the [modeling parameters](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md#parameters-to-tune) and/or updating the training data - i.e. if you want to make it more flexible to capture more variety in the input data - both of which might improve the model under different conditions. As Maria [mentioned previously](https://github.com/google/deepvariant/issues/698#issuecomment-1681392580), training is done on chromosome 1-19, then evaluation on 21-22, with a test on 20. Usually all training is done on some data, then evaluated on another for picking the best model, and finally the best model would be tested with the test data. Let me know if I should expand on anything. Hope it helps,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081:2016,flexible,flexible,2016,,https://github.com/google/deepvariant/issues/706#issuecomment-1711096081,1,['flexible'],['flexible']
Modifiability,"thon3.8/dist-packages/tensorflow/python/framework/ops.py:5351(_device_function_stack); 58954 0.117 0.000 0.200 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/dtypes.py:719(as_dtype); 6669 0.113 0.000 0.118 0.000 /usr/local/lib/python3.8/dist-packages/gast/gast.py:15(create_node); 155316 0.108 0.000 0.183 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:6314(get_default_graph); 6015 0.105 0.000 0.105 0.000 {built-in method tensorflow.python.util._tf_stack.extract_stack_for_op}; 6015 0.104 0.000 0.149 0.000 /usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py:1879(_NodeDef); ```. What I'm seeing above is something quite interesting. The execution seems heavy on [eager execution rather than graph execution](https://blog.tensorflow.org/2018/08/code-with-eager-execution-run-with-graphs.html), which could be the culprit. The thing is that there are some quick ways now to make it fast with almost no major code-rewrite, though there are other natural optimizations that are relatively obvious given the analysis. For the `make_examples` analysis, the task is much simpler to fix by localizing I/O through just a shared memory model and/or pass-by-reference approach, as illustrated by the call distribution (with an eye on the serialization/deserialization of ProtoBuf if that becomes more heavily used):. ```; Wed May 17 17:51:33 2023 make_examples_profile.txt. 2790682 function calls (2732833 primitive calls) in 11.860 seconds. Ordered by: internal time; List reduced from 11503 to 100 due to restriction <100>. ncalls tottime percall cumtime percall filename:lineno(function); 171/168 1.927 0.011 1.940 0.012 {built-in method _imp.create_dynamic}; 3159 0.930 0.000 0.930 0.000 {method 'read' of '_io.BufferedReader' objects}; 19018 0.694 0.000 0.694 0.000 {built-in method posix.stat}; 2 0.586 0.293 0.586 0.293 {method 'counts' of 'deepvariant.python.allelecounter.AlleleCounter' objects}; 3136 0.475 0.000 0.47",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:5939,rewrite,rewrite,5939,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['rewrite'],['rewrite']
Modifiability,"thread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection API: None; - Use the config file: None; Model Optimizer version:. [ SUCCESS ] Generated IR version 10 model.; [ SUCCESS ] XML file: /home/pichuan/./model.xml; [ SUCCESS ] BIN file: /home/pichuan/./model.bin; [ SUCCESS ] Total execution time: 24.29 seconds.; [ SUCCESS ] Memory consumed: 761 M",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:3028,inherit,inherited,3028,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,6,"['inherit', 'layers']","['inherited', 'layers']"
Modifiability,"to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at d5b57ca93e5 Merge pull request #58598 from tensorflow/vinila21-patch-1; WARNING: current bazel installation is not a release version.; Do you wish to build TensorFlow with ROCm support? [y/N]: No ROCm support will be enabled for TensorFlow. Do you wish to build TensorFlow with CUDA support? [y/N]: No CUDA support will be enabled for TensorFlow. Do you wish to download a fresh release of clang? (Experimental) [y/N]: Clang will not be downloaded. Please specify optimization flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [Mon 05 Jun 2023 04:03:16 PM UTC] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Run",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804:7914,config,configuring,7914,,https://github.com/google/deepvariant/issues/657#issuecomment-1577053804,2,['config'],['configuring']
Modifiability,"tput:; ```; Collecting package metadata (repodata.json): done; Solving environment: / Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found to be incompatible with each other:. Output in format: Requested package -> Available versions. Package google-cloud-sdk conflicts for:; google-cloud-sdk[version='<243.0.0']; deepvariant -> google-cloud-sdk. Package python conflicts for:; python=2.7; deepvariant -> python[version='2.7.*|>=2.7,<2.8.0a0']; google-cloud-sdk[version='<243.0.0'] -> python=2.7; deepvariant -> boost -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Package setuptools conflicts for:; deepvariant -> protobuf -> setuptools; python=2.7 -> pip -> setuptools; ```. **I also tried to install clean environment with Python 2.7 before installing deepvariant**. ```; conda create -n deepvariant python=2.7; conda activate deepvariant; conda install -c bioconda deepvariant; ```. Output:. ```; Collecting package metadata (repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; Solving environment: | Found conflicts! Looking for incompatible packages. failed . UnsatisfiableError: The following specifications were found; to be incompatible with the existing python installation in your environment:. Specifications:. - deepvariant -> python[version='3.4.*|3.5.*|3.6.*|>=3.5,<3.6.0a0|>=3.8,<3.9.0a0|>=3.7,<3.8.0a0|>=3.6,<3.7.0a0|>=3.6|3.7.*']. Your python: python=2.7. If python is on the left-most side of the chain, that's the version you've asked for.; When python appears to the right, that indicates that the thing on the left is somehow; not available for the python version you are constrained to. Note that conda will not; change your python version to a different minor version unless you explicitly specify; that.; ```. Do you have any idea what is causing the problem and how to proceed, please?",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177#issuecomment-584370219:3138,flexible,flexible,3138,,https://github.com/google/deepvariant/issues/177#issuecomment-584370219,1,['flexible'],['flexible']
Modifiability,"vice.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf; W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I1219 05:41:38.123045 139694699493120 estimator.py:191] Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': Cluster",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:1356,config,config,1356,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,1,['config'],['config']
Modifiability,"y.png. And specifically, for a while it was puzzling to us why model.ckpt-0 was much less accurate. After looking much closely into TensorFlow and the behavior or warmstarting, here is what we found:. It turns out that there's some subtlety to what exactly gets loaded in warm starting. It's documented here:; https://www.tensorflow.org/api_docs/python/tf/train/warm_start; Specifically, this:; ```; vars_to_warm_start: [Optional] One of the following:; * A regular expression (string) that captures which variables to warm-start (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1287,variab,variables,1287,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['variab'],['variables']
Modifiability,"yes, I think this is a real bug that still exists.; Due to the distributed nature of the cloud process, some machines may get shards that are all empty. Also, we actually only supply one of the shards to each process, so (1) doesn't really apply (there is no 'next shard').; You can reproduce this by adding ""--shards 64"" to the quickstart test data configuration in https://cloud.google.com/genomics/deepvariant.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27#issuecomment-355996061:350,config,configuration,350,,https://github.com/google/deepvariant/issues/27#issuecomment-355996061,1,['config'],['configuration']
Modifiability,"you pointed to. `gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" --scopes ""compute-rw,storage-full,cloud-platform"" --image-family ubuntu-1604-lts --image-project ubuntu-os-cloud --machine-type n1-standard-8 --boot-disk-size=200GB --zone us-west1-b --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure`. then I downloaded and built DeepVariant (here you could tweak different build optimization settings, but for now I left it alone):. ```; git clone https://github.com/google/deepvariant.git; cd deepvariant; ./build-prereq.sh; ./build_release_binaries.sh; ```. Then I downloaded and set up some of the variables from the case study doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-case-study.md); I had to change N_SHARDS to be 8 since we have 8 cpus on this instance. Then I ran make_examples in training mode on a small portion of the genome to create some labeled training data. I adapted the command line from the one used in the case study. You would also want to randomly shuffle the data but I didn't do that here. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --confident_regions ""${TRUTH_BED}"" \; --truth_variants ""${TRUTH_VCF}"" \; --examples ""${EXAMPLES}"" \; --regions ""20:10,000,000-12,000,000"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1; ```. The --confident_regions, and --truth_variants are how you supply the truth data to the program in order to create the labels. Then I created a data.pbtxt config file that is described in the training doc (https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-model-training.md). It looks like:; ```; > cat output/data.pbtxt ; name: ""my-training-dataset""; tfrecord_path: ""/home/rpoplin/case-study/output/HG002.examples.tfrecord-?????-of-00008.gz""; num_examples: 150; ```. Then I la",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46#issuecomment-363256889:1147,adapt,adapted,1147,,https://github.com/google/deepvariant/issues/46#issuecomment-363256889,1,['adapt'],['adapted']
Modifiability,"you want to pursue a deeper the story, what proportion of time do you want to make it science-driven versus engineering-driven?. The first question you don't have to answer, and is more obvious to you than me -- and it would affect how you pursue the second -- so I'll tackle the second one :) The science-driven one is not so much goal-driven, but rather trying to uncover the mechanism that the model the organism operates through. This can be a rabbit hole as you hypothesis-test the model's response through different experiments. So if you want to pursue de-novo assembly, ask yourself why you got good results previously? That's why I mentioned the ""panel of clonals"", which has the same basic idea. You are starting with a closer variant in its molecular evolution, than a reference which might be quite distant and/or mixed in specific loci or contigs as compared to your clone. . Regarding would I trust a low frequency region given the evidence? When there are multiple variables that start to accumulate with an experiment, I usually start breaking down towards the root cause with simpler experiments. What does low frequency region mean in terms of sequencing regarding your organism's behavior? For example, you can validate known regions in your clone you have verified via other assays to confirm that the sequencing results match -- it might not even be low frequency. Next, as you mentioned, you check for heterozygous k-mer pairs, which you can investigate via tools GenomeScope and Smudgeplot as [presented in the following paper](https://www.nature.com/articles/s41467-020-14998-3). For example, if you look at the figure of the k-mer spectra of diploid Arabidopsis thaliana, it has a similar distribution as your original GQ plot (assuming correlation of GQ with coverage, which would need to be validated):. ![image](https://github.com/google/deepvariant/assets/6555937/6e225f54-b836-4c0d-a4af-88142066bace). They mentioned that _*""for a diploid species, increasing heterozygosi",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917:1220,variab,variables,1220,,https://github.com/google/deepvariant/issues/682#issuecomment-1659358917,2,['variab'],['variables']
Modifiability,"zation flags to use during compilation when bazel option ""--config=opt"" is specified [Default is -Wno-sign-compare]: . Would you like to interactively configure ./WORKSPACE for Android builds? [y/N]: Not configuring the WORKSPACE for Android builds. Preconfigured Bazel build configs. You can use any of the below by adding ""--config=<>"" to your build command. See .bazelrc for more details.; 	--config=mkl 	# Build with MKL support.; 	--config=mkl_aarch64 	# Build with oneDNN and Compute Library for the Arm Architecture (ACL).; 	--config=monolithic 	# Config for mostly static monolithic build.; 	--config=numa 	# Build with NUMA support.; 	--config=dynamic_kernels	# (Experimental) Build kernels into separate shared objects.; 	--config=v1 	# Build with TensorFlow 1 API instead of TF 2 API.; Preconfigured Bazel build configs to DISABLE default on features:; 	--config=nogcp 	# Disable GCP support.; 	--config=nonccl 	# Disable NVIDIA NCCL support.; Configuration finished; ========== [jue 18 ago 2022 14:11:54 CEST] Stage 'Set pyparsing to 2.2.0 for CLIF.' starting; WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -parsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution -yparsing (/usr/local/lib/python3.8/dist-packages); WARNING: Ignoring invalid distribution - (/usr/local/lib/python3.8/dist-packages); Found existing installation: pyparsing 3.0.9; Uninstalling pyparsing-3.0.9:; Successfully uninstalled pyparsing-3.0.9; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Using pip 22.2.2 from /root/.local/lib/python3.8/site-pack",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279:8294,Config,Configuration,8294,,https://github.com/google/deepvariant/issues/552#issuecomment-1219417279,1,['Config'],['Configuration']
Performance," (see tf.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.; ```; Because in our code, we use a regular expression like this:; https://github.com/google/deepvariant/blob/aff131aac3bd0cb63ee8314e32bcbf5590987fb8/deepvariant/modeling.py#L361. This means that only the variables in the TRAINABLE_VARIABLES get loaded when we warm start.; This explains why warm starting from a checkpoint does not produce the same (or even similar) results as the inference/prediction mode. For example, Batch norms were not being warmstarted. If you do want to load **everything**, one trick we've tried is to change to code into a list:; ```; vars_to_warm_start=['|'.join(vars_to_include)]); ```; which then tricks the code into the model of loading everything (not just TRAINABLE_VARIABLES). But, it doesn't seem very desirable because it also loads things like global_step. The best way is probably to include just BatchNorm and the Trainable variables we want, but it'll require more refactoring here. If try with the small code change above and observe the log -- you'll see that a lot more vars are being used in warm-starting now. (But note that you'll likely want to exclude some variables such as global_step, because those will affect your learning rate). Note that even when warmstarting from an existing checkpoint, we don't currently recommend ""fine-tuning"" with very few steps. Even with our small TPU training case study, we showed an example to train up to 50k steps before we pick a model.; So if you want to use our code to fine tune on your data, please proceed with caution and make sure you carefully benchmark the accuracy and robustness of the resulting model. . Thanks for sharing your findings here. If you have more questions or anything you want to discuss, please feel free to bring up here. (And, adding my",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185#issuecomment-494919509:1848,load,loads,1848,,https://github.com/google/deepvariant/issues/185#issuecomment-494919509,2,['load'],['loads']
Performance, //deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1248,cache,cache,1248,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," //deepvariant:tf_utils_test PASSED in 3.8s; //deepvariant:utils_test PASSED in 0.1s; //deepvariant:variant_caller_test PASSED in 2.4s; //deepvariant:variant_calling_test PASSED in 0.1s; //deepvariant/environment_tests:env_smoke_test PASSED in 0.4s; //deepvariant/environment_tests:protobuf_implementation_test PASSED in 0.8s; //deepvariant/labeler:customized_classes_labeler_test PASSED in 1.8s; //deepvariant/labeler:labeled_examples_to_vcf_test PASSED in 1.9s; //deepvariant/labeler:positional_labeler_test PASSED in 1.8s; //deepvariant/labeler:variant_labeler_test PASSED in 1.8s; //deepvariant/python:allelecounter_wrap_test PASSED in 1.7s; //deepvariant/python:variant_calling_wrap_test PASSED in 1.8s; //deepvariant/realigner:aligner_test PASSED in 1.7s; //deepvariant/realigner:fast_pass_aligner_test PASSED in 0.1s; //deepvariant/realigner:realigner_test PASSED in 3.1s; //deepvariant/realigner:ssw_test PASSED in 0.1s; //deepvariant/realigner:window_selector_test PASSED in 1.8s; //deepvariant/realigner/allele_count_linear:generate_trained_model_test PASSED in 2.7s; //deepvariant/realigner/allele_count_linear:model_evaluation_test PASSED in 4.3s; //deepvariant/realigner/python:debruijn_graph_wrap_test PASSED in 1.8s; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.5s; //deepvariant/labeler:haplotype_labeler_test FAILED in 2.6s; /root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant:make_examples_test PASSED in 13.4s; Stats over 2 runs: max = 13.4s, min = 12.8s, avg = 13.1s, dev = 0.3s; //deepvariant:model_eval_test PASSED in 40.9s; Stats over 10 runs: max = 40.9s, min = 2.8s, avg = 7.3s, dev = 11.3s; //deepvariant:model_train_test PASSED in 120.0s; Stats over 10 runs: max = 120.0s, min = 3.0s, avg = 38.6s, dev = 44.3s; ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-464686381:3197,cache,cache,3197,,https://github.com/google/deepvariant/issues/123#issuecomment-464686381,1,['cache'],['cache']
Performance," 18:49:41) ; [GCC 9.4.0] on linux; Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.; >>> import dataclasses; Traceback (most recent call last):; File ""<stdin>"", line 1, in <module>; ModuleNotFoundError: No module named 'dataclasses'; >>> import tensorflow; >>> quit(); (dv) dpipe@3a2ea3796f25:/app/dpipe$ pip freeze; absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work; aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1623682242746/work; altair @ file:///home/conda/feedstock_root/build_artifacts/altair_1640796027299/work; astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work; async-timeout==3.0.1; attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1671632566681/work; blinker @ file:///home/conda/feedstock_root/build_artifacts/blinker_1664823096650/work; brotlipy==0.7.0; cached-property @ file:///home/conda/feedstock_root/build_artifacts/cached_property_1615209429212/work; cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1640686991047/work; certifi==2021.5.30; cffi @ file:///home/conda/feedstock_root/build_artifacts/cffi_1631636256886/work; chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1610093477613/work; charset-normalizer @ file:///home/conda/feedstock_root/build_artifacts/charset-normalizer_1661170624537/work; click @ file:///home/conda/feedstock_root/build_artifacts/click_1621503698523/work; contextlib2 @ file:///home/conda/feedstock_root/build_artifacts/contextlib2_1624848568296/work; crcmod @ file:///home/conda/feedstock_root/build_artifacts/crcmod_1624134625038/work; cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1634230300355/work; entrypoints @ file:///home/conda/feedstock_root/build_artifacts/entrypoints_1643888246732/work; gast==0.2.2; google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1686724583094/work; google-auth-oauthlib @ file:///home/co",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553:2278,cache,cached-property,2278,,https://github.com/google/deepvariant/issues/664#issuecomment-1593835553,2,['cache'],"['cached-property', 'cachetools']"
Performance," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:model_eval_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log); (06:29:12) INFO: From Testing //deepvariant:model_eval_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:44552,cache,cache,44552,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:48986,cache,cache,48986,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:57722,cache,cache,57722,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:14) FAIL: //deepvariant:model_eval_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log); (06:29:14) INFO: From Testing //deepvariant:model_eval_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:55573,cache,cache,55573,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_eval_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_eval_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportErro",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:59871,cache,cache,59871,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant/python:variant_calling_wrap_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log); (06:29:21) INFO: From Testing //deepvariant/python:variant_calling_wrap_test:; ==================== Test output for //deepvariant/python:variant_calling_wrap_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/deepvariant/python/variant_calling_wrap_test.py"", line 38, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/python/variant_calling_wrap_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Tra",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:112725,cache,cache,112725,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," Analysed 168 targets (0 packages loaded).; (06:29:07) INFO: Found 130 targets and 38 test targets...; (06:29:07) [1 / 7] [-----] BazelWorkspaceStatusAction stable-status.txt; (06:29:08) FAIL: //deepvariant/labeler:variant_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log); (06:29:08) INFO: From Testing //deepvariant/labeler:variant_labeler_test:; ==================== Test output for //deepvariant/labeler:variant_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/variant_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/variant_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Trace",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:12117,cache,cache,12117,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," There were 1689 SNP FN and 832 SNP FP. The same sample with the current version of DeepVariant has 1328 SNP FN and 749 SNP FP. . For Indels, the PrecisionFDA submission has 4175 Indel FN and 2839 Indel FP. The current version of DeepVariant has 1428 Indel FN and 924 Indel FP, a reduction in error of almost 50% compared to the most accurate Indel entry in Precision FDA Truth Challenge. The DeepVariant paper has the evaluation numbers for the first open source version (https://www.nature.com/articles/nbt.4235) and compares these results of this with the PrecisionFDA entries. 3) There are good other checks which can provide an indirect estimate of quality and which do not require a particular characterized samples. For example, you can call the same sample with GATK and DeepVariant and take the calls only made in one sample or the other. Comparison of the TiTv for those calls present on one or the other can tell you which (on average) has higher quality (indicated by higher TiTv in the singletons for that caller). We perform these evaluations internally as well and would welcome feedback about a similar analysis from you on your own samples. . 4) When DeepVariant evaluates a candidate, it can call it as a homozygous variant, heterozygous variant, or indicate that it believes that although there is evidence for a variant at a position, the true call for this position is reference (0/0). In the paper referenced, I believe that these reference calls were considered as failing a filter. However, it is not the case that these are variant calls that were made and had to be removed. Directly taking the genotype for each call would arrive at the same number of variants. In effect, these were not really variant calls to filter. They were rows in the VCF that already did not indicate variation. . We would be enthusiastic to collaborate with you to benchmark DeepVariant against other methods on your own samples with various preparations and coverages if you like. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165#issuecomment-476392585:2096,perform,perform,2096,,https://github.com/google/deepvariant/issues/165#issuecomment-476392585,2,['perform'],['perform']
Performance," _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97768,cache,cache,97768,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," a De Bruijn graph of each. It then performs a Smith-Waterman re-alignment of reads to the assemblies. You can think of this as a more exhaustive version of the candidate haplotype generation performed in GATK. As a result of this re-assembly, there may be some differences between the alleles that DeepVariant constructs and those constructed in GATK (and this may contribute to differences in AD). See the deepvariant/realigner folder for all of the associated code. With respect to GT and GQ, these are the primary outputs of the convolutional neural network classifier. The classifier estimates the probability of HOM REF, HET, and HOM ALT states. The GT is the most probably state as determined by the classifier. The GQ should correspond to the likelihood calculated for that GT (and as a result, this should correspond to PL). With respect to the calibration of GQ and recommendations for filtering. One observation we have about DeepVariant is that the genotype qualities seem to quite accurately reflect the empirical error probability (see Figure 2 Panel C of - https://www.nature.com/articles/nbt.4235). This fact, combined with the observation that the GQ scores produced by DeepVariant are quite normally distributed, means that you have flexibility to shift them slightly higher or lower if you prefer higher precision or higher recall. . If anything, DeepVariant seems to be slightly on the conservative side outside of the confident regions, so I would likely recommend you not perform additional hard filtering on GQ, and consider REF calls with GQ below 10 or 20 to be more like no-calls. If you have not yet read the Nature Biotechnology manuscript, I would recommend that as another good overview https://www.nature.com/articles/nbt.423. If you are curious to compare calls between DeepVariant and other technologies, I would also recommend that you use metrics like TiTv ratio or dbSNP fraction on the calls that are shared and singletons between the two methods. Thanks,; Andrew",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/135#issuecomment-450712530:2053,perform,perform,2053,,https://github.com/google/deepvariant/issues/135#issuecomment-450712530,1,['perform'],['perform']
Performance," a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Mon 05 Jun 2023 01:42:38 AM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6879,cache,cached,6879,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,2,['cache'],['cached']
Performance," completes as expected when using the CPU image with the same dataset and parameters. Initially, I thought TensorRT issues might be causing this stop, but I'll share the logs with you to get your perspective and an extra set of eyes on the problem. **Command:** ; ```; ( time sudo docker run --runtime=nvidia --gpus 1\; -v ${HOME}:${HOME} \; -w ${HOME} \; google/deepvariant:1.6.1-gpu \; train \; --config=""${BASE}/dv_config.py"":base \; --config.train_dataset_pbtxt=""${BASE}/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""${BASE}/validation_set.pbtxt"" \; --config.init_checkpoint=""${BASE}/checkpoint/deepvariant.wgs.ckpt"" \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```; ```; I0508 17:53:46.544947 140534986602304 train.py:384] Starting epoch 0; I0508 17:53:46.545100 140534986602304 train.py:391] Performing initial evaluation of warmstart model.; I0508 17:53:46.545171 140534986602304 train.py:361] Running tune at step=0 epoch=0; I0508 17:53:46.545287 140534986602304 train.py:366] Tune step 0 / 15 (0.0%); I0508 17:54:10.069682 140512707213056 logging_writer.py:48] [0] tune/categorical_accuracy=0.22617188096046448, tune/categorical_crossentropy=1.3209192752838135, tune/f1_het=0.02283571846783161, tune/f1_homalt=0.09889934211969376, tune/f1_homref=0.843934178352356, tune/f1_macro=0.3218897581100464, tune/f1_micro=0.22617188096046448, tune/f1_weighted=0.21346084773540497, tune/false_negatives_1=6123.0, tune/false_positives_1=5727.0, tune/loss=1.3209190368652344, tune/precision_1=0.21375617384910583, tune/precision_het=0.19323670864105225, tune/precision_homalt=0.05127762258052826, tune/precision_homref=0.9494163393974304, tune/recall_1=0.20273438096046448, tune/recall_het=0.007176175247877836, tune/recall_homalt=0.834269642829895, tune/recall_homref=0.6971428394317627, tune/true_negatives_1=9633.0, tun",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904:1216,Perform,Performing,1216,,https://github.com/google/deepvariant/issues/819#issuecomment-2101161904,1,['Perform'],['Performing']
Performance," cp ""$BUILD_DIR/tools/clif/python/utils/proto_util.init.cc"" clif/python/utils/; # install; export C_INCLUDE_PATH=/home/qilibj/inst/include; export CPLUS_INCLUDE_PATH=/home/qilibj/inst/include; ""$CLIF_PIP"" install .; # echo ""SUCCESS - To use pyclif, run $CLIF_VIRTUALENV/bin/pyclif.""; python setup.py bdist_wheel; # Note: pyclif should be installed into virtualenv; ""$CLIF_PIP"" install pyclif-0.3-cp27-none-linux_ppc64le.whl; pip install dist/pyclif-0.3-cp27-cp27m-linux_ppc64le.whl. # verify; python -c ""from clif.python.proto import start"". # link for deepvariant; ln -s /home/qilibj/inst/clif /usr/local/; ```. ## Opencv-python 3.4.5.20. Git repository: [https://github.com/skvark/opencv-python](https://github.com/skvark/opencv-python). ```bash; # Checkout repository and submodules; git clone https://github.com/skvark/opencv-python.git; cd opencv-python/; # fetch the tags to your local repository; git fetch --all --tags --prune; # check out tag 3.4.5.20; git checkout tags/20; # load submoduel; git submodule update --init --recursive. # Dependency; pip install pyparsing; yum install qt-devel; # Build; python setup.py bdist_wheel. # Insatll; pip install dist/opencv_python-3.4.5.20-cp27-cp27mu-linux_ppc64le.whl. # Verify in a new session; python -c ""import cv2""; ```. ## DV Prerequisite. ```bash; ####################################################################; # misc setup; ####################################################################. # development packages; yum install python2-pkgconfig zip zlib-devel unzip curl -y; # python packages; yum install python-devel python-pip python-wheel -y. ####################################################################; # python packages; ####################################################################. # python 2 required; echo ""$(python --version)""; echo ""$(pip --version)"". # Install python packages; pip install contextlib2; pip install enum34; pip install intervaltree; pip install 'mock>=2.0.0'. # pip install 'numpy==1.1",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123#issuecomment-469190994:15439,load,load,15439,,https://github.com/google/deepvariant/issues/123#issuecomment-469190994,2,['load'],['load']
Performance," echo ""For debugging:""; # pip3 show nvidia-tensorrt; # TENSORRT_PATH=$(python3 -c 'import tensorrt; print(tensorrt.__path__[0])'); # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer.so.8"" ""${TENSORRT_PATH}/libnvinfer.so.7""; # sudo ln -sf ""${TENSORRT_PATH}/libnvinfer_plugin.so.8"" ""${TENSORRT_PATH}/libnvinfer_plugin.so.7""; # export LD_LIBRARY_PATH=""${LD_LIBRARY_PATH-}:${TENSORRT_PATH}""; # sudo ldconfig; # # Just in case this still doesn't work, we link them.; # # This is a workaround that we might want to get rid of, if we can make sure; # # setting LD_LIBRARY_PATH and `sudo ldconfig`` works.; # if [[ ! -e /usr/local/nvidia/lib ]]; then; # sudo mkdir -p /usr/local/nvidia/lib; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer.so.7"" /usr/local/nvidia/lib/libnvinfer.so.7; # sudo ln -sf ""${TENSORRT_PATH}//libnvinfer_plugin.so.7"" /usr/local/nvidia/lib/libnvinfer_plugin.so.7; # fi; # fi; ```. The output for `./run-prereq.sh`:; ```; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Mon 05 Jun 2023 01:42:32 AM UTC] Stage 'Misc setup' starting; ========== [Mon 05 Jun 2023 01:42:33 AM UTC] Stage 'Update package list' starting; ========== [Mon 05 Jun 2023 01:42:34 AM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Mon 05 Jun 2023 01:42:35 AM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5403k 0 --:--:-- --:--:-- --:--:-- 5403k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518:6269,Load,Load,6269,,https://github.com/google/deepvariant/issues/657#issuecomment-1575933518,1,['Load'],['Load']
Performance," https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:pileup_image_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log); (06:29:19) INFO: From Testing //deepvariant:pileup_image_test:; ==================== Test output for //deepvariant:pileup_image_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/deepvariant/pileup_image_test.py"", line 43, in <module>; from third_party.nucleus.io import fasta; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/pileup_image_test.runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 60, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>;",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:93093,cache,cache,93093,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," line 595, in ListObjects; global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/gslib/third_party/storage_apitools/storage_v1_client.py"", line 1237, in List; config, request, global_params=global_params); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/base_api.py"", line 701, in _RunMethod; http, http_request, **opts); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 341, in MakeRequest; check_response_func=check_response_func); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/apitools/apitools/base/py/http_wrapper.py"", line 391, in _MakeRequestNoRetry; redirections=redirections, connection_type=connection_type); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1570, in request; (response, content) = self._request(conn, authority, uri, request_uri, method, body, headers, redirections, cachekey); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1317, in _request; (response, content) = self._conn_request(conn, request_uri, method, body, headers); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1252, in _conn_request; conn.connect(); File ""/home/ydliu/anaconda3/envs/py2.7/share/google-cloud-sdk-166.0.0-0/platform/gsutil/third_party/httplib2/python2/httplib2/__init__.py"", line 1018, in connect; sock.connect((self.host, self.port)); File ""/home/ydliu/anaconda3/envs/py2.7/lib/python2.7/socket.py"", line 228, in meth; return getattr(self._sock,name)(*args); socket.timeout: timed out. return code: 1. ()",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/228#issuecomment-549130970:6709,cache,cachekey,6709,,https://github.com/google/deepvariant/issues/228#issuecomment-549130970,1,['cache'],['cachekey']
Performance," make_examples.py:1381] Created 28 examples. real	0m10.204s; user	0m5.490s; sys	0m3.310s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 13:07:08.439639 140638419556096 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 13:07:08.488881: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 13:07:08.491470: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x58612b0 executing computations on platform Host. Devices:; 2019-12-13 13:07:08.491562: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 13:07:08.495160: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 13:07:08.525229 140638419556096 modeling.py:560] Initializing model with random parameters; W1213 13:07:08.527353 140638419556096 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpr4M5u5; I1213 13:07:08.528274 140638419556096 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe88a5b6190>, '_model_dir': '/tmp/tmpr4M5u5', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-565441661:3650,Tune,Tune,3650,,https://github.com/google/deepvariant/issues/249#issuecomment-565441661,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance," min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_10_of_10/test.log. Executed 24 out of 38 tests: 14 tests pass and 24 fail locally.; There were tests whose specified size is too big. Use the --test_verbose_timeout_warnings command line option to see which ones these are.; (06:29:2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:126232,cache,cache,126232,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) [2,484 / 2,523] 17 / 38 tests, 3 failed; Testing //deepvariant:model_train_test [0s (9 actions)] ... (39 actions, 2 running); (06:29:09) FAIL: //deepvariant:model_train_test (shard 2 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 2 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 2 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:21606,cache,cache,21606,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) [2,502 / 2,523] 22 / 38 tests, 8 failed; Testing //deepvariant:model_train_test [0s (5 actions)] ... (21 actions, 2 running); (06:29:15) FAIL: //deepvariant:model_train_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_9_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_in",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:62541,cache,cache,62541,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123833,cache,cache,123833,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance," on many other aspects such as how to pull multiple workers to orchestrate a distributed workflow, or how to run with GPU (which involves installing GPU driver, using the binaries that are built for GPU, etc).; If you want to run on GPU, and if you have everything set up already (such as installing GPU driver correctly), you should be able to do it pretty much the same way. But instead of `sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""`, you'll pull from gcr.io/deepvariant-docker/**deepvariant_gpu** which is built for GPU.; We have also documented it here:; https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-details.md#call_variants in case you need to build the binaries yourself. Note that even though using GPUs is faster, the overall cost might not be better depending on many other factors. Again, you can look at the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) as an example of how they configure their run.; If you end up doing more experiments to compare different configurations in your workflow, we would love to learn more about it as well. In addition to the [GCP Cloud runner](https://cloud.google.com/genomics/docs/tutorials/deepvariant) that @nmousavi 's team maintains, we also have seen other examples such as https://github.com/atgenomix/deepvariant-on-spark (and their [WGS case study](https://github.com/atgenomix/deepvariant-on-spark/blob/master/docs/wgs-case-study.md) reports run time as well). In terms of how much details we include on the DeepVariant GitHub page --; Even though I'm personally very interested in the performance and cost of these implementations, I also need to consider the trade-off of the amount of details we include, because too much information can also end up being confusing. If you have more suggestions on how to organize the documentation better in the future, please let me know. Even now it's already a bit messy and I would like to simplify it further. Thanks!",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151#issuecomment-461426712:1821,perform,performance,1821,,https://github.com/google/deepvariant/issues/151#issuecomment-461426712,2,['perform'],['performance']
Performance, pool 'FileTransfer' minSize=10; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunn,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2192,cache,cache,2192,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,2,['cache'],['cache']
Performance," the reference needed for reading the cram file. Pysam uses a similar approach. . **The REF_PATH and REF_CACHE**; One of the key concepts in CRAM is that it is uses reference based compression. This means that Samtools needs the reference genome sequence in order to decode a CRAM file. Samtools uses the MD5 sum of the each reference sequence as the key to link a CRAM file to the reference genome used to generate it. By default Samtools checks the reference MD5 sums (@SQ “M5” auxiliary tag) in the directory pointed to by $REF_PATH environment variable (if it exists), falling back to querying the European Bioinformatics Institute (EBI) reference genome server, and further falling back to the @SQ “UR” field if these are not found. While the EBI have an MD5 reference server for downloading reference sequences over http, we recommend use of a local MD5 cache. We have provided with Samtools a basic script (misc/seq_cache_populate.pl) to convert your local yeast.fasta to a directory tree of reference sequence MD5 sums:. <samtools_src_dir>/misc/seq_cache_populate.pl -root /some_dir/cache yeast.fasta; export REF_PATH=/some_dir/cache/%2s/%2s/%s:http://www.ebi.ac.uk/ena/cram/md5/%s; export REF_CACHE=/some_dir/cache/%2s/%2s/%s; REF_PATH is a colon separated list of directories in which to search for files named after the sequence M5 field. The : in http:// is not considered to be a separator. Hence using the above setting, any CRAM files that are not cached locally may still be looked up remotely. In this example “%2s/%2s/%s” means the first two digits of the M5 field followed by slash, the next two digits and slash, and then the remaining 28 digits. This helps to avoid one large directory with thousands of files in it. The REF_CACHE environment variable is used to indicate that any downloaded reference sequences should be stored locally in this directory in order to avoid subsequent downloads. This should normally be set to the same location as the first directory in REF_PATH.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/38#issuecomment-372143466:1125,cache,cache,1125,,https://github.com/google/deepvariant/issues/38#issuecomment-372143466,4,['cache'],"['cache', 'cached']"
Performance," the runtime myself, so over the weekend, I tried building and running the version with OpenVINO to observe the behavior of call_variants. Specifically, I did something similar to https://github.com/google/deepvariant/blob/r1.0/scripts/run_wgs_runtime_test_docker.sh - but I incorporate your changes, and build the docker with OpenVINO, and made sure I ran call_variants with OpenVINO. Here is a strange thing I found in my log:. ```; ...; W1019 04:32:52.604453 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave >; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Opti>; 2020-10-19 04:32:52.727391: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_thread>; W1019 04:32:52.748747 140673783289600 deprecation.py:323] From /tmp/Bazel.runfiles_mdh0lz62/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from >; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the f>; I1019 09:14:02.799481 140673783289600 call_variants.py:520] Writing calls to /tmp/tmpll7hkhwu/call_variants_output.tfrecord.gz; I1019 09:14:02.804919 140673783289600 call_variants.py:538] Processed 1 examples in 1 batches [0.284 sec per 100]; I1019 09:14:04.482554 140673783289600 call_variants.py:538] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1019 09:14:06.172387 140673783289600 call_variants.py:538] Processed 30001 examples in 59 batches [0.011 sec per 100]; I1019 09:14:07.867975 140673783289600 call_variants.py:538] Processed 45001 examples in 88 batches [0.011 sec pe",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-712402658:1039,Tune,Tune,1039,,https://github.com/google/deepvariant/pull/363#issuecomment-712402658,1,['Tune'],['Tune']
Performance," the underlying data is not the same and the goals usually are not equal. Therefore the approach for optimizing model is a journey of discovery performed via hyperparameter tuning. For example, Google uses [Vizier](https://github.com/google/vizier), but the idea falls into one of five general camps: . * Manual Tuning; * Random Search; * Grid Search; * Bayesian Optimization; * Tree-structured Parzen estimators . Here is a [link to an article](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide) that provides a nice summary of them - with [another describing them more visually](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/) - and [a link to another nice article](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) describing what happens during hyperparameter tuning. There are other ways, but they become niche and sometimes based on the data, private. Usually this training is performed automatically [as shown here](https://blog.floydhub.com/guide-to-hyperparameters-search-for-deep-learning-models/), but you can generate the search space yourself like in this simple example - though there are [more parameters you can select from](https://github.com/google/deepvariant/blob/r1.5/deepvariant/modeling.py#L68-L110):. $`1)`$ Learning rate usually changes as you get closer to optimal accuracy, since you are getting closer to optimal and do not overshoot the local minimal in the hyperplane. If you are starting from an untrained model, you want learn as much as possible with the default value of `0.064`, but the closer you get to optimal you want to minimize it to something like `0.0005`. If let's say learning rate decreases exponentially with accuracy - meaning you want to tweak the model less as you become more accurate - then it would be something like `learning_rate` $= (1-(e^{accuracy-1})^\alpha)/\gamma$, where $\alpha = 5$ and $\gamma=0.1$, resulting in a chart like this:. !",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294:1389,perform,performed,1389,,https://github.com/google/deepvariant/issues/706#issuecomment-1720360294,2,['perform'],['performed']
Performance," this?. 3. I want to show the messages I got from running Ubuntu 20.04 on Mac's UTM.; settings.sh has been modified:; ```; export CUDNN_INSTALL_PATH=""/usr/lib/aarch64-linux-gnu"" #instead of /usr/lib/x86_64-linux-gnu; # orig: export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export DV_USE_GCP_OPTIMIZED_TF_WHL=""0"" #force using CPU only tensorflow; ```. The message when running:; ```; > sudo su; > source settings.sh; > ./run-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Sun 04 Jun 2023 11:11:08 PM UTC] Stage 'Misc setup' starting; ========== [Sun 04 Jun 2023 11:11:09 PM UTC] Stage 'Update package list' starting; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Sun 04 Jun 2023 11:11:10 PM UTC] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2518k 100 2518k 0 0 5671k 0 --:--:-- --:--:-- --:--:-- 5671k; Collecting pip; Using cached pip-23.1.2-py3-none-any.whl (2.1 MB); Installing collected packages: pip; Attempting uninstall: pip; Found existing installation: pip 23.1.2; Uninstalling pip-23.1.2:; Successfully uninstalled pip-23.1.2; WARNING: The scripts pip, pip3 and pip3.8 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-23.1.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; Python 3.8.10; pip 23.1.2 from /root/.local/lib/python3.8/site-packages/pip (python 3.8); ========== [Sun 04 Jun 2023 11:11:13 PM UTC] Stage 'Install python3 packages' start",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276:1273,cache,cached,1273,,https://github.com/google/deepvariant/issues/657#issuecomment-1575779276,1,['cache'],['cached']
Performance,""", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3010,cache,cache,3010,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; alternate_bases: ""C""; end: 11; reference_name: ""20""; start: 10; , reference_bases: ""A""; alternate_bases: ""C""; end: 21; reference_name: ""20""; start: 20; ]) (__main__.HaplotypeLabelerClassUnitTest); test_make_labeler_ref(truths=[], expected_end=21, expected_start=9, bufsize=0, candidates=[reference_bases: ""A""; ----------------------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-package",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1362,cache,cache,1362,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,1,['cache'],['cache']
Performance,"'numpy.random.mtrand.RandomState' objects}; 5759 0.094 0.000 0.937 0.000 <frozen importlib._bootstrap_external>:1498(find_spec); 23926 0.089 0.000 0.109 0.000 /usr/lib/python3.8/enum.py:313(__call__); 3136 0.089 0.000 1.439 0.000 <frozen importlib._bootstrap_external>:1034(get_data); 473 0.083 0.000 0.120 0.000 <frozen importlib._bootstrap_external>:1317(_path_hooks); 1417/288 0.077 0.000 0.151 0.001 /usr/lib/python3.8/sre_parse.py:493(_parse); 4910 0.069 0.000 0.069 0.000 {method 'PythonNext' of 'third_party.nucleus.io.python.sam_reader.SamIterable' objects}; 3614/1 0.069 0.000 11.965 11.965 {built-in method builtins.exec}; 701 0.066 0.000 0.066 0.000 {built-in method posix.listdir}; 15152 0.063 0.000 0.268 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:69(_wrapreduction); 32755 0.062 0.000 0.135 0.000 <frozen importlib._bootstrap_external>:121(_path_join); 171/133 0.060 0.000 0.178 0.001 {built-in method _imp.exec_dynamic}; 1 0.059 0.059 0.059 0.059 {method 'calls_from_allele_counts' of 'deepvariant.python.variant_calling_multisample.VariantCaller' objects}; 23323 0.058 0.000 0.163 0.000 /usr/lib/python3.8/inspect.py:2489(__init__); 15563/15359 0.053 0.000 0.384 0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}; 3601 0.053 0.000 0.344 0.000 /usr/lib/python3.8/inspect.py:1102(getfullargspec); 15152 0.051 0.000 0.319 0.000 /usr/local/lib/python3.8/dist-packages/numpy/core/fromnumeric.py:2703(amax); 12155 0.049 0.000 0.060 0.000 {built-in method builtins.max}; 40200 0.049 0.000 0.049 0.000 {built-in method third_party.nucleus.util.python.utils.read_overlaps_region}; ```. In any case, the most optimized approach is some sort shared memory model with minimal overhead to keep the code nimble and easy to integrate or extend. The code itself is pleasantly easy to go through given the nice design, which makes it quite amendable to that. In the meantime, pass-by-reference probably could be an easier win. . Thank you,; Paul",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826:10032,optimiz,optimized,10032,,https://github.com/google/deepvariant/issues/650#issuecomment-1552327826,2,['optimiz'],['optimized']
Performance,"('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:20) FAIL: //deepvariant/labeler:labeled_examples_to_vcf_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log); (06:29:20) INFO: From Testing //deepvariant/labeler:labeled_examples_to_vcf_test:; ==================== Test output for //deepvariant/labeler:labeled_examples_to_vcf_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/deepvariant/labeler/labeled_examples_to_vcf_test.py"", line 39, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/labeled_examples_to_vcf_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # p",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:103726,cache,cache,103726,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) [2,488 / 2,523] 19 / 38 tests, 5 failed; Testing //deepvariant:data_providers_test; 0s local ... (35 actions, 2 running); (06:29:10) FAIL: //deepvariant:data_providers_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log); (06:29:10) INFO: From Testing //deepvariant:data_providers_test:; ==================== Test output for //deepvariant:data_providers_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/data_providers_test.runfiles/com_google_deepvariant/deepvariant/data_providers_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorfl",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:30890,cache,cache,30890,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"**Thank you!** . **Once adding in the location of the bed file, which is definitely not empty (download link for bed file here: https://we.tl/t-7EXAoGz8RT). using this code:** . ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=72:00:00; #SBATCH --mem-per-cpu=64GB. module purge; module load parallel; module load singularity. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=Polyposis_Exome_Analysis/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fa.gz \; --reads=Polyposis_Exome_Analysis/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam \; --regions=Polyposis_Exome_Analysis/deepvariant/agilent_bedfile/agilent_human_region.hg38.bed \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **I get the error:** ; ; ```; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to make sure they are not resulting in set of empty region to process. This also happens if you use ""chr20"" for a BAM where contig names don't have ""chr""s (or vice versa).; parallel: This job failed:; /opt/deepv",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214:629,load,load,629,,https://github.com/google/deepvariant/issues/527#issuecomment-1067792214,2,['load'],['load']
Performance,", Critical Path: 13.35s; (06:29:21) INFO: 43 processes: 43 local.; (06:29:21) INFO: Build completed, 24 tests FAILED, 44 total actions; //deepvariant:allelecounter_test (cached) PASSED in 0.5s; //deepvariant:dv_vcf_constants_test (cached) PASSED in 1.7s; //deepvariant:exclude_contigs_test (cached) PASSED in 0.9s; //deepvariant:postprocess_variants_lib_test (cached) PASSED in 0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:117966,cache,cached,117966,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cached']
Performance,", in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:19) FAIL: //deepvariant:make_examples_test (shard 2 of 2) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log); (06:29:19) INFO: From Testing //deepvariant:make_examples_test (shard 2 of 2):; ==================== Test output for //deepvariant:make_examples_test (shard 2 of 2):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/deepvariant/make_examples_test.py"", line 48, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvarian",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:97382,cache,cache,97382,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,", in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-28:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. (similar records from other workers repeating here ...)",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449:3630,queue,queues,3630,,https://github.com/google/deepvariant/issues/804#issuecomment-2308162449,1,['queue'],['queues']
Performance,",000"" was ok without shards,; however, for WES.bed I got a similar error as below. . ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". I1213 19:19:36.445342 140624564107008 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2019-12-13 19:19:36.497919: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2697915000 Hz; 2019-12-13 19:19:36.499703: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x52dd110 executing computations on platform Host. Devices:; 2019-12-13 19:19:36.499773: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2019-12-13 19:19:36.503204: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1213 19:19:36.524235 140624564107008 modeling.py:560] Initializing model with random parameters; W1213 19:19:36.526154 140624564107008 estimator.py:1760] Using temporary folder as model directory: /tmp/tmp6q1g_L; I1213 19:19:36.526648 140624564107008 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe550820190>, '_model_dir': '/tmp/tmp6q1g_L', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249#issuecomment-565657419:1019,Tune,Tune,1019,,https://github.com/google/deepvariant/issues/249#issuecomment-565657419,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"----------------------------------------------------------; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 37, in testPartExecutor; yield; File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/third_party/unittest3_backport/case.py"", line 162, in run; testMethod(); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/absl_py/absl/testing/parameterized.py"", line 256, in bound_param_test; test_method(self, **testcase_params); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/haplotype_labeler_test.py"", line 371, in test_make_labeler_ref; ranges.make_range('20', expected_start, expected_end)); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 948, in assert_called_once_with; return self.assert_called_with(*args, **kwargs); File ""/home/qilibj/inst/lib/python2.7/site-packages/mock/mock.py"", line 937, in assert_called_with; six.raise_from(AssertionError(_error_message(cause)), cause); File ""/root/.cache/bazel/_bazel_root/dc155a991b1776fcc65387121539d20a/execroot/com_google_deepvariant/bazel-out/ppc-opt/bin/deepvariant/labeler/haplotype_labeler_test.runfiles/six_archive/six.py"", line 718, in raise_from; raise value; AssertionError: Expected call: query(reference_name: ""20""; start: 9; end: 21; ); Actual call: query(reference_name: ""20""; start: 9; end: 1 <== this ends with 1 due to the mock object returns 1; ); ```",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154#issuecomment-464683367:1909,cache,cache,1909,,https://github.com/google/deepvariant/issues/154#issuecomment-464683367,2,['cache'],['cache']
Performance,--input_pattern_list=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples/tune_all/*tune_examples*tfrecord-000*-of-00040; # --output_pattern_prefix=/storage/scratch/iee/dj20y461/Stickleback/G_aculeatus/FITNESS/DV_training//examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled; #; ```. and . ```; ls /home/examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:9823,tune,tune,9823,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,-?????-of-?????.tfrecord.gz; ```; gives the desired file list again:. ```; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00000-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00001-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00002-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00003-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00004-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00005-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00006-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00007-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00008-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00009-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00010-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00011-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00012-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00013-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00014-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00015-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_all_tune_examples_inc_downsampled_05.shuffled-00016-of-00020.tfrecord.gz; ./examples_shuffled/tune/All_samples_al,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666:10267,tune,tune,10267,,https://github.com/google/deepvariant/issues/876#issuecomment-2324108666,1,['tune'],['tune']
Performance,". for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:08) FAIL: //deepvariant/realigner:aligner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log); (06:29:08) INFO: From Testing //deepvariant/realigner:aligner_test:; ==================== Test output for //deepvariant/realigner:aligner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/deepvariant/realigner/aligner_test.py"", line 40, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/aligner_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:14885,cache,cache,14885,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,".4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124356,cache,cache,124356,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,".9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) [2,512 / 2,523] 28 / 38 tests, 14 failed; Testing //deepvariant/realigner:realigner_test; 0s local ... (11 actions, 2 running); (06:29:18) FAIL: //deepvariant/realigner:realigner_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner:realigner_test:; ==================== Test output for //deepvariant/realigner:realigner_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/realigner_test.runfiles/com_google_deepvariant/deepvariant/realigner/realigner_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_t",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:88202,cache,cache,88202,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,".org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:17) FAIL: //deepvariant:variant_caller_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log); (06:29:17) INFO: From Testing //deepvariant:variant_caller_test:; ==================== Test output for //deepvariant:variant_caller_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/deepvariant/variant_caller_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/variant_caller_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:75516,cache,cache,75516,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,".pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:18) FAIL: //deepvariant/realigner/allele_count_linear:generate_trained_model_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log); (06:29:18) INFO: From Testing //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; ==================== Test output for //deepvariant/realigner/allele_count_linear:generate_trained_model_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/realigner/allele_count_linear/generate_trained_model_test.runfiles/com_google_deepvariant/deepvariant/realigner/allele_count_linear/generate_trained_model_test.py"", line 39, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python im",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:80331,cache,cache,80331,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,//deepvariant:model_eval_test (Summary); /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107177,cache,cache,107177,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-11-28 03:33:00.752221: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.766410 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:375: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; 2020-11-28 03:33:02.961537: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; W1128 03:33:02.980482 139674856871680 deprecation.py:323] From /tmp/Bazel.runfiles_rud4ovxa/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I1128 03:46:54.531774 139674856871680 call_variants.py:434] Writing calls to /tmp/tmp0gfwv278/call_variants_output.tfrecord.gz; I1128 03:46:54.533843 139674856871680 call_variants.py:452] Processed 1 examples in 1 batches [0.123 sec per 100]; I1128 03:46:56.165715 139674856871680 call_variants.py:452] Processed 15001 examples in 30 batches [0.011 sec per 100]; I1128 03:46:57.810235 139674856871680 call_variants.py:452] Processed 30001 examples in 59",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363#issuecomment-735276922:2251,Tune,Tune,2251,,https://github.com/google/deepvariant/pull/363#issuecomment-735276922,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --; checkpoint ""/opt/models/wgs/model.ckpt""; I1217 09:08:41.108182 139680301201152 call_variants.py:313] Set KMP_BLOCKTIME to 0; 2020-12-17 09:08:41.511115: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; 2020-12-17 09:08:42.039849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz; 2020-12-17 09:08:42.070759: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x5abc760 executing computations on platform Host. Devices:; 2020-12-17 09:08:42.070838: I tensorflow/compiler/xla/service/service.cc:158] StreamExecutor device (0): <undefined>, <undefined>; 2020-12-17 09:08:42.092135: I tensorflow/core/common_runtime/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for ; best performance.; I1217 09:08:42.442809 139680301201152 modeling.py:560] Initializing model with random parameters; W1217 09:08:42.445099 139680301201152 estimator.py:1760] Using temporary folder as model directory: /tmp/tmpKfC9_T; I1217 09:08:42.445868 139680301201152 estimator.py:201] Using config: {'_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_tra; in_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f09a30d11d0>, '_model_dir': '/tmp/tmpKfC9_T', '_protocol': Non; e, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn':; None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_clus; ter': 0, ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/399#issuecomment-749313156:2281,Tune,Tune,2281,,https://github.com/google/deepvariant/issues/399#issuecomment-749313156,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md#run-deepvariant-on-chromosome-20-alignments. I added `--call_variants_extra_args ""use_openvino=true""`. So my command is:; ```; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.1.0""; mkdir -p deepvariant1. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --call_variants_extra_args ""use_openvino=true""; ```. This actually worked for me. I'll paste some logs around the model conversion:; ```; Instructions for updating:; Use `tf.compat.v1.graph_util.remove_training_nodes`; Model Optimizer arguments:; Common parameters:; - Path to the Input Model: /home/pichuan/model.pb; - Path for generated IR: /home/pichuan/.; - IR output name: model; - Log level: ERROR; - Batch: Not specified, inherited from the model; - Input layers: Not specified, inherited from the model; - Output layers: Not specified, inherited from the model; - Input shapes: Not specified, inherited from the model; - Mean values: [128,128,128,128,128,128,128,128,128]; - Scale values: Not specified; - Scale factor: 128.0; - Precision of IR: FP32; - Enable fusing: True; - Enable grouped convolutions fusing: True; - Move mean values to preprocess section: False; - Reverse input channels: False; TensorFlow specific parameters:; - Input model in text protobuf format: False; - Path to model dump for TensorBoard: None; - List of shared libraries with TensorFlow custom layers implementation: None; - Update the configuration file with input/output node names: None; - Use configuration file used to generate the model with Object Detection AP",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/404#issuecomment-761309014:2824,Optimiz,Optimizer,2824,,https://github.com/google/deepvariant/issues/404#issuecomment-761309014,1,['Optimiz'],['Optimizer']
Performance,/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/tf_utils_test/test.log; //deepvariant:variant_caller_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlo,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:119519,cache,cache,119519,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/labeler/customized_classes_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:customized_classes_labeler_test:; ==================== Test output for //deepvariant/labeler:customized_classes_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/customized_classes_labeler_test.py"", line 41, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/customized_classes_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 82, in <module>; from tensorflow.python.lib.io import python_io; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py""",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:33720,cache,cache,33720,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (sha,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107351,cache,cache,107351,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_de",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124530,cache,cache,124530,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1771,cache,cache,1771,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/si",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107699,cache,cache,107699,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124878,cache,cache,124878,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107873,cache,cache,107873,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:125052,cache,cache,125052,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; (06:29:20) FAIL: //deepvariant:model_eval_test (shard 9 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log); (06:29:20) INFO: From Testing //deepvariant:model_eval_test (shard 9 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 9 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:107525,cache,cache,107525,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; //deepvariant:model_train_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_2_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:124704,cache,cache,124704,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_9_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/s",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:1945,cache,cache,1945,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:12) FAIL: //deepvariant:haplotypes_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log); (06:29:12) INFO: From Testing //deepvariant:haplotypes_test:; ==================== Test output for //deepvariant:haplotypes_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/deepvariant/haplotypes_test.py"", line 43, in <module>; from third_party.nucleus.testing import test_utils; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/haplotypes_test.runfiles/com_google_deepvariant/third_party/nucleus/testing/test_utils.py"", line 47, in <module>; from tensorflow.python.platform import gfile; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent ca",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:42461,cache,cache,42461,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_dee",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123179,cache,cache,123179,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,0.1s; //deepvariant:resources_test (cached) PASSED in 1.7s; //deepvariant:utils_test (cached) PASSED in 0.5s; //deepvariant:variant_calling_test (cached) PASSED in 0.6s; //deepvariant/environment_tests:env_smoke_test (cached) PASSED in 0.7s; //deepvariant/environment_tests:protobuf_implementation_test (cached) PASSED in 1.2s; //deepvariant/realigner:fast_pass_aligner_test (cached) PASSED in 0.6s; //deepvariant/realigner:ssw_test (cached) PASSED in 0.5s; //deepvariant/realigner/python:ssw_misc_test (cached) PASSED in 0.9s; //deepvariant/realigner/python:ssw_wrap_test (cached) PASSED in 1.2s; //deepvariant/vendor:timer_test (cached) PASSED in 0.7s; //deepvariant:call_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/call_variants_test/test.log; //deepvariant:data_providers_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/data_providers_test/test.log; //deepvariant:haplotypes_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/haplotypes_test/test.log; //deepvariant:modeling_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/modeling_test/test.log; //deepvariant:pileup_image_test FAILED in 0.3s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/pileup_image_test/test.log; //deepvariant:postprocess_variants_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log; //deepvariant:tf_utils_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:118259,cache,cache,118259,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,0; maxSize=288; workQueue=LinkedBlockingQueue[10000]; allowCoreThreadTimeout=false; Jun-08 12:02:05.348 [main] DEBUG nextflow.cli.CmdRun -; Version: 22.10.7 build 5853; Created: 18-02-2023 20:32 UTC (19-02-2023 02:02 IDT); System: Linux 5.4.0-146-generic; Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 14.0.2+12-Ubuntu-120.04; Encoding: UTF-8 (UTF-8); Process: 683315@victor [127.0.1.1]; CPUs: 96 - Mem: 503.5 GB (137.9 GB) - Swap: 108 GB (107.9 GB); Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Work-dir: /data/shared/clinical/LongRead/Pipeline/work [ext2/ext3]; Jun-08 12:02:05.365 [main] DEBUG nextflow.Session - Script base path does not exist or is not a directory: /data/shared/clinical/LongRead/Pipeline/bin; Jun-08 12:02:05.372 [main] DEBUG nextflow.executor.ExecutorFactory - Extension executors providers=[]; Jun-08 12:02:05.380 [main] DEBUG nextflow.Session - Observer factory: DefaultObserverFactory; Jun-08 12:02:05.398 [main] DEBUG nextflow.cache.CacheFactory - Using Nextflow cache factory: nextflow.cache.DefaultCacheFactory; Jun-08 12:02:05.407 [main] DEBUG nextflow.util.CustomThreadPool - Creating default thread pool > poolSize: 97; maxThreads: 1000; Jun-08 12:02:05.482 [main] DEBUG nextflow.Session - Session start; Jun-08 12:02:05.642 [main] DEBUG nextflow.script.ScriptRunner - > Launching execution; Jun-08 12:02:05.728 [main] DEBUG nextflow.executor.ExecutorFactory - << taskConfig executor: null; Jun-08 12:02:05.738 [main] DEBUG n.processor.LocalPollingMonitor - Creating local task monitor for executor 'local' > cpus=96; memory=503.5 GB; capacity=96; pollInterval=100ms; dumpInterval=5m; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Workflow process names [dsl2]: pbc_varicall; Jun-08 12:02:05.804 [main] DEBUG nextflow.Session - Igniting dataflow network (2); Jun-08 12:02:05.809 [main] DEBUG nextflow.processor.TaskProcessor - Starting process > pbc_varicall; Jun-08 12:02:05.810 [main] DEBUG nextflow.script.ScriptRunner - > Awaiting termination; J,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:2198,Cache,CacheFactory,2198,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,3,"['Cache', 'cache']","['CacheFactory', 'cache']"
Performance,"0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; (05:40:51) FAIL: //deepvariant:model_eval_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log); (05:40:51) INFO: From Testing //deepvariant:model_eval_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_int",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:2567,cache,cache,2567,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test.log; //deepvariant/realigner/allele_count_linear:generate_trained_model_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/generate_trained_model_test/test.log; //deepvariant/realigner/allele_count_linear:model_evaluation_test FAILED in 0.5s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/allele_count_linear/model_evaluation_test/test.log; //deepvariant/realigner/python:debruijn_graph_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:122135,cache,cache,122135,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"0f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/python/debruijn_graph_wrap_test/test.log; //deepvariant:make_examples_test FAILED in 2 out of 2 in 0.4s; Stats over 2 runs: max = 0.4s, min = 0.4s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test FAILED in 10 out of 10 in 0.4s; Stats over 10 runs: max = 0.4s, min = 0.3s, avg = 0.4s, dev = 0.0s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_1_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_8_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_6_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_7_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_5_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_4_of_10/test.log; /root/.cache/bazel/_bazel_root/ce699a1ca024",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:123659,cache,cache,123659,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"1) In the blog, if you look at [Jason's Jupyter notebooks](https://github.com/cschin/DeepDiveDeepVariant), you'll see him using `--norealign_reads` which skips the preprocessing step described in the paper. That's why he's getting the `T` which a is a possible sequencing error. 2) The general realignment heuristic is the following: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/realigner.py#L439-L454. 3) The ploidy dictates the most likely haplotypes which is 2 :). 3) The haploytype realignment also been optimized a bit since that paper. Take a look at this section of the code: https://github.com/google/deepvariant/blob/r0.8/deepvariant/realigner/fast_pass_aligner.cc#L117-L166. 4) Yes, the realignment is basically used to update the reads' interpretation via the best-representative haplotype, trying to approach what might be closest-truth if the sequencer were relatively perfect. It's basically a way to correct. 5) Variant calling comes later, and is performed via the following, which is called from [make_examples.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/make_examples.py):. https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_caller.py ; https://github.com/google/deepvariant/blob/r0.8/deepvariant/variant_calling.cc. 6) The [predict(...) in call_variants.py](https://github.com/google/deepvariant/blob/r0.8/deepvariant/call_variants.py#L374-L378) emits the `genotype_probabilities`, which Jason used to demonstrate the low probability of `T` as a sequencing error. Just like any machine-learning trained model, if it's not seen the pattern before it will not predict it with a high match - no real magic here :). 7) It might help to read the [deepvariant.proto](https://github.com/google/deepvariant/blob/r0.8/deepvariant/protos/deepvariant.proto), which you can treat like a spec-design document of the general idea behind DeepVariant. The [details doc](https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-d",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/197#issuecomment-512112524:1099,perform,performed,1099,,https://github.com/google/deepvariant/issues/197#issuecomment-512112524,2,['perform'],['performed']
Performance,"12.1 NW_018085313.1 NW_018085314.1 NW_018085315.1 NW_018085316.1 NW_018085317.1 NW_018085318.1 NW_018085319.1 NW_018085320.1 NW_018085321.1 NW_018085322.1 NW_018085323.1 NW_018085324.1 NW_018085325.1 NW_018085326.1 NW_018085327.1 NW_018085328.1 NW_018085329.1 NW_018085330.1 NW_018085331.1 NW_018085332.1 NW_018085333.1 NW_018085334.1 NW_018085335.1 NW_018085336.1 NW_018085337.1 NW_018085338.1 NW_018085339.1 NW_018085340.1 NW_018085341.1 NW_018085342.1 NW_018085343.1 NW_018085344.1 NW_018085345.1 NW_018085346.1 NW_018085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1; > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB; > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter.; > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions; > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5; > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database...; > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete!; > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up.; > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5; > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads; > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles; > [71420] [2",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:12458,load,load,12458,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['load'],['load']
Performance,"1312 call_variants.py:462] Processed 300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s; sys 10m18.739s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 772, in write_variants_to_vcf; with vcf.VcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_writer.py"", line 174, in __init__; self._writer = self._native_writer(output_path, **kwargs); File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 309, in _native_writer; return NativeVcfWriter(; File ""/tmp/Bazel.runfiles_21tufdoh/runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 287, in __init__; self._writer = vcf_writer.VcfWriter.to_file(output_path, header,; ValueError: UNKNOWN: Could not open variants_path: /data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz. real 0m7.906s; user 0m8.421s; sys 0m8.363s. Work dir:; /data/shared/clinical",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:11273,optimiz,optimized,11273,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,4,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"18085347.1 NW_018085348.1 NW_018085349.1 NW_018085350.1 NW_018085351.1 NW_018085352.1 NW_018085353.1 NW_018085354.1 NW_018085355.1 NW_018085356.1 NW_018085357.1 NW_018085358.1 NW_018085359.1 NW_018085360.1 NW_018085361.1 NW_018085362.1 NW_018085363.1 NW_018085364.1 NW_018085365.1 NW_018085366.1 NW_018085367.1 NW_018085368.1 NC_000845.1; > [71420] [2024-04-03 09:10:42.642] [GLnexus] [info] db_get_contigs GLnexus.DB; > [71420] [2024-04-03 09:10:42.789] [GLnexus] [info] Beginning bulk load with no range filter.; > [71420] [2024-04-03 10:09:37.111] [GLnexus] [info] Loaded 5 datasets with 5 samples; 846365851832 bytes in 8659464665 BCF records (882 duplicate) in 414215 buckets. Bucket max 2856376 bytes, 28997 records. 0 BCF records skipped due to caller-specific exceptions; > [71420] [2024-04-03 10:09:37.141] [GLnexus] [info] Created sample set *@5; > [71420] [2024-04-03 10:09:37.142] [GLnexus] [info] Flushing database...; > [71420] [2024-04-03 10:11:17.432] [GLnexus] [info] Bulk load complete!; > [71420] [2024-04-03 10:11:17.482] [GLnexus] [warning] Processing full length of 613 contigs, as no --bed was provided. Providing a BED file with regions of interest, if applicable, can speed this up.; > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] found sample set *@5; > [71420] [2024-04-03 10:11:17.509] [GLnexus] [info] discovering alleles in 613 range(s) on 126 threads; > [71420] [2024-04-03 10:17:12.989] [GLnexus] [info] discovered 3689057 alleles; > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] unified to 159191 sites cleanly with 159684 ALT alleles. 1 ALT alleles were additionally included in monoallelic sites and 1704795 were filtered out on quality thresholds.; > [71420] [2024-04-03 10:17:15.093] [GLnexus] [info] Finishing database compaction...; > [71420] [2024-04-03 10:17:17.832] [GLnexus] [info] genotyping 159191 sites; sample set = *@5 mem_budget = 0 threads = 128; > [71420] [2024-04-03 10:19:30.901] [GLnexus] [info] genotyping complete!; > [71420] [2024-0",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378:12961,load,load,12961,,https://github.com/google/deepvariant/issues/778#issuecomment-2033814378,1,['load'],['load']
Performance,"19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=80; ```. ```; processor : 87; vendor_id : GenuineIntel; cpu family : 6; model : 79; model name : Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz; stepping : 1; microcode : 0xb000040; cpu MHz : 1201.050; cache size : 56320 KB; physical id : 1; siblings : 44; core id : 28; cpu cores : 22; apicid : 121; initial apicid : 121; fpu : yes; fpu_exception : yes; cpuid level : 20; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa itlb_multihit mmio_stale_data; bogomips : 4401.96; clflush size : 64; cache_alignment : 64; address sizes : 46 bits physical, 48 bits virtual; power management:; ```. ```; *-display ; description: VGA compatible controller; product: G200eR2; vendor: Matrox Electronics Systems Ltd.; physical id: 0; bus info: pci@0000:0b:00.0; version: 01; width: 32 bits; clock: 33MHz; capabilities: pm vga_controller bus_master cap_list rom; configuration: driver=mgag200 latency=0 maxlatency=32 mingnt=16; resources: irq:19 mem",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205:1354,cache,cache,1354,,https://github.com/google/deepvariant/issues/452#issuecomment-1495875205,1,['cache'],['cache']
Performance,20516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/variant_caller_test/test.log; //deepvariant/labeler:customized_classes_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120483,cache,cache,120483,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:21) FAIL: //deepvariant:postprocess_variants_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/postprocess_variants_test/test.log); (06:29:21) INFO: From Testing //deepvariant:postprocess_variants_test:; ==================== Test output for //deepvariant:postprocess_variants_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants_test.runfiles/com_google_deepvariant/deepvariant/postprocess_variants_test.py"", line 46, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportError(msg); ImportError: Traceback (most recent call last):; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:115205,cache,cache,115205,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"300001 examples in 586 batches [0.079 sec per 100]; I0608 12:16:37.228438 139794368661312 call_variants.py:462] Processed 350001 examples in 684 batches [0.079 sec per 100]; I0608 12:17:07.588583 139794368661312 call_variants.py:468] Processed 390233 examples in 763 batches [0.078 sec per 100]; I0608 12:17:07.588791 139794368661312 call_variants.py:471] Done calling variants from a total of 390233 examples. real 5m9.540s; user 294m6.601s; sys 10m18.739s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/data/shared/clinical/LongRead/Data/resources/Homo_sapiens_assembly38.fasta"" --infile ""/tmp/tmpye305c9i/call_variants_output.tfrecord.gz"" --outfile ""/data/shared/clinical/LongRead/Data//Analysis/out_m84011_220902_175841_NFR_sif.vcf.gz"". 2023-06-08 12:17:08.837575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0608 12:17:10.554912 140524419180352 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: HG002; 2023-06-08 12:17:10.560565: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmpye305c9i/call_variants_output.tfrecord.gz; 2023-06-08 12:17:12.148564: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 390233; I0608 12:17:15.074928 140524419180352 postprocess_variants.py:1037] CVO sorting took 0.07523852189381917 minutes; I0608 12:17:15.075901 140524419180352 postprocess_variants.py:1040] Transforming call_variants_output to variants.; I0608 12:17:15.089107 140524419180352 postprocess_variants.py:1061] Writing variants to VCF.; I0608 12:17:15.089209 140524419180352 postprocess_variants.py:771] Writing output to VCF file: /data/shared/clinical/LongRead/Data//Analysis/out_m840",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308:7033,optimiz,optimized,7033,,https://github.com/google/deepvariant/issues/659#issuecomment-1581991308,4,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"338] Shape of input examples: [100, 221, 9]; 2023-12-19 05:41:37.985946: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-12-19 05:41:38.002687: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2100000000 Hz; 2023-12-19 05:41:38.008172: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5c08e10 initialized for platform Host (this does not guarantee that XLA will be used). Devices:; 2023-12-19 05:41:38.008269: I tensorflow/compiler/xla/service/service.cc:176] StreamExecutor device (0): Host, Default Version; 2023-12-19 05:41:38.008459: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpz9j4xfvf; W1219 05:41:38.122338 139694699493120 estimator.py:1844] Using temporary folder as model directory: /tmp/tmpz9j4xfvf; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpz9j4xfvf', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}; I1219 05:41:38.123045 ",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013:1073,Tune,Tune,1073,,https://github.com/google/deepvariant/issues/757#issuecomment-1864800013,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"3390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:11) ERROR: /home/user/Documents/deepvariant-r1.5/WORKSPACE:108:14: fetching python_configure rule //external:local_config_python: Traceback (most recent call last):; 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 271, column 40, in _python_autoconf_impl; 		_create_local_python_repository(repository_ctx); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 214, column 22, in _create_local_python_repository; 		_check_python_lib(repository_ctx, python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk i",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:1712,cache,cache,1712,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,1,['cache'],['cache']
Performance,"46 bytes; samtools index: failed to create index for ""ENCFF528VXT.bam""; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$; paul@gubuntu:~/data/luisa$ cd ~/deepvariant/bazel-bin; paul@gubuntu:~/deepvariant/bazel-bin$ PYTHONPATH=. /usr/bin/python deepvariant/make_examples --mode calling --ref /home/paul/data/luisa/hg19.fa --reads /home/paul/data/luisa/ENCFF528VXT.bam --examples /home/paul/data/luisa/shardedExamples/examples.tfrecord@2.gz --regions chr20:10,000,000-10,010,000 --task 0; WARNING: Logging before flag parsing goes to stderr.; I0307 16:27:52.052795 140569100494592 client.py:1004] Timeout attempting to reach GCE metadata service.; W0307 16:27:52.112967 140569100494592 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; [W::bam_hdr_read] EOF marker is absent. The input is probably truncated; Traceback (most recent call last):; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1056, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 225, in default_options; with genomics_io.make_sam_reader(flags_obj.reads) as sam_reader:; File ""/home/paul/.cache/bazel/_bazel_paul/26d0829f7bba5a6d91f862666f0d8ee5/execroot/genomics/bazel-out/k8-opt/bin/deepvariant/make_examples.runfiles",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/52#issuecomment-371293506:2090,cache,cache,2090,,https://github.com/google/deepvariant/issues/52#issuecomment-371293506,2,['cache'],['cache']
Performance,4b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.log; //deepvariant/realigner:window_selector_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/window_selector_test/test,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120952,cache,cache,120952,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:09) FAIL: //deepvariant:model_train_test (shard 3 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_3_of_10/test.log); (06:29:09) INFO: From Testing //deepvariant:model_train_test (shard 3 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 3 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:18931,cache,cache,18931,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant/labeler:positional_labeler_test (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log); (06:29:10) INFO: From Testing //deepvariant/labeler:positional_labeler_test:; ==================== Test output for //deepvariant/labeler:positional_labeler_test:; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/deepvariant/labeler/positional_labeler_test.py"", line 40, in <module>; from third_party.nucleus.io import vcf; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/labeler/positional_labeler_test.runfiles/com_google_deepvariant/third_party/nucleus/io/vcf.py"", line 69, in <module>; from third_party.nucleus.io import genomics_reader; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvari",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:25452,cache,cache,25452,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:10) FAIL: //deepvariant:model_train_test (shard 1 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_1_of_10/test.log); (06:29:10) INFO: From Testing //deepvariant:model_train_test (shard 1 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 1 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:28269,cache,cache,28269,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 6 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_6_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 6 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 6 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:35548,cache,cache,35548,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:11) FAIL: //deepvariant:model_train_test (shard 7 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_7_of_10/test.log); (06:29:11) INFO: From Testing //deepvariant:model_train_test (shard 7 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 7 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:37703,cache,cache,37703,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:13) FAIL: //deepvariant:model_eval_test (shard 10 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_eval_test/shard_10_of_10/test.log); (06:29:13) INFO: From Testing //deepvariant:model_eval_test (shard 10 of 10):; ==================== Test output for //deepvariant:model_eval_test (shard 10 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_eval_test.runfiles/com_google_deepvariant/deepvariant/model_eval_test.py"", line 43, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise ImportEr",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:51136,cache,cache,51136,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:15) FAIL: //deepvariant:model_train_test (shard 4 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_4_of_10/test.log); (06:29:15) INFO: From Testing //deepvariant:model_train_test (shard 4 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 4 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:64313,cache,cache,64313,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 5 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_5_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 5 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 5 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:66468,cache,cache,66468,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"58, in <module>; from tensorflow.python.pywrap_tensorflow_internal import *; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 28, in <module>; _pywrap_tensorflow_internal = swig_import_helper(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py"", line 24, in swig_import_helper; _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description); ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory. Failed to load the native TensorFlow runtime. See https://www.tensorflow.org/install/errors. for some common reasons and solutions. Include the entire stack trace; above this error message when asking for help.; ================================================================================; (06:29:16) FAIL: //deepvariant:model_train_test (shard 8 of 10) (see /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/model_train_test/shard_8_of_10/test.log); (06:29:16) INFO: From Testing //deepvariant:model_train_test (shard 8 of 10):; ==================== Test output for //deepvariant:model_train_test (shard 8 of 10):; Traceback (most recent call last):; File ""/root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/bin/deepvariant/model_train_test.runfiles/com_google_deepvariant/deepvariant/model_train_test.py"", line 44, in <module>; import tensorflow as tf; File ""/root/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 49, in <module>; from tensorflow.python import pywrap_tensorflow; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py"", line 74, in <module>; raise Import",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:68623,cache,cache,68623,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/customized_classes_labeler_test/test.log; //deepvariant/labeler:haplotype_labeler_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/haplotype_labeler_test/test.log; //deepvariant/labeler:labeled_examples_to_vcf_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/labeled_examples_to_vcf_test/test.log; //deepvariant/labeler:positional_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/positional_labeler_test/test.log; //deepvariant/labeler:variant_labeler_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/labeler/variant_labeler_test/test.log; //deepvariant/python:allelecounter_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/allelecounter_wrap_test/test.log; //deepvariant/python:variant_calling_wrap_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/python/variant_calling_wrap_test/test.log; //deepvariant/realigner:aligner_test FAILED in 0.4s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/aligner_test/test.log; //deepvariant/realigner:realigner_test FAILED in 0.2s; /root/.cache/bazel/_bazel_root/ce699a1ca024b3cb0e615720516790f5/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/realigner/realigner_test/test.,MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145#issuecomment-463596181:120718,cache,cache,120718,,https://github.com/google/deepvariant/issues/145#issuecomment-463596181,1,['cache'],['cache']
Performance,"75c978033f9adc7c2a/external/org_tensorflow/third_party/py/python_configure.bzl"", line 138, column 25, in _check_python_lib; 		auto_config_fail(""Invalid python library path: %s"" % python_lib); 	File ""/home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/third_party/remote_config/common.bzl"", line 12, column 9, in auto_config_fail; 		fail(""%sConfiguration Error:%s %s\n"" % (red, no_color, msg)); Error in fail: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Repository go_sdk instantiated at:; /home/user/Documents/deepvariant-r1.5/WORKSPACE:116:14: in <toplevel>; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/org_tensorflow/tensorflow/workspace0.bzl:134:20: in workspace; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/com_github_grpc_grpc/bazel/grpc_extra_deps.bzl:36:27: in grpc_extra_deps; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:431:28: in go_register_toolchains; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:130:21: in go_download_sdk; Repository rule _go_download_sdk defined at:; /home/user/.cache/bazel/_bazel_user/7cc1383e03390275c978033f9adc7c2a/external/io_bazel_rules_go/go/private/sdk.bzl:117:35: in <toplevel>; (11:50:12) ERROR: Analysis of target '//:binaries' failed; build aborted: Configuration Error: Invalid python library path: ""; (11:50:12) INFO: Elapsed time: 0.552s; (11:50:12) INFO: 0 processes.; (11:50:12) FAILED: Build did NOT complete successfully (0 packages loaded, 0 t\; argets configured); ```; Do you have any suggestions? Thank you for your time!; Looks like we are getting there for this one. Just fyi, fortunately, I was able to run deepvariant on another unix system using singularity pull.; After updating the numpy and implementing the GPU command -nv, it runs pretty fast and error-free.",MatchSource.ISSUE_COMMENT,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838:3312,cache,cache,3312,,https://github.com/google/deepvariant/issues/657#issuecomment-1584813838,3,"['cache', 'load']","['cache', 'loaded']"
