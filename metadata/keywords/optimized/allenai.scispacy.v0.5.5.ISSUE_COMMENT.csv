quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb08",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2723,error,error,2723,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['error'],['error']
Availability,"6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden; building 'nmslib' extension; creating build; creating build/temp.linux-x86_64-3.6; creating build/temp.linux-x86_64-3.6/nmslib; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcf",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:1951,error,error,1951,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['error'],['error']
Availability,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed.; I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:; JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST; the Spacy-2 correctly predicts all 3 entities above; whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents.; Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395:170,down,downstream,170,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395,1,['down'],['downstream']
Availability,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```; ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:; ERROR: running bdist_wheel; running build; running build_ext; creating tmp; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden; building 'nmslib' extension; creating build; creating build/temp.linux-x86_64-3.6; creating build/temp.linux-x86_64-3.6/nmslib; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direct",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:78,error,error,78,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['error'],['error']
Availability,"Hi @ChantalvanSon ,. Great that you got it working, sorry it was a bit tricky. You raise some good points - there is one way that you can not require your function which loads all the pieces which is this:. ```python; from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ); class UMLS2020KnowledgeBase(KnowledgeBase):; def __init__(; self,; file_path: str = ""path/to/2020AA.json"",; ):; super().__init__(file_path). # Admittedly this is a bit of a hack, because we are mutating a global object.; # However, it's just a kind of registry, so maybe it's ok.; DEFAULT_PATHS[""umls2020""] = CustomLinkerPaths_2020AA; DEFAULT_KNOWLEDGE_BASES[""umls2020""] = UMLS2020KnowledgeBase. linker = CandidateGenerator(name=""umls2020""). ```. Overall, we have it like this so that we can present the simplest possible interface to people who are using scispacy (i.e being able to just pass names to get particular linkers rather than having to know the internals of how the linker is implemented). However I definitely see your point that we should try to make this a bit nicer. In another project I used to work on, we had the concept of using a decorator to register this type of info with the base class, so it can construct itself. That might be a bit of overkill here, but maybe we could provide a function which does this global mutation for you and throws intelligent errors if you e.g try to overwrite something in there? . I think you're right that we need to fix this if we want people to frequently be able to create their own very specific/custom linkers though so thanks for raising it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169:1695,error,errors,1695,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169,1,['error'],['errors']
Availability,"Hi @DeNeutoy, thanks for the information! I'm happy to share that I managed to create my custom Entity Linker based on the 2020AA release. It took a bit longer (~8 hours) to build the ANN index, but this could very well be because of the size of my UMLS subset (all level 0 sources + SNOMED). . Overall, it was not tóó difficult to do, but I think some small changes in the code would make it even easier. I don't have an answer to this question myself yet -- if I have time and I do think of a good solution, I will try to see if I can help out by creating a PR. But I think it comes down to the following:. `CandidateGenerator()` currently accepts a pre-trained linker (`umls` or `mesh`), for which the default `LinkerPaths` have been defined globally in the `candidate_generation.py`. While it is possible to provide your own `ann_index`, `tfidf_vectorizer`, `ann_concept_aliases_list` and `kb`, these will first have to be loaded using `load_approximate_nearest_neighbours_index`, and this one only accepts a `LinkerPaths` object. So I ended up writing something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model compon",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:585,down,down,585,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['down'],['down']
Availability,"I am running the following commands in macOS v12.2.1 and python 3.9.10; ```; python; Python 3.9.10 (main, Jan 15 2022, 11:48:04) ; [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```; python -m pip install --upgrade pip; pip install spacy; pip install spacy-transformers; pip install scispacy; ```. When executing ; `pip install scispacy ` I get the following error:; ```; Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy; Attempting uninstall: click; Found existing installation: click 8.0.4; Uninstalling click-8.0.4:; Successfully uninstalled click-8.0.4; Attempting uninstall: typer; Found existing installation: typer 0.4.0; Uninstalling typer-0.4.0:; Successfully uninstalled typer-0.4.0; Attempting uninstall: spacy; Found existing installation: spacy 3.2.2; Uninstalling spacy-3.2.2:; Successfully uninstalled spacy-3.2.2; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible.; Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018:363,error,error,363,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018,1,['error'],['error']
Availability,"I'd be willing to do this and submit a PR for it. Not sure if it as simple as running `scripts/create_linker.py` on the MRCONSO.rrf file or if I'd need to download the entire UMLS and run `scripts/export_umls_json.py`. Also not sure if I could include the data for those files in the PR due to size or if I'd need to retrain and publish the models themselves which I am sure I don't have permissions for... I think going forward making this process as simple as possible should be a requirement so no matter your load users can easily update the primary (UMLS) knowledge base to keep it up to date. The first paragraph here raises a general question I had, is the UMLS data used only for the NER or is it a larger part of the model? I.e. if I created my own EntityLinker using 2022AB UMLS, would that solve this ""outdated"" issue?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227:155,down,download,155,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227,1,['down'],['download']
Availability,"I'm coming. I run the follow command.; 1.`conda create --name vega_scispacy_2 python=3.9 -y`; 2.`conda activate vega_scispacy_2`; 3.`pip list`; 4.`pip install scispacy`; 5.`pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz`; 6.`vim demo_scispacy.py` and copy the demo code; 7.`cat demo_scispacy.py`; 8.`pip list|grep scispacy`; 9.`pip list|grep en_core_sci`; 10.`python -V`; 11.`python demo_scispacy.py`; 12.I Got Success result, Hey. 13.**But I don't know why the previous error, unbelieveable.**; . The all log are as following.; ```log; (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda create --name vega_scispacy_2 python=3.9 -y; Collecting package metadata (current_repodata.json): done; Solving environment: done. ==> WARNING: A newer version of conda exists. <==; current version: 4.9.2; latest version: 22.11.1. Please update conda by running. $ conda update -n base -c defaults conda. ## Package Plan ##. environment location: /home/zhangx/anaconda3/envs/vega_scispacy_2. added / updated specs:; - python=3.9. The following NEW packages will be INSTALLED:. _libgcc_mutex pkgs/main/linux-64::_libgcc_mutex-0.1-main; _openmp_mutex pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu; ca-certificates pkgs/main/linux-64::ca-certificates-2022.10.11-h06a4308_0; certifi pkgs/main/linux-64::certifi-2022.9.24-py39h06a4308_0; ld_impl_linux-64 pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1; libffi pkgs/main/linux-64::libffi-3.4.2-h6a678d5_6; libgcc-ng pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1; libgomp pkgs/main/linux-64::libgomp-11.2.0-h1234567_1; libstdcxx-ng pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1; ncurses pkgs/main/linux-64::ncurses-6.3-h5eee18b_3; openssl pkgs/main/linux-64::openssl-1.1.1s-h7f8727e_0; pip pkgs/main/linux-64::pip-22.3.1-py39h06a4308_0; python pkgs/main/linux-64::python-3.9.15-h7a1cb2a_2; readline pkgs/main/linux-64::readline-8.2-h5eee18b_0; setuptools pkg",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:531,error,error,531,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['error'],['error']
Availability,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python; !mv /root/.scispacy/ /content/gdrive/MyDrive/test/; !ls /content/gdrive/MyDrive/test/.scispacy/; >>> datasets; ```. To update the environment variable, as described:. ```python; import os; os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'; ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python; nlp.add_pipe(; ""scispacy_linker"",; config={; ""resolve_abbreviations"": True,; ""linker_name"": ""umls"",; ""cache_folder"": ""/content/gdrive/MyDrive/test/""}); ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940:561,down,download,561,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940,1,['down'],['download']
Availability,"No problem!. In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:216,down,down,216,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,1,['down'],['down']
Availability,"Thanks for the suggestion @chrishmorris! While your idea is reasonable, incorporating that human intuition into the dependency parsing model is quite difficult. See https://spacy.io/api/dependencyparser for more details on the dependency parsing model. A simpler way to incorporate this idea would be to add lots of examples of the form you describe to the training corpus. I will likely not be doing this for scispacy, but if you were to create your own corpus, I'd be happy to help you figure out how to use it in our training scripts to train your own model! And feel free to open another issue if you end up going down that route and would like some help.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/451#issuecomment-1289932724:618,down,down,618,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/451#issuecomment-1289932724,1,['down'],['down']
Availability,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/352#issuecomment-843634145:89,robust,robust,89,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352#issuecomment-843634145,1,['robust'],['robust']
Availability,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); Traceback (most recent call last):; File ""linker.py"", line 12, in <module>; linker = UmlsEntityLinker(resolve_abbreviations=True); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__; self.candidate_generator = candidate_generator or CandidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; ob",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:224,error,error,224,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['error'],['error']
Availability,"ain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Li",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:2278,down,downstream,2278,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['down'],['downstream']
Availability,"ain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.;",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:6784,down,downstream,6784,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['down'],['downstream']
Availability,"ain/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_160",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2716,error,error,2716,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['error'],['error']
Availability,"for scispacy `pipeline` gives . ```; [('attribute_ruler',; <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),; ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]; ```. Where as regular spacy gives. ```; [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]; ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right?. scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:885,mask,masked,885,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['mask'],['masked']
Availability,"he pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the necessary files directly when initiating a `CandidateGenerator`, so t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:2109,avail,available,2109,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['avail'],['available']
Availability,"heel-0.37.1-pyhd3eb1b0_0; xz pkgs/main/linux-64::xz-5.2.8-h5eee18b_0; zlib pkgs/main/linux-64::zlib-1.2.13-h5eee18b_0. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate vega_scispacy_2; #; # To deactivate an active environment, use; #; # $ conda deactivate. (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list; Package Version; ---------- ---------; certifi 2022.9.24; pip 22.3.1; setuptools 65.5.0; wheel 0.37.1; WARNING: The repository located at pypi.douban.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host pypi.douban.com'.; WARNING: There was an error checking the latest version of pip.; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install scispacy; Looking in indexes: http://pypi.douban.com/simple; Collecting scispacy; Downloading http://pypi.doubanio.com/packages/6d/f2/a55ed36940e481e1823c71047e5b3b90a2cb516f59f25b63a57e60e3f8c3/scispacy-0.5.1-py3-none-any.whl (44 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.9/44.9 kB 1.3 MB/s eta 0:00:00; Collecting numpy; Downloading http://pypi.doubanio.com/packages/4c/b9/038abd6fbd67b05b03cb1af590cfc02b7f1e5a37af7ac6a868f5093c29f5/numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 7.7 MB/s eta 0:00:00; Collecting spacy<3.5.0,>=3.4.0; Downloading http://pypi.doubanio.com/packages/f6/8e/1ee7c934aeb18bb6a77b8f7b3d9a301acd8aaedfc5f07c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:3166,avail,available,3166,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['avail'],['available']
Availability,"ile ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:6096,error,error,6096,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['error'],['error']
Availability,"ile ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from sci",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:9613,error,error,9613,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['error'],['error']
Availability,"inux-x86_64-3.6/nmslib/similarity_search; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:2086,error,error,2086,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['error'],['error']
Availability,"iplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'; ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:11221,error,error,11221,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['error'],['error']
Availability,"ision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'; ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:10166,error,error,10166,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['error'],['error']
Availability,"leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,; 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec'])",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:8976,down,downstream,8976,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['down'],['downstream']
Availability,"ll_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:6246,error,error,6246,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['error'],['error']
Availability,"n or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlp",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:9608,error,error,9608,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['error'],['error']
Availability,"ng transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate vega_scispacy_2; #; # To deactivate an active environment, use; #; # $ conda deactivate. (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list; Package Version; ---------- ---------; certifi 2022.9.24; pip 22.3.1; setuptools 65.5.0; wheel 0.37.1; WARNING: The repository located at pypi.douban.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host pypi.douban.com'.; WARNING: There was an error checking the latest version of pip.; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install scispacy; Looking in indexes: http://pypi.douban.com/simple; Collecting scispacy; Downloading http://pypi.doubanio.com/packages/6d/f2/a55ed36940e481e1823c71047e5b3b90a2cb516f59f25b63a57e60e3f8c3/scispacy-0.5.1-py3-none-any.whl (44 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.9/44.9 kB 1.3 MB/s eta 0:00:00; Collecting numpy; Downloading http://pypi.doubanio.com/packages/4c/b9/038abd6fbd67b05b03cb1af590cfc02b7f1e5a37af7ac6a868f5093c29f5/numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 7.7 MB/s eta 0:00:00; Collecting spacy<3.5.0,>=3.4.0; Downloading http://pypi.doubanio.com/packages/f6/8e/1ee7c934aeb18bb6a77b8f7b3d9a301acd8aaedfc5f07c300871f3c6f1ff/spacy-3.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:3343,error,error,3343,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['error'],['error']
Availability,"t are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and rec",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:5391,mask,masked,5391,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['mask'],['masked']
Availability,"to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:4462,down,downstream,4462,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['down'],['downstream']
Availability,"xtensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error using a conda env on mac m1 (Issue #455). What was the error you got?. —; Reply to this email directly, view it on GitHub<https://github.com/allenai/scispacy/issues/455#issuecomment-1309570650>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIIB7TJZCTFHCICGZWG47KTWHQ4WZANCNFSM6AAAAAARYG27ME>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:15529,error,error,15529,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['error'],['error']
Deployability," for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key] = env.best_match(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1075, in best_match. return self.obtain(req, installer). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:3692,install,install-,3692,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install-']
Deployability, murmurhash (1.0.2); nbconvert (5.6.0); nbformat (4.4.0); netifaces (0.10.4); nmslib (1.8.1); notebook (6.0.1); numpy (1.17.2); oauth (1.0.1); olefile (0.45.1); pandocfilters (1.4.2); parso (0.5.1); pbr (3.1.1); pexpect (4.7.0); pickleshare (0.7.5); Pillow (6.1.0); pip (9.0.1); plac (0.9.6); preshed (3.0.2); prometheus-client (0.7.1); prompt-toolkit (2.0.9); protobuf (3.9.2); ptyprocess (0.6.0); pyasn1 (0.4.7); pybind11 (2.4.2); pycairo (1.16.2); pycrypto (2.6.1); pycups (1.9.73); Pygments (2.4.2); pygobject (3.26.1); pymacaroons (0.13.0); PyNaCl (1.1.2); pyRFC3339 (1.0); pyrsistent (0.15.4); python-apt (1.6.4); python-dateutil (2.8.0); python-debian (0.1.32); pytz (2018.3); pyxdg (0.25); PyYAML (5.1.2); pyzmq (18.1.0); qtconsole (4.5.5); reportlab (3.4.0); requests (2.22.0); requests-unixsocket (0.1.5); rsa (3.4.2); s3transfer (0.2.1); scikit-learn (0.21.3); scipy (1.3.1); scispacy (0.2.3); screen-resolution-extra (0.0.0); SecretStorage (2.3.1); Send2Trash (1.5.0); setuptools (41.2.0); simplegeneric (0.8.1); simplejson (3.13.2); six (1.12.0); spacy (2.1.8); srsly (0.1.0); system-service (0.3); systemd-python (234); tensorboard (1.14.0); tensorflow (1.14.0); tensorflow-estimator (1.14.0); tensorflow-gpu (1.14.0); termcolor (1.1.0); terminado (0.8.2); testpath (0.4.2); thinc (7.1.1); torch (1.2.0); torchvision (0.4.0); tornado (6.0.3); tqdm (4.36.1); traitlets (4.3.2); ubuntu-drivers-common (0.0.0); ufw (0.36); unattended-upgrades (0.1); urllib3 (1.25.6); usb-creator (0.3.3); wadllib (1.3.2); wasabi (0.2.2); wcwidth (0.1.7); webencodings (0.5.1); Werkzeug (0.16.0); wheel (0.33.6); widgetsnbextension (3.5.1); wrapt (1.11.2); xkit (0.0.0); zope.interface (4.3.2)`. and my full code snippet is :; `In [1]: import spacy . In [2]: import scispacy . In [3]: from scispacy.umls_linking import UmlsEntityLinker . In [4]: nlp = spacy.load('en_core_sci_sm') . In [5]: linker = UmlsEntityLinker(resolve_abbreviations=True) ; fish: “ipython” terminated by signal SIGKILL (Forced quit)`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949:2727,upgrade,upgrades,2727,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949,1,['upgrade'],['upgrades']
Deployability," or `mesh`), for which the default `LinkerPaths` have been defined globally in the `candidate_generation.py`. While it is possible to provide your own `ann_index`, `tfidf_vectorizer`, `ann_concept_aliases_list` and `kb`, these will first have to be loaded using `load_approximate_nearest_neighbours_index`, and this one only accepts a `LinkerPaths` object. So I ended up writing something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:1650,release,release,1650,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['release'],['release']
Deployability," os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement alr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10139,install,installed,10139,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installed']
Deployability," these will first have to be loaded using `load_approximate_nearest_neighbours_index`, and this one only accepts a `LinkerPaths` object. So I ended up writing something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:1813,release,release,1813,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['release'],['release']
Deployability," vega_scispacy_2; #; # To deactivate an active environment, use; #; # $ conda deactivate. (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list; Package Version; ---------- ---------; certifi 2022.9.24; pip 22.3.1; setuptools 65.5.0; wheel 0.37.1; WARNING: The repository located at pypi.douban.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host pypi.douban.com'.; WARNING: There was an error checking the latest version of pip.; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install scispacy; Looking in indexes: http://pypi.douban.com/simple; Collecting scispacy; Downloading http://pypi.doubanio.com/packages/6d/f2/a55ed36940e481e1823c71047e5b3b90a2cb516f59f25b63a57e60e3f8c3/scispacy-0.5.1-py3-none-any.whl (44 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.9/44.9 kB 1.3 MB/s eta 0:00:00; Collecting numpy; Downloading http://pypi.doubanio.com/packages/4c/b9/038abd6fbd67b05b03cb1af590cfc02b7f1e5a37af7ac6a868f5093c29f5/numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 7.7 MB/s eta 0:00:00; Collecting spacy<3.5.0,>=3.4.0; Downloading http://pypi.doubanio.com/packages/f6/8e/1ee7c934aeb18bb6a77b8f7b3d9a301acd8aaedfc5f07c300871f3c6f1ff/spacy-3.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 1.9 MB/s eta 0:00:00; Collecting requests<3.0.0,>=2.0.0; Downloading http://pypi.doubanio.com/packages/ca/91/6d9b8ccacd041",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:3487,install,install,3487,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['install'],['install']
Deployability,"(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Req",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:5953,install,install-,5953,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install-']
Deployability,"(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 M",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:9470,install,install-,9470,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install-']
Deployability,"(setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu; Found existing installation: docutils 0.15.2; Uninstalling docutils-0.15.2:; Successfully uninstalled docutils-0.15.2; Running setup.py install for nmslib: started; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:4610,install,installation,4610,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,6,['install'],"['install', 'installation']"
Deployability,", psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0); Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0); Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14682,release,releases,14682,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['release'],['releases']
Deployability,".1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:3399,install,installer,3399,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installer']
Deployability,".1.0,>=3.0.0->scispacy) (3.1.2); Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (60.3.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->scispacy) (3.0.9); Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->scispacy) (5.2.1); Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->scispacy) (4.2.0); Requirement already satisfied: click<7.2.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->scispacy) (7.1.2); Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->scispacy) (2.1.1); Installing collected packages: spacy; Attempting uninstall: spacy; Found existing installation: spacy 3.2.4; Uninstalling spacy-3.2.4:; Successfully uninstalled spacy-3.2.4; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; spacy-transformers 1.1.6 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.8 which is incompatible.; medspacy 0.2.0.0 requires spacy<3.2.0,>=3.1.3, but you have spacy 3.0.8 which is incompatible.; en-core-sci-scibert 0.5.0 requires spacy<3.3.0,>=3.2.3, but you have spacy 3.0.8 which is incompatible.; docanalysis 0.1.1 requires spacy==3.0.7, but you have spacy 3.0.8 which is incompatible.; Successfully installed spacy-3.0.8; ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:6120,install,installation,6120,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,3,['install'],"['installation', 'installed']"
Deployability,"0gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10265,install,install,10265,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Deployability,"1) The version on the demo is probably not the latest release version. I should check and update that.; 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context; ```; In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:; ...: text = f""The drug {drug_name} is used to treat the virus""; ...: doc = nlp(text); ...: print(doc.ents); ...: ; (mesna,); (); (mebane,); (); (); (aspirin,); (); (); (scopolamine,); (entamine,); (valimine,); (henirin,); (); (); ```. Looks like it is also sensitive to capitalization; ```; In [56]: doc = nlp(""Remdesivir is a chemical""); In [57]: doc.ents; Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents; Out[59]: (); ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines).; 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/336#issuecomment-800691659:54,release,release,54,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336#issuecomment-800691659,2,"['release', 'update']","['release', 'update']"
Deployability,"11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/ana",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:6858,install,installer,6858,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installer']
Deployability,"3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error using a conda env on mac m1 (Issue #455). What was the error you got?. —; Reply to this email directly, view it on GitHub<https://github.com/allenai/scispacy/issues/455#issuecomment-1309570650>, or unsubscribe<h",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14741,install,installed,14741,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installed']
Deployability,"8f6b_0 None. pip pkgs/main/osx-arm64::pip-22.2.2-py39hca03da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2241,install,install,2241,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Deployability,"File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key] = env.best_match(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1075, in best_match. return self.obtain(req, installer). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1087, in obtain. return installer(requirement). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 945, in fetch_build_egg. return fetch_build_egg(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. o",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:4626,install,installer,4626,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['install'],['installer']
Deployability,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```; ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:; ERROR: running bdist_wheel; running build; running build_ext; creating tmp; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden; building 'nmslib' extension; creating build; creating build/temp.linux-x86_64-3.6; creating build/temp.linux-x86_64-3.6/nmslib; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direct",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:30,install,install,30,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,4,['install'],"['install', 'install-', 'installation']"
Deployability,"Hello,. I just simply ran the following commands:. ```; pip install spacy; spacy evaluate en_core_sci_sm /path/to/data; spacy evaluate en_core_sci_md /path/to/data; ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/140#issuecomment-518931086:60,install,install,60,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140#issuecomment-518931086,1,['install'],['install']
Deployability,"Hi @DeNeutoy, thanks for the information! I'm happy to share that I managed to create my custom Entity Linker based on the 2020AA release. It took a bit longer (~8 hours) to build the ANN index, but this could very well be because of the size of my UMLS subset (all level 0 sources + SNOMED). . Overall, it was not tóó difficult to do, but I think some small changes in the code would make it even easier. I don't have an answer to this question myself yet -- if I have time and I do think of a good solution, I will try to see if I can help out by creating a PR. But I think it comes down to the following:. `CandidateGenerator()` currently accepts a pre-trained linker (`umls` or `mesh`), for which the default `LinkerPaths` have been defined globally in the `candidate_generation.py`. While it is possible to provide your own `ann_index`, `tfidf_vectorizer`, `ann_concept_aliases_list` and `kb`, these will first have to be loaded using `load_approximate_nearest_neighbours_index`, and this one only accepts a `LinkerPaths` object. So I ended up writing something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model compon",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:130,release,release,130,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['release'],['release']
Deployability,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/284#issuecomment-718151109:95,install,installing,95,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284#issuecomment-718151109,2,['install'],['installing']
Deployability,"Hi Dan,; Thanks for the input. I am working on a medical use case and i need some inputs :; 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ?; 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/143#issuecomment-620607287:228,pipeline,pipeline,228,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143#issuecomment-620607287,1,['pipeline'],['pipeline']
Deployability,"Hi Daniel,. I tried to install nmslib again in a new py 3.9 environment and it looks like it actually did install but there were issues. The entire log from the install is below. I also successfully installed scispacy from pip which is great! I included the install log for scispacy below the log for nmslib. Maybe I did something wrong before when I created the env for scispacy?. I will be testing it soon and will let you know if anything else comes up. Thank you for following up with me on this. Cheers, Brian. ### create env log ###. (base) ***@***.*** ~ % conda create -n scispacy python=3.9. Collecting package metadata (current_repodata.json): done. Solving environment: done. ## Package Plan ##. environment location: /Users/briang/opt/anaconda3/envs/scispacy. added / updated specs:. - python=3.9. The following NEW packages will be INSTALLED:. ca-certificates pkgs/main/osx-arm64::ca-certificates-2022.10.11-hca03da5_0 None. certifi pkgs/main/osx-arm64::certifi-2022.9.24-py39hca03da5_0 None. libcxx pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 None. libffi pkgs/main/osx-arm64::libffi-3.4.2-hc377ac9_4 None. ncurses pkgs/main/osx-arm64::ncurses-6.3-h1a28f6b_3 None. openssl pkgs/main/osx-arm64::openssl-1.1.1s-h1a28f6b_0 None. pip pkgs/main/osx-arm64::pip-22.2.2-py39hca03da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:23,install,install,23,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,6,"['install', 'update']","['install', 'installed', 'updated']"
Deployability,"Hi, @vgainullin,. You can simply add the full url to the model to your requirements.txt file. e.g. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz; `",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/224#issuecomment-624936824:151,release,releases,151,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/224#issuecomment-624936824,1,['release'],['releases']
Deployability,"I am running the following commands in macOS v12.2.1 and python 3.9.10; ```; python; Python 3.9.10 (main, Jan 15 2022, 11:48:04) ; [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```; python -m pip install --upgrade pip; pip install spacy; pip install spacy-transformers; pip install scispacy; ```. When executing ; `pip install scispacy ` I get the following error:; ```; Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy; Attempting uninstall: click; Found existing installation: click 8.0.4; Uninstalling click-8.0.4:; Successfully uninstalled click-8.0.4; Attempting uninstall: typer; Found existing installation: typer 0.4.0; Uninstalling typer-0.4.0:; Successfully uninstalled typer-0.4.0; Attempting uninstall: spacy; Found existing installation: spacy 3.2.2; Uninstalling spacy-3.2.2:; Successfully uninstalled spacy-3.2.2; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible.; Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018:201,install,install,201,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018,11,"['install', 'upgrade']","['install', 'installation', 'installed', 'upgrade']"
Deployability,"I'd be willing to do this and submit a PR for it. Not sure if it as simple as running `scripts/create_linker.py` on the MRCONSO.rrf file or if I'd need to download the entire UMLS and run `scripts/export_umls_json.py`. Also not sure if I could include the data for those files in the PR due to size or if I'd need to retrain and publish the models themselves which I am sure I don't have permissions for... I think going forward making this process as simple as possible should be a requirement so no matter your load users can easily update the primary (UMLS) knowledge base to keep it up to date. The first paragraph here raises a general question I had, is the UMLS data used only for the NER or is it a larger part of the model? I.e. if I created my own EntityLinker using 2022AB UMLS, would that solve this ""outdated"" issue?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227:535,update,update,535,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227,1,['update'],['update']
Deployability,"I'm coming. I run the follow command.; 1.`conda create --name vega_scispacy_2 python=3.9 -y`; 2.`conda activate vega_scispacy_2`; 3.`pip list`; 4.`pip install scispacy`; 5.`pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz`; 6.`vim demo_scispacy.py` and copy the demo code; 7.`cat demo_scispacy.py`; 8.`pip list|grep scispacy`; 9.`pip list|grep en_core_sci`; 10.`python -V`; 11.`python demo_scispacy.py`; 12.I Got Success result, Hey. 13.**But I don't know why the previous error, unbelieveable.**; . The all log are as following.; ```log; (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda create --name vega_scispacy_2 python=3.9 -y; Collecting package metadata (current_repodata.json): done; Solving environment: done. ==> WARNING: A newer version of conda exists. <==; current version: 4.9.2; latest version: 22.11.1. Please update conda by running. $ conda update -n base -c defaults conda. ## Package Plan ##. environment location: /home/zhangx/anaconda3/envs/vega_scispacy_2. added / updated specs:; - python=3.9. The following NEW packages will be INSTALLED:. _libgcc_mutex pkgs/main/linux-64::_libgcc_mutex-0.1-main; _openmp_mutex pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu; ca-certificates pkgs/main/linux-64::ca-certificates-2022.10.11-h06a4308_0; certifi pkgs/main/linux-64::certifi-2022.9.24-py39h06a4308_0; ld_impl_linux-64 pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1; libffi pkgs/main/linux-64::libffi-3.4.2-h6a678d5_6; libgcc-ng pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1; libgomp pkgs/main/linux-64::libgomp-11.2.0-h1234567_1; libstdcxx-ng pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1; ncurses pkgs/main/linux-64::ncurses-6.3-h5eee18b_3; openssl pkgs/main/linux-64::openssl-1.1.1s-h7f8727e_0; pip pkgs/main/linux-64::pip-22.3.1-py39h06a4308_0; python pkgs/main/linux-64::python-3.9.15-h7a1cb2a_2; readline pkgs/main/linux-64::readline-8.2-h5eee18b_0; setuptools pkg",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:151,install,install,151,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,5,"['install', 'release', 'update']","['install', 'releases', 'update']"
Deployability,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python; !mv /root/.scispacy/ /content/gdrive/MyDrive/test/; !ls /content/gdrive/MyDrive/test/.scispacy/; >>> datasets; ```. To update the environment variable, as described:. ```python; import os; os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'; ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python; nlp.add_pipe(; ""scispacy_linker"",; config={; ""resolve_abbreviations"": True,; ""linker_name"": ""umls"",; ""cache_folder"": ""/content/gdrive/MyDrive/test/""}); ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940:349,update,update,349,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940,1,['update'],['update']
Deployability,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/338#issuecomment-801431956:242,pipeline,pipeline,242,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338#issuecomment-801431956,2,['pipeline'],['pipeline']
Deployability,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so; ```; In [5]: nlp.get_pipe('scispacy_linker'); Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator; Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator; Out[8]: <function __main__.<lambda>(x)>; ```. so you would do. ```; nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>; nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb; ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/337#issuecomment-801431110:824,install,install,824,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337#issuecomment-801431110,1,['install'],['install']
Deployability,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); Traceback (most recent call last):; File ""linker.py"", line 12, in <module>; linker = UmlsEntityLinker(resolve_abbreviations=True); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__; self.candidate_generator = candidate_generator or CandidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; ob",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:265,install,installed,265,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['install'],['installed']
Deployability,"When installing scispacy, I get similar resolver issues. Am I doing something wrong?. ```; emanuelfarruda@Mannys-MacBook-Pro-2021 ~ % pip3 install scispacy; Requirement already satisfied: scispacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.4.0); Requirement already satisfied: scikit-learn>=0.20.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.1.1); Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.1.0); Requirement already satisfied: nmslib>=1.7.3.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.1.1); Requirement already satisfied: pysbd in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (0.3.4); Collecting spacy<3.1.0,>=3.0.0; Using cached spacy-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB); Requirement already satisfied: conllu in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (4.4.2); Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.15.1); Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.22.4); Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.1); Requirement already satisfied: pybind11<2.6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1); Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0); Requirement already satisfied: sc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:5,install,installing,5,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,2,['install'],"['install', 'installing']"
Deployability,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/410#issuecomment-1028469710:181,patch,patch,181,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410#issuecomment-1028469710,1,['patch'],['patch']
Deployability,"_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", l",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:6916,install,installer,6916,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installer']
Deployability,"adata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/ana",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:3341,install,installer,3341,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installer']
Deployability,"andidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21.1.3; plac 0.9.6; preshed 3.0.5; psutil 5.8.0; pyasn1 0.4.8; pybind11 2.6.1; pysbd 0.3.4; python-dateutil 2.8.2; PyYAML 5.4.1; requests 2.26.0; rsa 4.7.2; s3transfer 0.5.0; scikit-learn 0.22.2; scipy 1.7.0; scispacy 0.2.4; setuptools 39.0.1; six 1.16.0; spacy 2.2.1; srsly 1.0.5; thinc 7.1.1; threadpoolctl 2.2.0; tqdm 4.61.2; typing-extensions 3.10.0.0; urllib3 1.26.6; wasabi 0.8.2; zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replaci",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:2148,install,installed,2148,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['install'],['installed']
Deployability,"da3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:9837,install,install,9837,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Deployability,"esult -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden; building 'nmslib' extension; creating build; creating build/temp.linux-x86_64-3.6; creating build/temp.linux-x86_64-3.6/nmslib; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cb",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:1676,install,install-,1676,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['install'],['install-']
Deployability,"for scispacy `pipeline` gives . ```; [('attribute_ruler',; <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),; ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]; ```. Where as regular spacy gives. ```; [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]; ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right?. scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:14,pipeline,pipeline,14,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,4,['pipeline'],['pipeline']
Deployability,"geBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21.1.3; plac 0.9.6; preshed 3.0.5; psutil 5.8.0; pyasn1 0.4.8; pybind11 2.6.1; pysbd 0.3.4; python-dateutil 2.8.2; PyYAML 5.4.1; requests 2.26.0; rsa 4.7.2; s3transfer 0.5.0; scikit-learn 0.22.2; scipy 1.7.0; scispacy 0.2.4; setuptools 39.0.1; six 1.16.0; spacy 2.2.1; srsly 1.0.5; thinc 7.1.1; threadpoolctl 2.2.0; tqdm 4.61.2; typing-extensions 3.10.0.0; urllib3 1.26.6; wasabi 0.8.2; zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:2944,install,installed,2944,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,2,"['install', 'release']","['installed', 'release']"
Deployability,"iting something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the nece",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:2017,release,release,2017,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['release'],['release']
Deployability,"kB); Collecting blis<0.8.0,>=0.7.8; Downloading http://pypi.doubanio.com/packages/28/b6/e1cdfcf4ada40bef7c0511576231df20ac94a15baeb7ceaab2a180463268/blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 9.2 MB/s eta 0:00:00; Collecting click<9.0.0,>=7.1.1; Downloading http://pypi.doubanio.com/packages/c2/f1/df59e28c642d583f7dacffb1e0965d0e00b218e0186d7858ac5233dce840/click-8.1.3-py3-none-any.whl (96 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 7.3 MB/s eta 0:00:00; Collecting MarkupSafe>=2.0; Downloading http://pypi.doubanio.com/packages/df/06/c515c5bc43b90462e753bc768e6798193c6520c9c7eb2054c7466779a9db/MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, pysbd, pybind11, psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:13943,install,installed,13943,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['install'],['installed']
Deployability,"le>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key] = env.best_match(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1075, in best_match. return self.obtain(req, installer). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1087, in obtain. return installer(requirement). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 945, in fetch_build_egg. return fetch_build_egg(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:4771,install,installer,4771,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['install'],['installer']
Deployability,"lib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key] = env.best_match(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1075, in best_match. return self.obtain(req, installer). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:7209,install,install-,7209,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install-']
Deployability,"ne. ncurses pkgs/main/osx-arm64::ncurses-6.3-h1a28f6b_3 None. openssl pkgs/main/osx-arm64::openssl-1.1.1s-h1a28f6b_0 None. pip pkgs/main/osx-arm64::pip-22.2.2-py39hca03da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-sepa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2109,install,install,2109,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Deployability,"ntic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0); Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0); Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.28.1); Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/zhangx/anacon",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14840,release,releases,14840,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['release'],['releases']
Deployability,"onda activate vega_scispacy_2`; 3.`pip list`; 4.`pip install scispacy`; 5.`pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz`; 6.`vim demo_scispacy.py` and copy the demo code; 7.`cat demo_scispacy.py`; 8.`pip list|grep scispacy`; 9.`pip list|grep en_core_sci`; 10.`python -V`; 11.`python demo_scispacy.py`; 12.I Got Success result, Hey. 13.**But I don't know why the previous error, unbelieveable.**; . The all log are as following.; ```log; (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda create --name vega_scispacy_2 python=3.9 -y; Collecting package metadata (current_repodata.json): done; Solving environment: done. ==> WARNING: A newer version of conda exists. <==; current version: 4.9.2; latest version: 22.11.1. Please update conda by running. $ conda update -n base -c defaults conda. ## Package Plan ##. environment location: /home/zhangx/anaconda3/envs/vega_scispacy_2. added / updated specs:; - python=3.9. The following NEW packages will be INSTALLED:. _libgcc_mutex pkgs/main/linux-64::_libgcc_mutex-0.1-main; _openmp_mutex pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu; ca-certificates pkgs/main/linux-64::ca-certificates-2022.10.11-h06a4308_0; certifi pkgs/main/linux-64::certifi-2022.9.24-py39h06a4308_0; ld_impl_linux-64 pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1; libffi pkgs/main/linux-64::libffi-3.4.2-h6a678d5_6; libgcc-ng pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1; libgomp pkgs/main/linux-64::libgomp-11.2.0-h1234567_1; libstdcxx-ng pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1; ncurses pkgs/main/linux-64::ncurses-6.3-h5eee18b_3; openssl pkgs/main/linux-64::openssl-1.1.1s-h7f8727e_0; pip pkgs/main/linux-64::pip-22.3.1-py39h06a4308_0; python pkgs/main/linux-64::python-3.9.15-h7a1cb2a_2; readline pkgs/main/linux-64::readline-8.2-h5eee18b_0; setuptools pkgs/main/linux-64::setuptools-65.5.0-py39h06a4308_0; sqlite pkgs/main/linux-64::sqlite-3.40.0-h5082",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:1089,update,updated,1089,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['update'],['updated']
Deployability,"owledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the necessary files directly when initiating a `CandidateGenerator`, so that you could do something like the following:. ```; candidate_generator = CandidateGenerator(; ann_index=""path/to/ann_index"",; tfidf_vectorizer=""p",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:2284,release,release,2284,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['release'],['release']
Deployability,"py"", line 1087, in obtain. return installer(requirement). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 945, in fetch_build_egg. return fetch_build_egg(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'des",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:5760,install,install-,5760,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install-']
Deployability,"py"", line 1087, in obtain. return installer(requirement). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 945, in fetch_build_egg. return fetch_build_egg(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:9277,install,install-,9277,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install-']
Deployability,"rivate/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10208,install,install,10208,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Deployability,"s/vega_scispacy_2/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.1); Building wheels for collected packages: en-core-sci-sm; Building wheel for en-core-sci-sm (setup.py) ... done; Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870856 sha256=e99e476d22293a04ce498b2a9a3ed2514cdadebb4b9fa5794ebf40b51d05587c; Stored in directory: /home/zhangx/.cache/pip/wheels/f5/2e/39/9c9d425a1d34c06409420f7c65c5e10a56f7b149a3c37cdfa6; Successfully built en-core-sci-sm; Installing collected packages: en-core-sci-sm; Successfully installed en-core-sci-sm-0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ vim demo_scispacy.py; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ cat demo_scispacy.py; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""); (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep scispacy; scispacy 0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep en_core_sci; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep en_core_sci*; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ python -V; Python 3.9.15; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/00",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:21737,pipeline,pipeline,21737,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['pipeline'],['pipeline']
Deployability,"satisfied: confection<1.0.0,>=0.0.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.0.3); Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (8.1.3); Requirement already satisfied: MarkupSafe>=2.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.1); Building wheels for collected packages: en-core-sci-sm; Building wheel for en-core-sci-sm (setup.py) ... done; Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870856 sha256=e99e476d22293a04ce498b2a9a3ed2514cdadebb4b9fa5794ebf40b51d05587c; Stored in directory: /home/zhangx/.cache/pip/wheels/f5/2e/39/9c9d425a1d34c06409420f7c65c5e10a56f7b149a3c37cdfa6; Successfully built en-core-sci-sm; Installing collected packages: en-core-sci-sm; Successfully installed en-core-sci-sm-0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ vim demo_scispacy.py; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ cat demo_scispacy.py; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""); (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:21320,install,installed,21320,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['install'],['installed']
Deployability,"stalled MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0); Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0); Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.28.1); Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (4.64.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14949,release,releases,14949,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['release'],['releases']
Deployability,"t__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key] = env.best_match(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1075, in best_match. return self.obtain(req, installer). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 1087, in obtain. return installer(requirement). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 945, in fetch_build_egg. return fetch_build_egg(self, req). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py"", line 87, in fetch_build_egg. wheel.install_as_egg(dist_location). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 110, in install_as_egg. self._install_as_egg(destination_eggdir, zf). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:5044,install,installer,5044,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['install'],['installer']
Deployability,"t_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (7",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:9927,install,install,9927,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Deployability,"ting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:3470,install,installer,3470,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installer']
Deployability,"tl, spacy-loggers, spacy-legacy, smart-open, pysbd, pybind11, psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0); Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0); Requirement already satisfied: requests<3.0.0",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14623,install,install,14623,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['install'],['install']
Deployability,"uptools/wheel.py"", line 118, in _install_as_egg. self._convert_metadata(zf, destination_eggdir, dist_info, egg_info). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/wheel.py"", line 162, in _convert_metadata. os.rename(dist_info, egg_info). OSError: [Errno 66] Directory not empty: '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Co",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:9890,install,installed,9890,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installed']
Deployability,"v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed building wheel for nmslib. Running setup.py clean for nmslib. error: subprocess-exited-with-error. × python setup.py clean did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module>. File ""<pip-setuptools-caller>"", line 34, in <module>. File ""/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/setup.py"", line 170, in <module>. setup(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 86, in setup. _install_setup_requires(attrs). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/__init__.py"", line 80, in _install_setup_requires. dist.fetch_build_eggs(dist.setup_requires). File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py"", line 875, in fetch_build_eggs. resolved_dists = pkg_resources.working_set.resolve(. File ""/Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/pkg_resources/__init__.py"", line 789, in resolve. dist = best[req.key]",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:6987,install,installer,6987,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['installer']
Deployability,"wing:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:1341,integrat,integration,1341,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,1,['integrat'],['integration']
Deployability,"xtensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error using a conda env on mac m1 (Issue #455). What was the error you got?. —; Reply to this email directly, view it on GitHub<https://github.com/allenai/scispacy/issues/455#issuecomment-1309570650>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIIB7TJZCTFHCICGZWG47KTWHQ4WZANCNFSM6AAAAAARYG27ME>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:15521,install,install,15521,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['install'],['install']
Energy Efficiency,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. ; @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:; ```; ncalls tottime percall cumtime percall filename:lineno(function); 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode); ```; I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:; 1. `pyarrow` to store the alias list ; 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)?. Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/402#issuecomment-952661338:455,efficient,efficient,455,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402#issuecomment-952661338,1,['efficient'],['efficient']
Energy Efficiency,"gh-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used fo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:2133,power,powerful,2133,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['power'],['powerful']
Energy Efficiency,"gh-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:6639,power,powerful,6639,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['power'],['powerful']
Energy Efficiency,"itation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,; 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:7996,power,power,7996,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['power'],['power']
Energy Efficiency,"jective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1944,power,power,1944,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,2,['power'],['power']
Energy Efficiency,naconda-project 0.8.2 ; asn1crypto 0.24.0 ; astor 0.8.0 ; astroid 2.1.0 ; astropy 3.1 ; atomicwrites 1.2.1 ; attrs 18.2.0 ; awscli 1.16.261 ; Babel 2.6.0 ; backcall 0.1.0 ; backports.os 0.1.1 ; backports.shutil-get-terminal-size 1.0.0 ; beautifulsoup4 4.6.3 ; bitarray 0.8.3 ; bkcharts 0.2 ; blaze 0.11.3 ; bleach 3.0.2 ; blis 0.4.1 ; bokeh 1.0.2 ; boto 2.49.0 ; botocore 1.12.251 ; Bottleneck 1.2.1 ; certifi 2018.11.29; cffi 1.11.5 ; chardet 3.0.4 ; Click 7.0 ; cloudpickle 0.6.1 ; clyent 1.2.2 ; colorama 0.4.1 ; conllu 2.2 ; contextlib2 0.5.5 ; cryptography 2.4.2 ; cupy 6.4.0 ; cycler 0.10.0 ; cymem 2.0.2 ; Cython 0.29.2 ; cytoolz 0.9.0.1 ; dask 1.0.0 ; datashape 0.5.4 ; decorator 4.3.0 ; defusedxml 0.5.0 ; distributed 1.25.1 ; docutils 0.14 ; en-core-sci-lg 0.2.3 ; en-core-web-sm 2.2.0 ; entrypoints 0.2.3 ; et-xmlfile 1.0.1 ; fastcache 1.0.2 ; fastrlock 0.4 ; filelock 3.0.10 ; Flask 1.0.2 ; Flask-Cors 3.0.7 ; gast 0.2.2 ; gevent 1.3.7 ; glob2 0.6 ; gmpy2 2.0.8 ; google-pasta 0.1.7 ; greenlet 0.4.15 ; grpcio 1.23.0 ; h5py 2.8.0 ; heapdict 1.0.0 ; html5lib 1.0.1 ; idna 2.8 ; imageio 2.4.1 ; imagesize 1.1.0 ; importlib-metadata 0.6 ; ipykernel 5.1.0 ; ipython 7.2.0 ; ipython-genutils 0.2.0 ; ipywidgets 7.4.2 ; isort 4.3.4 ; itsdangerous 1.1.0 ; jdcal 1.4 ; jedi 0.13.2 ; jeepney 0.4 ; Jinja2 2.10 ; jmespath 0.9.4 ; joblib 0.14.0 ; jsonschema 2.6.0 ; jupyter 1.0.0 ; jupyter-client 5.2.4 ; jupyter-console 6.0.0 ; jupyter-core 4.4.0 ; jupyterlab 0.35.3 ; jupyterlab-server 0.2.0 ; Keras-Applications 1.0.8 ; Keras-Preprocessing 1.1.0 ; keyring 17.0.0 ; kiwisolver 1.0.1 ; lazy-object-proxy 1.3.1 ; libarchive-c 2.8 ; lief 0.9.0 ; llvmlite 0.26.0 ; locket 0.2.0 ; lxml 4.2.5 ; Markdown 3.1.1 ; MarkupSafe 1.1.0 ; matplotlib 3.0.2 ; mccabe 0.6.1 ; mistune 0.8.4 ; mkl-fft 1.0.6 ; mkl-random 1.0.2 ; more-itertools 4.3.0 ; mpmath 1.1.0 ; msgpack 0.5.6 ; multipledispatch 0.6.0 ; murmurhash 1.0.2 ; nbconvert 5.4.0 ; nbformat 4.4.0 ; networkx 2.2 ; nltk 3.4 ; nmslib 1.8.1 ; nose 1.3.7 ; n,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868:1246,green,greenlet,1246,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868,1,['green'],['greenlet']
Energy Efficiency,"ormer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1606,power,powerful,1606,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,4,['power'],"['power', 'powerful']"
Energy Efficiency,"rom citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any ta",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:3485,power,power,3485,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['power'],['power']
Integrability, murmurhash (1.0.2); nbconvert (5.6.0); nbformat (4.4.0); netifaces (0.10.4); nmslib (1.8.1); notebook (6.0.1); numpy (1.17.2); oauth (1.0.1); olefile (0.45.1); pandocfilters (1.4.2); parso (0.5.1); pbr (3.1.1); pexpect (4.7.0); pickleshare (0.7.5); Pillow (6.1.0); pip (9.0.1); plac (0.9.6); preshed (3.0.2); prometheus-client (0.7.1); prompt-toolkit (2.0.9); protobuf (3.9.2); ptyprocess (0.6.0); pyasn1 (0.4.7); pybind11 (2.4.2); pycairo (1.16.2); pycrypto (2.6.1); pycups (1.9.73); Pygments (2.4.2); pygobject (3.26.1); pymacaroons (0.13.0); PyNaCl (1.1.2); pyRFC3339 (1.0); pyrsistent (0.15.4); python-apt (1.6.4); python-dateutil (2.8.0); python-debian (0.1.32); pytz (2018.3); pyxdg (0.25); PyYAML (5.1.2); pyzmq (18.1.0); qtconsole (4.5.5); reportlab (3.4.0); requests (2.22.0); requests-unixsocket (0.1.5); rsa (3.4.2); s3transfer (0.2.1); scikit-learn (0.21.3); scipy (1.3.1); scispacy (0.2.3); screen-resolution-extra (0.0.0); SecretStorage (2.3.1); Send2Trash (1.5.0); setuptools (41.2.0); simplegeneric (0.8.1); simplejson (3.13.2); six (1.12.0); spacy (2.1.8); srsly (0.1.0); system-service (0.3); systemd-python (234); tensorboard (1.14.0); tensorflow (1.14.0); tensorflow-estimator (1.14.0); tensorflow-gpu (1.14.0); termcolor (1.1.0); terminado (0.8.2); testpath (0.4.2); thinc (7.1.1); torch (1.2.0); torchvision (0.4.0); tornado (6.0.3); tqdm (4.36.1); traitlets (4.3.2); ubuntu-drivers-common (0.0.0); ufw (0.36); unattended-upgrades (0.1); urllib3 (1.25.6); usb-creator (0.3.3); wadllib (1.3.2); wasabi (0.2.2); wcwidth (0.1.7); webencodings (0.5.1); Werkzeug (0.16.0); wheel (0.33.6); widgetsnbextension (3.5.1); wrapt (1.11.2); xkit (0.0.0); zope.interface (4.3.2)`. and my full code snippet is :; `In [1]: import spacy . In [2]: import scispacy . In [3]: from scispacy.umls_linking import UmlsEntityLinker . In [4]: nlp = spacy.load('en_core_sci_sm') . In [5]: linker = UmlsEntityLinker(resolve_abbreviations=True) ; fish: “ipython” terminated by signal SIGKILL (Forced quit)`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949:2917,wrap,wrapt,2917,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949,2,"['interface', 'wrap']","['interface', 'wrapt']"
Integrability,"(setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu; Found existing installation: docutils 0.15.2; Uninstalling docutils-0.15.2:; Successfully uninstalled docutils-0.15.2; Running setup.py install for nmslib: started; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:4454,wrap,wrapt,4454,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['wrap'],['wrapt']
Integrability,".1.0,>=3.0.0->scispacy) (3.1.2); Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (60.3.1); Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->scispacy) (3.0.9); Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->scispacy) (5.2.1); Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.1.0,>=3.0.0->scispacy) (4.2.0); Requirement already satisfied: click<7.2.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->scispacy) (7.1.2); Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from jinja2->spacy<3.1.0,>=3.0.0->scispacy) (2.1.1); Installing collected packages: spacy; Attempting uninstall: spacy; Found existing installation: spacy 3.2.4; Uninstalling spacy-3.2.4:; Successfully uninstalled spacy-3.2.4; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; spacy-transformers 1.1.6 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.8 which is incompatible.; medspacy 0.2.0.0 requires spacy<3.2.0,>=3.1.3, but you have spacy 3.0.8 which is incompatible.; en-core-sci-scibert 0.5.0 requires spacy<3.3.0,>=3.2.3, but you have spacy 3.0.8 which is incompatible.; docanalysis 0.1.1 requires spacy==3.0.7, but you have spacy 3.0.8 which is incompatible.; Successfully installed spacy-3.0.8; ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:6225,depend,dependency,6225,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,2,['depend'],['dependency']
Integrability,.8.1 ; nose 1.3.7 ; notebook 5.7.4 ; numba 0.41.0 ; numexpr 2.6.8 ; numpy 1.15.4 ; numpydoc 0.8.0 ; odo 0.5.1 ; olefile 0.46 ; openpyxl 2.5.12 ; packaging 18.0 ; pandas 0.23.4 ; pandocfilters 1.4.2 ; parso 0.3.1 ; partd 0.3.9 ; path.py 11.5.0 ; pathlib2 2.3.3 ; patsy 0.5.1 ; pep8 1.7.1 ; pexpect 4.6.0 ; pickleshare 0.7.5 ; Pillow 5.3.0 ; pip 18.1 ; pkginfo 1.4.2 ; plac 0.9.6 ; pluggy 0.8.0 ; ply 3.11 ; preshed 3.0.2 ; prometheus-client 0.5.0 ; prompt-toolkit 2.0.7 ; protobuf 3.9.1 ; psutil 5.4.8 ; ptyprocess 0.6.0 ; py 1.7.0 ; pyasn1 0.4.7 ; pybind11 2.4.3 ; pycodestyle 2.4.0 ; pycosat 0.6.3 ; pycparser 2.19 ; pycrypto 2.6.1 ; pycurl 7.43.0.2 ; pyflakes 2.0.0 ; Pygments 2.3.1 ; pylint 2.2.2 ; pyodbc 4.0.25 ; pyOpenSSL 18.0.0 ; pyparsing 2.3.0 ; PySocks 1.6.8 ; pytest 4.0.2 ; pytest-arraydiff 0.3 ; pytest-astropy 0.5.0 ; pytest-doctestplus 0.2.0 ; pytest-openfiles 0.3.1 ; pytest-remotedata 0.3.1 ; python-dateutil 2.7.5 ; pytz 2018.7 ; PyWavelets 1.0.1 ; PyYAML 3.13 ; pyzmq 17.1.2 ; QtAwesome 0.5.3 ; qtconsole 4.4.3 ; QtPy 1.5.2 ; requests 2.21.0 ; rope 0.11.0 ; rsa 3.4.2 ; ruamel-yaml 0.15.46 ; s3transfer 0.2.1 ; scikit-image 0.14.1 ; scikit-learn 0.21.3 ; scipy 1.1.0 ; scispacy 0.2.3 ; seaborn 0.9.0 ; SecretStorage 3.1.0 ; Send2Trash 1.5.0 ; setuptools 40.6.3 ; simplegeneric 0.8.1 ; singledispatch 3.4.0.3 ; six 1.12.0 ; snowballstemmer 1.2.1 ; sortedcollections 1.0.1 ; sortedcontainers 2.1.0 ; spacy 2.2.1 ; Sphinx 1.8.2 ; sphinxcontrib-websupport 1.1.0 ; spyder 3.3.2 ; spyder-kernels 0.3.0 ; SQLAlchemy 1.2.15 ; srsly 0.1.0 ; statsmodels 0.9.0 ; sympy 1.3 ; tables 3.4.4 ; tblib 1.3.2 ; termcolor 1.1.0 ; terminado 0.8.1 ; testpath 0.4.2 ; thinc 7.1.1 ; thinc-gpu-ops 0.0.4 ; toolz 0.9.0 ; tornado 5.1.1 ; tqdm 4.28.1 ; traitlets 4.3.2 ; unicodecsv 0.14.1 ; urllib3 1.24.1 ; wasabi 0.2.2 ; wcwidth 0.1.7 ; webencodings 0.5.1 ; Werkzeug 0.14.1 ; wheel 0.32.3 ; widgetsnbextension 3.4.2 ; wrapt 1.11.2 ; wurlitzer 1.0.2 ; xlrd 1.2.0 ; XlsxWriter 1.1.2 ; xlwt 1.3.0 ; zict 0.1.3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868:4141,wrap,wrapt,4141,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868,1,['wrap'],['wrapt']
Integrability,"Happens to me also, trying to install inside docker also resulted in the same error - but the installation does succeed later on (I guess when another library that depends on it tries to install it). ```; ERROR: Complete output from command /usr/local/bin/python -u -c 'import setuptools, tokenize;__file__='""'""'/tmp/pip-install-wtawfp29/nmslib/setup.py'""'""';f=getattr(tokenize, '""'""'open'""'""', open)(__file__);code=f.read().replace('""'""'\r\n'""'""', '""'""'\n'""'""');f.close();exec(compile(code, __file__, '""'""'exec'""'""'))' bdist_wheel -d /tmp/pip-wheel-60emi_hb --python-tag cp36:; ERROR: running bdist_wheel; running build; running build_ext; creating tmp; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjatnwgsk.cpp -o tmp/tmpjatnwgsk.o -std=c++14; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I/usr/local/include/python3.6m -c /tmp/tmpjo6vs9_6.cpp -o tmp/tmpjo6vs9_6.o -fvisibility=hidden; building 'nmslib' extension; creating build; creating build/temp.linux-x86_64-3.6; creating build/temp.linux-x86_64-3.6/nmslib; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or direct",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:164,depend,depends,164,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['depend'],['depends']
Integrability,"Hi @ChantalvanSon ,. Great that you got it working, sorry it was a bit tricky. You raise some good points - there is one way that you can not require your function which loads all the pieces which is this:. ```python; from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ); class UMLS2020KnowledgeBase(KnowledgeBase):; def __init__(; self,; file_path: str = ""path/to/2020AA.json"",; ):; super().__init__(file_path). # Admittedly this is a bit of a hack, because we are mutating a global object.; # However, it's just a kind of registry, so maybe it's ok.; DEFAULT_PATHS[""umls2020""] = CustomLinkerPaths_2020AA; DEFAULT_KNOWLEDGE_BASES[""umls2020""] = UMLS2020KnowledgeBase. linker = CandidateGenerator(name=""umls2020""). ```. Overall, we have it like this so that we can present the simplest possible interface to people who are using scispacy (i.e being able to just pass names to get particular linkers rather than having to know the internals of how the linker is implemented). However I definitely see your point that we should try to make this a bit nicer. In another project I used to work on, we had the concept of using a decorator to register this type of info with the base class, so it can construct itself. That might be a bit of overkill here, but maybe we could provide a function which does this global mutation for you and throws intelligent errors if you e.g try to overwrite something in there? . I think you're right that we need to fix this if we want people to frequently be able to create their own very specific/custom linkers though so thanks for raising it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169:1138,interface,interface,1138,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169,1,['interface'],['interface']
Integrability,"I am running the following commands in macOS v12.2.1 and python 3.9.10; ```; python; Python 3.9.10 (main, Jan 15 2022, 11:48:04) ; [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```; python -m pip install --upgrade pip; pip install spacy; pip install spacy-transformers; pip install scispacy; ```. When executing ; `pip install scispacy ` I get the following error:; ```; Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy; Attempting uninstall: click; Found existing installation: click 8.0.4; Uninstalling click-8.0.4:; Successfully uninstalled click-8.0.4; Attempting uninstall: typer; Found existing installation: typer 0.4.0; Uninstalling typer-0.4.0:; Successfully uninstalled typer-0.4.0; Attempting uninstall: spacy; Found existing installation: spacy 3.2.2; Uninstalling spacy-3.2.2:; Successfully uninstalled spacy-3.2.2; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible.; Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018:936,depend,dependency,936,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018,2,['depend'],['dependency']
Integrability,"I ideally wanted to include scispacy as a dependency of a package for more novice programmers to have some simple access to biomedical NER and using WSL and/or navigating dependency (python, scispacy, etc) versions seems like mental overhead I want to avoid. Is there a way this model could be re-trained using spacy's new entity linker itself? Could that accomplish the same NEL while benefiting from scispacy's models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663:42,depend,dependency,42,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663,2,['depend'],['dependency']
Integrability,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking.; I'll try that, thanks!. BTW, In trying sbd with `en_core_sci_md`, scispacy performs well.; However, there's some minor tokenization problem and if custom rules are added, it can be prevented.; https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16; (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/126#issuecomment-504710956:536,depend,depends,536,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126#issuecomment-504710956,1,['depend'],['depends']
Integrability,"Thanks for the suggestion @chrishmorris! While your idea is reasonable, incorporating that human intuition into the dependency parsing model is quite difficult. See https://spacy.io/api/dependencyparser for more details on the dependency parsing model. A simpler way to incorporate this idea would be to add lots of examples of the form you describe to the training corpus. I will likely not be doing this for scispacy, but if you were to create your own corpus, I'd be happy to help you figure out how to use it in our training scripts to train your own model! And feel free to open another issue if you end up going down that route and would like some help.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/451#issuecomment-1289932724:116,depend,dependency,116,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/451#issuecomment-1289932724,4,"['depend', 'rout']","['dependency', 'dependencyparser', 'route']"
Integrability,"ib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished wit",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:2317,wrap,wrapt,2317,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['wrap'],['wrapt']
Integrability,"py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:3574,wrap,wrapt,3574,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['wrap'],['wrapt']
Integrability,"rch/src; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/space; creating build/temp.linux-x86_64-3.6/nmslib/similarity_search/src/method; gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; B",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:2271,wrap,wrapt,2271,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['wrap'],['wrapt']
Integrability,"wing:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:1341,integrat,integration,1341,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,1,['integrat'],['integration']
Modifiability,"(setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu; Found existing installation: docutils 0.15.2; Uninstalling docutils-0.15.2:; Successfully uninstalled docutils-0.15.2; Running setup.py install for nmslib: started; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:3832,config,configobj,3832,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,2,['config'],"['configobj', 'configparser']"
Modifiability,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python; !mv /root/.scispacy/ /content/gdrive/MyDrive/test/; !ls /content/gdrive/MyDrive/test/.scispacy/; >>> datasets; ```. To update the environment variable, as described:. ```python; import os; os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'; ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python; nlp.add_pipe(; ""scispacy_linker"",; config={; ""resolve_abbreviations"": True,; ""linker_name"": ""umls"",; ""cache_folder"": ""/content/gdrive/MyDrive/test/""}); ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940:372,variab,variable,372,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940,4,"['config', 'enhance', 'variab']","['config', 'enhancement', 'variable']"
Modifiability,"core-sci-sm; Building wheel for en-core-sci-sm (setup.py) ... done; Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870856 sha256=e99e476d22293a04ce498b2a9a3ed2514cdadebb4b9fa5794ebf40b51d05587c; Stored in directory: /home/zhangx/.cache/pip/wheels/f5/2e/39/9c9d425a1d34c06409420f7c65c5e10a56f7b149a3c37cdfa6; Successfully built en-core-sci-sm; Installing collected packages: en-core-sci-sm; Successfully installed en-core-sci-sm-0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ vim demo_scispacy.py; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ cat demo_scispacy.py; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""); (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep scispacy; scispacy 0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep en_core_sci; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep en_core_sci*; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ python -V; Python 3.9.15; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ python demo_scispacy.py; Abbreviation Definition; SBMA (6, 7) Spinal and bulbar muscular atrophy; SBMA (33, 34) Spinal",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:21849,inherit,inherited,21849,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['inherit'],['inherited']
Modifiability,"py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:3501,config,configobj,3501,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['config'],['configobj']
Modifiability,"we're using spacy 3's new config system and [spacy projects](https://spacy.io/usage/projects). So our project file lives [here](https://github.com/allenai/scispacy/blob/master/project.yml) and our configs live [here](https://github.com/allenai/scispacy/tree/master/configs). You should be able to follow these as a guide, an basically just run the ner training commands, but with your data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/316#issuecomment-792961359:26,config,config,26,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316#issuecomment-792961359,3,['config'],"['config', 'configs']"
Performance," (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'; ```. For convenience, he",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:9756,load,load,9756,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['load'],['load']
Performance," -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispac",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10397,cache,cached,10397,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance," 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); Traceback (most recent call last):; File ""linker.py"", line 12, in <module>; linker = UmlsEntityLinker(resolve_abbreviations=True); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__; self.candidate_generator = candidate_generator or CandidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21.1.3; plac 0.9.6; preshed 3.0.5; psutil 5.8.0; pyasn1 0.4.8; pybind11 2.6.1; pysbd 0.3.4; python-dateutil 2.8.2; PyYAML 5.4.1; requests 2.26.0; rsa 4.7.2; s3transfer 0.5.0; scikit-learn 0.22.2; scipy 1.7.0; scispacy 0.2.4; setuptoo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:1744,load,loads,1744,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['load'],['loads']
Performance," install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10832,cache,cached,10832,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance, murmurhash (1.0.2); nbconvert (5.6.0); nbformat (4.4.0); netifaces (0.10.4); nmslib (1.8.1); notebook (6.0.1); numpy (1.17.2); oauth (1.0.1); olefile (0.45.1); pandocfilters (1.4.2); parso (0.5.1); pbr (3.1.1); pexpect (4.7.0); pickleshare (0.7.5); Pillow (6.1.0); pip (9.0.1); plac (0.9.6); preshed (3.0.2); prometheus-client (0.7.1); prompt-toolkit (2.0.9); protobuf (3.9.2); ptyprocess (0.6.0); pyasn1 (0.4.7); pybind11 (2.4.2); pycairo (1.16.2); pycrypto (2.6.1); pycups (1.9.73); Pygments (2.4.2); pygobject (3.26.1); pymacaroons (0.13.0); PyNaCl (1.1.2); pyRFC3339 (1.0); pyrsistent (0.15.4); python-apt (1.6.4); python-dateutil (2.8.0); python-debian (0.1.32); pytz (2018.3); pyxdg (0.25); PyYAML (5.1.2); pyzmq (18.1.0); qtconsole (4.5.5); reportlab (3.4.0); requests (2.22.0); requests-unixsocket (0.1.5); rsa (3.4.2); s3transfer (0.2.1); scikit-learn (0.21.3); scipy (1.3.1); scispacy (0.2.3); screen-resolution-extra (0.0.0); SecretStorage (2.3.1); Send2Trash (1.5.0); setuptools (41.2.0); simplegeneric (0.8.1); simplejson (3.13.2); six (1.12.0); spacy (2.1.8); srsly (0.1.0); system-service (0.3); systemd-python (234); tensorboard (1.14.0); tensorflow (1.14.0); tensorflow-estimator (1.14.0); tensorflow-gpu (1.14.0); termcolor (1.1.0); terminado (0.8.2); testpath (0.4.2); thinc (7.1.1); torch (1.2.0); torchvision (0.4.0); tornado (6.0.3); tqdm (4.36.1); traitlets (4.3.2); ubuntu-drivers-common (0.0.0); ufw (0.36); unattended-upgrades (0.1); urllib3 (1.25.6); usb-creator (0.3.3); wadllib (1.3.2); wasabi (0.2.2); wcwidth (0.1.7); webencodings (0.5.1); Werkzeug (0.16.0); wheel (0.33.6); widgetsnbextension (3.5.1); wrapt (1.11.2); xkit (0.0.0); zope.interface (4.3.2)`. and my full code snippet is :; `In [1]: import spacy . In [2]: import scispacy . In [3]: from scispacy.umls_linking import UmlsEntityLinker . In [4]: nlp = spacy.load('en_core_sci_sm') . In [5]: linker = UmlsEntityLinker(resolve_abbreviations=True) ; fish: “ipython” terminated by signal SIGKILL (Forced quit)`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949:3134,load,load,3134,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949,1,['load'],['load']
Performance," the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'; ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?u",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:10004,load,load,10004,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['load'],['load']
Performance," these will first have to be loaded using `load_approximate_nearest_neighbours_index`, and this one only accepts a `LinkerPaths` object. So I ended up writing something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). r",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:1967,load,loading,1967,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['load'],['loading']
Performance,"+14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:2882,cache,cache-,2882,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['cache'],['cache-']
Performance,",>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14198,cache,cached,14198,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"---------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler,",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:3127,cache,cache-,3127,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['cache'],['cache-']
Performance,"-Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -fPIC -I./nmslib/similarity_search/include -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/usr/local/include/python3.6m -I/root/.local/include/python3.6m -I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:2406,cache,cache-,2406,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['cache'],['cache-']
Performance,".0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14100,cache,cached,14100,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,".2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2392,cache,cached,2392,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13820,cache,cached,13820,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 insta",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2470,cache,cached,2470,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10310,cache,cached,10310,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13913,cache,cached,13913,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10490,cache,cached,10490,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13537,cache,cached,13537,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12705,cache,cached,12705,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-a",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13219,cache,cached,13219,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"4::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/installer.py:27: SetuptoolsDeprecationWarning: setuptools.installer is deprecated. Requirements should be satisfied by a PEP 517 installer. warnings.warn(. Traceback (most recent call last):. File ""<string>"", line 2, in <module",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2564,cache,cached,2564,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->sc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:11880,cache,cached,11880,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12353,cache,cached,12353,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:11807,cache,cached,11807,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"> I have a feeling this is related to some combination of Windows and python version...but I'm not sure. I was able to load the entity linker on my windows machine just fine. Are you able to load other json files using `json.load`?. I agree on the problem of combination of windows and python. Yes. I tried a simple json file to test the `json.load`, it work. Finally, I change to Ubuntu environment, it work successfully. I think the there are some Unicode issues on windows, but I haven't figured out. Really thanks for your reply.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/227#issuecomment-629885815:119,load,load,119,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/227#issuecomment-629885815,4,['load'],['load']
Performance,"@DeNeutoy This is the exact code I'm using:. ```; import scispacy; import spacy; from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""); nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586:123,load,load,123,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586,3,"['load', 'perform']","['load', 'performed']"
Performance,"B). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-m",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13123,cache,cached,13123,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"Hi @ChantalvanSon ,. Great that you got it working, sorry it was a bit tricky. You raise some good points - there is one way that you can not require your function which loads all the pieces which is this:. ```python; from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ); class UMLS2020KnowledgeBase(KnowledgeBase):; def __init__(; self,; file_path: str = ""path/to/2020AA.json"",; ):; super().__init__(file_path). # Admittedly this is a bit of a hack, because we are mutating a global object.; # However, it's just a kind of registry, so maybe it's ok.; DEFAULT_PATHS[""umls2020""] = CustomLinkerPaths_2020AA; DEFAULT_KNOWLEDGE_BASES[""umls2020""] = UMLS2020KnowledgeBase. linker = CandidateGenerator(name=""umls2020""). ```. Overall, we have it like this so that we can present the simplest possible interface to people who are using scispacy (i.e being able to just pass names to get particular linkers rather than having to know the internals of how the linker is implemented). However I definitely see your point that we should try to make this a bit nicer. In another project I used to work on, we had the concept of using a decorator to register this type of info with the base class, so it can construct itself. That might be a bit of overkill here, but maybe we could provide a function which does this global mutation for you and throws intelligent errors if you e.g try to overwrite something in there? . I think you're right that we need to fix this if we want people to frequently be able to create their own very specific/custom linkers though so thanks for raising it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169:170,load,loads,170,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169,1,['load'],['loads']
Performance,"Hi @DeNeutoy, thanks for the information! I'm happy to share that I managed to create my custom Entity Linker based on the 2020AA release. It took a bit longer (~8 hours) to build the ANN index, but this could very well be because of the size of my UMLS subset (all level 0 sources + SNOMED). . Overall, it was not tóó difficult to do, but I think some small changes in the code would make it even easier. I don't have an answer to this question myself yet -- if I have time and I do think of a good solution, I will try to see if I can help out by creating a PR. But I think it comes down to the following:. `CandidateGenerator()` currently accepts a pre-trained linker (`umls` or `mesh`), for which the default `LinkerPaths` have been defined globally in the `candidate_generation.py`. While it is possible to provide your own `ann_index`, `tfidf_vectorizer`, `ann_concept_aliases_list` and `kb`, these will first have to be loaded using `load_approximate_nearest_neighbours_index`, and this one only accepts a `LinkerPaths` object. So I ended up writing something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model compon",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:927,load,loaded,927,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['load'],['loaded']
Performance,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/402#issuecomment-951214733:135,load,loading,135,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402#issuecomment-951214733,3,"['cache', 'load']","['cache', 'loading']"
Performance,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. ; @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:; ```; ncalls tottime percall cumtime percall filename:lineno(function); 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode); ```; I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:; 1. `pyarrow` to store the alias list ; 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)?. Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/402#issuecomment-952661338:36,cache,cached,36,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402#issuecomment-952661338,3,"['cache', 'load']","['cached', 'load', 'loading']"
Performance,"Hi Ravina,; Thank you for the information. I am trying to extract drug name and disease; name from a sentence. So which model will be best fit for this and also can; you give me a clearity what GGP and CL entity mean in medical terms.; Thank you so much for your help.; -Regards,; Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>; wrote:. > import spacy; > nlp = spacy.load(""en_ner_craft_md""); > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells; > with immunosuppressive activity. ""; > ""They accumulate in tumor-bearing mice and humans with different types of; > cancer, including hepatocellular carcinoma (HCC).""; > doc = nlp(text); > for ent in doc.ents:; > print(ent, ent.label_); >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>; > .; >",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/79#issuecomment-467847381:399,load,load,399,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79#issuecomment-467847381,1,['load'],['load']
Performance,"I'd be willing to do this and submit a PR for it. Not sure if it as simple as running `scripts/create_linker.py` on the MRCONSO.rrf file or if I'd need to download the entire UMLS and run `scripts/export_umls_json.py`. Also not sure if I could include the data for those files in the PR due to size or if I'd need to retrain and publish the models themselves which I am sure I don't have permissions for... I think going forward making this process as simple as possible should be a requirement so no matter your load users can easily update the primary (UMLS) knowledge base to keep it up to date. The first paragraph here raises a general question I had, is the UMLS data used only for the NER or is it a larger part of the model? I.e. if I created my own EntityLinker using 2022AB UMLS, would that solve this ""outdated"" issue?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227:513,load,load,513,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227,1,['load'],['load']
Performance,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking.; I'll try that, thanks!. BTW, In trying sbd with `en_core_sci_md`, scispacy performs well.; However, there's some minor tokenization problem and if custom rules are added, it can be prevented.; https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16; (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/126#issuecomment-504710956:159,perform,performs,159,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126#issuecomment-504710956,1,['perform'],['performs']
Performance,"I/tmp/pip-install-wtawfp29/nmslib/.eggs/numpy-1.17.0-py3.6-linux-x86_64.egg/numpy/core/include -I/usr/local/include/python3.6m -c nmslib.cc -o build/temp.linux-x86_64-3.6/nmslib.o -O3 -march=native -fopenmp -DVERSION_INFO=""1.8.1"" -std=c++14 -fvisibility=hidden; nmslib.cc:16:10: fatal error: pybind11/pybind11.h: No such file or directory; #include <pybind11/pybind11.h>; ^~~~~~~~~~~~~~~~~~~~~; compilation terminated.; error: command 'gcc' failed with exit status 1; ----------------------------------------; ERROR: Failed building wheel for nmslib; Running setup.py clean for nmslib; Building wheel for wrapt (setup.py): started; Building wheel for wrapt (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:2647,cache,cache-,2647,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['cache'],['cache-']
Performance,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python; !mv /root/.scispacy/ /content/gdrive/MyDrive/test/; !ls /content/gdrive/MyDrive/test/.scispacy/; >>> datasets; ```. To update the environment variable, as described:. ```python; import os; os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'; ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python; nlp.add_pipe(; ""scispacy_linker"",; config={; ""resolve_abbreviations"": True,; ""linker_name"": ""umls"",; ""cache_folder"": ""/content/gdrive/MyDrive/test/""}); ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940:103,cache,cached,103,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940,4,['cache'],"['cache', 'cached']"
Performance,"No problem!. In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:726,perform,performance,726,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,1,['perform'],['performance']
Performance,"Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). C",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12272,cache,cached,12272,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/378#issuecomment-880502034:66,load,loaded,66,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378#issuecomment-880502034,1,['load'],['loaded']
Performance,"When installing scispacy, I get similar resolver issues. Am I doing something wrong?. ```; emanuelfarruda@Mannys-MacBook-Pro-2021 ~ % pip3 install scispacy; Requirement already satisfied: scispacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.4.0); Requirement already satisfied: scikit-learn>=0.20.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.1.1); Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.1.0); Requirement already satisfied: nmslib>=1.7.3.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.1.1); Requirement already satisfied: pysbd in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (0.3.4); Collecting spacy<3.1.0,>=3.0.0; Using cached spacy-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB); Requirement already satisfied: conllu in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (4.4.2); Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.15.1); Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.22.4); Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.1); Requirement already satisfied: pybind11<2.6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1); Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0); Requirement already satisfied: sc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:933,cache,cached,933,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,1,['cache'],['cached']
Performance,"act code I'm using:. ```; import scispacy; import spacy; from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""); nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignanc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586:1075,perform,performed,1075,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586,1,['perform'],['performed']
Performance,"any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Succes",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13728,cache,cached,13728,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); Traceback (most recent call last):; File ""linker.py"", line 12, in <module>; linker = UmlsEntityLinker(resolve_abbreviations=True); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__; self.candidate_generator = candidate_generator or CandidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21.1.3; plac 0.9.6; preshed 3.0.5; psutil 5.8.0; pyasn1 0.4.8; pybind11 2.6.1; pysbd 0.3.4; python-dateutil 2.8.2; PyYAML 5.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:1591,load,load,1591,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['load'],['load']
Performance,"cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolct",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13435,cache,cached,13435,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Usin",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12615,cache,cached,12615,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"ched conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:11981,cache,cached,11981,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"cting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12942,cache,cached,12942,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"d(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignancy degree, also antigen expression features does not show any type of correlation between malignancy degree and CD24 positivity or with CD24 e",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586:1176,perform,performed,1176,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586,1,['perform'],['performed']
Performance,"da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead. warnings.warn(. /Users/briang/opt/anaconda3",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2282,cache,cached,2282,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'; ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSI",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:9879,load,load,9879,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['load'],['load']
Performance,"fe>=2.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.1); Building wheels for collected packages: en-core-sci-sm; Building wheel for en-core-sci-sm (setup.py) ... done; Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870856 sha256=e99e476d22293a04ce498b2a9a3ed2514cdadebb4b9fa5794ebf40b51d05587c; Stored in directory: /home/zhangx/.cache/pip/wheels/f5/2e/39/9c9d425a1d34c06409420f7c65c5e10a56f7b149a3c37cdfa6; Successfully built en-core-sci-sm; Installing collected packages: en-core-sci-sm; Successfully installed en-core-sci-sm-0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ vim demo_scispacy.py; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ cat demo_scispacy.py; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""); (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep scispacy; scispacy 0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep en_core_sci; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list|grep en_core_sci*; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ python -V; Python 3.9.15; (vega_scispacy_2) zhang",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:21672,load,load,21672,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['load'],['load']
Performance,"fied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached c",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12439,cache,cached,12439,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"g cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12072,cache,cached,12072,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"hash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, sciki",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13624,cache,cached,13624,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"hy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14013,cache,cached,14013,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"ing pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ___",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14277,cache,cached,14277,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"ip-ephem-wheel-cache-097rpwoy/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd; Building wheel for absl-py (setup.py): started; Building wheel for absl-py (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ee/98/38/46cbcc5a93cfea5492d19c38562691ddb23b940176c14f7b48; Building wheel for gast (setup.py): started; Building wheel for gast (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd; Building wheel for termcolor (setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, sciki",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:3366,cache,cache-,3366,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['cache'],['cache-']
Performance,"iplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); # nlpSciLg = spacy.load(""en_core_sci_lg"", disable = ['ner', 'parser', 'tagger', 'lemmatizer']); nlpSciMd.add_pipe('pysbd_sentencizer'); nlpSciSm.add_pipe('pysbd_sentencizer'); ```. error. ```; ---------------------------------------------------------------------------; ModuleNotFoundError Traceback (most recent call last); <ipython-input-3-45556ac5415d> in <module>(); 1 import spacy; 2 import scispacy; ----> 3 from scispacy.custom_sentence_segmentater import pysbd_sentencizer; 4 nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); 5 nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']). ModuleNotFoundError: No module named 'scispacy.custom_sentence_segmentater'; ```. For convenience, here are the colab notebooks where I tried to code. scispacy. https://colab.research.google.com/drive/1EleinjhYDaqU3OYb4u1odSItEY7-KP4U?usp=sharing. spacy. https://colab.research.google.com/drive/1UCh65W-yEYZzOhWDrqL_ACKSbjxWXbGI?usp=sharing. pysbd_sentencizer. https://colab.research.google.com/drive/1jYetA7G4RdRHDGmXxl3ToSBBpzw6BE36?usp=sharing. side note: in the first notebook you can see there's an error getting the small model to work.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:10486,load,load,10486,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,2,['load'],['load']
Performance,"iting something like the following (based on how it's done for the pre-trained `umls` and `mesh` linkers in `candidate_generation.py`):. ```; import json; import joblib. from scispacy.linking_utils import UmlsKnowledgeBase; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths,; load_approximate_nearest_neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the nece",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:2081,load,load,2081,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['load'],['load']
Performance,"jective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1957,perform,performance,1957,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,2,['perform'],['performance']
Performance,"nda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12179,cache,cached,12179,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"neighbours_index,; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the necessary files directly when initiating a `CandidateGenerator`, so that you could do something like the following:. ```; candidate_generator = CandidateGenerator(; ann_index=""path/to/ann_index"",; tfidf_vectorizer=""path/to/tfidf_vectorizer"",; ann_concept_aliases_list=""path/to/ann_concept_aliases_list"",; kb=""path/to/kb"",; ); ```. I ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:2296,load,load,2296,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['load'],['load']
Performance,"ng>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13332,cache,cached,13332,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"nvs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:11718,cache,cached,11718,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murm",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:11621,cache,cached,11621,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"rom requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12526,cache,cached,12526,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"satisfied: confection<1.0.0,>=0.0.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.0.3); Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (8.1.3); Requirement already satisfied: MarkupSafe>=2.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.1.1); Building wheels for collected packages: en-core-sci-sm; Building wheel for en-core-sci-sm (setup.py) ... done; Created wheel for en-core-sci-sm: filename=en_core_sci_sm-0.5.1-py3-none-any.whl size=15870856 sha256=e99e476d22293a04ce498b2a9a3ed2514cdadebb4b9fa5794ebf40b51d05587c; Stored in directory: /home/zhangx/.cache/pip/wheels/f5/2e/39/9c9d425a1d34c06409420f7c65c5e10a56f7b149a3c37cdfa6; Successfully built en-core-sci-sm; Installing collected packages: en-core-sci-sm; Successfully installed en-core-sci-sm-0.5.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ vim demo_scispacy.py; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ cat demo_scispacy.py; import spacy. from scispacy.abbreviation import AbbreviationDetector. nlp = spacy.load(""en_core_sci_sm""). # Add the abbreviation pipe to the spacy pipeline.; nlp.add_pipe(""abbreviation_detector""). doc = nlp(""Spinal and bulbar muscular atrophy (SBMA) is an \; inherited motor neuron disease caused by the expansion \; of a polyglutamine tract within the androgen receptor (AR). \; SBMA can be caused by this easily.""). print(""Abbreviation"", ""\t"", ""Definition""); for abrv in doc._.abbreviations:; print(f""{abrv} \t ({abrv.start}, {abrv.end}) {abrv._.long_form}""); (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:21147,cache,cache,21147,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['cache'],['cache']
Performance,"ssible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached thre",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10987,cache,cached,10987,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"sults. Use at your own risk.; UserWarning); /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); Traceback (most recent call last):; File ""linker.py"", line 12, in <module>; linker = UmlsEntityLinker(resolve_abbreviations=True); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__; self.candidate_generator = candidate_generator or CandidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:1483,load,load,1483,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['load'],['load']
Performance,"t https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:11078,cache,cached,11078,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"th/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the necessary files directly when initiating a `CandidateGenerator`, so that you could do something like the following:. ```; candidate_generator = CandidateGenerator(; ann_index=""path/to/ann_index"",; tfidf_vectorizer=""path/to/tfidf_vectorizer"",; ann_concept_aliases_list=""path/to/ann_concept_aliases_list"",; kb=""path/to/kb"",; ); ```. I hope I explained my thoughts properly :-) Thanks and keep up the good work!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:2467,load,load,2467,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,3,['load'],['load']
Performance,"ting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-py3-none-any.whl (181 kB). Collecting spacy-legacy<3.1.0,>=3.0.10. Using cached spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB). Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4. Using cached pydantic-1.10.2-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB). Collecting cymem<2.1.0,>=2.0.2. Using cached cymem-2.0.7-cp39-cp39-macosx_11_0_arm64.whl (31 kB). Collecting catalogue<2.1.0,>=2.0.6. Using cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:13036,cache,cached,13036,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Performance,"tion, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0); Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0); Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.28.1); Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14891,cache,cached,14891,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['cache'],['cached']
Performance,"up.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10917,cache,cached,10917,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['cache'],['cached']
Safety," not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:2520,predict,prediction,2520,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['predict'],['prediction']
Safety," not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; He",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:7026,predict,prediction,7026,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['predict'],['prediction']
Safety,"Ah! The Abbreviation detector detects abbreviations defined in text, but does not try to disambiguate arbitrary abbreviations. So in a document, you had `blah blah... computed tomography angiography (CTA) was used ... After, CTA was done.`, the abbreviation detector would find the definition and connect it to its uses in the document. Hopefully that's clearer?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/189#issuecomment-570049396:21,detect,detector,21,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/189#issuecomment-570049396,3,['detect'],"['detector', 'detects']"
Safety,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed.; I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:; JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST; the Spacy-2 correctly predicts all 3 entities above; whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents.; Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395:451,predict,predicts,451,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395,3,['predict'],"['predicted', 'predicts']"
Safety,"I ideally wanted to include scispacy as a dependency of a package for more novice programmers to have some simple access to biomedical NER and using WSL and/or navigating dependency (python, scispacy, etc) versions seems like mental overhead I want to avoid. Is there a way this model could be re-trained using spacy's new entity linker itself? Could that accomplish the same NEL while benefiting from scispacy's models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663:252,avoid,avoid,252,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663,1,['avoid'],['avoid']
Safety,"No problem!. In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:321,detect,detection,321,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,2,['detect'],['detection']
Safety,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/352#issuecomment-843634145:257,predict,predict,257,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352#issuecomment-843634145,1,['predict'],['predict']
Safety,"Unfortunately still the same issue. I also checked your post in: https://www.kaggle.com/daking/extracting-entities-linked-to-umls-with-scispacy and tried to use the same packages version but no success. Here the stack trace error with the list of packages/versions installed:. `/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); /data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/sklearn/base.py:315: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.20.3 when using version 0.22.2. This might lead to breaking code or invalid results. Use at your own risk.; UserWarning); Traceback (most recent call last):; File ""linker.py"", line 12, in <module>; linker = UmlsEntityLinker(resolve_abbreviations=True); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_linking.py"", line 68, in __init__; self.candidate_generator = candidate_generator or CandidateGenerator(); File ""/data/home/fsa/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/candidate_generation.py"", line 129, in __init__; self.umls = umls or UmlsKnowledgeBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; ob",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:543,risk,risk,543,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,2,['risk'],['risk']
Safety,"geBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21.1.3; plac 0.9.6; preshed 3.0.5; psutil 5.8.0; pyasn1 0.4.8; pybind11 2.6.1; pysbd 0.3.4; python-dateutil 2.8.2; PyYAML 5.4.1; requests 2.26.0; rsa 4.7.2; s3transfer 0.5.0; scikit-learn 0.22.2; scipy 1.7.0; scispacy 0.2.4; setuptools 39.0.1; six 1.16.0; spacy 2.2.1; srsly 1.0.5; thinc 7.1.1; threadpoolctl 2.2.0; tqdm 4.61.2; typing-extensions 3.10.0.0; urllib3 1.26.6; wasabi 0.8.2; zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:3137,safe,safe,3137,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['safe'],['safe']
Safety,"wing:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1 on the st21pv subset. It would also be helpful to know the coverage of our kb on this subset as well as what % of annotated concepts in this subset have definitions. I will try to work on these things over the next couple of weeks.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:1985,detect,detection,1985,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,1,['detect'],['detection']
Security,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed.; I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:; JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST; the Spacy-2 correctly predicts all 3 entities above; whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents.; Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395:242,validat,validation,242,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395,1,['validat'],['validation']
Security,"Hi Daniel,. I tried to install nmslib again in a new py 3.9 environment and it looks like it actually did install but there were issues. The entire log from the install is below. I also successfully installed scispacy from pip which is great! I included the install log for scispacy below the log for nmslib. Maybe I did something wrong before when I created the env for scispacy?. I will be testing it soon and will let you know if anything else comes up. Thank you for following up with me on this. Cheers, Brian. ### create env log ###. (base) ***@***.*** ~ % conda create -n scispacy python=3.9. Collecting package metadata (current_repodata.json): done. Solving environment: done. ## Package Plan ##. environment location: /Users/briang/opt/anaconda3/envs/scispacy. added / updated specs:. - python=3.9. The following NEW packages will be INSTALLED:. ca-certificates pkgs/main/osx-arm64::ca-certificates-2022.10.11-hca03da5_0 None. certifi pkgs/main/osx-arm64::certifi-2022.9.24-py39hca03da5_0 None. libcxx pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 None. libffi pkgs/main/osx-arm64::libffi-3.4.2-hc377ac9_4 None. ncurses pkgs/main/osx-arm64::ncurses-6.3-h1a28f6b_3 None. openssl pkgs/main/osx-arm64::openssl-1.1.1s-h1a28f6b_0 None. pip pkgs/main/osx-arm64::pip-22.2.2-py39hca03da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:859,certificate,certificates,859,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,2,['certificate'],"['certificates', 'certificates-']"
Security,"I ideally wanted to include scispacy as a dependency of a package for more novice programmers to have some simple access to biomedical NER and using WSL and/or navigating dependency (python, scispacy, etc) versions seems like mental overhead I want to avoid. Is there a way this model could be re-trained using spacy's new entity linker itself? Could that accomplish the same NEL while benefiting from scispacy's models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663:114,access,access,114,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663,1,['access'],['access']
Security,"ain/linux-64::tk-8.6.12-h1ccaba5_0; tzdata pkgs/main/noarch::tzdata-2022g-h04d1e81_0; wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0; xz pkgs/main/linux-64::xz-5.2.8-h5eee18b_0; zlib pkgs/main/linux-64::zlib-1.2.13-h5eee18b_0. Preparing transaction: done; Verifying transaction: done; Executing transaction: done; #; # To activate this environment, use; #; # $ conda activate vega_scispacy_2; #; # To deactivate an active environment, use; #; # $ conda deactivate. (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list; Package Version; ---------- ---------; certifi 2022.9.24; pip 22.3.1; setuptools 65.5.0; wheel 0.37.1; WARNING: The repository located at pypi.douban.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host pypi.douban.com'.; WARNING: There was an error checking the latest version of pip.; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install scispacy; Looking in indexes: http://pypi.douban.com/simple; Collecting scispacy; Downloading http://pypi.doubanio.com/packages/6d/f2/a55ed36940e481e1823c71047e5b3b90a2cb516f59f25b63a57e60e3f8c3/scispacy-0.5.1-py3-none-any.whl (44 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.9/44.9 kB 1.3 MB/s eta 0:00:00; Collecting numpy; Downloading http://pypi.doubanio.com/packages/4c/b9/038abd6fbd67b05b03cb1af590cfc02b7f1e5a37af7ac6a868f5093c29f5/numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 7.7 MB/s eta 0:00:00; Collecting spacy<3.5",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:3110,secur,secure,3110,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['secur'],['secure']
Security,"and copy the demo code; 7.`cat demo_scispacy.py`; 8.`pip list|grep scispacy`; 9.`pip list|grep en_core_sci`; 10.`python -V`; 11.`python demo_scispacy.py`; 12.I Got Success result, Hey. 13.**But I don't know why the previous error, unbelieveable.**; . The all log are as following.; ```log; (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda create --name vega_scispacy_2 python=3.9 -y; Collecting package metadata (current_repodata.json): done; Solving environment: done. ==> WARNING: A newer version of conda exists. <==; current version: 4.9.2; latest version: 22.11.1. Please update conda by running. $ conda update -n base -c defaults conda. ## Package Plan ##. environment location: /home/zhangx/anaconda3/envs/vega_scispacy_2. added / updated specs:; - python=3.9. The following NEW packages will be INSTALLED:. _libgcc_mutex pkgs/main/linux-64::_libgcc_mutex-0.1-main; _openmp_mutex pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu; ca-certificates pkgs/main/linux-64::ca-certificates-2022.10.11-h06a4308_0; certifi pkgs/main/linux-64::certifi-2022.9.24-py39h06a4308_0; ld_impl_linux-64 pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1; libffi pkgs/main/linux-64::libffi-3.4.2-h6a678d5_6; libgcc-ng pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1; libgomp pkgs/main/linux-64::libgomp-11.2.0-h1234567_1; libstdcxx-ng pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1; ncurses pkgs/main/linux-64::ncurses-6.3-h5eee18b_3; openssl pkgs/main/linux-64::openssl-1.1.1s-h7f8727e_0; pip pkgs/main/linux-64::pip-22.3.1-py39h06a4308_0; python pkgs/main/linux-64::python-3.9.15-h7a1cb2a_2; readline pkgs/main/linux-64::readline-8.2-h5eee18b_0; setuptools pkgs/main/linux-64::setuptools-65.5.0-py39h06a4308_0; sqlite pkgs/main/linux-64::sqlite-3.40.0-h5082296_0; tk pkgs/main/linux-64::tk-8.6.12-h1ccaba5_0; tzdata pkgs/main/noarch::tzdata-2022g-h04d1e81_0; wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0; xz pkgs/main/linux-64::xz-5.2.8-h5eee18b_0; zlib pkgs/ma",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:1286,certificate,certificates,1286,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,2,['certificate'],"['certificates', 'certificates-']"
Testability," cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error us",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14448,log,loggers,14448,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['log'],['loggers']
Testability," http://pypi.doubanio.com/packages/bb/b7/380c9e4cd71263f03d16f8a92c0e44c9bdef38777e1a7dde1f47ba996bac/scipy-1.9.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 33.8/33.8 MB 8.2 MB/s eta 0:00:00; Collecting spacy-legacy<3.1.0,>=3.0.10; Downloading http://pypi.doubanio.com/packages/9f/3d/5024f88696db0ef37e3a3a0ddde60d9a43932e0ed68c2387cd966acf776d/spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB); Collecting packaging>=20.0; Downloading http://pypi.doubanio.com/packages/8f/7b/42582927d281d7cb035609cd3a543ffac89b74f3f4ee8e1c50914bcb57eb/packaging-22.0-py3-none-any.whl (42 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 30.5 MB/s eta 0:00:00; Collecting murmurhash<1.1.0,>=0.28.0; Downloading http://pypi.doubanio.com/packages/ce/4a/139a0f0ed47afc324843357b021233f5cf16e4b28fd0d322f0ec54ee6d0e/murmurhash-1.0.9-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB); Collecting spacy-loggers<2.0.0,>=1.0.0; Downloading http://pypi.doubanio.com/packages/62/8c/814e0bd139a8c94b50298be3a4e640d90cdce78efe0099e373a767b7d854/spacy_loggers-1.0.4-py3-none-any.whl (11 kB); Collecting cymem<2.1.0,>=2.0.2; Downloading http://pypi.doubanio.com/packages/b1/62/c615d7ff20647b1c568eac00a94df1e88e7c379646659eb0be6e346cadfe/cymem-2.0.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB); Collecting jinja2; Downloading http://pypi.doubanio.com/packages/bc/c3/f068337a370801f372f2f8f6bad74a5c140f6fda3d9de154052708dd3c65/Jinja2-3.1.2-py3-none-any.whl (133 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.1/133.1 kB 3.8 MB/s eta 0:00:00; Collecting srsly<3.0.0,>=2.4.3; Downloading http://pypi.doubanio.com/packages/c2/6c/39ea8715b9096d97e16474278fca96256dd3f128723ea6e4325107cfca9a/srsly-2.4.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 490.7/490.7 kB 7.2 MB/s eta 0:00:00; Collecting wasabi<1.1.0,>=0.9.1; ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:8646,log,loggers,8646,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['log'],['loggers']
Testability, murmurhash (1.0.2); nbconvert (5.6.0); nbformat (4.4.0); netifaces (0.10.4); nmslib (1.8.1); notebook (6.0.1); numpy (1.17.2); oauth (1.0.1); olefile (0.45.1); pandocfilters (1.4.2); parso (0.5.1); pbr (3.1.1); pexpect (4.7.0); pickleshare (0.7.5); Pillow (6.1.0); pip (9.0.1); plac (0.9.6); preshed (3.0.2); prometheus-client (0.7.1); prompt-toolkit (2.0.9); protobuf (3.9.2); ptyprocess (0.6.0); pyasn1 (0.4.7); pybind11 (2.4.2); pycairo (1.16.2); pycrypto (2.6.1); pycups (1.9.73); Pygments (2.4.2); pygobject (3.26.1); pymacaroons (0.13.0); PyNaCl (1.1.2); pyRFC3339 (1.0); pyrsistent (0.15.4); python-apt (1.6.4); python-dateutil (2.8.0); python-debian (0.1.32); pytz (2018.3); pyxdg (0.25); PyYAML (5.1.2); pyzmq (18.1.0); qtconsole (4.5.5); reportlab (3.4.0); requests (2.22.0); requests-unixsocket (0.1.5); rsa (3.4.2); s3transfer (0.2.1); scikit-learn (0.21.3); scipy (1.3.1); scispacy (0.2.3); screen-resolution-extra (0.0.0); SecretStorage (2.3.1); Send2Trash (1.5.0); setuptools (41.2.0); simplegeneric (0.8.1); simplejson (3.13.2); six (1.12.0); spacy (2.1.8); srsly (0.1.0); system-service (0.3); systemd-python (234); tensorboard (1.14.0); tensorflow (1.14.0); tensorflow-estimator (1.14.0); tensorflow-gpu (1.14.0); termcolor (1.1.0); terminado (0.8.2); testpath (0.4.2); thinc (7.1.1); torch (1.2.0); torchvision (0.4.0); tornado (6.0.3); tqdm (4.36.1); traitlets (4.3.2); ubuntu-drivers-common (0.0.0); ufw (0.36); unattended-upgrades (0.1); urllib3 (1.25.6); usb-creator (0.3.3); wadllib (1.3.2); wasabi (0.2.2); wcwidth (0.1.7); webencodings (0.5.1); Werkzeug (0.16.0); wheel (0.33.6); widgetsnbextension (3.5.1); wrapt (1.11.2); xkit (0.0.0); zope.interface (4.3.2)`. and my full code snippet is :; `In [1]: import spacy . In [2]: import scispacy . In [3]: from scispacy.umls_linking import UmlsEntityLinker . In [4]: nlp = spacy.load('en_core_sci_sm') . In [5]: linker = UmlsEntityLinker(resolve_abbreviations=True) ; fish: “ipython” terminated by signal SIGKILL (Forced quit)`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949:2553,test,testpath,2553,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949,1,['test'],['testpath']
Testability," not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, w",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:2447,benchmark,benchmark,2447,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['benchmark'],['benchmark']
Testability," not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; He",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:6953,benchmark,benchmark,6953,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['benchmark'],['benchmark']
Testability," satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.4). Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from requests<3.0.0,>=2.0.0->scispacy) (2022.9.24). Collecting urllib3<1.27,>=1.21.1. Using cached urllib3-1.26.12-py2.py3-none-any.whl (140 kB). Collecting charset-normalizer<3,>=2. Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB). Collecting idna<4,>=2.5. Using cached idna-3.4-py3-none-any.whl (61 kB). Collecting scipy>=1.3.2. Using cached scipy-1.9.3-cp39-cp39-macosx_12_0_arm64.whl (28.6 MB). Collecting threadpoolctl>=2.0.0. Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB). Collecting thinc<8.2.0,>=8.1.0. Using cached thinc-8.1.5-cp39-cp39-macosx_11_0_arm64.whl (694 kB). Collecting spacy-loggers<2.0.0,>=1.0.0. Using cached spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB). Collecting wasabi<1.1.0,>=0.9.1. Using cached wasabi-0.10.1-py3-none-any.whl (26 kB). Collecting packaging>=20.0. Using cached packaging-21.3-py3-none-any.whl (40 kB). Collecting tqdm<5.0.0,>=4.38.0. Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB). Collecting typer<0.5.0,>=0.3.0. Using cached typer-0.4.2-py3-none-any.whl (27 kB). Collecting murmurhash<1.1.0,>=0.28.0. Using cached murmurhash-1.0.9-cp39-cp39-macosx_11_0_arm64.whl (19 kB). Collecting jinja2. Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB). Requirement already satisfied: setuptools in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->scispacy) (65.5.0). Collecting preshed<3.1.0,>=3.0.2. Using cached preshed-3.0.8-cp39-cp39-macosx_11_0_arm64.whl (101 kB). Collecting pathy>=0.3.5. Using cached pathy-0.6.2-py3-none-any.whl (42 kB). Collecting langcodes<4.0.0,>=3.2.0. Using cached langcodes-3.3.0-",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:12150,log,loggers,12150,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['log'],['loggers']
Testability,.8.1 ; nose 1.3.7 ; notebook 5.7.4 ; numba 0.41.0 ; numexpr 2.6.8 ; numpy 1.15.4 ; numpydoc 0.8.0 ; odo 0.5.1 ; olefile 0.46 ; openpyxl 2.5.12 ; packaging 18.0 ; pandas 0.23.4 ; pandocfilters 1.4.2 ; parso 0.3.1 ; partd 0.3.9 ; path.py 11.5.0 ; pathlib2 2.3.3 ; patsy 0.5.1 ; pep8 1.7.1 ; pexpect 4.6.0 ; pickleshare 0.7.5 ; Pillow 5.3.0 ; pip 18.1 ; pkginfo 1.4.2 ; plac 0.9.6 ; pluggy 0.8.0 ; ply 3.11 ; preshed 3.0.2 ; prometheus-client 0.5.0 ; prompt-toolkit 2.0.7 ; protobuf 3.9.1 ; psutil 5.4.8 ; ptyprocess 0.6.0 ; py 1.7.0 ; pyasn1 0.4.7 ; pybind11 2.4.3 ; pycodestyle 2.4.0 ; pycosat 0.6.3 ; pycparser 2.19 ; pycrypto 2.6.1 ; pycurl 7.43.0.2 ; pyflakes 2.0.0 ; Pygments 2.3.1 ; pylint 2.2.2 ; pyodbc 4.0.25 ; pyOpenSSL 18.0.0 ; pyparsing 2.3.0 ; PySocks 1.6.8 ; pytest 4.0.2 ; pytest-arraydiff 0.3 ; pytest-astropy 0.5.0 ; pytest-doctestplus 0.2.0 ; pytest-openfiles 0.3.1 ; pytest-remotedata 0.3.1 ; python-dateutil 2.7.5 ; pytz 2018.7 ; PyWavelets 1.0.1 ; PyYAML 3.13 ; pyzmq 17.1.2 ; QtAwesome 0.5.3 ; qtconsole 4.4.3 ; QtPy 1.5.2 ; requests 2.21.0 ; rope 0.11.0 ; rsa 3.4.2 ; ruamel-yaml 0.15.46 ; s3transfer 0.2.1 ; scikit-image 0.14.1 ; scikit-learn 0.21.3 ; scipy 1.1.0 ; scispacy 0.2.3 ; seaborn 0.9.0 ; SecretStorage 3.1.0 ; Send2Trash 1.5.0 ; setuptools 40.6.3 ; simplegeneric 0.8.1 ; singledispatch 3.4.0.3 ; six 1.12.0 ; snowballstemmer 1.2.1 ; sortedcollections 1.0.1 ; sortedcontainers 2.1.0 ; spacy 2.2.1 ; Sphinx 1.8.2 ; sphinxcontrib-websupport 1.1.0 ; spyder 3.3.2 ; spyder-kernels 0.3.0 ; SQLAlchemy 1.2.15 ; srsly 0.1.0 ; statsmodels 0.9.0 ; sympy 1.3 ; tables 3.4.4 ; tblib 1.3.2 ; termcolor 1.1.0 ; terminado 0.8.1 ; testpath 0.4.2 ; thinc 7.1.1 ; thinc-gpu-ops 0.0.4 ; toolz 0.9.0 ; tornado 5.1.1 ; tqdm 4.28.1 ; traitlets 4.3.2 ; unicodecsv 0.14.1 ; urllib3 1.24.1 ; wasabi 0.2.2 ; wcwidth 0.1.7 ; webencodings 0.5.1 ; Werkzeug 0.14.1 ; wheel 0.32.3 ; widgetsnbextension 3.4.2 ; wrapt 1.11.2 ; wurlitzer 1.0.2 ; xlrd 1.2.0 ; XlsxWriter 1.1.2 ; xlwt 1.3.0 ; zict 0.1.3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868:3877,test,testpath,3877,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868,1,['test'],['testpath']
Testability,"15c5bc43b90462e753bc768e6798193c6520c9c7eb2054c7466779a9db/MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, pysbd, pybind11, psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14385,log,loggers-,14385,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['log'],['loggers-']
Testability,"> Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it!. We have both a requirement and capacity to work on this function, but may need some guidance on the spec. -Kate B., CDH (Databricks)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/388#issuecomment-2260935002:255,test,tests,255,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388#issuecomment-2260935002,1,['test'],['tests']
Testability,"> I have a feeling this is related to some combination of Windows and python version...but I'm not sure. I was able to load the entity linker on my windows machine just fine. Are you able to load other json files using `json.load`?. I agree on the problem of combination of windows and python. Yes. I tried a simple json file to test the `json.load`, it work. Finally, I change to Ubuntu environment, it work successfully. I think the there are some Unicode issues on windows, but I haven't figured out. Really thanks for your reply.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/227#issuecomment-629885815:329,test,test,329,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/227#issuecomment-629885815,1,['test'],['test']
Testability,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed.; I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:; JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST; the Spacy-2 correctly predicts all 3 entities above; whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents.; Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395:558,test,test,558,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395,1,['test'],['test']
Testability,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/284#issuecomment-718151109:458,log,logs,458,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284#issuecomment-718151109,2,"['benchmark', 'log']","['benchmark', 'logs']"
Testability,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/402#issuecomment-951214733:64,test,test,64,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402#issuecomment-951214733,1,['test'],['test']
Testability,"Hi Daniel,. I tried to install nmslib again in a new py 3.9 environment and it looks like it actually did install but there were issues. The entire log from the install is below. I also successfully installed scispacy from pip which is great! I included the install log for scispacy below the log for nmslib. Maybe I did something wrong before when I created the env for scispacy?. I will be testing it soon and will let you know if anything else comes up. Thank you for following up with me on this. Cheers, Brian. ### create env log ###. (base) ***@***.*** ~ % conda create -n scispacy python=3.9. Collecting package metadata (current_repodata.json): done. Solving environment: done. ## Package Plan ##. environment location: /Users/briang/opt/anaconda3/envs/scispacy. added / updated specs:. - python=3.9. The following NEW packages will be INSTALLED:. ca-certificates pkgs/main/osx-arm64::ca-certificates-2022.10.11-hca03da5_0 None. certifi pkgs/main/osx-arm64::certifi-2022.9.24-py39hca03da5_0 None. libcxx pkgs/main/osx-arm64::libcxx-14.0.6-h848a8c0_0 None. libffi pkgs/main/osx-arm64::libffi-3.4.2-hc377ac9_4 None. ncurses pkgs/main/osx-arm64::ncurses-6.3-h1a28f6b_3 None. openssl pkgs/main/osx-arm64::openssl-1.1.1s-h1a28f6b_0 None. pip pkgs/main/osx-arm64::pip-22.2.2-py39hca03da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:148,log,log,148,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,5,"['log', 'test']","['log', 'testing']"
Testability,"I'm coming. I run the follow command.; 1.`conda create --name vega_scispacy_2 python=3.9 -y`; 2.`conda activate vega_scispacy_2`; 3.`pip list`; 4.`pip install scispacy`; 5.`pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz`; 6.`vim demo_scispacy.py` and copy the demo code; 7.`cat demo_scispacy.py`; 8.`pip list|grep scispacy`; 9.`pip list|grep en_core_sci`; 10.`python -V`; 11.`python demo_scispacy.py`; 12.I Got Success result, Hey. 13.**But I don't know why the previous error, unbelieveable.**; . The all log are as following.; ```log; (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda create --name vega_scispacy_2 python=3.9 -y; Collecting package metadata (current_repodata.json): done; Solving environment: done. ==> WARNING: A newer version of conda exists. <==; current version: 4.9.2; latest version: 22.11.1. Please update conda by running. $ conda update -n base -c defaults conda. ## Package Plan ##. environment location: /home/zhangx/anaconda3/envs/vega_scispacy_2. added / updated specs:; - python=3.9. The following NEW packages will be INSTALLED:. _libgcc_mutex pkgs/main/linux-64::_libgcc_mutex-0.1-main; _openmp_mutex pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu; ca-certificates pkgs/main/linux-64::ca-certificates-2022.10.11-h06a4308_0; certifi pkgs/main/linux-64::certifi-2022.9.24-py39h06a4308_0; ld_impl_linux-64 pkgs/main/linux-64::ld_impl_linux-64-2.38-h1181459_1; libffi pkgs/main/linux-64::libffi-3.4.2-h6a678d5_6; libgcc-ng pkgs/main/linux-64::libgcc-ng-11.2.0-h1234567_1; libgomp pkgs/main/linux-64::libgomp-11.2.0-h1234567_1; libstdcxx-ng pkgs/main/linux-64::libstdcxx-ng-11.2.0-h1234567_1; ncurses pkgs/main/linux-64::ncurses-6.3-h5eee18b_3; openssl pkgs/main/linux-64::openssl-1.1.1s-h7f8727e_0; pip pkgs/main/linux-64::pip-22.3.1-py39h06a4308_0; python pkgs/main/linux-64::python-3.9.15-h7a1cb2a_2; readline pkgs/main/linux-64::readline-8.2-h5eee18b_0; setuptools pkg",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:566,log,log,566,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,2,['log'],['log']
Testability,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python; !mv /root/.scispacy/ /content/gdrive/MyDrive/test/; !ls /content/gdrive/MyDrive/test/.scispacy/; >>> datasets; ```. To update the environment variable, as described:. ```python; import os; os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'; ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python; nlp.add_pipe(; ""scispacy_linker"",; config={; ""resolve_abbreviations"": True,; ""linker_name"": ""umls"",; ""cache_folder"": ""/content/gdrive/MyDrive/test/""}); ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940:275,test,test,275,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940,5,['test'],['test']
Testability,"act code I'm using:. ```; import scispacy; import spacy; from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""); nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases according to malignanc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586:1121,test,tests,1121,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586,1,['test'],['tests']
Testability,"core-sci-sm==0.5.1) (3.0.8); Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.10.2); Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.0.10); Requirement already satisfied: jinja2 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.1.2); Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (3.3.0); Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.0.9); Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (1.0.4); Requirement already satisfied: setuptools in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (65.5.0); Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.0.8); Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.4.5); Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (6.3.0); Requirement already satisfied: typing-extensions>=4.1.0 in /home/zhangx/an",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:18039,log,loggers,18039,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['log'],['loggers']
Testability,"kB); Collecting blis<0.8.0,>=0.7.8; Downloading http://pypi.doubanio.com/packages/28/b6/e1cdfcf4ada40bef7c0511576231df20ac94a15baeb7ceaab2a180463268/blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 9.2 MB/s eta 0:00:00; Collecting click<9.0.0,>=7.1.1; Downloading http://pypi.doubanio.com/packages/c2/f1/df59e28c642d583f7dacffb1e0965d0e00b218e0186d7858ac5233dce840/click-8.1.3-py3-none-any.whl (96 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 7.3 MB/s eta 0:00:00; Collecting MarkupSafe>=2.0; Downloading http://pypi.doubanio.com/packages/df/06/c515c5bc43b90462e753bc768e6798193c6520c9c7eb2054c7466779a9db/MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, pysbd, pybind11, psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:13628,log,loggers,13628,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['log'],['loggers']
Testability,"l representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic con",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:2655,benchmark,benchmark,2655,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['benchmark'],['benchmark']
Testability,"l representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semanti",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:7161,benchmark,benchmark,7161,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['benchmark'],['benchmark']
Testability,"ne. ncurses pkgs/main/osx-arm64::ncurses-6.3-h1a28f6b_3 None. openssl pkgs/main/osx-arm64::openssl-1.1.1s-h1a28f6b_0 None. pip pkgs/main/osx-arm64::pip-22.2.2-py39hca03da5_0 None. python pkgs/main/osx-arm64::python-3.9.13-hbdb9e5c_2 None. readline pkgs/main/osx-arm64::readline-8.2-h1a28f6b_0 None. setuptools pkgs/main/osx-arm64::setuptools-65.5.0-py39hca03da5_0 None. sqlite pkgs/main/osx-arm64::sqlite-3.39.3-h1058600_0 None. tk pkgs/main/osx-arm64::tk-8.6.12-hb8d0fd4_0 None. tzdata pkgs/main/noarch::tzdata-2022f-h04d1e81_0 None. wheel pkgs/main/noarch::wheel-0.37.1-pyhd3eb1b0_0 None. xz pkgs/main/osx-arm64::xz-5.2.6-h1a28f6b_0 None. zlib pkgs/main/osx-arm64::zlib-1.2.13-h5a0b063_0 None. Proceed ([y]/n)? y. Preparing transaction: done. Verifying transaction: done. Executing transaction: done. #. # To activate this environment, use. #. # $ conda activate scispacy. #. # To deactivate an active environment, use. #. # $ conda deactivate. Retrieving notices: ...working... done. ### install nmslib log ###. (base) ***@***.*** ~ % conda activate scispacy. (scispacy) ***@***.*** ~ % CFLAGS=""-mavx -DWARN(a)=(a)"" pip install nmslib. Collecting nmslib. Using cached nmslib-2.1.1.tar.gz (188 kB). Preparing metadata (setup.py) ... done. Collecting pybind11<2.6.2. Using cached pybind11-2.6.1-py2.py3-none-any.whl (188 kB). Collecting psutil. Using cached psutil-5.9.4-cp38-abi3-macosx_11_0_arm64.whl (244 kB). Collecting numpy>=1.10.0. Using cached numpy-1.23.4-cp39-cp39-macosx_11_0_arm64.whl (13.4 MB). Building wheels for collected packages: nmslib. Building wheel for nmslib (setup.py) ... error. error: subprocess-exited-with-error. × python setup.py bdist_wheel did not run successfully. │ exit code: 1. ╰─> [33 lines of output]. Dependence list: ['pybind11<2.6.2', 'psutil', ""numpy>=1.10.0,<1.17 ; python_version=='2.7'"", ""numpy>=1.10.0 ; python_version>='3.5'""]. /Users/briang/opt/anaconda3/envs/scispacy/lib/python3.9/site-packages/setuptools/dist.py:771: UserWarning: Usage of dash-sepa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:2124,log,log,2124,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['log'],['log']
Testability,"rivate/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10225,log,log,10225,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['log'],['log']
Testability,"xtensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error using a conda env on mac m1 (Issue #455). What was the error you got?. —; Reply to this email directly, view it on GitHub<https://github.com/allenai/scispacy/issues/455#issuecomment-1309570650>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIIB7TJZCTFHCICGZWG47KTWHQ4WZANCNFSM6AAAAAARYG27ME>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:15144,log,loggers-,15144,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['log'],['loggers-']
Usability," In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:5092,learn,learn,5092,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability," abstracts. Here's an example. . en_core_sci_md:. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging fr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1479,learn,learning,1479,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learning']
Usability," cached catalogue-2.0.8-py3-none-any.whl (17 kB). Collecting srsly<3.0.0,>=2.4.3. Using cached srsly-2.4.5-cp39-cp39-macosx_11_0_arm64.whl (489 kB). Collecting pyparsing!=3.0.5,>=2.0.2. Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB). Collecting smart-open<6.0.0,>=5.2.1. Using cached smart_open-5.2.1-py3-none-any.whl (58 kB). Collecting typing-extensions>=4.1.0. Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error us",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:14630,learn,learn,14630,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['learn'],['learn']
Usability," language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-leve",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:5544,learn,learn,5544,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability, murmurhash (1.0.2); nbconvert (5.6.0); nbformat (4.4.0); netifaces (0.10.4); nmslib (1.8.1); notebook (6.0.1); numpy (1.17.2); oauth (1.0.1); olefile (0.45.1); pandocfilters (1.4.2); parso (0.5.1); pbr (3.1.1); pexpect (4.7.0); pickleshare (0.7.5); Pillow (6.1.0); pip (9.0.1); plac (0.9.6); preshed (3.0.2); prometheus-client (0.7.1); prompt-toolkit (2.0.9); protobuf (3.9.2); ptyprocess (0.6.0); pyasn1 (0.4.7); pybind11 (2.4.2); pycairo (1.16.2); pycrypto (2.6.1); pycups (1.9.73); Pygments (2.4.2); pygobject (3.26.1); pymacaroons (0.13.0); PyNaCl (1.1.2); pyRFC3339 (1.0); pyrsistent (0.15.4); python-apt (1.6.4); python-dateutil (2.8.0); python-debian (0.1.32); pytz (2018.3); pyxdg (0.25); PyYAML (5.1.2); pyzmq (18.1.0); qtconsole (4.5.5); reportlab (3.4.0); requests (2.22.0); requests-unixsocket (0.1.5); rsa (3.4.2); s3transfer (0.2.1); scikit-learn (0.21.3); scipy (1.3.1); scispacy (0.2.3); screen-resolution-extra (0.0.0); SecretStorage (2.3.1); Send2Trash (1.5.0); setuptools (41.2.0); simplegeneric (0.8.1); simplejson (3.13.2); six (1.12.0); spacy (2.1.8); srsly (0.1.0); system-service (0.3); systemd-python (234); tensorboard (1.14.0); tensorflow (1.14.0); tensorflow-estimator (1.14.0); tensorflow-gpu (1.14.0); termcolor (1.1.0); terminado (0.8.2); testpath (0.4.2); thinc (7.1.1); torch (1.2.0); torchvision (0.4.0); tornado (6.0.3); tqdm (4.36.1); traitlets (4.3.2); ubuntu-drivers-common (0.0.0); ufw (0.36); unattended-upgrades (0.1); urllib3 (1.25.6); usb-creator (0.3.3); wadllib (1.3.2); wasabi (0.2.2); wcwidth (0.1.7); webencodings (0.5.1); Werkzeug (0.16.0); wheel (0.33.6); widgetsnbextension (3.5.1); wrapt (1.11.2); xkit (0.0.0); zope.interface (4.3.2)`. and my full code snippet is :; `In [1]: import spacy . In [2]: import scispacy . In [3]: from scispacy.umls_linking import UmlsEntityLinker . In [4]: nlp = spacy.load('en_core_sci_sm') . In [5]: linker = UmlsEntityLinker(resolve_abbreviations=True) ; fish: “ipython” terminated by signal SIGKILL (Forced quit)`,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949:2284,simpl,simplegeneric,2284,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949,2,['simpl'],"['simplegeneric', 'simplejson']"
Usability,"(setup.py): started; Building wheel for termcolor (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6; Building wheel for PyYAML (setup.py): started; Building wheel for PyYAML (setup.py): finished with status 'done'; Stored in directory: /tmp/pip-ephem-wheel-cache-097rpwoy/wheels/ad/56/bc/1522f864feb2a358ea6f1a92b4798d69ac783a28e80567a18b; Successfully built networkx obonet nltk zc.lockfile configobj future pathspec treelib shortuuid nanotime humanize smart-open wrapt absl-py gast termcolor PyYAML; Failed to build nmslib; ERROR: awscli 1.16.209 has requirement colorama<=0.3.9,>=0.2.5, but you'll have colorama 0.4.1 which is incompatible.; Installing collected packages: decorator, networkx, ruamel.yaml, zc.lockfile, configobj, future, colorama, pyfiglet, wcwidth, Pillow, asciimatics, ply, jsonpath-ng, configparser, contextlib2, schema, pathspec, treelib, appdirs, pyasn1, shortuuid, nanotime, humanize, smmap2, gitdb2, gitpython, funcy, distro, grandalf, inflect, dvc, obonet, xlrd, python-dateutil, numpy, pandas, scipy, cycler, kiwisolver, matplotlib, seaborn, tqdm, boto, docutils, jmespath, botocore, s3transfer, boto3, smart-open, gensim, cymem, preshed, plac, murmurhash, wasabi, blis, srsly, thinc, spacy, rsa, PyYAML, awscli, conllu, pybind11, nmslib, joblib, scikit-learn, scispacy, nltk, keras-preprocessing, astor, protobuf, wrapt, absl-py, gast, markdown, grpcio, tb-nightly, tf-estimator-nightly, h5py, keras-applications, termcolor, google-pasta, tensorflow-gpu; Found existing installation: docutils 0.15.2; Uninstalling docutils-0.15.2:; Successfully uninstalled docutils-0.15.2; Running setup.py install for nmslib: started; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: still running...; Running setup.py install for nmslib: finished with status 'done'. ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215:4393,learn,learn,4393,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-517332215,1,['learn'],['learn']
Usability,", catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (22.0); Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (0.7.0); Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.1->en-core-sci-sm==0.5.1) (2.28.1); Requirement already satis",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14770,simpl,simple,14770,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['simpl'],['simple']
Usability,".0; Using cached spacy-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB); Requirement already satisfied: conllu in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (4.4.2); Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.15.1); Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.22.4); Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.1); Requirement already satisfied: pybind11<2.6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1); Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0); Requirement already satisfied: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (1.8.1); Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (8.0.17); Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.3.2); Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.8.2); Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (4.62.3); Requirement already satisfied: blis<0.8.0,>",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:1934,learn,learn,1934,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,1,['learn'],['learn']
Usability,.8.1 ; nose 1.3.7 ; notebook 5.7.4 ; numba 0.41.0 ; numexpr 2.6.8 ; numpy 1.15.4 ; numpydoc 0.8.0 ; odo 0.5.1 ; olefile 0.46 ; openpyxl 2.5.12 ; packaging 18.0 ; pandas 0.23.4 ; pandocfilters 1.4.2 ; parso 0.3.1 ; partd 0.3.9 ; path.py 11.5.0 ; pathlib2 2.3.3 ; patsy 0.5.1 ; pep8 1.7.1 ; pexpect 4.6.0 ; pickleshare 0.7.5 ; Pillow 5.3.0 ; pip 18.1 ; pkginfo 1.4.2 ; plac 0.9.6 ; pluggy 0.8.0 ; ply 3.11 ; preshed 3.0.2 ; prometheus-client 0.5.0 ; prompt-toolkit 2.0.7 ; protobuf 3.9.1 ; psutil 5.4.8 ; ptyprocess 0.6.0 ; py 1.7.0 ; pyasn1 0.4.7 ; pybind11 2.4.3 ; pycodestyle 2.4.0 ; pycosat 0.6.3 ; pycparser 2.19 ; pycrypto 2.6.1 ; pycurl 7.43.0.2 ; pyflakes 2.0.0 ; Pygments 2.3.1 ; pylint 2.2.2 ; pyodbc 4.0.25 ; pyOpenSSL 18.0.0 ; pyparsing 2.3.0 ; PySocks 1.6.8 ; pytest 4.0.2 ; pytest-arraydiff 0.3 ; pytest-astropy 0.5.0 ; pytest-doctestplus 0.2.0 ; pytest-openfiles 0.3.1 ; pytest-remotedata 0.3.1 ; python-dateutil 2.7.5 ; pytz 2018.7 ; PyWavelets 1.0.1 ; PyYAML 3.13 ; pyzmq 17.1.2 ; QtAwesome 0.5.3 ; qtconsole 4.4.3 ; QtPy 1.5.2 ; requests 2.21.0 ; rope 0.11.0 ; rsa 3.4.2 ; ruamel-yaml 0.15.46 ; s3transfer 0.2.1 ; scikit-image 0.14.1 ; scikit-learn 0.21.3 ; scipy 1.1.0 ; scispacy 0.2.3 ; seaborn 0.9.0 ; SecretStorage 3.1.0 ; Send2Trash 1.5.0 ; setuptools 40.6.3 ; simplegeneric 0.8.1 ; singledispatch 3.4.0.3 ; six 1.12.0 ; snowballstemmer 1.2.1 ; sortedcollections 1.0.1 ; sortedcontainers 2.1.0 ; spacy 2.2.1 ; Sphinx 1.8.2 ; sphinxcontrib-websupport 1.1.0 ; spyder 3.3.2 ; spyder-kernels 0.3.0 ; SQLAlchemy 1.2.15 ; srsly 0.1.0 ; statsmodels 0.9.0 ; sympy 1.3 ; tables 3.4.4 ; tblib 1.3.2 ; termcolor 1.1.0 ; terminado 0.8.1 ; testpath 0.4.2 ; thinc 7.1.1 ; thinc-gpu-ops 0.0.4 ; toolz 0.9.0 ; tornado 5.1.1 ; tqdm 4.28.1 ; traitlets 4.3.2 ; unicodecsv 0.14.1 ; urllib3 1.24.1 ; wasabi 0.2.2 ; wcwidth 0.1.7 ; webencodings 0.5.1 ; Werkzeug 0.14.1 ; wheel 0.32.3 ; widgetsnbextension 3.4.2 ; wrapt 1.11.2 ; wurlitzer 1.0.2 ; xlrd 1.2.0 ; XlsxWriter 1.1.2 ; xlwt 1.3.0 ; zict 0.1.3,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868:3388,learn,learn,3388,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/179#issuecomment-547827868,2,"['learn', 'simpl']","['learn', 'simplegeneric']"
Usability,"/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by t",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:4152,learn,learning,4152,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learning']
Usability,"1) The version on the demo is probably not the latest release version. I should check and update that.; 2/3/4) First, this is a model, so inconsistent and surprising output is likely, and some memorization is likely (@DeNeutoy looks like data augmentation could help a lot here). Second, the BC5CDR corpus was annotated with specific guidelines (https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf) which you may want to read and see if they align with your expectations of what would be annotated as a chemical. Here is some output of a mix of real and made up chemical names. I don't really conclude anything from this, other than that the model is definitely using some combination of the form of the name itself and the context; ```; In [29]: for drug_name in [""mesna"", ""remdesivir"", ""mebane"", ""relidate"", ""novila"", ""aspirin"", ""coloxal"", ""inovivir"", ""scopolamine"", ""entamine"", ""valimine"", ""henirin"", ""noonirin"", ""halirin""]:; ...: text = f""The drug {drug_name} is used to treat the virus""; ...: doc = nlp(text); ...: print(doc.ents); ...: ; (mesna,); (); (mebane,); (); (); (aspirin,); (); (); (scopolamine,); (entamine,); (valimine,); (henirin,); (); (); ```. Looks like it is also sensitive to capitalization; ```; In [56]: doc = nlp(""Remdesivir is a chemical""); In [57]: doc.ents; Out[57]: (Remdesivir,). In [58]: doc = nlp(""remdesivir is a chemical""). In [59]: doc.ents; Out[59]: (); ```. I don't have much else to add at the moment. We were thinking about running some data augmentation experiments to try to improve the NER, but haven't done it yet (I'd be thrilled to have a contribution along those lines).; 5) Definitely the model takes into account the context that the word occurs in, so it is not wholly surprising to me that the same word could be classified differently in different contexts.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/336#issuecomment-800691659:334,guid,guidelines,334,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/336#issuecomment-800691659,1,['guid'],['guidelines']
Usability,"> Hi, I think there are others that would like to have this function as well, but I will likely not have time to work on it in the near future. I would welcome a contribution with this function though, if you would be interested in creating a PR and some tests for it!. We have both a requirement and capacity to work on this function, but may need some guidance on the spec. -Kate B., CDH (Databricks)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/388#issuecomment-2260935002:354,guid,guidance,354,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388#issuecomment-2260935002,1,['guid'],['guidance']
Usability,"> I have a feeling this is related to some combination of Windows and python version...but I'm not sure. I was able to load the entity linker on my windows machine just fine. Are you able to load other json files using `json.load`?. I agree on the problem of combination of windows and python. Yes. I tried a simple json file to test the `json.load`, it work. Finally, I change to Ubuntu environment, it work successfully. I think the there are some Unicode issues on windows, but I haven't figured out. Really thanks for your reply.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/227#issuecomment-629885815:309,simpl,simple,309,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/227#issuecomment-629885815,1,['simpl'],['simple']
Usability,"@DeNeutoy This is the exact code I'm using:. ```; import scispacy; import spacy; from spacy import displacy. # nlp = spacy.load(""en_ner_jnlpba_md""); nlp = spacy.load(""en_core_sci_md""). text = """"""The purpose of our study was to learn the distribution characteristics of cancer stem cell markers (CD24, CD44) in invasive carcinomas with different grade and molecular subtype. For research was used 1324 postoperative breast cancer samples, from which were selected 393 patient with invasive ductal carcinoma samples examined 2008-2012 in Laboratory of ""Pathgeo Union of Pathologist"" is and N.Kipshidze Central University Hospital. The age range is between 23-73 year. For all cases were performed immunohistochemical study using ER, PR, Her2, Ki67, CK5- molecular markers (Leica Microsystems). For identify cancer stem cells mononuclear antibodies CD24 (BIOCARE MEDICAL, CD44 - Clone 156-3C11; CD24 - Clone SN3b) were used. Association of CD44/CD24 expression in different subtypes of cells, between clinicopathological parameters and different biological characteristics were performed by Pearson correlation and usind X2 tests. Obtained quantitative statistical analyses were performed by using SPSS V.19.0 program. Statistically significant were considered 95% of confidence interval. The data shows, that towards G1-G3, amount of CD44 positive cases increased twice. CD44 positive cases are evenly distributed between Luminal A, Luminal B, HER2+, triple negative basal like cell subtypes and in significantly less (4,8 times) in Her2+ cases. Maximum amount of CD44 negative cases is shown in Luminal A subtype, which could be possible cause of better prognosis and high sensitivity for chemotherapy. For one's part such aggressive subtypes of breast cancer as Luminal B and basal like cell type, are characterized by CD44 positive and antigen high expression, which can be reason of aggressive nature of this types and also reason of chemotherapy resistance. As well as amount of CD24 positive cases",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586:227,learn,learn,227,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/141#issuecomment-518274586,1,['learn'],['learn']
Usability,"@dakinggg Hi, thanks for the guidance but I am still struggling to find the particular repo you mentioned using windows subsystem. Could you please provide me a link ?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/487#issuecomment-1628438150:29,guid,guidance,29,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/487#issuecomment-1628438150,1,['guid'],['guidance']
Usability,"@danielkingai2 , thank you for the feedback. This advice works for me!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/351#issuecomment-844219051:35,feedback,feedback,35,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/351#issuecomment-844219051,1,['feedback'],['feedback']
Usability,"Ah! The Abbreviation detector detects abbreviations defined in text, but does not try to disambiguate arbitrary abbreviations. So in a document, you had `blah blah... computed tomography angiography (CTA) was used ... After, CTA was done.`, the abbreviation detector would find the definition and connect it to its uses in the document. Hopefully that's clearer?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/189#issuecomment-570049396:354,clear,clearer,354,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/189#issuecomment-570049396,1,['clear'],['clearer']
Usability,"As an original author of explosion/spaCy#8138 (which has been closed), I **still** keep trying to figure out what has changed.; I have a case where the 'accuracy' in the downstream application has dropped over **20%**, despite Spacy training validation scores dropping less than 5%. There is a clear, consistent case where for my triplet of entities such as:; JOHN BROWN and JANE BROWN as trustees of JOHN AND JANE FAMILY TRUST; the Spacy-2 correctly predicts all 3 entities above; whereas Spacy-3 only predicts the first one (JANE BROWN) in 200 out of 1000 test documents.; Honnibal suggested there was some change in 'dropping entities' that can not be predicted, and perhaps that change is doing more than envisioned. I am trying to see if I can reproduce the same behavior using other data sets.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395:294,clear,clear,294,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342#issuecomment-886833395,1,['clear'],['clear']
Usability,"By default strings are converted into lowercase by TfidfVectorizer. https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html; ```; lowercase : bool, default=True; Convert all characters to lowercase before tokenizing.; ```. So isn't it that when removing duplicate aliases, we should ignore the case?; In that case, in the example mentioned by @ChantalvanSon ; `'NIVOLUMAB', 'nivolumab', 'Nivolumab'`; becomes same?; So this can lead to further reduction in size of concept_aliases.json. TfIdf vectorizer is called at; https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L410; ```; tfidf_vectorizer = TfidfVectorizer(; analyzer=""char_wb"", ngram_range=(3, 3), min_df=10, dtype=numpy.float32; ); ```; which means we are using the default value for the parameter `lowercase`. ### A question:; @DeNeutoy @danielkingai2; As we change the list of concept aliases, it would also change the vector representation of these concept aliases since the document frequency of the char trigram vocabulary also changes.; Isn't that going to impact the similarity score of entity candidate with the concept aliases?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/252#issuecomment-778171062:83,learn,learn,83,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/252#issuecomment-778171062,1,['learn'],['learn']
Usability,Closing due to no clear direction forward...,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/473#issuecomment-1572468225:18,clear,clear,18,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/473#issuecomment-1572468225,1,['clear'],['clear']
Usability,"Got it. Thank you. On Thu, Feb 28, 2019 at 11:07 PM Daniel King <notifications@github.com>; wrote:. > The annotation guidelines for the BC5CDR data (which that model was; > trained on can) can be found here: https://www.nlm.nih.gov/mesh/trees.html.; > and that document says that a chemical is defined as the Drugs and; > Chemicals [D] branch of Mesh 2015, which can be found here:; > https://www.nlm.nih.gov/mesh/trees.html; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/allenai/scispacy/issues/79#issuecomment-468365573>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AIJA8Jtk66RUle1ioW0x66AetjWsNIxHks5vSBPhgaJpZM4bURNb>; > .; >",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/79#issuecomment-468376081:117,guid,guidelines,117,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79#issuecomment-468376081,1,['guid'],['guidelines']
Usability,"Great - also please let us know any feedback you have when using the models, or additional features you'd love. . I'm not sure what task you are working on (entity linking maybe?) but you might also be interested in the knowledge base we have curated from UMLS - it is quite large and covers 99.9% of the entities which occur in the MedMentions annotations. You can read more about it here:; https://github.com/allenai/scispacy#umlsentitylinker-alpha-feature",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/126#issuecomment-504686539:36,feedback,feedback,36,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126#issuecomment-504686539,1,['feedback'],['feedback']
Usability,"Hello,. I just simply ran the following commands:. ```; pip install spacy; spacy evaluate en_core_sci_sm /path/to/data; spacy evaluate en_core_sci_md /path/to/data; ```. `en_core_sci_sm`, `en_core_sci_md` and `/path/to/data` are all officially provided by your repo. `spacy` version is 2.1.6",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/140#issuecomment-518931086:15,simpl,simply,15,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/140#issuecomment-518931086,1,['simpl'],['simply']
Usability,"Hi @ChantalvanSon ,. Great that you got it working, sorry it was a bit tricky. You raise some good points - there is one way that you can not require your function which loads all the pieces which is this:. ```python; from scispacy.candidate_generation import DEFAULT_PATHS, DEFAULT_KNOWLEDGE_BASES; from scispacy.candidate_generation import (; CandidateGenerator,; LinkerPaths; ). CustomLinkerPaths_2020AA = LinkerPaths(; ann_index=""path/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ); class UMLS2020KnowledgeBase(KnowledgeBase):; def __init__(; self,; file_path: str = ""path/to/2020AA.json"",; ):; super().__init__(file_path). # Admittedly this is a bit of a hack, because we are mutating a global object.; # However, it's just a kind of registry, so maybe it's ok.; DEFAULT_PATHS[""umls2020""] = CustomLinkerPaths_2020AA; DEFAULT_KNOWLEDGE_BASES[""umls2020""] = UMLS2020KnowledgeBase. linker = CandidateGenerator(name=""umls2020""). ```. Overall, we have it like this so that we can present the simplest possible interface to people who are using scispacy (i.e being able to just pass names to get particular linkers rather than having to know the internals of how the linker is implemented). However I definitely see your point that we should try to make this a bit nicer. In another project I used to work on, we had the concept of using a decorator to register this type of info with the base class, so it can construct itself. That might be a bit of overkill here, but maybe we could provide a function which does this global mutation for you and throws intelligent errors if you e.g try to overwrite something in there? . I think you're right that we need to fix this if we want people to frequently be able to create their own very specific/custom linkers though so thanks for raising it!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169:1120,simpl,simplest,1120,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-651504169,1,['simpl'],['simplest']
Usability,"Hi @JohnGiorgi ,. This is caused by a couple of things:. 1. In your colab, the instance is not installing the right nmslib package. The entity linker uses nmslib, an approximate nearest neighbour library to do sparse nearest neighbour search over tf-idf vectors for entities. I don't entirely understand why but colab is not installing a version of nmslib which is compiled to use the features of the CPU that the colab clearly has. If you look in the colab logs, you will see this:. `Your CPU supports instructions that this binary was not compiled to use: SSE3 SSE4.1 SSE4.2 AVX AVX2`. When I run that benchmark on my macbook, I get a 2x (3.1s) slowdown from using the linker, not an 8x slowdown, but it is correctly using all the instruction sets apart from AVX2. 2. The Entity linker does unfortunately use a lot of memory, because of the search. 3. UMLS is an extremely big KB - 2.3M concepts. We have other ones which are much, much smaller, and higher precision. E.g the `mesh` linker only has around 30k entities and is much cleaner. Using MESH, the runtime (without changing the EFS paramter, see below) is 2.41s. 4. The Candidate Generator which the entity linker uses has a parameter which controls the speed/precision trade off for the approximate nearest neighbours search (the numbers here are measuring ANN queries per second). ![image](https://user-images.githubusercontent.com/16001974/97484674-4acf3300-1916-11eb-924e-12941df0fd6e.png). By default, we set this parameter to `200`. You can change this value to speed up the search substantially, for a small ish cost in recall:. https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L204. Let me know if that's helpful!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/284#issuecomment-718151109:420,clear,clearly,420,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/284#issuecomment-718151109,1,['clear'],['clearly']
Usability,"Hi @MichalMalyska, thank you for your reply! ideally we want to test using the same model. I there any computation that happens during loading we could cache? Or is the duration simply caused by loading the weights?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/402#issuecomment-951214733:178,simpl,simply,178,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402#issuecomment-951214733,1,['simpl'],['simply']
Usability,"Hi @dakinggg, files are effectively cached, so it is simply about loading the UMLS index. ; @MichalMalyska, yes, this is approximately what I get (profiling output in the opening post). . The profiler shows that most of the time is spent decoding `json` objects:; ```; ncalls tottime percall cumtime percall filename:lineno(function); 3359672 16.912 0.000 16.912 0.000 .../python3.8/json/decoder.py:343(raw_decode); ```; I am wondering if there is a more efficient way to store, load and query the data. Furthermore, the current solution is very memory intensive (RAM usage spikes at 8GB RAM when running the above example). Two ideas for improvement are:; 1. `pyarrow` to store the alias list ; 2. `faiss` to improve upon the current nearest neighbour search (at least in terms of speed)?. Those are only suggestion as I don't know enough about the inner working of `scipacy`. Regarding my project, this issue is not critical, but that might be a nice improvement for the library.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/402#issuecomment-952661338:53,simpl,simply,53,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/402#issuecomment-952661338,1,['simpl'],['simply']
Usability,"Hi @rshah1990 , @fcggamou ; This function trains a linker:; https://github.com/allenai/scispacy/blob/master/scispacy/candidate_generation.py#L325. which takes a `KnowledgeBase`: https://github.com/allenai/scispacy/blob/master/scispacy/linking_utils.py#L45. which reads JSON/JSONL with the following simple format:. ```; # Json per entity you have:; {; ""concept_id"": ""The ID for the concept"",; ""canonical_name"": ""MyEntity"",; ""aliases"": [""List of alternative ways to refer to the entity""],; ""definition"": ""Longer form def of entity"", # optional; ""types"": [""The type of the entity""] # optional; }. ```. Also note that this will only work if you work off of the master branch.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/234#issuecomment-640720518:299,simpl,simple,299,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/234#issuecomment-640720518,1,['simpl'],['simple']
Usability,"Hi @ulc0 I think the original issue is a reasonable description! Are there any particular areas you are looking for guidance on? If you'd like to propose a design, I'd be happy to take a look here.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/388#issuecomment-2282944344:116,guid,guidance,116,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/388#issuecomment-2282944344,1,['guid'],['guidance']
Usability,"Hi Dan,; Thanks for the input. I am working on a medical use case and i need some inputs :; 1. Is there a licence needed to use Scispacy and Spacy in our application in production to be hosted on AWS ?; 2. We are building a NLP pipeline in which last step is to put the medical abstract text into LDA for unsupervised clustering in order to find most relevant medical abstract based on medical search terms I have following issue ..i am unable to figure out how to pass the 'Doc' data structure to LDA algorithm and also do you have a medical LDA ..as current LDA are based out of english non medical text learning nning",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/143#issuecomment-620607287:606,learn,learning,606,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/143#issuecomment-620607287,1,['learn'],['learning']
Usability,"Hi Ravina,; Thank you for the information. I am trying to extract drug name and disease; name from a sentence. So which model will be best fit for this and also can; you give me a clearity what GGP and CL entity mean in medical terms.; Thank you so much for your help.; -Regards,; Sujeet. On Wed, Feb 27, 2019 at 4:48 PM Ravina More <notifications@github.com>; wrote:. > import spacy; > nlp = spacy.load(""en_ner_craft_md""); > text = ""Myeloid derived suppressor cells (MDSC) are immature myeloid cells; > with immunosuppressive activity. ""; > ""They accumulate in tumor-bearing mice and humans with different types of; > cancer, including hepatocellular carcinoma (HCC).""; > doc = nlp(text); > for ent in doc.ents:; > print(ent, ent.label_); >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/allenai/scispacy/issues/79#issuecomment-467824413>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AIJA8MEMvXQs4z6EF6NlPbRZzEI34sg0ks5vRmmbgaJpZM4bURNb>; > .; >",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/79#issuecomment-467847381:180,clear,clearity,180,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79#issuecomment-467847381,1,['clear'],['clearity']
Usability,"Hi, @vgainullin,. You can simply add the full url to the model to your requirements.txt file. e.g. `https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz; `",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/224#issuecomment-624936824:26,simpl,simply,26,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/224#issuecomment-624936824,1,['simpl'],['simply']
Usability,"Hi, I'm not exactly sure what the question is, but generally speaking, these are imperfect machine learning models, and will make mistakes.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/501#issuecomment-1859007832:99,learn,learning,99,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/501#issuecomment-1859007832,1,['learn'],['learning']
Usability,"I am running the following commands in macOS v12.2.1 and python 3.9.10; ```; python; Python 3.9.10 (main, Jan 15 2022, 11:48:04) ; [Clang 13.0.0 (clang-1300.0.29.3)] on darwin. ```. ```; python -m pip install --upgrade pip; pip install spacy; pip install spacy-transformers; pip install scispacy; ```. When executing ; `pip install scispacy ` I get the following error:; ```; Installing collected packages: threadpoolctl, scipy, pysbd, pybind11, psutil, conllu, click, typer, scikit-learn, nmslib, spacy, scispacy; Attempting uninstall: click; Found existing installation: click 8.0.4; Uninstalling click-8.0.4:; Successfully uninstalled click-8.0.4; Attempting uninstall: typer; Found existing installation: typer 0.4.0; Uninstalling typer-0.4.0:; Successfully uninstalled typer-0.4.0; Attempting uninstall: spacy; Found existing installation: spacy 3.2.2; Uninstalling spacy-3.2.2:; Successfully uninstalled spacy-3.2.2; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; spacy-transformers 1.1.4 requires spacy<4.0.0,>=3.1.3, but you have spacy 3.0.7 which is incompatible.; Successfully installed click-7.1.2 conllu-4.4.1 nmslib-2.1.1 psutil-5.9.0 pybind11-2.6.1 pysbd-0.3.4 scikit-learn-1.0.2 scipy-1.8.0 scispacy-0.4.0 spacy-3.0.7 threadpoolctl-3.1.0 typer-0.3.2. ```. Following your advice I run successfully the SciSpacy example provided at https://allenai.github.io/scispacy/. Thanks. Achilleas",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018:483,learn,learn,483,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/417#issuecomment-1046103018,2,['learn'],"['learn', 'learn-']"
Usability,"I did my best to match everything to the old versions, and our reported accuracy didn't drop much I don't think, but there are a bunch of hyperparams that we haven't really done any search over, just tried to use whatever spacy is using. If you wanted to play around with retraining with different hyperparameters or something, all the training scripts should be clear from project.yml",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/342#issuecomment-885233934:363,clear,clear,363,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/342#issuecomment-885233934,1,['clear'],['clear']
Usability,"I ideally wanted to include scispacy as a dependency of a package for more novice programmers to have some simple access to biomedical NER and using WSL and/or navigating dependency (python, scispacy, etc) versions seems like mental overhead I want to avoid. Is there a way this model could be re-trained using spacy's new entity linker itself? Could that accomplish the same NEL while benefiting from scispacy's models?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663:107,simpl,simple,107,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/473#issuecomment-1542392663,1,['simpl'],['simple']
Usability,"I'd be willing to do this and submit a PR for it. Not sure if it as simple as running `scripts/create_linker.py` on the MRCONSO.rrf file or if I'd need to download the entire UMLS and run `scripts/export_umls_json.py`. Also not sure if I could include the data for those files in the PR due to size or if I'd need to retrain and publish the models themselves which I am sure I don't have permissions for... I think going forward making this process as simple as possible should be a requirement so no matter your load users can easily update the primary (UMLS) knowledge base to keep it up to date. The first paragraph here raises a general question I had, is the UMLS data used only for the NER or is it a larger part of the model? I.e. if I created my own EntityLinker using 2022AB UMLS, would that solve this ""outdated"" issue?",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227:68,simpl,simple,68,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/460#issuecomment-1494600227,2,['simpl'],['simple']
Usability,"I'm using scispacy mainly for sbd, and yes my tasks are Entity typing and linking.; I'll try that, thanks!. BTW, In trying sbd with `en_core_sci_md`, scispacy performs well.; However, there's some minor tokenization problem and if custom rules are added, it can be prevented.; https://gist.github.com/izuna385/512a9c62868c751a8290a9676f994d16; (Maybe this isn't scispacy's problem.). > also please let us know any feedback you have when using the models, or additional features you'd love. Of course I will. (Whether by e−mail or issue depends on the situation.)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/126#issuecomment-504710956:414,feedback,feedback,414,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/126#issuecomment-504710956,1,['feedback'],['feedback']
Usability,If you don't see any immediate problems I could try NearestNeighbors from scikit learn? I know that is frequently used with tfidf vectors. Appears one of their metrics is cosine distance as well. . Edited NearestNeighbors,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/pull/481#issuecomment-1548211369:81,learn,learn,81,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/pull/481#issuecomment-1548211369,1,['learn'],['learn']
Usability,"Just to be clear, the ""vectors"" are word2vec vectors trained on pubmed abstracts",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/387#issuecomment-894378110:11,clear,clear,11,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/387#issuecomment-894378110,1,['clear'],['clear']
Usability,"Makes sense. So it seems to pretty much be working with a bit of a workaround. The files are initially cached to `/root/.scispacy/datasets/`. After caching, move the cache folder to a permanent folder on Google drive:. ```python; !mv /root/.scispacy/ /content/gdrive/MyDrive/test/; !ls /content/gdrive/MyDrive/test/.scispacy/; >>> datasets; ```. To update the environment variable, as described:. ```python; import os; os.environ['SCISPACY_CACHE'] = '/content/gdrive/MyDrive/test/.scispacy/'; ```. However, this alone does not find the cached files. It will re-download the files again. In order to see the new environment variable, it's necessary to restart the runtime: `Runtime->Restart runtime`. Now when running the entity linker, it will see the _permanently_ cached files. So is an enhancement necessary? It'd definitely be easier and more foolproof to simply add a parameter such as `cache_folder` to the `nlp.add_pipe()` method. For example:. ```python; nlp.add_pipe(; ""scispacy_linker"",; config={; ""resolve_abbreviations"": True,; ""linker_name"": ""umls"",; ""cache_folder"": ""/content/gdrive/MyDrive/test/""}); ```. which would then be used to look for a subfolder `.scispacy`, i.e. `/content/gdrive/MyDrive/test/.scispacy/` in this case.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940:860,simpl,simply,860,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/415#issuecomment-1023716940,1,['simpl'],['simply']
Usability,"No problem!. In answer to your questions:. 1. Right, the concept annotations are not used because we only are looking at string similarity to aliases from the kb. 2. Yes this is a bit complicated. It basically comes down to the following:. There are 3 things we want to evaluate when doing end to end linking: 1) mention detection, 2) candidate generation and 3) choosing the right candidate. It's clear how to ablate the mention detection - just use gold mentions. To do the same thing with respect to the candidate selection part, you can say ""for the mentions for which the top K contains the gold concept (i.e we did the generation step successfully), what is my accuracy with respect to this subset?"". This separates out performance between the generation step and the selection step. The reason I then took the recall@1/recall@k is that the linker is currently only based on string similarity, so to get a single output you just take the one with the highest score. The reason that this is not quite accurate is that it is possible that the datapoints that you remove for a given K are ""hard"" in some sense, and the accuracy over this subset for top 1 is actually higher than over the whole dataset (this is likely to be the case), which is why this number is a lower bound of the normalized accuracy. . I am currently working on some integration stuff with `allennlp` (see https://github.com/allenai/allennlp/pull/3040) to make it easier to hook in more complicated models, and I also want to try the entity linker from spacy too. I've just re-read the medmentions paper and it seems like they added much more detail about the st21pv subset - previously when I'd read it I thought it was much more specific to IR. I think possibly we should switch over to this subset as it sounds more likely to reflect what people actually want out of an entity linker. It seems like some useful evaluations would be the following:. - P/R/F1 on the st21pv subset with gold mentions; - mention detection P/R/F1",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016:398,clear,clear,398,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/134#issuecomment-511104016,1,['clear'],['clear']
Usability,"Sorry, it is a bit long.; ```python ; atomicwrites 1.3.0 ; attrs 19.3.0 ; autopep8 1.4.4 ; awscli 1.16.260 ; blis 0.2.4 ; boto3 1.9.249 ; botocore 1.12.250 ; certifi 2019.9.11; chardet 3.0.4 ; colorama 0.4.1 ; conllu 2.2 ; cymem 2.0.2 ; Cython 0.29.13 ; docutils 0.15.2 ; en-core-sci-lg 0.2.3 ; en-core-sci-md 0.2.3 ; en-core-sci-sm 0.2.4 ; en-core-web-md 2.1.0 ; en-core-web-sm 2.1.0 ; en-ner-craft-md 0.2.3 ; en-ner-jnlpba-md 0.2.3 ; entrypoints 0.3 ; ez-setup 0.9 ; flake8 3.7.9 ; idna 2.8 ; importlib-metadata 0.23 ; jmespath 0.9.4 ; joblib 0.14.0 ; jsonschema 2.6.0 ; mccabe 0.6.1 ; more-itertools 7.2.0 ; murmurhash 1.0.2 ; neuralcoref 4.0 ; nmslib 1.7.3.6 ; numpy 1.17.3 ; packaging 19.2 ; pandas 0.25.3 ; pip 19.3.1 ; pkg-resources 0.0.0 ; plac 0.9.6 ; pluggy 0.13.0 ; preshed 2.0.1 ; py 1.8.0 ; pyasn1 0.4.7 ; pybind11 2.4.3 ; pycodestyle 2.5.0 ; pyflakes 2.1.1 ; pyparsing 2.4.2 ; pyrsistent 0.15.5 ; pytest 5.2.1 ; python-dateutil 2.8.0 ; pytz 2019.3 ; PyYAML 5.1.2 ; requests 2.22.0 ; rsa 3.4.2 ; s3transfer 0.2.1 ; scikit-learn 0.21.3 ; scipy 1.3.1 ; scispacy 0.2.3 ; setuptools 39.0.1 ; six 1.12.0 ; spacy 2.1.3 ; srsly 0.1.0 ; thinc 7.0.8 ; tqdm 4.36.1 ; urllib3 1.25.6 ; wasabi 0.4.0 ; wcwidth 0.1.7 ; XlsxWriter 1.2.6 ; zipp 0.6.0 ; ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/186#issuecomment-562707675:1035,learn,learn,1035,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/186#issuecomment-562707675,1,['learn'],['learn']
Usability,"Sorry, my internet paused for a second, and it looks like I made a duplicate issue (#284)",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/283#issuecomment-716705972:19,pause,paused,19,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/283#issuecomment-716705972,1,['pause'],['paused']
Usability,"Thanks a lot guys, I think I will simply use two different models loaded with the different linkers",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/378#issuecomment-880502034:34,simpl,simply,34,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378#issuecomment-880502034,1,['simpl'],['simply']
Usability,"Thanks for reply. Sounds like simple but effective for abbreviation, for `short term(longer term)` and the vise versa, which is very often seen at scientific paper. Personally, abbreviation expansion is interesting to me, since it's closely related to EL. So in future I may try another algorithm and report results!. Thanks to your advice, I now add pipe before each in-doc mention is going to be ANNsearched, and am checking recall per each K. I'll add result when evaluation is complete.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/199#issuecomment-591349955:30,simpl,simple,30,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/199#issuecomment-591349955,1,['simpl'],['simple']
Usability,"Thanks for the suggestion @chrishmorris! While your idea is reasonable, incorporating that human intuition into the dependency parsing model is quite difficult. See https://spacy.io/api/dependencyparser for more details on the dependency parsing model. A simpler way to incorporate this idea would be to add lots of examples of the form you describe to the training corpus. I will likely not be doing this for scispacy, but if you were to create your own corpus, I'd be happy to help you figure out how to use it in our training scripts to train your own model! And feel free to open another issue if you end up going down that route and would like some help.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/451#issuecomment-1289932724:97,intuit,intuition,97,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/451#issuecomment-1289932724,2,"['intuit', 'simpl']","['intuition', 'simpler']"
Usability,"The annotation guidelines for the BC5CDR data (which that model was trained on can) can be found here: https://biocreative.bioinformatics.udel.edu/media/store/files/2015/bc5_CDR_data_guidelines.pdf. and that document says that a chemical is defined as the Drugs and Chemicals [D] branch of Mesh 2015, which can be found here: https://www.nlm.nih.gov/mesh/trees.html",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/79#issuecomment-468365573:15,guid,guidelines,15,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/79#issuecomment-468365573,1,['guid'],['guidelines']
Usability,"The other github issue i linked to shows how you can convert the `Span` objects to serializable json (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144). You would simply add this function as a final pipe in your scispacy pipeline. This would mean that your pipeline produces serializable documents, which should work fine with multiprocessing.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/338#issuecomment-801431956:184,simpl,simply,184,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338#issuecomment-801431956,1,['simpl'],['simply']
Usability,"There are a few options, but I think the simplest will be a minimal change to your previous setup. You should be able to modify the pipe after it has been added like so; ```; In [5]: nlp.get_pipe('scispacy_linker'); Out[5]: <scispacy.linking.EntityLinker at 0x7f0c9a94e940>. In [6]: nlp.get_pipe('scispacy_linker').candidate_generator; Out[6]: <scispacy.candidate_generation.CandidateGenerator at 0x7f0c9a94e5b0>. In [7]: nlp.get_pipe('scispacy_linker').candidate_generator = lambda x: x. In [8]: nlp.get_pipe('scispacy_linker').candidate_generator; Out[8]: <function __main__.<lambda>(x)>; ```. so you would do. ```; nlp.get_pipe('scispacy_linker').candidate_generator = <your candidate generator>; nlp.get_pipe('scispacy_linker').kb = <your candidate generator>.kb; ```. Alternatively, you could just fork the library and install scispacy from your fork, and then you can add whatever linker paths you want to the necessary objects.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/337#issuecomment-801431110:41,simpl,simplest,41,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/337#issuecomment-801431110,1,['simpl'],['simplest']
Usability,"This is definitely a limitation of the current approach and we would love to have a more robust entity linker that does exactly what you describe. Take in the context of the text and the definition (and maybe type and aliases) of the candidate entities and predict which one is correct. I don't know if a simple sentence similarity would be enough or not, but is something you could evaluate on the medmentions dataset, which is how we evaluated the current entity linker. . The distance is the same for all of these because they all have an alias that is exactly `sex`.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/352#issuecomment-843634145:305,simpl,simple,305,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/352#issuecomment-843634145,1,['simpl'],['simple']
Usability,"Well that is annoying. I'm pretty sure this is exactly what it says, the `Span` object is not serializable. The `AbbreviationDetector` stores the abbreviations as spans on the `Doc`, and then multiprocessing needs to be able to serialize the objects that get worked on. I think the easiest solution is to do something like here (https://github.com/allenai/scispacy/issues/205#issuecomment-597273144, converting the abbreviations to anything serializable should do, json, https://spacy.io/api/span#as_doc, etc) to make your docs serializable. I'm pretty sure this will work, although not 100%. You might also be able to do the parallelization yourself and get around this, but the first solution is probably simpler assuming that it works.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/338#issuecomment-800727560:707,simpl,simpler,707,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/338#issuecomment-800727560,1,['simpl'],['simpler']
Usability,"When installing scispacy, I get similar resolver issues. Am I doing something wrong?. ```; emanuelfarruda@Mannys-MacBook-Pro-2021 ~ % pip3 install scispacy; Requirement already satisfied: scispacy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (0.4.0); Requirement already satisfied: scikit-learn>=0.20.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.1.1); Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.1.0); Requirement already satisfied: nmslib>=1.7.3.6 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.1.1); Requirement already satisfied: pysbd in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (0.3.4); Collecting spacy<3.1.0,>=3.0.0; Using cached spacy-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB); Requirement already satisfied: conllu in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (4.4.2); Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.15.1); Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.22.4); Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.1); Requirement already satisfied: pybind11<2.6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1); Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0); Requirement already satisfied: sc",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:327,learn,learn,327,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,1,['learn'],['learn']
Usability,"Yeah, this is a limitation of the abbreviation algorithm, sorry about that. I suspect there is not a simple fix for this. Depending on your use case and tradeoffs, you could try to patch this yourself by looking one word back from the abbreviation returned and seeing if it starts with the first letter of the short form, or something like that.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/410#issuecomment-1028469710:101,simpl,simple,101,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/410#issuecomment-1028469710,1,['simpl'],['simple']
Usability,You can create a conda environment with python 3.6 like so `conda create -n myenv python=3.6`. See more about working with conda environments here: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/137#issuecomment-547052133:200,guid,guide,200,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/137#issuecomment-547052133,1,['guid'],['guide']
Usability,"^This is correct. I would suggest instantiating the linkers separately, and running them in a loop and then doing whatever you want with the output. something like; ```; doc = nlp_without_linker(text); for linker in linker:; linked_doc = linker(doc); # save/clear whatever doc state you want; ```",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/378#issuecomment-880180407:258,clear,clear,258,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/378#issuecomment-880180407,1,['clear'],['clear']
Usability,"activate. (base) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ conda activate vega_scispacy_2; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip list; Package Version; ---------- ---------; certifi 2022.9.24; pip 22.3.1; setuptools 65.5.0; wheel 0.37.1; WARNING: The repository located at pypi.douban.com is not a trusted or secure host and is being ignored. If this repository is available via HTTPS we recommend you use HTTPS instead, otherwise you may silence this warning and allow it anyway with '--trusted-host pypi.douban.com'.; WARNING: There was an error checking the latest version of pip.; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install scispacy; Looking in indexes: http://pypi.douban.com/simple; Collecting scispacy; Downloading http://pypi.doubanio.com/packages/6d/f2/a55ed36940e481e1823c71047e5b3b90a2cb516f59f25b63a57e60e3f8c3/scispacy-0.5.1-py3-none-any.whl (44 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.9/44.9 kB 1.3 MB/s eta 0:00:00; Collecting numpy; Downloading http://pypi.doubanio.com/packages/4c/b9/038abd6fbd67b05b03cb1af590cfc02b7f1e5a37af7ac6a868f5093c29f5/numpy-1.23.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 7.7 MB/s eta 0:00:00; Collecting spacy<3.5.0,>=3.4.0; Downloading http://pypi.doubanio.com/packages/f6/8e/1ee7c934aeb18bb6a77b8f7b3d9a301acd8aaedfc5f07c300871f3c6f1ff/spacy-3.4.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.5 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.5/6.5 MB 1.9 MB/s eta 0:00:00; Collecting requests<3.0.0,>=2.0.0; Downloading http://pypi.doubanio.com/packages/ca/91/6d9b8ccacd0412c08820f72cebaa4f0c0441b5cda699c90f618b6f8a1b42/requests-2.28.1-py3-none-any.whl",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:3548,simpl,simple,3548,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['simpl'],['simple']
Usability,"arm64.egg/numpy-1.23.4.dist-info' -> '/private/var/folders/1c/1wcfh1095clg_2ppxfmm2_fc0000gn/T/pip-install-ov_6b84v/nmslib_1602d4079c614d059176fdb0892700e3/.eggs/numpy-1.23.4-py3.9-macosx-11.1-arm64.egg/EGG-INFO'. [end of output]. note: This error originates from a subprocess, and is likely not a problem with pip. ERROR: Failed cleaning build dir for nmslib. Failed to build nmslib. Installing collected packages: pybind11, psutil, numpy, nmslib. Running setup.py install for nmslib ... done. DEPRECATION: nmslib was installed using the legacy 'setup.py install' method, because a wheel could not be built for it. A possible replacement is to fix the wheel build issue reported above. Discussion can be found at https://github.com/pypa/pip/issues/8368. Successfully installed nmslib-2.1.1 numpy-1.23.4 psutil-5.9.4 pybind11-2.6.1. ### install scispacy log ###. (scispacy) ***@***.*** ~ % pip install scispacy. Collecting scispacy. Using cached scispacy-0.5.1-py3-none-any.whl (44 kB). Collecting scikit-learn>=0.20.3. Using cached scikit_learn-1.1.3-cp39-cp39-macosx_12_0_arm64.whl (7.7 MB). Collecting joblib. Using cached joblib-1.2.0-py3-none-any.whl (297 kB). Requirement already satisfied: numpy in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (1.23.4). Requirement already satisfied: nmslib>=1.7.3.6 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from scispacy) (2.1.1). Collecting spacy<3.5.0,>=3.4.0. Using cached spacy-3.4.2-cp39-cp39-macosx_11_0_arm64.whl (6.5 MB). Collecting pysbd. Using cached pysbd-0.3.4-py3-none-any.whl (71 kB). Collecting conllu. Using cached conllu-4.5.2-py2.py3-none-any.whl (16 kB). Collecting requests<3.0.0,>=2.0.0. Using cached requests-2.28.1-py3-none-any.whl (62 kB). Requirement already satisfied: pybind11<2.6.2 in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1). Requirement already satisfied: psutil in ./opt/anaconda3/envs/scispacy/lib/python3.9/site-packa",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:10376,learn,learn,10376,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['learn'],['learn']
Usability,"eta 0:00:00; Collecting MarkupSafe>=2.0; Downloading http://pypi.doubanio.com/packages/df/06/c515c5bc43b90462e753bc768e6798193c6520c9c7eb2054c7466779a9db/MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, pysbd, pybind11, psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in indexes: http://pypi.douban.com/simple; Collecting https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Using cached https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz (15.9 MB); Preparing metadata (setup.py) ... done; Requirement already satisfied: spacy<3.5.0,>=3.4.1 in /home/zhangx/anaconda3/envs/vega_scispacy_2/lib/python3.9/site-packages (from en-core-sci-sm==0.5.1) (3.4.3); Requirement already satisfied: packaging>=20.0 in /home/zhangx/anaconda3/envs/vega_",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:14291,learn,learn-,14291,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['learn'],['learn-']
Usability,"for scispacy `pipeline` gives . ```; [('attribute_ruler',; <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),; ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]; ```. Where as regular spacy gives. ```; [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]; ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right?. scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTE",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:586,learn,learn,586,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability,fulclient (0.13.5); lazr.uri (1.0.3); louis (3.5.0); macaroonbakery (1.1.3); Mako (1.0.7); Markdown (3.1.1); MarkupSafe (1.1.1); mistune (0.8.4); murmurhash (1.0.2); nbconvert (5.6.0); nbformat (4.4.0); netifaces (0.10.4); nmslib (1.8.1); notebook (6.0.1); numpy (1.17.2); oauth (1.0.1); olefile (0.45.1); pandocfilters (1.4.2); parso (0.5.1); pbr (3.1.1); pexpect (4.7.0); pickleshare (0.7.5); Pillow (6.1.0); pip (9.0.1); plac (0.9.6); preshed (3.0.2); prometheus-client (0.7.1); prompt-toolkit (2.0.9); protobuf (3.9.2); ptyprocess (0.6.0); pyasn1 (0.4.7); pybind11 (2.4.2); pycairo (1.16.2); pycrypto (2.6.1); pycups (1.9.73); Pygments (2.4.2); pygobject (3.26.1); pymacaroons (0.13.0); PyNaCl (1.1.2); pyRFC3339 (1.0); pyrsistent (0.15.4); python-apt (1.6.4); python-dateutil (2.8.0); python-debian (0.1.32); pytz (2018.3); pyxdg (0.25); PyYAML (5.1.2); pyzmq (18.1.0); qtconsole (4.5.5); reportlab (3.4.0); requests (2.22.0); requests-unixsocket (0.1.5); rsa (3.4.2); s3transfer (0.2.1); scikit-learn (0.21.3); scipy (1.3.1); scispacy (0.2.3); screen-resolution-extra (0.0.0); SecretStorage (2.3.1); Send2Trash (1.5.0); setuptools (41.2.0); simplegeneric (0.8.1); simplejson (3.13.2); six (1.12.0); spacy (2.1.8); srsly (0.1.0); system-service (0.3); systemd-python (234); tensorboard (1.14.0); tensorflow (1.14.0); tensorflow-estimator (1.14.0); tensorflow-gpu (1.14.0); termcolor (1.1.0); terminado (0.8.2); testpath (0.4.2); thinc (7.1.1); torch (1.2.0); torchvision (0.4.0); tornado (6.0.3); tqdm (4.36.1); traitlets (4.3.2); ubuntu-drivers-common (0.0.0); ufw (0.36); unattended-upgrades (0.1); urllib3 (1.25.6); usb-creator (0.3.3); wadllib (1.3.2); wasabi (0.2.2); wcwidth (0.1.7); webencodings (0.5.1); Werkzeug (0.16.0); wheel (0.33.6); widgetsnbextension (3.5.1); wrapt (1.11.2); xkit (0.0.0); zope.interface (4.3.2)`. and my full code snippet is :; `In [1]: import spacy . In [2]: import scispacy . In [3]: from scispacy.umls_linking import UmlsEntityLinker . In [4]: nlp = spacy.loa,MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949:2138,learn,learn,2138,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/166#issuecomment-541316949,1,['learn'],['learn']
Usability,"geBase(); File ""/data/home/***/Prodigy/Linker/linker-env/lib/python3.7/site-packages/scispacy/umls_utils.py"", line 47, in __init__; raw = json.load(open(cached_path(file_path))); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 296, in load; parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw); File ""/home/***/anaconda3/lib/python3.7/json/__init__.py"", line 348, in loads; return _default_decoder.decode(s); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/home/***/anaconda3/lib/python3.7/json/decoder.py"", line 353, in raw_decode; obj, end = self.scan_once(s, idx); json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 79986649 (char 79986648)`. ****. The installed package:. Package Version; ------------------ ---------; awscli 1.20.5; blis 0.4.1; botocore 1.21.5; catalogue 1.0.0; certifi 2021.5.30; charset-normalizer 2.0.3; colorama 0.4.3; conllu 4.4; cymem 2.0.5; docutils 0.15.2; en-core-sci-sm 0.2.4; idna 3.2; importlib-metadata 4.6.1; jmespath 0.10.0; joblib 1.0.1; murmurhash 1.0.5; nmslib 2.1.1; numpy 1.21.1; pip 21.1.3; plac 0.9.6; preshed 3.0.5; psutil 5.8.0; pyasn1 0.4.8; pybind11 2.6.1; pysbd 0.3.4; python-dateutil 2.8.2; PyYAML 5.4.1; requests 2.26.0; rsa 4.7.2; s3transfer 0.5.0; scikit-learn 0.22.2; scipy 1.7.0; scispacy 0.2.4; setuptools 39.0.1; six 1.16.0; spacy 2.2.1; srsly 1.0.5; thinc 7.1.1; threadpoolctl 2.2.0; tqdm 4.61.2; typing-extensions 3.10.0.0; urllib3 1.26.6; wasabi 0.8.2; zipp 3.5.0. For the sklearn warning, I installed the version: . `DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.`",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492:2700,learn,learn,2700,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/357#issuecomment-885555492,1,['learn'],['learn']
Usability,"icating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-leve",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:5588,learn,learn,5588,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,2,['learn'],"['learn', 'learning']"
Usability,"itation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,; 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:8035,learn,learn,8035,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability,"kB); Collecting blis<0.8.0,>=0.7.8; Downloading http://pypi.doubanio.com/packages/28/b6/e1cdfcf4ada40bef7c0511576231df20ac94a15baeb7ceaab2a180463268/blis-0.7.9-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.2/10.2 MB 9.2 MB/s eta 0:00:00; Collecting click<9.0.0,>=7.1.1; Downloading http://pypi.doubanio.com/packages/c2/f1/df59e28c642d583f7dacffb1e0965d0e00b218e0186d7858ac5233dce840/click-8.1.3-py3-none-any.whl (96 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 7.3 MB/s eta 0:00:00; Collecting MarkupSafe>=2.0; Downloading http://pypi.doubanio.com/packages/df/06/c515c5bc43b90462e753bc768e6798193c6520c9c7eb2054c7466779a9db/MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB); Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, pysbd, pybind11, psutil, packaging, numpy, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, typer, srsly, scipy, requests, pydantic, preshed, nmslib, jinja2, blis, scikit-learn, pathy, confection, thinc, spacy, scispacy; Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 nmslib-2.1.1 numpy-1.23.5 packaging-22.0 pathy-0.10.1 preshed-3.0.8 psutil-5.9.4 pybind11-2.6.1 pydantic-1.10.2 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.2.0 scipy-1.9.3 scispacy-0.5.1 smart-open-6.3.0 spacy-3.4.3 spacy-legacy-3.0.10 spacy-loggers-1.0.4 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.7.0 typing-extensions-4.4.0 urllib3-1.26.13 wasabi-0.10.1; (vega_scispacy_2) zhangx@pve-gpu:~/a_project/q_vegaPython/000.vega_daily/daily_60_scispacy_demo$ pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.1/en_core_sci_sm-0.5.1.tar.gz; Looking in in",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:13880,learn,learn,13880,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['learn'],['learn']
Usability,"lenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,; 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from s",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:8664,learn,learning,8664,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learning']
Usability,"leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,; 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. I also tried the pysbd_sentencizer, but got an error getting it to work . ```; import spacy; import scispacy; from scispacy.custom_sentence_segmentater import pysbd_sentencizer; nlpSciMd = spacy.load(""en_core_sci_md"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec']); nlpSciSm = spacy.load(""en_core_sci_sm"", disable = ['ner', 'parser', 'tagger', 'lemmatizer', 'attributeruler', 'tok2vec'])",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:8901,learn,learn,8901,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability,"ncizer.Sentencizer at 0x7f1a59754640>)]; ```. Where as regular spacy gives. ```; [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]; ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right?. scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-leve",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1082,learn,learn,1082,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,2,['learn'],"['learn', 'learning']"
Usability,"nues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related ",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:3674,simpl,simply,3674,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['simpl'],['simply']
Usability,"on3.10/site-packages (from scispacy) (4.4.2); Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (2.15.1); Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scispacy) (1.22.4); Requirement already satisfied: psutil in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (5.9.1); Requirement already satisfied: pybind11<2.6.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from nmslib>=1.7.3.6->scispacy) (2.6.1); Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (3.1.0); Requirement already satisfied: scipy>=1.3.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from scikit-learn>=0.20.3->scispacy) (1.8.1); Requirement already satisfied: thinc<8.1.0,>=8.0.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (8.0.17); Requirement already satisfied: typer<0.4.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.3.2); Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (1.8.2); Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (4.62.3); Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from spacy<3.1.0,>=3.0.0->scispacy) (0.7.7); Requirement already satisfied: preshed",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839:2108,learn,learn,2108,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/438#issuecomment-1164780839,1,['learn'],['learn']
Usability,"ormer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publi",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1600,learn,learn,1600,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,2,['learn'],['learn']
Usability,"rom citation prediction, to document classification and recommendation.; We show that SPECTER outperforms a variety of competitive baselines on the benchmark.; As the pace of scientific publication continues to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al., 2018; Devlin et al., 2019; Yang et al., 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al., 2017; Chen et al., 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any ta",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:3524,learn,learn,3524,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability,"spacy Docs are not really editable. I think the simplest way is to convert to string, replace the part of the string you want to, and then reprocess to a spacy doc.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/418#issuecomment-1049403649:48,simpl,simplest,48,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/418#issuecomment-1049403649,1,['simpl'],['simplest']
Usability,"th/to/nmslib_index.bin"",; tfidf_vectorizer=""path/to//nmslib_index.bin"",; tfidf_vectors=""path/to/tfidf_vectorizer.joblib"",; concept_aliases_list=""path/to/concept_aliases.json"",; ). # set default release; DEFAULT_RELEASE = ""2020AA""; DEFAULT_KB_PATH = ""path/to/2020AA.json""; DEFAULT_PATHS = {""2020AA"": CustomLinkerPaths_2020AA}. def load_candidate_generator(; release: str = DEFAULT_RELEASE, kb_path: str = DEFAULT_KB_PATH,; ) -> CandidateGenerator:; """"""Loads a pre-trained custom scispacy candidate generator by; loading the different model components from disk. release: str; The name of the pretrained candidate generator to load. ; Currently, the only available (and default) is ""2020AA"". kb_path: str; Path to the Knowledge Base in JSON format as required by scispacy.; """""". # create LinkerPaths; linker_paths = DEFAULT_PATHS[release]. # load ann_index, tfifd_vectorizer and ann_concept_aliases_list; ann_index = load_approximate_nearest_neighbours_index(linker_paths=linker_paths); tfidf_vectorizer = joblib.load(linker_paths.tfidf_vectorizer); with open(linker_paths.concept_aliases_list, ""r"") as f:; ann_concept_aliases_list = json.load(f). # load UMLS KnowledgeBase (converted json file); umls_kb = UmlsKnowledgeBase(file_path=kb_path). # create candidate generator; candidate_generator = CandidateGenerator(; ann_index=ann_index,; tfidf_vectorizer=tfidf_vectorizer,; ann_concept_aliases_list=ann_concept_aliases_list,; kb=umls_kb,; ). return candidate_generator; ```. I'm not sure if this makes sense, but think it would be great if instead, you could simply provide the paths to the necessary files directly when initiating a `CandidateGenerator`, so that you could do something like the following:. ```; candidate_generator = CandidateGenerator(; ann_index=""path/to/ann_index"",; tfidf_vectorizer=""path/to/tfidf_vectorizer"",; ann_concept_aliases_list=""path/to/ann_concept_aliases_list"",; kb=""path/to/kb"",; ); ```. I hope I explained my thoughts properly :-) Thanks and keep up the good work!!",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323:3015,simpl,simply,3015,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/237#issuecomment-650269323,1,['simpl'],['simply']
Usability,"the JSON file only has UMLS CUIs, so getting HPO will take an extra step. https://github.com/allenai/scispacy/blob/e9f0daeae9a76c644166f852f1d8a101e77d9593/scispacy/linking_utils.py#L113 . Do wish these JSON files had extra item of HPO ids. The only way to get the HPO terms is either use UMLS to crosswalk or simply use the HPO ontology files to extract the HPO-UMLS CUI mappings.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/253#issuecomment-669674267:310,simpl,simply,310,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/253#issuecomment-669674267,1,['simpl'],['simply']
Usability,"tially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.; Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.; Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging fr",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:5985,learn,learning,5985,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learning']
Usability,"to increase, Natural Language Processing (NLP) tools that help users to search, discover and understand the scientific literature have become critical.; In recent years, substantial improvements in NLP tools have been brought about by pretrained neural language models (LMs) (Radford et al.,; 2018; Devlin et al.,; 2019; Yang et al.,; 2019).; While such models are widely used for representing individual words ∗ Equal contribution 1 https://github.com/allenai/specter or sentences, extensions to whole-document embeddings are relatively underexplored.; Likewise, methods that do use inter-document signals to produce whole-document embeddings (Tu et al.,; 2017; Chen et al.,; 2019) have yet to incorporate stateof-the-art pretrained LMs.; Here, we study how to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al.,; 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al.,; 2017) language models (e.g., SciBERT (Beltagy et al.,; 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most relat",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:8185,simpl,simply,8185,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['simpl'],['simply']
Usability,"to leverage the power of pretrained language models to learn embeddings for scientific documents.; A paper’s title and abstract provide rich semantic content about the paper, but, as we show in this work, simply passing these textual fields to an “off-the-shelf” pretrained language model—even a state-of-the-art model tailored to scientific text like the recent SciBERT (Beltagy et al., 2019)—does not result in accurate paper representations.; The language modeling objectives used to pretrain the model do not lead it to output representations that are helpful for document-level tasks such as topic classification or recommendation.; In this paper, we introduce a new method for learning general-purpose vector representations of scientific documents.; Our system, SPECTER, 2 incorporates inter-document context into the Transformer (Vaswani et al., 2017) language models (e.g., SciBERT (Beltagy et al., 2019)) to learn document representations that are effective across a wide-variety of downstream tasks, without the need for any task-specific fine-tuning of the pretrained language model.; We specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective.; Unlike many prior works, at inference time, our model does not require any citation information.; This is critical for embedding new papers that have not yet been cited.; In experiments, we show that SPECTER’s representations substantially outperform the state; ```. en_core_web_sm. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:4387,learn,learn,4387,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability,"tribute_ruler',; <spacy.pipeline.attributeruler.AttributeRuler at 0x7f1a5969e3c0>),; ('sentencizer', <spacy.pipeline.sentencizer.Sentencizer at 0x7f1a59754640>)]; ```. Where as regular spacy gives. ```; [('sentencizer', <spacy.pipeline.pipes.Sentencizer at 0x7f821ef95e50>)]; ```. So it looks like scispacy adds a custom attribute_ruler, but both scispacy and spacy use the same sentencizer? Does that sound right?. scispacy gives much better results than spacy for abstracts. Here's an example. . en_core_sci_md:. ```; Abstract Our goal is to learn task-independent representations of academic papers.; Inspired by the recent success of pretrained Transformer language models across various NLP tasks, we use the Transformer model architecture as basis of encoding the input paper.; Existing LMs such as BERT, however, are primarily based on masked language modeling objective, only considering intra-document context and do not use any inter-document information.; This limits their ability to learn optimal document representations.; To learn high-quality documentlevel representations we propose using citations as an inter-document relatedness signal and formulate it as a triplet loss learning objective.; We then pretrain the model on a large corpus of citations using this objective, encouraging it to output representations that are more similar for papers that share a citation link than for those that do not.; Representation learning is a critical ingredient for natural language processing systems.; Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.; For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.; We propose SPECTER, a new method to generate document-leve",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592:1038,learn,learn,1038,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/327#issuecomment-795142592,1,['learn'],['learn']
Usability,"we're using spacy 3's new config system and [spacy projects](https://spacy.io/usage/projects). So our project file lives [here](https://github.com/allenai/scispacy/blob/master/project.yml) and our configs live [here](https://github.com/allenai/scispacy/tree/master/configs). You should be able to follow these as a guide, an basically just run the ner training commands, but with your data.",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/316#issuecomment-792961359:315,guid,guide,315,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/316#issuecomment-792961359,1,['guid'],['guide']
Usability,"xtensions-4.4.0-py3-none-any.whl (26 kB). Collecting confection<1.0.0,>=0.0.1. Using cached confection-0.0.3-py3-none-any.whl (32 kB). Collecting blis<0.8.0,>=0.7.8. Using cached blis-0.7.9-cp39-cp39-macosx_11_0_arm64.whl (1.1 MB). Collecting click<9.0.0,>=7.1.1. Using cached click-8.1.3-py3-none-any.whl (96 kB). Collecting MarkupSafe>=2.0. Using cached MarkupSafe-2.1.1-cp39-cp39-macosx_10_9_universal2.whl (17 kB). Installing collected packages: wasabi, cymem, urllib3, typing-extensions, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, scipy, pysbd, pyparsing, murmurhash, MarkupSafe, langcodes, joblib, idna, conllu, click, charset-normalizer, catalogue, blis, typer, srsly, scikit-learn, requests, pydantic, preshed, packaging, jinja2, pathy, confection, thinc, spacy, scispacy. Successfully installed MarkupSafe-2.1.1 blis-0.7.9 catalogue-2.0.8 charset-normalizer-2.1.1 click-8.1.3 confection-0.0.3 conllu-4.5.2 cymem-2.0.7 idna-3.4 jinja2-3.1.2 joblib-1.2.0 langcodes-3.3.0 murmurhash-1.0.9 packaging-21.3 pathy-0.6.2 preshed-3.0.8 pydantic-1.10.2 pyparsing-3.0.9 pysbd-0.3.4 requests-2.28.1 scikit-learn-1.1.3 scipy-1.9.3 scispacy-0.5.1 smart-open-5.2.1 spacy-3.4.2 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.5 thinc-8.1.5 threadpoolctl-3.1.0 tqdm-4.64.1 typer-0.4.2 typing-extensions-4.4.0 urllib3-1.26.12 wasabi-0.10.1. ________________________________; From: Daniel King ***@***.***>; Sent: Wednesday, November 9, 2022 7:12 PM; To: allenai/scispacy ***@***.***>; Cc: Brian Griner, PhD ***@***.***>; Author ***@***.***>; Subject: Re: [allenai/scispacy] nmslib install error using a conda env on mac m1 (Issue #455). What was the error you got?. —; Reply to this email directly, view it on GitHub<https://github.com/allenai/scispacy/issues/455#issuecomment-1309570650>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AIIB7TJZCTFHCICGZWG47KTWHQ4WZANCNFSM6AAAAAARYG27ME>.; You are receiving this because you authored the thread.Message ID: ***@***.***>",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146:15050,learn,learn-,15050,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/455#issuecomment-1309793146,1,['learn'],['learn-']
Usability,"━━━━━━━━━━━ 62.8/62.8 kB 4.0 MB/s eta 0:00:00; Collecting pysbd; Downloading http://pypi.doubanio.com/packages/48/0a/c99fb7d7e176f8b176ef19704a32e6a9c6aafdf19ef75a187f701fc15801/pysbd-0.3.4-py3-none-any.whl (71 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.1/71.1 kB 3.2 MB/s eta 0:00:00; Collecting nmslib>=1.7.3.6; Downloading http://pypi.doubanio.com/packages/b2/9b/e888adcc689d17da4dbc5fd471b814ebb498fcf0e6aa7e4cc6be5869a344/nmslib-2.1.1-cp39-cp39-manylinux2010_x86_64.whl (13.3 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 9.4 MB/s eta 0:00:00; Collecting joblib; Downloading http://pypi.doubanio.com/packages/91/d4/3b4c8e5a30604df4c7518c562d4bf0502f2fa29221459226e140cf846512/joblib-1.2.0-py3-none-any.whl (297 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 7.4 MB/s eta 0:00:00; Collecting conllu; Downloading http://pypi.doubanio.com/packages/6a/b6/0d634bd79865d03a866764e4ca9ff392ec8e4bf0bd97b7385a5ef29e8fbf/conllu-4.5.2-py2.py3-none-any.whl (16 kB); Collecting scikit-learn>=0.20.3; Downloading http://pypi.doubanio.com/packages/83/b5/0436307cb4f91ba280c74746fde7c89bed7a87703a2bf6e21791f56ce6de/scikit_learn-1.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.5 MB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.5/9.5 MB 9.7 MB/s eta 0:00:00; Collecting pybind11<2.6.2; Downloading http://pypi.doubanio.com/packages/00/84/fc9dc13ee536ba5e6b8fd10ce368fea5b738fe394c3b296cde7c9b144a92/pybind11-2.6.1-py2.py3-none-any.whl (188 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.5/188.5 kB 6.2 MB/s eta 0:00:00; Collecting psutil; Downloading http://pypi.doubanio.com/packages/6e/c8/784968329c1c67c28cce91991ef9af8a8913aa5a3399a6a8954b1380572f/psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB); ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 kB 8.8 MB/s eta 0:00:00; Collecting charset-normalizer<3,>=2; Downloading http://pypi.doubanio.com/packages/db/51/a507c856293ab05",MatchSource.ISSUE_COMMENT,allenai,scispacy,v0.5.5,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208:5623,learn,learn,5623,https://allenai.github.io/scispacy/,https://github.com/allenai/scispacy/issues/459#issuecomment-1352631208,1,['learn'],['learn']
